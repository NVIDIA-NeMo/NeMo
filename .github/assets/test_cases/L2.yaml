L2_Community_LLM_Checkpoints_tests_Llama: |
  CUDA_VISIBLE_DEVICES=0 python scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \
  --input_name_or_path=/home/TestData/nlp/megatron_llama/llama-ci-hf-tiny \
  --output_path=/home/TestData/nlp/megatron_llama/llama_ci.nemo \
  --precision=16
L2_Community_LLM_Checkpoints_tests_Llama3: |
  CUDA_VISIBLE_DEVICES=0 python scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \
  --input_name_or_path=/home/TestData/nlp/megatron_llama/llama3-ci-hf \
  --output_path=/home/TestData/nlp/megatron_llama/llama3-ci-hf/llama3_ci.nemo \
  --precision=16
  rm -f /home/TestData/nlp/megatron_llama/llama3-ci-hf/llama3_ci.nemo
L2_Community_LLM_Checkpoints_tests_StarCoder: |
  python scripts/checkpoint_converters/convert_starcoder_hf_to_nemo.py \
  --input_name_or_path /home/TestData/nlp/megatron_gpt/starcoder-ci-hf \
  --output_path /home/TestData/nlp/megatron_gpt/starcoder-ci-hf
  rm -f /home/TestData/nlp/megatron_gpt/starcoder-ci-hf/megatron_starcoder_tp1_pp1.nemo
L2_Community_LLM_Checkpoints_tests_Falcon: |
  python scripts/checkpoint_converters/convert_falcon_hf_to_nemo.py \
  --input_name_or_path /home/TestData/nlp/megatron_gpt/falcon-ci-hf \
  --output_path /home/TestData/nlp/megatron_gpt/falcon-ci-hf/falcon_ci.nemo
  rm -f /home/TestData/nlp/megatron_gpt/falcon-ci-hf/falcon_ci.nemo
L2_PTQ_Llama2_Export_Only: |
  python examples/nlp/language_modeling/megatron_llama_quantization.py \
  model_file=/home/TestData/nlp/megatron_llama/llama_ci.nemo \
  quantization.algorithm=null \
  model_save=/home/TestData/nlp/megatron_llama/ci_baseline

  rm -rf /home/TestData/nlp/megatron_llama/ci_baseline
L2_PTQ_Llama2_FP8: |
  python examples/nlp/language_modeling/megatron_llama_quantization.py \
  model_file=/home/TestData/nlp/megatron_llama/llama_ci.nemo \
  tensor_model_parallel_size=2 \
  trainer.devices=2 \
  quantization.calib_dataset=/home/TestData/nlp/test_quantization/test.json \
  quantization.algorithm=fp8 \
  quantization.num_calib_size=8 \
  inference.batch_size=2 \
  export.inference_tensor_parallel=2 \
  model_save=/home/TestData/nlp/megatron_llama/ci_fp8.qnemo

  rm -rf /home/TestData/nlp/megatron_llama/ci_fp8.qnemo
L2_PTQ_Llama2_INT8_SQ: |
  python examples/nlp/language_modeling/megatron_llama_quantization.py \
  model_file=/home/TestData/nlp/megatron_llama/llama_ci.nemo \
  quantization.calib_dataset=/home/TestData/nlp/test_quantization/test.json \
  quantization.algorithm=int8_sq \
  quantization.num_calib_size=8 \
  inference.batch_size=2 \
  model_save=/home/TestData/nlp/megatron_llama/ci_int8_sq.qnemo

  rm -rf /home/TestData/nlp/megatron_llama/ci_int8_sq.qnemo
L2_Speech_to_Text_EMA: |
  python examples/asr/asr_ctc/speech_to_text_ctc.py \
  model.train_ds.manifest_filepath=/home/TestData/an4_dataset/an4_train.json \
  model.validation_ds.manifest_filepath=/home/TestData/an4_dataset/an4_val.json \
  trainer.devices=2 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=True \
  +exp_manager.ema.enable=True \
  exp_manager.exp_dir=examples/asr/speech_to_text_results
  rm -rf examples/asr/speech_to_text_results
L2_Speaker_dev_run_Speaker_Recognition: |
  python examples/speaker_tasks/recognition/speaker_reco.py \
  model.train_ds.batch_size=10 \
  model.validation_ds.batch_size=2 \
  model.train_ds.manifest_filepath=/home/TestData/an4_speaker/train.json \
  model.validation_ds.manifest_filepath=/home/TestData/an4_speaker/dev.json \
  model.decoder.num_classes=2 \
  trainer.max_epochs=10 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=True \
  exp_manager.exp_dir=examples/speaker_tasks/recognition/speaker_recognition_results
  rm -rf examples/speaker_tasks/recognition/speaker_recognition_results
L2_Speaker_dev_run_Speaker_Diarization: |
  python examples/speaker_tasks/diarization/neural_diarizer/multiscale_diar_decoder.py \
  model.diarizer.speaker_embeddings.model_path=titanet_large \
  model.train_ds.batch_size=5 \
  model.validation_ds.batch_size=5 \
  model.train_ds.emb_dir=examples/speaker_tasks/diarization/speaker_diarization_results \
  model.validation_ds.emb_dir=examples/speaker_tasks/diarization/speaker_diarization_results \
  model.train_ds.manifest_filepath=/home/TestData/an4_diarizer/simulated_train/msdd_data.50step.json \
  model.validation_ds.manifest_filepath=/home/TestData/an4_diarizer/simulated_valid/msdd_data.50step.json \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=True \
  exp_manager.exp_dir=examples/speaker_tasks/diarization/speaker_diarization_results
  rm -rf examples/speaker_tasks/diarization/speaker_diarization_results
L2_Speaker_dev_run_Speech_to_Label: |
  python examples/asr/speech_classification/speech_to_label.py \
  model.train_ds.manifest_filepath=/home/TestData/speech_commands/train_manifest.json \
  model.validation_ds.manifest_filepath=/home/TestData/speech_commands/test_manifest.json \
  model.test_ds.manifest_filepath=/home/TestData/speech_commands/test_manifest.json \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=True \
  model.preprocessor._target_=nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor \
  ~model.preprocessor.window_size \
  ~model.preprocessor.window_stride \
  ~model.preprocessor.window \
  ~model.preprocessor.n_mels \
  ~model.preprocessor.n_mfcc \
  ~model.preprocessor.n_fft \
  exp_manager.exp_dir=examples/asr/speech_to_label_results
  rm -rf examples/asr/speech_to_label_results
L2_Speaker_dev_run_Speaker_Diarization_with_ASR_Inference: |
  python examples/speaker_tasks/diarization/clustering_diarizer/offline_diar_with_asr_infer.py \
  diarizer.manifest_filepath=/home/TestData/an4_diarizer/an4_manifest.json \
  diarizer.speaker_embeddings.model_path=/home/TestData/an4_diarizer/spkr.nemo \
  diarizer.speaker_embeddings.parameters.save_embeddings=True \
  diarizer.speaker_embeddings.parameters.window_length_in_sec=[1.5] \
  diarizer.speaker_embeddings.parameters.shift_length_in_sec=[0.75] \
  diarizer.speaker_embeddings.parameters.multiscale_weights=[1.0] \
  diarizer.asr.model_path=QuartzNet15x5Base-En \
  diarizer.asr.parameters.asr_based_vad=True \
  diarizer.out_dir=examples/speaker_tasks/diarization/speaker_diarization_asr_results
  rm -rf examples/speaker_tasks/diarization/speaker_diarization_asr_results
L2_Speaker_dev_run_Clustering_Diarizer_Inference: |
  python examples/speaker_tasks/diarization/clustering_diarizer/offline_diar_infer.py \
  diarizer.manifest_filepath=/home/TestData/an4_diarizer/an4_manifest.json \
  diarizer.speaker_embeddings.model_path=/home/TestData/an4_diarizer/spkr.nemo \
  diarizer.speaker_embeddings.parameters.save_embeddings=True \
  diarizer.speaker_embeddings.parameters.window_length_in_sec=1.5 \
  diarizer.speaker_embeddings.parameters.shift_length_in_sec=0.75 \
  diarizer.speaker_embeddings.parameters.multiscale_weights=null \
  diarizer.vad.model_path=/home/TestData/an4_diarizer/MatchboxNet_VAD_3x2.nemo \
  diarizer.out_dir=examples/speaker_tasks/diarization/clustering_diarizer_results
  rm -rf examples/speaker_tasks/diarization/clustering_diarizer_results
L2_Speaker_dev_run_Neural_Diarizer_Inference: |
  python examples/speaker_tasks/diarization/neural_diarizer/multiscale_diar_decoder_infer.py \
  diarizer.manifest_filepath=/home/TestData/an4_diarizer/an4_manifest.json \
  diarizer.msdd_model.model_path=/home/TestData/an4_diarizer/diar_msdd_telephonic.nemo \
  diarizer.speaker_embeddings.parameters.save_embeddings=True \
  diarizer.vad.model_path=/home/TestData/an4_diarizer/MatchboxNet_VAD_3x2.nemo \
  diarizer.out_dir=examples/speaker_tasks/diarization/neural_diarizer_results
  rm -rf examples/speaker_tasks/diarization/neural_diarizer_results
L2_Speaker_dev_run_Multispeaker_ASR_Data_Simulation: |
  python tools/speech_data_simulator/multispeaker_simulator.py \
  --config-path=conf --config-name=data_simulator.yaml \
  data_simulator.random_seed=42 \
  data_simulator.manifest_filepath=/home/TestData/LibriSpeechShort/dev-clean-align-short.json \
  data_simulator.outputs.output_dir=./test_simulator \
  data_simulator.session_config.num_sessions=2 \
  data_simulator.session_config.session_length=60
  rm -rf ./test_simulator
L2_ASR_Multi-dataloader_dev_run_Speech_to_Text_multi-dataloader: |
  python examples/asr/asr_ctc/speech_to_text_ctc.py \
  model.train_ds.manifest_filepath=/home/TestData/an4_dataset/an4_train.json \
  model.validation_ds.manifest_filepath=[/home/TestData/an4_dataset/an4_val.json,/home/TestData/an4_dataset/an4_val.json] \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  +trainer.num_sanity_val_steps=1 \
  exp_manager.exp_dir=examples/asr/speech_to_text_results
  rm -rf examples/asr/speech_to_text_results
L2_ASR_Multi-dataloader_dev_run_Speech_to_Label_multi-dataloader: |
  python examples/asr/speech_classification/speech_to_label.py \
  model.train_ds.manifest_filepath=/home/TestData/speech_commands/train_manifest.json \
  model.validation_ds.manifest_filepath=[/home/TestData/speech_commands/test_manifest.json,/home/TestData/speech_commands/test_manifest.json] \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  +trainer.num_sanity_val_steps=1 \
  model.preprocessor._target_=nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor \
  ~model.preprocessor.window_size \
  ~model.preprocessor.window_stride \
  ~model.preprocessor.window \
  ~model.preprocessor.n_mels \
  ~model.preprocessor.n_mfcc \
  ~model.preprocessor.n_fft \
  exp_manager.exp_dir=examples/asr/speech_to_label_results
  rm -rf examples/asr/speech_to_label_results
L2_ASR_Adapters_Linear_Adapters: |
  python examples/asr/asr_adapters/train_asr_adapter.py \
  model.pretrained_model="stt_en_conformer_ctc_small" \
  model.adapter.adapter_name="an4" \
  model.adapter.linear.in_features=176 \
  model.train_ds.manifest_filepath=/home/TestData/an4_dataset/an4_train.json \
  model.validation_ds.manifest_filepath=/home/TestData/an4_dataset/an4_val.json \
  trainer.max_steps=5 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=True \
  exp_manager.exp_dir=examples/asr/speech_to_text_adapters_results
  rm -rf examples/asr/speech_to_text_adapters_results
L2_ASR_Adapters_RelPos_MHA_Adapters: |
  python examples/asr/asr_adapters/train_asr_adapter.py \
  model.pretrained_model="stt_en_conformer_ctc_small" \
  model.adapter.adapter_name="encoder:an4" \
  model.adapter.adapter_type="tiny_attn" \
  model.adapter.tiny_attn.n_feat=176 \
  model.train_ds.manifest_filepath=/home/TestData/an4_dataset/an4_train.json \
  model.validation_ds.manifest_filepath=/home/TestData/an4_dataset/an4_val.json \
  trainer.max_steps=5 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=True \
  exp_manager.exp_dir=examples/asr/speech_to_text_adapters_mha_results
  rm -rf examples/asr/speech_to_text_adapters_mha_results
L2_Speech_Transcription_Speech_to_Text_Transcribe: |
  python examples/asr/transcribe_speech.py \
  pretrained_name="QuartzNet15x5Base-En" \
  audio_dir="/home/TestData/an4_transcribe/test_subset/" \
  output_filename="stt_test_res.json" \
  amp=true
  rm -rf stt_test_res.json
L2_Transducer_alignment_Running_pytest: |
  pytest tests/collections/asr/decoding/rnnt_alignments_check.py --durations=-1
L2_Segmentation_Tool_Parallel_ctc_segmentation_test_L2_Eng_CitriNet_with_wav: |
  cd tools/ctc_segmentation && \
  TIME=`date +"%Y-%m-%d-%T"` && \
  /bin/bash run_segmentation.sh \
  --MODEL_NAME_OR_PATH="stt_en_citrinet_512_gamma_0_25" \
  --DATA_DIR=/home/TestData/ctc_segmentation/eng \
  --OUTPUT_DIR=/home/TestData/ctc_segmentation/eng/output${TIME} \
  --LANGUAGE=en \
  --USE_NEMO_NORMALIZATION="TRUE" && \
  python /home/TestData/ctc_segmentation/verify_alignment.py \
  -r /home/TestData/ctc_segmentation/eng/eng_valid_segments_1.7.txt \
  -g /home/TestData/ctc_segmentation/eng/output${TIME}/verified_segments/nv_test_segments.txt && \
  rm -rf /home/TestData/ctc_segmentation/eng/output${TIME}
L2_Segmentation_Tool_Parallel_ctc_segmentation_test_L2_Ru_QN_with_mp3: |
  cd tools/ctc_segmentation && \
  TIME=`date +"%Y-%m-%d-%T"` && \
  /bin/bash run_segmentation.sh \
  --MODEL_NAME_OR_PATH=/home/TestData/ctc_segmentation/QuartzNet15x5-Ru-e512-wer14.45.nemo \
  --DATA_DIR=/home/TestData/ctc_segmentation/ru \
  --OUTPUT_DIR=/home/TestData/ctc_segmentation/ru/output${TIME} \
  --LANGUAGE=ru \
  --ADDITIONAL_SPLIT_SYMBOLS=";" && \
  python /home/TestData/ctc_segmentation/verify_alignment.py \
  -r /home/TestData/ctc_segmentation/ru/valid_ru_segments_1.7.txt \
  -g /home/TestData/ctc_segmentation/ru/output${TIME}/verified_segments/ru_segments.txt && \
  rm -rf /home/TestData/ctc_segmentation/ru/output${TIME}
L2_G2P_Models_G2P_Conformer_training_evaluation_and_inference: |
  cd examples/tts/g2p && \
      TIME=`date +"%Y-%m-%d-%T"` && OUTPUT_DIR_CONFORMER=output_ctc_${TIME} && \
      python g2p_train_and_evaluate.py \
          train_manifest=/home/TestData/g2p/g2p.json \
          validation_manifest=/home/TestData/g2p/g2p.json \
          model.test_ds.manifest_filepath=/home/TestData/g2p/g2p.json \
          model.tokenizer.dir=/home/TestData/g2p/tokenizer_spe_unigram_v512 \
          trainer.max_epochs=1 \
          model.max_source_len=64 \
          trainer.devices=1 \
          do_training=True \
          do_testing=True \
          exp_manager.exp_dir=${OUTPUT_DIR_CONFORMER} \
          +exp_manager.use_datetime_version=False\
          +exp_manager.version=test \
          --config-name=g2p_conformer_ctc && \
      python g2p_inference.py \
          pretrained_model=${OUTPUT_DIR_CONFORMER}/G2P-Conformer-CTC/test/checkpoints/G2P-Conformer-CTC.nemo \
          manifest_filepath=/home/TestData/g2p/g2p.json \
          phoneme_field=text
L2_G2P_Models_HeteronymClassificationModel_training_evaluation_and_inference: |
  cd examples/tts/g2p && \
      TIME=`date +"%Y-%m-%d-%T"` && OUTPUT_DIR=output_${TIME} && \
      python g2p_heteronym_classification_train_and_evaluate.py \
          train_manifest=/home/TestData/g2p/manifest.json \
          validation_manifest=/home/TestData/g2p/manifest.json \
          test_manifest=/home/TestData/g2p/manifest.json \
          model.wordids=/home/TestData/g2p/wordids.tsv \
          trainer.max_epochs=1 \
          model.max_seq_length=64 \
          do_training=True \
          do_testing=True \
          exp_manager.exp_dir=${OUTPUT_DIR} \
          +exp_manager.use_datetime_version=False\
          +exp_manager.version=test && \
      python g2p_heteronym_classification_inference.py \
          manifest=/home/TestData/g2p/manifest.json \
          pretrained_model=${OUTPUT_DIR}/HeteronymClassification/test/checkpoints/HeteronymClassification.nemo \
          output_manifest=preds.json
L2_Dialogue_Classification_Intent_and_slot_classification_using_SGDQA: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  model.dataset.data_dir=/home/TestData/nlp/sgd_small \
  model.dataset.dialogues_example_dir=sgd_gen_bert_outputs \
  model.dataset.task_name=debug_sample \
  trainer.max_steps=1 \
  trainer.max_epochs=1 \
  model.train_ds.batch_size=2 \
  model.validation_ds.batch_size=2 \
  model.test_ds.batch_size=2 \
  model.dataset.num_tasks=6 \
  model.nemo_path=null \
  trainer.val_check_interval=0.0 \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=bert-base-cased \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf sgd_gen_bert_outputs
L2_Dialogue_Classification_Intent_and_slot_classification_using_IntentSlotClassificationModel: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  model.dataset.data_dir=/home/TestData/nlp/processed_assistant \
  model.dataset.dialogues_example_dir=sgd_gen_bert_intent_classification_outputs \
  model.dataset.task=assistant \
  trainer.max_steps=1 \
  trainer.max_epochs=1 \
  model.train_ds.batch_size=2 \
  model.validation_ds.batch_size=2 \
  model.test_ds.batch_size=2 \
  model.nemo_path=null \
  trainer.val_check_interval=0.0 \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=bert-base-uncased \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf sgd_gen_bert_intent_classification_outputs
L2_Dialogue_Classification_Intent_classification_using_ZeroShotIntentModel: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  do_training=False \
  model.dataset.data_dir=/home/TestData/nlp/drive_thru_revised \
  model.original_nemo_checkpoint=/home/TestData/nlp/drive_thru_revised/zeroshotintent_en_bert_base_uncased.nemo \
  model.dataset.dialogues_example_dir=sgd_gen_zero_shot_intent_classification_outputs \
  model.dataset.task=zero_shot \
  model.dataset.prompt_template="This example is" \
  trainer.max_steps=1 \
  trainer.max_epochs=1 \
  model.train_ds.batch_size=2 \
  model.validation_ds.batch_size=2 \
  model.test_ds.batch_size=2 \
  model.nemo_path=null \
  trainer.val_check_interval=0.0 \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=bert-base-uncased \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf sgd_gen_zero_shot_intent_classification_outputs
L2_Dialogue_Classification_Design_Intent_classification_using_ZeroShotIntentModel: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  do_training=False \
  model.dataset.data_dir=/home/TestData/nlp/design_dataset \
  model.original_nemo_checkpoint=/home/TestData/nlp/drive_thru_revised/zeroshotintent_en_bert_base_uncased.nemo \
  model.dataset.dialogues_example_dir=design_zero_shot_intent_classification_outputs \
  model.dataset.task=design \
  model.dataset.prompt_template="This example is related to" \
  model.library=megatron \
  trainer.max_steps=1 \
  trainer.max_epochs=1 \
  model.train_ds.batch_size=2 \
  model.validation_ds.batch_size=2 \
  model.test_ds.batch_size=2 \
  model.nemo_path=null \
  trainer.val_check_interval=0.0 \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=bert-base-uncased \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf design_zero_shot_intent_classification_outputs
L2_Dialogue_Classification_Design_Intent_classification_using_ZeroShotIntentModel_BART_Classifier: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  do_training=False \
  model.dataset.data_dir=/home/TestData/nlp/design_dataset \
  model.original_nemo_checkpoint=/home/TestData/nlp/drive_thru_revised/zeroshotintent_en_bert_base_uncased.nemo \
  model.dataset.dialogues_example_dir=design_zero_shot_intent_classification_bart_outputs \
  model.dataset.task=design \
  model.dataset.prompt_template="This example is related to" \
  model.library=huggingface \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=bert-base-uncased \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf design_zero_shot_intent_classification_bart_outputs
L2_Dialogue_Classification_Design_Intent_classification_using_DialogueNearestNeighbourModel: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  do_training=False \
  model.dataset.data_dir=/home/TestData/nlp/design_dataset \
  model.dataset.dialogues_example_dir=design_dialogue_nearest_neighbour_classification_outputs \
  model.dataset.task=design \
  model.dataset.prompt_template="" \
  model.library=huggingface \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=sentence-transformers/all-MiniLM-L6-v2 \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf design_dialogue_nearest_neighbour_classification_outputs
L2_Dialogue_Generation_Dialogue_Answer_Extender_using_DialogueS2SGenerationModel: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  do_training=False \
  model.dataset.data_dir=/home/TestData/nlp/ms-marco-qa \
  model.dataset.dialogues_example_dir=answer_extender_s2s \
  model.dataset.task=ms_marco \
  model.library=huggingface \
  model.dataset.debug_mode=True \
  trainer.max_steps=1 \
  trainer.max_epochs=1 \
  model.train_ds.batch_size=2 \
  model.validation_ds.batch_size=2 \
  model.test_ds.batch_size=2 \
  model.nemo_path=null \
  trainer.val_check_interval=0.0 \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=facebook/bart-large \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf answer_extender_s2s
L2_Dialogue_Generation_Dialogue_SGD_Based_Answer_Extender_using_DialogueS2SGenerationModel: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  do_training=False \
  model.dataset.data_dir=/home/TestData/nlp/sgd_small \
  model.dataset.dialogues_example_dir=sgd_answer_extender_s2s \
  model.dataset.task_name=debug_sample \
  model.dataset.task=sgd_generation \
  model.dataset.input_field=utterance+system_actions \
  model.dataset.output_field=system_utterance \
  model.dataset.use_cache=false \
  model.dataset.system_utterance=next_turn \
  model.dataset.debug_mode=True \
  model.dataset.prompt_template=slots_values \
  model.library=huggingface \
  trainer.max_steps=1 \
  trainer.max_epochs=1 \
  model.train_ds.batch_size=2 \
  model.validation_ds.batch_size=2 \
  model.test_ds.batch_size=2 \
  model.nemo_path=null \
  trainer.val_check_interval=0.0 \
  trainer.devices=1 \
  model.language_model.pretrained_model_name=facebook/bart-large \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf sgd_answer_extender_s2s
L2_COPY_Dialogue_Answer_Extender_using_DialogueGPTGenerationModel: |
  cd examples/nlp/dialogue && \
  python dialogue.py \
  do_training=False \
  model.dataset.data_dir=/home/TestData/nlp/ms-marco-qa \
  model.dataset.dialogues_example_dir=answer_extender \
  model.library=huggingface \
  model.dataset.task=ms_marco \
  model.dataset.debug_mode=True \
  trainer.val_check_interval=0.0 \
  trainer.devices=1 \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name=gpt2 \
  trainer.accelerator=gpu \
  exp_manager=null  && \
  rm -rf answer_extender
L2_Duplex_Text_Normalization_with_Tarred_dataset: |
  cd examples/nlp/duplex_text_normalization && \
  python duplex_text_normalization_train.py \
  data.validation_ds.data_path=/home/TestData/nlp/duplex_text_norm/small_test.tsv \
  mode=tn \
  lang=en \
  tagger_model.do_training=false \
  decoder_model.transformer=t5-small \
  data.validation_ds.batch_size=2 \
  data.train_ds.use_cache=false \
  data.validation_ds.use_cache=false \
  data.test_ds.batch_size=2 \
  data.train_ds.decoder_data_augmentation=false \
  data.train_ds.num_workers=2 \
  decoder_trainer.devices=[0,1] \
  decoder_trainer.accelerator="gpu" \
  data.train_ds.use_tarred_dataset=true \
  +decoder_trainer.fast_dev_run=true \
  decoder_exp_manager.create_checkpoint_callback=false \
  data.train_ds.tar_metadata_file=/home/TestData/nlp/duplex_text_norm/tarred_small/metadata.json \
  data.test_ds.use_cache=false \
  data.test_ds.data_path=/home/TestData/nlp/duplex_text_norm/small_test.tsv
L2_BERT_Text_Classification_with_BERT_Test: |
  cd examples/nlp/text_classification && \
  python text_classification_with_bert.py \
  model.dataset.num_classes=6 \
  model.train_ds.file_path=/home/TestData/nlp/retail_text_classification/train.tsv \
  model.validation_ds.file_path=/home/TestData/nlp/retail_text_classification/dev.tsv \
  model.language_model.pretrained_model_name=distilbert-base-uncased \
  model.train_ds.batch_size=10 \
  model.dataset.max_seq_length=50 \
  model.dataset.use_cache=false \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=true \
  exp_manager=null
L2_Parallel_BERT_Question-Answering_SQUAD_v1_1: |
  # Cannot do fast_dev_run because squad needs whole dev dataset
  cd examples/nlp/question_answering && \
  python question_answering.py \
  model.train_ds.file=/home/TestData/nlp/squad_mini/v1.1/train-v1.1.json \
  model.dataset.use_cache=false \
  model.validation_ds.file=/home/TestData/nlp/squad_mini/v1.1/dev-v1.1.json \
  model.test_ds.file=/home/TestData/nlp/squad_mini/v1.1/dev-v1.1.json \
  model.train_ds.batch_size=2 \
  model.train_ds.num_samples=2 \
  model.validation_ds.batch_size=2 \
  model.validation_ds.num_samples=2 \
  model.test_ds.num_samples=2 \
  model.test_ds.batch_size=2 \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  model.language_model.pretrained_model_name=bert-base-uncased \
  model.dataset.version_2_with_negative=false \
  trainer.precision=16 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  exp_manager=null
L2_Parallel_BERT_Question-Answering_SQUAD_v2_0: |
  # Cannot do fast_dev_run because squad needs whole dev dataset
  cd examples/nlp/question_answering && \
  python question_answering.py \
  model.train_ds.file=/home/TestData/nlp/squad_mini/v2.0/train-v2.0.json \
  model.dataset.use_cache=false \
  model.train_ds.batch_size=2 \
  model.train_ds.num_samples=2 \
  model.validation_ds.batch_size=2 \
  model.validation_ds.num_samples=2 \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  model.validation_ds.file=/home/TestData/nlp/squad_mini/v2.0/dev-v2.0.json \
  model.language_model.pretrained_model_name=bert-base-uncased \
  model.dataset.version_2_with_negative=true \
  trainer.precision=16 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  exp_manager=null
L2_Parallel_BART_Question-Answering_SQUAD_v1_1: |
  cd examples/nlp/question_answering && \
  python question_answering.py \
  model.train_ds.file=/home/TestData/nlp/squad_mini/v1.1/train-v1.1.json \
  model.dataset.use_cache=false \
  model.dataset.check_if_answer_in_context=false \
  model.validation_ds.file=/home/TestData/nlp/squad_mini/v1.1/dev-v1.1.json \
  model.test_ds.file=/home/TestData/nlp/squad_mini/v1.1/dev-v1.1.json \
  model.train_ds.batch_size=2 \
  model.train_ds.num_samples=2 \
  model.validation_ds.batch_size=2 \
  model.validation_ds.num_samples=2 \
  model.test_ds.num_samples=2 \
  model.test_ds.batch_size=2 \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  model.language_model.pretrained_model_name=facebook/bart-base \
  model.dataset.version_2_with_negative=false \
  trainer.precision=16 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  exp_manager=null
L2_Parallel_BART_Question-Answering_SQUAD_v2_0: |
  cd examples/nlp/question_answering && \
  python question_answering.py \
  model.train_ds.file=/home/TestData/nlp/squad_mini/v2.0/train-v2.0.json \
  model.dataset.use_cache=false \
  model.dataset.check_if_answer_in_context=false \
  model.train_ds.batch_size=2 \
  model.train_ds.num_samples=2 \
  model.validation_ds.batch_size=2 \
  model.validation_ds.num_samples=2 \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  model.validation_ds.file=/home/TestData/nlp/squad_mini/v2.0/dev-v2.0.json \
  model.language_model.pretrained_model_name=facebook/bart-base \
  model.dataset.version_2_with_negative=true \
  trainer.precision=16 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  exp_manager=null
L2_Parallel_GPT2_Question-Answering_SQUAD_v1_1: |
  cd examples/nlp/question_answering && \
  python question_answering.py \
  model.train_ds.file=/home/TestData/nlp/squad_mini/v1.1/train-v1.1.json \
  model.dataset.use_cache=false \
  model.dataset.check_if_answer_in_context=false \
  model.validation_ds.file=/home/TestData/nlp/squad_mini/v1.1/dev-v1.1.json \
  model.test_ds.file=/home/TestData/nlp/squad_mini/v1.1/dev-v1.1.json \
  model.train_ds.batch_size=2 \
  model.train_ds.num_samples=2 \
  model.validation_ds.batch_size=2 \
  model.validation_ds.num_samples=2 \
  model.test_ds.num_samples=2 \
  model.test_ds.batch_size=2 \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  model.language_model.pretrained_model_name=gpt2 \
  model.dataset.version_2_with_negative=false \
  trainer.precision=16 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  exp_manager=null
L2_Parallel_GPT2_Question-Answering_SQUAD_v2_0: |
  cd examples/nlp/question_answering && \
  python question_answering.py \
  model.train_ds.file=/home/TestData/nlp/squad_mini/v2.0/train-v2.0.json \
  model.dataset.use_cache=false \
  model.dataset.check_if_answer_in_context=false \
  model.train_ds.batch_size=2 \
  model.train_ds.num_samples=2 \
  model.validation_ds.batch_size=2 \
  model.validation_ds.num_samples=2 \
  trainer.max_epochs=1 \
  trainer.max_steps=1 \
  model.validation_ds.file=/home/TestData/nlp/squad_mini/v2.0/dev-v2.0.json \
  model.language_model.pretrained_model_name=gpt2 \
  model.dataset.version_2_with_negative=true \
  trainer.precision=16 \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  exp_manager=null
L2_Intent_and_Slot_Classification_Tasks_Intent_and_Slot_Classification: |
  cd examples/nlp/intent_slot_classification && \
  python intent_slot_classification.py \
  model.data_dir=/home/TestData/nlp/retail \
  model.validation_ds.prefix=dev \
  model.test_ds.prefix=dev \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=true \
  exp_manager.exp_dir=checkpoints
  rm -rf checkpoints
L2_Intent_and_Slot_Classification_Tasks_Multi-Label_Intent_and_Slot_Classification: |
  cd examples/nlp/intent_slot_classification && \
  python multi_label_intent_slot_classification.py \
  model.data_dir=/home/TestData/nlp/new_multiatis \
  model.validation_ds.prefix=dev \
  model.test_ds.prefix=dev \
  trainer.devices=1 \
  +trainer.fast_dev_run=true \
  exp_manager.exp_dir=checkpoints2
  rm -rf checkpoints2
L2_Parallel_NLP_Examples2_NER_finetuning_from_pretrained_Test: |
  cd examples/nlp/token_classification && \
  python token_classification_train.py \
  pretrained_model=ner_en_bert \
  model.dataset.data_dir=/home/TestData/nlp/ner/ \
  model.train_ds.batch_size=2 \
  model.dataset.use_cache=false \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=true \
  model.dataset.class_balancing="weighted_loss" \
  exp_manager.exp_dir=null
L2_Parallel_NLP_Examples2_Punctuation_and_capitalization_finetuning_from_pretrained_test: |
  cd examples/nlp/token_classification && \
  data_dir="$(mktemp -d -p "$(pwd)")" && \
  cp /home/TestData/nlp/token_classification_punctuation/*.txt "${data_dir}"/ && \
  python punctuation_capitalization_train_evaluate.py \
    pretrained_model=punctuation_en_bert \
    model.train_ds.ds_item="${data_dir}" \
    model.validation_ds.ds_item="${data_dir}" \
    model.test_ds.ds_item="${data_dir}" \
    +model.train_ds.use_cache=false \
    +model.validation_ds.use_cache=false \
    +model.test_ds.use_cache=false \
    trainer.devices=1 \
    trainer.accelerator="gpu" \
    +trainer.fast_dev_run=true \
    exp_manager.exp_dir=null && \
  rm -rf "${data_dir}"
L2_Parallel_NLP_Examples2_NER_with_TurkuNLP__bert-base-finnish-cased-v1: |
  cd examples/nlp/token_classification && \
  python token_classification_train.py \
  model.dataset.data_dir=/home/TestData/nlp/token_classification_punctuation/ \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=true \
  model.dataset.use_cache=false \
  model.language_model.pretrained_model_name="TurkuNLP/bert-base-finnish-cased-v1" \
  exp_manager.exp_dir=null
L2_Parallel_NLP_Examples2_Evaluation_script_for_Token_Classification: |
  python examples/nlp/token_classification/token_classification_evaluate.py \
  model.dataset.data_dir=/home/TestData/nlp/ner/ \
  model.dataset.use_cache=false \
  pretrained_model=/home/TestData/nlp/pretrained_models/NER_Model_with_BERT_base_uncased.nemo
L2_Parallel_NLP_Examples2_Evaluation_script_for_Punctuation: |
  data_dir="$(mktemp -d -p "$(pwd)")" && \
  cp /home/TestData/nlp/token_classification_punctuation/*.txt "${data_dir}"/ && \
  python examples/nlp/token_classification/punctuation_capitalization_train_evaluate.py \
    +do_training=false \
    +do_testing=true \
    model.test_ds.ds_item="${data_dir}" \
    ~model.train_ds \
    ~model.validation_ds \
    +model.test_ds.use_cache=false \
    pretrained_model=/home/TestData/nlp/pretrained_models/Punctuation_Capitalization_with_DistilBERT_base_uncased.nemo && \
  rm -rf "${data_dir}"
L2_Parallel_NLP_Examples2_Punctuation_Capitalization_2GPUs_with_DistilBERT_Finetuning_on_other_data: |
  cd examples/nlp/token_classification && \
  output_dir="$(mktemp -d -p "$(pwd)")" && \
  tmp_data_dir="$(mktemp -d -p "$(pwd)")" && \
  cp /home/TestData/nlp/token_classification_punctuation/*.txt "${tmp_data_dir}"/ && \
  python punctuation_capitalization_train_evaluate.py \
    model.train_ds.use_tarred_dataset=false \
    model.train_ds.ds_item="${tmp_data_dir}" \
    model.validation_ds.ds_item="${tmp_data_dir}" \
    model.test_ds.ds_item="${tmp_data_dir}" \
    model.language_model.pretrained_model_name=distilbert-base-uncased \
    +model.train_ds.use_cache=false \
    +model.validation_ds.use_cache=false \
    +model.test_ds.use_cache=false \
    trainer.devices=[0,1] \
    trainer.accelerator="gpu" \
    trainer.strategy=ddp \
    trainer.max_epochs=1 \
    +exp_manager.explicit_log_dir="${output_dir}" \
    +do_testing=true && \
  tmp_data_dir_2="$(mktemp -d -p "$(pwd)")" && \
  mv "${tmp_data_dir}"/* "${tmp_data_dir_2}" && \
  rm -rf "${tmp_data_dir}" && \
  python punctuation_capitalization_train_evaluate.py \
    model.train_ds.use_tarred_dataset=false \
    model.train_ds.ds_item="${tmp_data_dir_2}" \
    model.validation_ds.ds_item="${tmp_data_dir_2}" \
    model.test_ds.ds_item="${tmp_data_dir_2}" \
    pretrained_model="${output_dir}/checkpoints/Punctuation_and_Capitalization.nemo" \
    +model.train_ds.use_cache=false \
    +model.validation_ds.use_cache=false \
    +model.test_ds.use_cache=false \
    trainer.devices=[0,1] \
    trainer.accelerator="gpu" \
    trainer.strategy=ddp \
    trainer.max_epochs=1 \
    exp_manager=null && \
  rm -rf /workspace/NeMo/examples/nlp/token_classification/nemo_experiments \
    "${tmp_data_dir_2}" \
    "${output_dir}"
L2_Pretraining_BERT_pretraining_from_Text: |
  "cd examples/nlp/language_modeling && \\\n  python bert_pretraining.py \\\n  --config-name=bert_pretraining_from_text_config.yaml \\\n  trainer.devices=1 \\\n  trainer.accelerator=\"gpu\" \\\n  trainer.precision=16 \\\n  +trainer.fast_dev_run=true \\\n  model.train_ds.data_file=/home/TestData/nlp/wikitext-2/train.txt  \\\n  model.train_ds.batch_size=32 \\\n  model.validation_ds.data_file=/home/TestData/nlp/wikitext-2/valid.txt  \\\n  model.validation_ds.batch_size=32 \\\n  model.language_model.config_file=/home/TestData/nlp/bert_configs/bert_3200.json \\\n  model.optim.lr=0.01 \\\n  model.optim.sched.warmup_ratio=0.1 \\\n  model.tokenizer.tokenizer_name=sentencepiece \\\n  model.tokenizer.tokenizer_model=/home/TestData/nlp/wikitext-2/tokenizer_bpe_v3193/tokenizer.model \\\n  model.mask_prob=0.15 \\\n  model.short_seq_prob=0.1 \\\n  exp_manager.exp_dir=PretrainingBERTFromText \\\n  \nrm -f /home/TestData/nlp/wikitext-2/*.pkl\n#rm -rf examples/nlp/language_modeling/PretrainingBERTFromText\n"
L2_Pretraining_BERT_from_Preprocessed: |
  "cd examples/nlp/language_modeling && \\\n  python bert_pretraining.py \\\n  --config-name=bert_pretraining_from_preprocessed_config.yaml \\\n  trainer.devices=1 \\\n  trainer.accelerator=\"gpu\" \\\n  trainer.precision=16 \\\n  +trainer.fast_dev_run=false \\\n  +trainer.max_epochs=1 \\\n  +trainer.limit_val_batches=0 \\\n  +trainer.limit_train_batches=1 \\\n  model.train_ds.data_file=/home/TestData/nlp/wiki_book_mini/training \\\n  model.train_ds.batch_size=8 \\\n  model.language_model.lm_checkpoint=/home/TestData/nlp/bert_ckpts/nemo1.0/bert_base_uncased_mlm_final_1074591_nemo1.0.pt \\\n  model.language_model.config_file=/home/TestData/nlp/bert_configs/uncased_L-12_H-768_A-12.json \\\n  model.optim.lr=0.875e-4 \\\n  model.optim.weight_decay=0.01 \\\n  model.optim.sched.warmup_ratio=0.01 \\\n  exp_manager.exp_dir=PretrainingBERTFromPreprocessed \\\n  exp_manager.create_checkpoint_callback=False \\\n  \n#rm -rf examples/nlp/language_modeling/PretrainingBERTFromPreprocessed\n"
L2_Entity_Linking_Self_Alignment_Pretraining_BERT: |
  cd examples/nlp/entity_linking && \
  python self_alignment_pretraining.py \
  project_dir=. \
  trainer.val_check_interval=3 \
  model.raw_data=None \
  model.train_ds.data_file=/home/TestData/nlp/entity_linking/tiny_example_train_pairs.tsv \
  model.validation_ds.data_file=/home/TestData/nlp/entity_linking/tiny_example_validation_pairs.tsv \
  model.train_ds.batch_size=8 \
  model.validation_ds.batch_size=8 \
  exp_manager.exp_dir=null
L2_NMT_Attention_is_All_You_Need_Training_NMT_Training_Post-LN: |
 "python examples/nlp/machine_translation/enc_dec_nmt.py \\\n  --config-path=conf \\\n  --config-name=aayn_base \\\n  do_testing=false \\\n  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \\\n  model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \\\n  model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \\\n  model.encoder.num_layers=1 \\\n  model.encoder.hidden_size=64 \\\n  model.encoder.inner_size=256 \\\n  model.decoder.num_layers=1 \\\n  model.decoder.hidden_size=64 \\\n  model.decoder.inner_size=256 \\\n  +model.optim.capturable=True \\\n  trainer.devices=1 \\\n  trainer.accelerator=\"gpu\" \\\n  +trainer.val_check_interval=2 \\\n  +trainer.limit_val_batches=1 \\\n  +trainer.max_steps=2 \\\n  trainer.precision=16 \\\n  +exp_manager.explicit_log_dir=examples/nlp/machine_translation/nmt_results \\\n  +exp_manager.create_checkpoint_callback=true\n  \npython examples/nlp/machine_translation/enc_dec_nmt.py \\\n  --config-path=conf \\\n  --config-name=aayn_base \\\n  do_testing=true \\\n  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \\\n  model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \\\n  model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \\\n  model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \\\n  model.encoder.num_layers=1 \\\n  model.encoder.hidden_size=64 \\\n  model.encoder.inner_size=256 \\\n  model.decoder.num_layers=1 \\\n  model.decoder.hidden_size=64 \\\n  model.decoder.inner_size=256 \\\n  +model.optim.capturable=True \\\n  trainer.devices=1 \\\n  trainer.accelerator=\"gpu\" \\\n  +trainer.val_check_interval=10 \\\n  +trainer.limit_val_batches=1 \\\n  +trainer.limit_test_batches=1 \\\n  +trainer.max_steps=10 \\\n  +exp_manager.explicit_log_dir=examples/nlp/machine_translation/nmt_results \\\n  +exp_manager.create_checkpoint_callback=true \\\n  +exp_manager.resume_if_exists=True\n  \nrm -rf examples/nlp/machine_translation/nmt_results\n"
L2_NMT_Attention_is_All_You_Need_Training_NMT_Training_Pre-LN: |
  cd examples/nlp/machine_translation && \
  python enc_dec_nmt.py \
  --config-path=conf \
  --config-name=aayn_base \
  do_testing=true \
  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \
  model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \
  model.encoder.pre_ln=true \
  model.decoder.pre_ln=true \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=true \
  +trainer.limit_test_batches=2 \
  exp_manager=null
L2_NMT_Attention_is_All_You_Need_Training_NMT_Multi-Validation: |
  cd examples/nlp/machine_translation && \
  python enc_dec_nmt.py \
  --config-path=conf \
  --config-name=aayn_base \
  do_testing=true \
  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-en-de.src \
  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-en-de.ref \
  model.validation_ds.src_file_name=[/home/TestData/nlp/nmt/toy_data/wmt13-en-de.src,/home/TestData/nlp/nmt/toy_data/wmt14-en-de.src] \
  model.validation_ds.tgt_file_name=[/home/TestData/nlp/nmt/toy_data/wmt13-en-de.ref,/home/TestData/nlp/nmt/toy_data/wmt14-en-de.ref] \
  model.test_ds.src_file_name=[/home/TestData/nlp/nmt/toy_data/wmt13-en-de.src,/home/TestData/nlp/nmt/toy_data/wmt14-en-de.src] \
  model.test_ds.tgt_file_name=[/home/TestData/nlp/nmt/toy_data/wmt13-en-de.ref,/home/TestData/nlp/nmt/toy_data/wmt14-en-de.ref] \
  model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \
  model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/spm_4k_ende.model \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=true \
  +trainer.limit_test_batches=2 \
  exp_manager=null
L2_NMT_Attention_is_All_You_Need_Inference: |
  cd examples/nlp/machine_translation && \
  python nmt_transformer_infer.py \
  --model=/home/TestData/nlp/nmt/toy_data/enes_v16k_s100k_6x6.nemo \
  --srctext=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.test.src \
  --tgtout=/home/TestData/nlp/nmt/toy_data/out.txt \
  --target_lang en \
  --source_lang de
L2_NMT_Attention_is_All_You_Need_Finetuning: |
  cd examples/nlp/machine_translation && \
  python enc_dec_nmt_finetune.py \
  model_path=/home/TestData/nlp/nmt/toy_data/enes_v16k_s100k_6x6.nemo \
  trainer.devices=1 \
  ~trainer.max_epochs \
  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  +trainer.val_check_interval=10 \
  +trainer.limit_val_batches=1 \
  +trainer.limit_test_batches=1 \
  +trainer.max_steps=10 \
  +exp_manager.exp_dir=examples/nlp/machine_translation/nmt_finetune \
  +exp_manager.create_checkpoint_callback=True \
  +exp_manager.checkpoint_callback_params.monitor=val_sacreBLEU \
  +exp_manager.checkpoint_callback_params.mode=max \
  +exp_manager.checkpoint_callback_params.save_best_model=true

  rm -rf examples/nlp/machine_translation/nmt_finetune
L2_NMT_Tarred_Dataset_Creation_Auto_Tarred_Dataset_Creation: |
  cd examples/nlp/machine_translation && \
  python enc_dec_nmt.py \
  --config-path=conf \
  --config-name=aayn_base \
  do_training=false \
  model.preproc_out_dir=$PWD/preproc_out_dir \
  model.train_ds.use_tarred_dataset=true \
  model.train_ds.n_preproc_jobs=2 \
  model.train_ds.lines_per_dataset_fragment=500 \
  model.train_ds.num_batches_per_tarfile=10 \
  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.encoder_tokenizer.vocab_size=2000 \
  model.decoder_tokenizer.vocab_size=2000 \
  ~model.test_ds \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.fast_dev_run=true \
  exp_manager=null \
L2_NMT_Tarred_Dataset_Creation_Script_Tarred_Dataset_Creation: |
  cd examples/nlp/machine_translation && \
  python create_tarred_parallel_dataset.py \
  --src_fname /home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  --tgt_fname /home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  --out_dir $PWD/out_dir \
  --encoder_tokenizer_vocab_size=2000 \
  --decoder_tokenizer_vocab_size=2000 \
  --tokens_in_batch=1000 \
  --lines_per_dataset_fragment=500 \
  --num_batches_per_tarfile=10 \
  --n_preproc_jobs=2 \
L2_Megatron_NMT_Training_TP2: |
  python examples/nlp/machine_translation/megatron_nmt_training.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  +trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/machine_translation/megatron_nmt_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation='swiglu' \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method='block' \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation='swiglu' \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method='block' \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.micro_batch_size=2 \
  model.global_batch_size=4 \
  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  model.train_ds.num_workers=1 \
  model.validation_ds.num_workers=1 \
  ~model.test_ds \
  model.train_ds.dataset_type=text_memmap \
  model.encoder_tokenizer.library=sentencepiece \
  model.encoder_tokenizer.model=/home/TestData/nlp/nmt/toy_data/spm_64k_all_langs_plus_en.model \
  model.decoder_tokenizer.library=sentencepiece \
  model.decoder_tokenizer.model=/home/TestData/nlp/nmt/toy_data/spm_64k_all_langs_plus_en.model
  # Change val_check_interval to 1 for resume as the len(dataloder) is 1 due to max_steps being the same as that of training and Lightning 2.0 raises an error
  # if val_check_interval > len(dataloder: https://github.com/Lightning-AI/lightning/blob/2.0.6/src/lightning/pytorch/loops/fit_loop.py#L259 at the beginning of fit_loop.run()
  python examples/nlp/machine_translation/megatron_nmt_training.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  +trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/machine_translation/megatron_nmt_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation='swiglu' \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method='block' \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation='swiglu' \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method='block' \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.micro_batch_size=2 \
  model.global_batch_size=4 \
  model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
  model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
  model.train_ds.num_workers=1 \
  model.validation_ds.num_workers=1 \
  ~model.test_ds \
  model.train_ds.dataset_type=text_memmap \
  model.encoder_tokenizer.library=sentencepiece \
  model.encoder_tokenizer.model=/home/TestData/nlp/nmt/toy_data/spm_64k_all_langs_plus_en.model \
  model.decoder_tokenizer.library=sentencepiece \
  model.decoder_tokenizer.model=/home/TestData/nlp/nmt/toy_data/spm_64k_all_langs_plus_en.model
  rm -rf examples/nlp/machine_translation/megatron_nmt_results
L2_Megatron_BART_Perceiver_MIM_Training_TP2: |
  python examples/nlp/language_modeling/megatron_bart_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/megatron_mim_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.arch=perceiver \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation='swiglu' \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method='block' \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation='swiglu' \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method='block' \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.micro_batch_size=2 \
  model.global_batch_size=4 \
  model.data.data_impl=text_mmap \
  model.data.data_prefix=[1.0,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src] \
  model.data.splits_string='"800,100,100"' \
  model.data.whole_word_masking=False \
  model.tokenizer.library=sentencepiece \
  model.tokenizer.model=/home/TestData/nlp/nmt/toy_data/spm_64k_all_langs_plus_en.model \
  ++model.hiddens.enc_output_name=z \
  ++model.hiddens.transform.q_z_given_x.cls_name=cond_gaussian \
  ++model.hiddens.transform.q_z_given_x.hidden_size=64 \
  ++model.hiddens.loss.mim.cls_name=a_mim \
  ++model.hiddens.loss.mim.loss_weight=0.5
  # Change val_check_interval to 1 for resume as the len(dataloder) is 1 due to max_steps being the same as that of training and Lightning 2.0 raises an error
  # if val_check_interval > len(dataloder: https://github.com/Lightning-AI/lightning/blob/2.0.6/src/lightning/pytorch/loops/fit_loop.py#L259 at the beginning of fit_loop.run()
  python examples/nlp/language_modeling/megatron_bart_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/megatron_mim_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.arch=perceiver \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation='swiglu' \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method='block' \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation='swiglu' \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method='block' \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.micro_batch_size=2 \
  model.global_batch_size=4 \
  model.data.data_impl=text_mmap \
  model.data.data_prefix=[1.0,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src] \
  model.data.splits_string='"800,100,100"' \
  model.data.whole_word_masking=False \
  model.tokenizer.library=sentencepiece \
  model.tokenizer.model=/home/TestData/nlp/nmt/toy_data/spm_64k_all_langs_plus_en.model \
  ++model.hiddens.enc_output_name=z \
  ++model.hiddens.transform.q_z_given_x.cls_name=cond_gaussian \
  ++model.hiddens.transform.q_z_given_x.hidden_size=64 \
  ++model.hiddens.loss.mim.cls_name=a_mim \
  ++model.hiddens.loss.mim.loss_weight=0.5
  rm -rf examples/nlp/language_modeling/megatron_mim_results
L2_Megatron_Bert_Pretraining_and_Resume_Training_with_Pipeline_Parallelism: |
  python examples/nlp/language_modeling/megatron_bert_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bert_pretrain_results \
  model.pipeline_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_bert/data/bert/vocab.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence,.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/bert_index_mappings

  python examples/nlp/language_modeling/megatron_bert_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=20 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bert_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.pipeline_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_bert/data/bert/vocab.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence,.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/bert_index_mappings

  rm -rf examples/nlp/language_modeling/bert_pretrain_results
  rm -rf examples/nlp/language_modeling/bert_index_mappings
L2_Megatron_Bert_Pretraining_and_Resume_Training: |
  python examples/nlp/language_modeling/megatron_bert_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bert_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.sequence_parallel=True \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_bert/data/bert/vocab.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence,.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/bert_index_mappings

  python examples/nlp/language_modeling/megatron_bert_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=20 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bert_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_bert/data/bert/vocab.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence,.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/bert_index_mappings

  rm -rf examples/nlp/language_modeling/bert_pretrain_results
  rm -rf examples/nlp/language_modeling/bert_index_mappings
L2_Megatron_Core_Bert_Pretraining_and_Resume_Training: |
  NVTE_FLASH_ATTN=0 python examples/nlp/language_modeling/megatron_bert_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=32 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bert_pretrain_results \
  model.mcore_bert=True \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.sequence_parallel=True \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_bert/data/bert/vocab.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method='block' \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence,.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/bert_index_mappings

  NVTE_FLASH_ATTN=0 python examples/nlp/language_modeling/megatron_bert_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=20 \
  trainer.precision=32 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bert_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.mcore_bert=True \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_bert/data/bert/vocab.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method='block' \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence,.5,/home/TestData/nlp/megatron_bert/data/bert/simple_wiki_bert_preproc_text_sentence] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/bert_index_mappings

  rm -rf examples/nlp/language_modeling/bert_pretrain_results
  rm -rf examples/nlp/language_modeling/bert_index_mappings
L2_Megatron_RETRO_Pretraining_and_Resume_Training: |
  python examples/nlp/language_modeling/megatron_retro_pretraining.py \
  trainer.num_nodes=1 \
  trainer.devices=2 \
  trainer.precision=bf16 \
  trainer.accelerator=gpu \
  model.data.data_prefix=['none'] \
  exp_manager.exp_dir=examples/nlp/language_modeling/mcore_retro_results \
  model.mcore_gpt=True \
  model.tensor_model_parallel_size=1 \
  model.pipeline_model_parallel_size=1 \
  model.optim.name=distributed_fused_adam \
  model.retro.retro_project_dir=/home/TestData/nlp/megatron_retro/mcore_retro/micro-wiki-core \
  model.data.num_workers=4 \
  model.micro_batch_size=1 \
  model.data.shuffle_documents=False \
  trainer.val_check_interval=30 \
  +trainer.num_sanity_val_steps=0 \
  model.init_method_std=0.023 \
  model.optim.lr=6.0e-4 \
  model.megatron_amp_O2=True \
  model.data.splits_string=\'\"98,2,0\"\' \
  model.data.dataloader_type=cyclic \
  trainer.max_steps=10

  python examples/nlp/language_modeling/megatron_retro_pretraining.py \
  trainer.num_nodes=1 \
  trainer.devices=2 \
  trainer.precision=bf16 \
  trainer.accelerator=gpu \
  model.data.data_prefix=['none'] \
  exp_manager.exp_dir=examples/nlp/language_modeling/mcore_retro_results \
  model.mcore_gpt=True \
  model.tensor_model_parallel_size=1 \
  model.pipeline_model_parallel_size=1 \
  model.optim.name=distributed_fused_adam \
  model.retro.retro_project_dir=/home/TestData/nlp/megatron_retro/mcore_retro/micro-wiki-core \
  model.data.num_workers=4 \
  model.micro_batch_size=1 \
  model.data.shuffle_documents=False \
  trainer.val_check_interval=30 \
  +trainer.num_sanity_val_steps=0 \
  model.init_method_std=0.023 \
  model.optim.lr=6.0e-4 \
  model.megatron_amp_O2=True \
  model.data.splits_string=\'\"98,2,0\"\' \
  model.data.dataloader_type=cyclic \
  trainer.max_steps=20

  rm -rf examples/nlp/language_modeling/mcore_retro_results
L2_Legacy_Megatron_RETRO_Pretraining_and_Resume_Training: |
  python examples/nlp/language_modeling/megatron_retro_pretraining_legacy.py \
  trainer.devices=2 \
  trainer.num_nodes=1 \
  trainer.accelerator=gpu \
  trainer.accumulate_grad_batches=1 \
  trainer.limit_val_batches=2 \
  exp_manager.resume_if_exists=True \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  trainer.val_check_interval=10 \
  exp_manager.exp_dir=examples/nlp/language_modeling/retro_legacy_results \
  model.data.data_prefix= \
  model.data.knn_index= \
  model.data.retrieval_prefix= \
  model.tensor_model_parallel_size=2 \
  model.micro_batch_size=4 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.chunk_size=32 \
  model.enc_num_layers=2 \
  model.dec_num_layers=2 \
  model.enc_cross_attention=[1] \
  model.dec_cross_attention=[1] \
  +model.data.mock=True

  python examples/nlp/language_modeling/megatron_retro_pretraining_legacy.py \
  trainer.devices=2 \
  trainer.num_nodes=1 \
  trainer.accelerator=gpu \
  trainer.accumulate_grad_batches=1 \
  trainer.limit_val_batches=2 \
  exp_manager.resume_if_exists=True \
  trainer.max_steps=20 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  trainer.val_check_interval=10 \
  exp_manager.exp_dir=examples/nlp/language_modeling/retro_legacy_results \
  model.data.data_prefix= \
  model.data.knn_index= \
  model.data.retrieval_prefix= \
  model.tensor_model_parallel_size=2 \
  model.micro_batch_size=4 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.chunk_size=32 \
  model.enc_num_layers=2 \
  model.dec_num_layers=2 \
  model.enc_cross_attention=[1] \
  model.dec_cross_attention=[1] \
  +model.data.mock=True

  rm -rf examples/nlp/language_modeling/retro_legacy_results
L2_BioMegatron_Bert_NER_Task: |
  python examples/nlp/token_classification/token_classification_train.py \
  exp_manager.exp_dir=examples/nlp/language_modeling/token_classification_results \
  trainer.max_epochs=1 \
  model.dataset.data_dir=/home/TestData/nlp/ner \
  model.language_model.pretrained_model_name=biomegatron345m_biovocab_30k_cased \
  model.tokenizer.tokenizer_name=null
  rm -rf examples/nlp/language_modeling/token_classification_results
L2_Megatron_GPT_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=3 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=1 \
  model.optim.sched.constant_steps=1 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.normalization=rmsnorm \
  model.bias=False \
  model.bias_activation_fusion=False \
  model.bias_dropout_add_fusion=False \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_granularity=full \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings

  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=6 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.normalization=rmsnorm \
  model.bias=False \
  model.bias_activation_fusion=False \
  model.bias_dropout_add_fusion=False \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_granularity=full \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings

  rm -rf examples/nlp/language_modeling/gpt_pretrain_results
  rm -rf examples/nlp/language_modeling/gpt_index_mappings
L2_Megatron_GPT_with_Rope_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=3 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=1 \
  model.optim.sched.constant_steps=1 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.position_embedding_type=rope \
  model.rotary_percentage=0.5 \
  model.normalization=rmsnorm \
  model.bias=False \
  model.bias_activation_fusion=False \
  model.bias_dropout_add_fusion=False \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_granularity=full \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings

   #  commented out to save time on github ci @adithyare
   # python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
   # trainer.devices=2 \
   # trainer.accelerator=gpu \
   # trainer.log_every_n_steps=1 \
   # trainer.val_check_interval=2 \
   # trainer.limit_val_batches=1 \
   # trainer.accumulate_grad_batches=1 \
   # trainer.max_steps=6 \
   # trainer.precision=16 \
   # trainer.gradient_clip_val=1.0 \
   # exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
   # exp_manager.resume_if_exists=True \
   # model.tensor_model_parallel_size=2 \
   # model.optim.name=fused_adam \
   # model.optim.lr=2e-4 \
   # model.optim.sched.warmup_steps=2 \
   # model.optim.sched.constant_steps=2 \
   # model.optim.sched.min_lr=8e-5 \
   # model.max_position_embeddings=128 \
   # model.encoder_seq_length=128 \
   # model.data.seq_length=128 \
   # model.position_embedding_type=rope \
   # model.rotary_percentage=0.5 \
   # model.normalization=rmsnorm \
   # model.bias=False \
   # model.bias_activation_fusion=False \
   # model.bias_dropout_add_fusion=False \
   # model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
   # model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
   # model.num_layers=8 \
   # model.hidden_size=256 \
   # model.num_attention_heads=8 \
   # model.activations_checkpoint_method=block \
   # model.activations_checkpoint_granularity=full \
   # model.activations_checkpoint_num_layers=1 \
   # model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
   # model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings"

  rm -rf examples/nlp/language_modeling/gpt_pretrain_results
  rm -rf examples/nlp/language_modeling/gpt_index_mappings
L2_Megatron_GPT_with_ALiBi_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=3 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=1 \
  model.optim.sched.constant_steps=1 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.position_embedding_type=alibi \
  model.normalization=rmsnorm \
  model.bias=False \
  model.bias_activation_fusion=False \
  model.bias_dropout_add_fusion=False \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_granularity=full \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings

  # not testing resume functionality to save time on ci @adithyare
  #python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  #trainer.devices=2 \
  #trainer.accelerator=gpu \
  #trainer.log_every_n_steps=1 \
  #trainer.val_check_interval=2 \
  #trainer.limit_val_batches=1 \
  #trainer.accumulate_grad_batches=1 \
  #trainer.max_steps=6 \
  #trainer.precision=16 \
  #trainer.gradient_clip_val=1.0 \
  #exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  #exp_manager.resume_if_exists=True \
  #model.tensor_model_parallel_size=2 \
  #model.optim.name=fused_adam \
  #model.optim.lr=2e-4 \
  #model.optim.sched.warmup_steps=2 \
  #model.optim.sched.constant_steps=2 \
  #model.optim.sched.min_lr=8e-5 \
  #model.max_position_embeddings=128 \
  #model.encoder_seq_length=128 \
  #model.data.seq_length=128 \
  #model.position_embedding_type=alibi \
  #model.normalization=rmsnorm \
  #model.bias=False \
  #model.bias_activation_fusion=False \
  #model.bias_dropout_add_fusion=False \
  #model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  #model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  #model.num_layers=8 \
  #model.hidden_size=256 \
  #model.num_attention_heads=8 \
  #model.activations_checkpoint_method=block \
  #model.activations_checkpoint_granularity=full \
  #model.activations_checkpoint_num_layers=1 \
  #model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  #model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings"

  rm -rf examples/nlp/language_modeling/gpt_pretrain_results
  rm -rf examples/nlp/language_modeling/gpt_index_mappings
L2_Megatron_GPT_with_KERPLE_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=3 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=1 \
  model.optim.sched.constant_steps=1 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.data.seq_length=128 \
  model.position_embedding_type=kerple \
  model.normalization=rmsnorm \
  model.bias=False \
  model.bias_activation_fusion=False \
  model.bias_dropout_add_fusion=False \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_granularity=full \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings

  # commented out to save time on github ci @adithyare
  #python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  #trainer.devices=2 \
  #trainer.accelerator=gpu \
  #trainer.log_every_n_steps=1 \
  #trainer.val_check_interval=2 \
  #trainer.limit_val_batches=1 \
  #trainer.accumulate_grad_batches=1 \
  #trainer.max_steps=6 \
  #trainer.precision=16 \
  #trainer.gradient_clip_val=1.0 \
  #exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  #exp_manager.resume_if_exists=True \
  #model.tensor_model_parallel_size=2 \
  #model.optim.name=fused_adam \
  #model.optim.lr=2e-4 \
  #model.optim.sched.warmup_steps=2 \
  #model.optim.sched.constant_steps=2 \
  #model.optim.sched.min_lr=8e-5 \
  #model.max_position_embeddings=128 \
  #model.encoder_seq_length=128 \
  #model.data.seq_length=128 \
  #model.position_embedding_type=kerple \
  #model.normalization=rmsnorm \
  #model.bias=False \
  #model.bias_activation_fusion=False \
  #model.bias_dropout_add_fusion=False \
  #model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  #model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  #model.num_layers=8 \
  #model.hidden_size=256 \
  #model.num_attention_heads=8 \
  #model.activations_checkpoint_method=block \
  #model.activations_checkpoint_granularity=full \
  #model.activations_checkpoint_num_layers=1 \
  #model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  #model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings"

  rm -rf examples/nlp/language_modeling/gpt_pretrain_results
  rm -rf examples/nlp/language_modeling/gpt_index_mappings
L2_Megatron_GPT_Pretraining_and_Resume_Training_PP2: |
  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  trainer.devices=2 \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=3 \
  trainer.precision=bf16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  model.pipeline_model_parallel_size=2 \
  model.tensor_model_parallel_size=1 \
  model.mcore_gpt=True \
  model.megatron_amp_O2=True \
  model.optim.name=distributed_fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=1 \
  model.optim.sched.constant_steps=1 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.activation=fast-swiglu \
  model.bias_activation_fusion=False \
  model.hidden_dropout=0.0 \
  model.attention_dropout=0.0 \
  model.transformer_block_type=normformer \
  model.headscale=True \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings

  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  trainer.devices=2 \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=6 \
  trainer.precision=bf16 \
  trainer.gradient_clip_val=1.0 \
  model.mcore_gpt=True \
  model.megatron_amp_O2=True \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.pipeline_model_parallel_size=2 \
  model.tensor_model_parallel_size=1 \
  model.optim.name=distributed_fused_adam \
  model.optim.lr=2e-4 \
  model.optim.sched.warmup_steps=2 \
  model.optim.sched.constant_steps=2 \
  model.optim.sched.min_lr=8e-5 \
  model.max_position_embeddings=128 \
  model.encoder_seq_length=128 \
  model.activation=fast-swiglu \
  model.bias_activation_fusion=False \
  model.hidden_dropout=0.0 \
  model.attention_dropout=0.0 \
  model.transformer_block_type=normformer \
  model.headscale=True \
  model.data.seq_length=128 \
  model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
  model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
  model.num_layers=8 \
  model.hidden_size=256 \
  model.num_attention_heads=8 \
  model.activations_checkpoint_method=block \
  model.activations_checkpoint_num_layers=1 \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings

  rm -rf examples/nlp/language_modeling/gpt_pretrain_results
  rm -rf examples/nlp/language_modeling/gpt_index_mappings
L2_Megatron_GPT_Finetuning_PP2: |
  python examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \
  trainer.devices=2 \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  +trainer.limit_val_batches=2 \
  trainer.max_steps=3 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_sft_results \
  model.pipeline_model_parallel_size=2 \
  model.tensor_model_parallel_size=1 \
  model.restore_from_path=/home/TestData/nlp/megatron_gpt/PP2/gpt_pp2_tp1.nemo \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.peft.peft_scheme=null \
  model.data.train_ds.micro_batch_size=1 \
  model.data.train_ds.global_batch_size=4 \
  model.data.train_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl,/home/TestData/nlp/megatron_sft/trec.jsonl] \
  model.data.train_ds.concat_sampling_probabilities=[0.3,0.7] \
  model.data.train_ds.num_workers=0 \
  model.data.test_ds.micro_batch_size=1 \
  model.data.test_ds.global_batch_size=1 \
  model.data.test_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.test_ds.names=[quarel] \
  model.data.validation_ds.micro_batch_size=1 \
  model.data.validation_ds.global_batch_size=1 \
  model.data.validation_ds.num_workers=0 \
  model.data.validation_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.validation_ds.names=[quarel]

  python examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \
  trainer.devices=2 \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  +trainer.limit_val_batches=2 \
  trainer.max_steps=3 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_sft_results \
  model.pipeline_model_parallel_size=2 \
  model.tensor_model_parallel_size=1 \
  model.restore_from_path=/home/TestData/nlp/megatron_gpt/PP2/gpt_pp2_tp1.nemo \
  model.optim.name=fused_adam \
  model.optim.lr=2e-4 \
  model.peft.peft_scheme=null \
  model.data.train_ds.micro_batch_size=1 \
  model.data.train_ds.global_batch_size=4 \
  model.data.train_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl,/home/TestData/nlp/megatron_sft/trec.jsonl] \
  model.data.train_ds.concat_sampling_probabilities=[0.3,0.7] \
  model.data.train_ds.num_workers=0 \
  model.data.test_ds.micro_batch_size=1 \
  model.data.test_ds.global_batch_size=1 \
  model.data.test_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.test_ds.names=[quarel] \
  model.data.validation_ds.micro_batch_size=1 \
  model.data.validation_ds.global_batch_size=1 \
  model.data.validation_ds.num_workers=0 \
  model.data.validation_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.validation_ds.names=[quarel]

  rm -rf examples/nlp/language_modeling/gpt_sft_results
L2_Megatron_GPT_Finetuning_StarCoder_PP1: |
  python examples/nlp/language_modeling/tuning/megatron_gpt_sft.py \
  trainer.devices=1 \
  trainer.num_nodes=1 \
  trainer.precision=32 \
  trainer.max_steps=4 \
  trainer.val_check_interval=4 \
  trainer.enable_checkpointing=False \
  +trainer.limit_val_batches=2 \
  +trainer.limit_test_batches=2 \
  exp_manager.checkpoint_callback_params.save_best_model=False \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_sft_results \
  model.optim.name=distributed_fused_adam \
  model.restore_from_path=/home/TestData/nlp/megatron_gpt/starcoder-ci-nemo/megatron_starcoder_tp1_pp1.nemo \
  model.tensor_model_parallel_size=1 \
  model.pipeline_model_parallel_size=1 \
  model.data.train_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.train_ds.num_workers=0 \
  model.data.test_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.validation_ds.num_workers=0 \
  model.data.validation_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.test_ds.num_workers=0 \
  model.data.train_ds.concat_sampling_probabilities=[1.0]

  rm -rf examples/nlp/language_modeling/gpt_sft_results
L2_Megatron_GPT_Embedding: |
  rm -rf /home/TestData/nlp/megatron_ir/working_dir

  python examples/nlp/information_retrieval/megatron_gpt_embedding_finetuning.py \
  exp_manager.exp_dir='/home/TestData/nlp/megatron_ir/working_dir' \
  model.global_batch_size=4 \
  model.micro_batch_size=4 \
  trainer.devices=1 \
  trainer.num_nodes=1 \
  trainer.max_epochs=null \
  trainer.max_steps=20 \
  trainer.val_check_interval=10 \
  model.restore_from_path='/home/TestData/nlp/megatron_gpt/mcore_45M/megatron_llama.nemo' \
  model.peft.lora_tuning.adapter_dim=8 \
  model.data.validation_ds.query_file_names=[/home/TestData/nlp/megatron_ir/test_query.jsonl] \
  model.data.validation_ds.doc_file_names=[/home/TestData/nlp/megatron_ir/test_doc.jsonl] \
  model.data.validation_ds.write_embeddings_to_file=True \
  model.data.validation_ds.output_file_path_prefix='/home/TestData/nlp/megatron_ir/working_dir/val_embs' \
  model.data.train_ds.file_names=[/home/TestData/nlp/megatron_ir/train.jsonl]


  python examples/nlp/information_retrieval/megatron_gpt_embedding_generate.py \
  trainer.devices=1 \
  trainer.num_nodes=1 \
  model.restore_from_path='/home/TestData/nlp/megatron_gpt/mcore_45M/megatron_llama.nemo' \
  model.peft.restore_from_path='/home/TestData/nlp/megatron_ir/working_dir/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning.nemo' \
  model.global_batch_size=4 \
  model.micro_batch_size=4 \
  model.peft.lora_tuning.adapter_dim=8 \
  model.data.test_ds.write_embeddings_to_file=True \
  model.data.test_ds.output_file_path_prefix='/home/TestData/nlp/megatron_ir/working_dir/test_embs' \
  model.data.test_ds.query_file_names=[/home/TestData/nlp/megatron_ir/test_query.jsonl] \
  model.data.test_ds.doc_file_names=[/home/TestData/nlp/megatron_ir/test_doc.jsonl]

  rm -rf /home/TestData/nlp/megatron_ir/working_dir
L2_Megatron_GPT_PEFT_Lora_PP2: |
  rm -rf examples/nlp/language_modeling/gpt_peft_lora_results_pp2

  python examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \
  trainer.devices=2 \
  trainer.log_every_n_steps=1 \
  trainer.max_epochs=9999 \
  trainer.max_steps=3 \
  trainer.val_check_interval=3 \
  ++trainer.limit_val_batches=2 \
  trainer.precision=16 \
  exp_manager.exp_dir=examples/nlp/language_modeling/gpt_peft_lora_results_pp2 \
  model.pipeline_model_parallel_size=2 \
  model.tensor_model_parallel_size=1 \
  model.restore_from_path=/home/TestData/nlp/megatron_gpt/PP2/gpt_pp2_tp1.nemo \
  model.peft.peft_scheme=lora \
  model.answer_only_loss=True \
  model.micro_batch_size=1 \
  model.global_batch_size=1 \
  model.data.train_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.train_ds.concat_sampling_probabilities=[1.0] \
  model.data.train_ds.num_workers=0 \
  model.data.validation_ds.num_workers=0 \
  model.data.validation_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.validation_ds.names=[quarel]

  rm -rf examples/nlp/language_modeling/gpt_peft_lora_results_pp2
L2_Megatron_GPT_PEFT_Lora_TP2: |
  rm -rf /home/TestData/nlp/lora_tuning_tp2

  python examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \
  trainer.devices=2 \
  trainer.log_every_n_steps=1 \
  trainer.max_epochs=9999 \
  trainer.max_steps=3 \
  trainer.val_check_interval=3 \
  ++trainer.limit_val_batches=2 \
  trainer.precision=16 \
  exp_manager.exp_dir=/home/TestData/nlp/lora_tuning_tp2 \
  model.pipeline_model_parallel_size=1 \
  model.tensor_model_parallel_size=2 \
  model.restore_from_path=/home/TestData/nlp/megatron_gpt/TP2/megatron_gpt_tp2.nemo \
  model.peft.peft_scheme='lora' \
  model.answer_only_loss=True \
  model.micro_batch_size=1 \
  model.global_batch_size=1 \
  model.data.train_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.train_ds.concat_sampling_probabilities=[1.0] \
  model.data.train_ds.num_workers=0 \
  model.data.validation_ds.num_workers=0 \
  model.data.validation_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.validation_ds.names=[quarel]

  python examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \
  model.restore_from_path=/home/TestData/nlp/megatron_gpt/TP2/megatron_gpt_tp2.nemo \
  model.peft.restore_from_path=/home/TestData/nlp/lora_tuning_tp2/megatron_gpt_peft_lora_tuning/checkpoints/megatron_gpt_peft_lora_tuning.nemo \
  model.tensor_model_parallel_size=2 \
  trainer.devices=2 \
  model.data.test_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel_4.jsonl] \
  model.data.test_ds.names=['quarel4'] \
  model.global_batch_size=2 \
  model.micro_batch_size=1 \
  model.data.test_ds.tokens_to_generate=10 \
  model.data.test_ds.write_predictions_to_file=True \
  model.data.test_ds.output_file_path_prefix='/home/TestData/nlp/lora_tuning_tp2/out' \
  inference.greedy=True \
  inference.repetition_penalty=1.0 \
  inference.outfile_path='/home/TestData/nlp/lora_tuning_tp2/out.jsonl'

  rm -rf /home/TestData/nlp/lora_tuning_tp2
L2_Megatron_GPT_Eval: |
  python examples/nlp/language_modeling/megatron_gpt_eval.py \
      gpt_model_file=/home/TestData/nlp/megatron_gpt/125M/megatron_gpt.nemo \
      prompts=['How to fix GPU memory? A:'] \
      tensor_model_parallel_size=1 \
      inference.tokens_to_generate=32 \
      trainer.precision=32
L2_Megatron_GPT_Eval_PP2: |
  python examples/nlp/language_modeling/megatron_gpt_eval.py \
      gpt_model_file=/home/TestData/nlp/megatron_gpt/PP2/gpt_pp2_tp1.nemo \
      server=False \
      tensor_model_parallel_size=1 \
      pipeline_model_parallel_size=2 \
      trainer.devices=2 \
      trainer.num_nodes=1 \
      trainer.precision=32
L2_Megatron_GPT_SFT_Eval_inference_seq_len_greaterThan_training_seq_len: |
  python examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \
      model.restore_from_path=/home/TestData/nlp/megatron_gpt_sft/megatron_gpt_rope_sft.nemo \
      model.peft.restore_from_path=null \
      model.data.test_ds.file_names=[/home/TestData/nlp/megatron_gpt_sft/sample.jsonl] \
      model.data.test_ds.names=[test] \
      model.data.test_ds.global_batch_size=1 \
      model.data.test_ds.micro_batch_size=1 \
      model.data.test_ds.tokens_to_generate=30 \
      model.data.test_ds.max_seq_length=6000 \
      model.data.test_ds.write_predictions_to_file=True \
      model.data.test_ds.output_file_path_prefix=examples/nlp/language_modeling/out \
      inference.greedy=True \
      inference.repetition_penalty=1.0 \
      inference.outfile_path=examples/nlp/language_modeling/out.jsonl && \
      rm -rf examples/nlp/language_modeling/out.jsonl
L2_Megatron_Change_Partitions_Reduce_TP_Num_Partitions_-2_to_1-_and_PP_Num_Partitions_-1_to_2: |
  python examples/nlp/language_modeling/megatron_change_num_partitions.py \
      --model_file /home/TestData/nlp/megatron_gpt/TP2/megatron_gpt_tp2.nemo \
      --target_file /home/TestData/nlp/megatron_gpt/TP2-Temp/test-reduce.nemo \
      --tensor_model_parallel_size 2 \
      --target_tensor_model_parallel_size 1 \
      --pipeline_model_parallel_size 1 \
      --target_pipeline_model_parallel_size 2

   rm /home/TestData/nlp/megatron_gpt/TP2-Temp/test-reduce.nemo
L2_Megatron_Change_Partitions_Increase_TP_Num_Partitions_-2_to_4-_and_PP_Num_Partitions_-1_to_2: |
  python examples/nlp/language_modeling/megatron_change_num_partitions.py \
      --model_file /home/TestData/nlp/megatron_gpt/TP2/megatron_gpt_tp2.nemo \
      --target_file /home/TestData/nlp/megatron_gpt/TP2-Temp/test-increase.nemo \
      --tensor_model_parallel_size 2 \
      --target_tensor_model_parallel_size 4 \
      --pipeline_model_parallel_size 1 \
      --target_pipeline_model_parallel_size 1

  rm /home/TestData/nlp/megatron_gpt/TP2-Temp/test-increase.nemo
L2_Megatron_T5_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.position_embedding_type=relative \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=fast-swiglu \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=pre_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src,.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings \
  model.data.data_impl=text_mmap \
  +model.data.data_impl_kwargs.newline_int=10 \
  +model.data.data_impl_kwargs.header_lines=0 \
  +model.data.data_impl_kwargs.workers=null \
  +model.data.data_impl_kwargs.sort_dataset_paths=False \
  model.share_token_embeddings=False \
  model.share_decoder_tokens_head_embeddings=False

  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.position_embedding_type=relative \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=fast-swiglu \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=pre_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src,.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings \
  model.data.data_impl=text_mmap \
  +model.data.data_impl_kwargs.newline_int=10 \
  +model.data.data_impl_kwargs.header_lines=0 \
  +model.data.data_impl_kwargs.workers=null \
  +model.data.data_impl_kwargs.sort_dataset_paths=False \
  model.share_token_embeddings=False \
  model.share_decoder_tokens_head_embeddings=False

  rm -rf examples/nlp/language_modeling/t5_pretrain_results
  rm -rf examples/nlp/language_modeling/t5_index_mappings
L2_Megatron_T5_with_ALiBi_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.position_embedding_type=alibi \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=swiglu \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=pre_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src,.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings \
  model.data.data_impl=text_mmap \
  +model.data.data_impl_kwargs.newline_int=10 \
  +model.data.data_impl_kwargs.header_lines=0 \
  +model.data.data_impl_kwargs.workers=null \
  +model.data.data_impl_kwargs.sort_dataset_paths=False \
  model.share_token_embeddings=False \
  model.share_decoder_tokens_head_embeddings=False

  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.position_embedding_type=alibi \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=swiglu \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=pre_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src,.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings \
  model.data.data_impl=text_mmap \
  +model.data.data_impl_kwargs.newline_int=10 \
  +model.data.data_impl_kwargs.header_lines=0 \
  +model.data.data_impl_kwargs.workers=null \
  +model.data.data_impl_kwargs.sort_dataset_paths=False \
  model.share_token_embeddings=False \
  model.share_decoder_tokens_head_embeddings=False

  rm -rf examples/nlp/language_modeling/t5_pretrain_results
  rm -rf examples/nlp/language_modeling/t5_index_mappings
L2_Megatron_T5_with_KERPLE_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.position_embedding_type=kerple \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=swiglu \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=pre_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src,.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings \
  model.data.data_impl=text_mmap \
  +model.data.data_impl_kwargs.newline_int=10 \
  +model.data.data_impl_kwargs.header_lines=0 \
  +model.data.data_impl_kwargs.workers=null \
  +model.data.data_impl_kwargs.sort_dataset_paths=False \
  model.share_token_embeddings=False \
  model.share_decoder_tokens_head_embeddings=False

  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.masked_softmax_fusion=False \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.position_embedding_type=kerple \
  model.decoder.num_layers=2 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=swiglu \
  model.decoder.masked_softmax_fusion=False \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=pre_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src,.5,/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings \
  model.data.data_impl=text_mmap \
  +model.data.data_impl_kwargs.newline_int=10 \
  +model.data.data_impl_kwargs.header_lines=0 \
  +model.data.data_impl_kwargs.workers=null \
  +model.data.data_impl_kwargs.sort_dataset_paths=False \
  model.share_token_embeddings=False \
  model.share_decoder_tokens_head_embeddings=False

  rm -rf examples/nlp/language_modeling/t5_pretrain_results
  rm -rf examples/nlp/language_modeling/t5_index_mappings
L2_Megatron_T5_Pretraining_and_Resume_Training_PP2: |
  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  model.pipeline_model_parallel_size=2 \
  model.pipeline_model_parallel_split_rank=1 \
  model.seq_length=256 \
  model.encoder.num_layers=4 \
  model.decoder.num_layers=1 \
  model.encoder.hidden_size=64 \
  model.decoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.decoder.num_attention_heads=8 \
  model.decoder.ffn_hidden_size=2048 \
  model.encoder.activation=gelu \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=post_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document,.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings

  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.pipeline_model_parallel_size=2 \
  model.pipeline_model_parallel_split_rank=1 \
  model.seq_length=256 \
  model.encoder.num_layers=4 \
  model.decoder.num_layers=1 \
  model.encoder.hidden_size=64 \
  model.decoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.decoder.num_attention_heads=8 \
  model.decoder.ffn_hidden_size=2048 \
  model.encoder.activation=gelu \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=post_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document,.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings

  rm -rf examples/nlp/language_modeling/t5_pretrain_results
  rm -rf examples/nlp/language_modeling/t5_index_mappings
L2_Megatron_T5_w_Mixture_of_Expert_Pretraining: |
  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  model.pipeline_model_parallel_split_rank=1 \
  model.seq_length=256 \
  model.encoder.num_layers=4 \
  model.decoder.num_layers=1 \
  model.encoder.num_moe_experts=4 \
  model.decoder.num_moe_experts=4 \
  model.encoder.moe_frequency=3 \
  model.decoder.moe_frequency=1 \
  model.encoder.hidden_size=64 \
  model.decoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.decoder.num_attention_heads=8 \
  model.decoder.ffn_hidden_size=2048 \
  model.encoder.activation=gelu \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=pre_ln \
  model.decoder.transformer_block_type=post_ln \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document,.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings

  rm -rf examples/nlp/language_modeling/t5_pretrain_results
  rm -rf examples/nlp/language_modeling/t5_index_mappings
L2_Megatron_UL2_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_t5_pretraining.py -cn megatron_ul2_config \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=normformer \
  model.encoder.headscale=True \
  model.decoder.num_layers=4 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=geglu \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.decoder.transformer_block_type=normformer \
  model.decoder.headscale=False \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document,.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings

  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=swiglu \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.encoder.transformer_block_type=normformer \
  model.encoder.headscale=True \
  model.decoder.num_layers=4 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=geglu \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.decoder.transformer_block_type=normformer \
  model.decoder.headscale=False \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document,.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document] \
  model.data.index_mapping_dir=examples/nlp/language_modeling/t5_index_mappings

  rm -rf examples/nlp/language_modeling/t5_pretrain_results
  rm -rf examples/nlp/language_modeling/t5_index_mappings
L2_Megatron_T5_Eval: |
  python examples/nlp/language_modeling/megatron_t5_eval.py \
      --model_file /home/TestData/nlp/megatron_t5/8m/megatron_t5_8m-refactor.nemo \
      --prompt 'How do I fix my GPU memory issue? I am seeing <mask> out of memory.' \
      --tensor_model_parallel_size 1
L2_Megatron_BART_Pretraining_and_Resume_Training_TP2: |
  python examples/nlp/language_modeling/megatron_bart_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=3 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bart_pretrain_results \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation='reglu' \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method='block' \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=4 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation='reglu' \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method='block' \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.data.data_prefix='{train:[1.0,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document],test:[/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document], validation:[/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document]}'

  python examples/nlp/language_modeling/megatron_bart_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=2 \
  trainer.limit_val_batches=5 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=6 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bart_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.tensor_model_parallel_size=2 \
  model.seq_length=128 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation='reglu' \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method='block' \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=4 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation='reglu' \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method='block' \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.data.data_prefix='{train:[1.0,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document],test:[/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document], validation:[/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document]}'

  rm -rf examples/nlp/language_modeling/bart_pretrain_results
L2_Megatron_BART_Pretraining_and_Resume_Training_PP2: |
  python examples/nlp/language_modeling/megatron_bart_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=10 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bart_pretrain_results \
  model.pipeline_model_parallel_size=2 \
  model.pipeline_model_parallel_split_rank=1 \
  model.seq_length=256 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=geglu \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=4 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=geglu \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.data.respect_document_boundaries=False \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document,.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document]

  python examples/nlp/language_modeling/megatron_bart_pretraining.py \
  trainer.devices=2 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  trainer.limit_val_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=10 \
  trainer.precision=16 \
  trainer.gradient_clip_val=1.0 \
  exp_manager.exp_dir=examples/nlp/language_modeling/bart_pretrain_results \
  exp_manager.resume_if_exists=True \
  model.pipeline_model_parallel_size=2 \
  model.pipeline_model_parallel_split_rank=1 \
  model.seq_length=256 \
  model.encoder.num_layers=4 \
  model.encoder.hidden_size=64 \
  model.encoder.num_attention_heads=8 \
  model.encoder.activation=geglu \
  model.encoder.bias_activation_fusion=False \
  model.encoder.activations_checkpoint_method=block \
  model.encoder.activations_checkpoint_num_layers=1 \
  model.decoder.num_layers=4 \
  model.decoder.hidden_size=64 \
  model.decoder.num_attention_heads=8 \
  model.decoder.activation=geglu \
  model.decoder.bias_activation_fusion=False \
  model.decoder.activations_checkpoint_method=block \
  model.decoder.activations_checkpoint_num_layers=1 \
  model.data.respect_document_boundaries=False \
  model.data.data_prefix=[.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document,.5,/home/TestData/nlp/megatron_t5/data/pile_val_small_bert_tokenizer_text_document]

  rm -rf examples/nlp/language_modeling/bart_pretrain_results
L2_Megatron_T5_GLUE_RTE: |
  python examples/nlp/language_modeling/megatron_t5_seq2seq_finetune.py \
  trainer.devices=1 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  +trainer.limit_val_batches=2 \
  +trainer.limit_test_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=2 \
  trainer.precision=16 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_glue_results \
  model.restore_from_path=/home/TestData/nlp/megatron_t5/8m/megatron_t5_8m-refactor.nemo \
  model.pipeline_model_parallel_size=1 \
  model.pipeline_model_parallel_split_rank=0 \
  model.data.train_ds.task_name=rte \
  model.data.train_ds.global_batch_size=4 \
  model.data.train_ds.micro_batch_size=2 \
  model.data.validation_ds.global_batch_size=2 \
  model.data.validation_ds.micro_batch_size=2 \
  model.data.train_ds.file_path=/home/TestData/nlp/megatron_t5/data/train_ci.tsv \
  model.data.validation_ds.task_name=rte \
  model.data.validation_ds.file_path=/home/TestData/nlp/megatron_t5/data/dev_ci.tsv

  rm -rf examples/nlp/language_modeling/t5_glue_results
L2_Megatron_T5_GLUE_XNLI: |
  python examples/nlp/language_modeling/megatron_t5_seq2seq_finetune.py \
  -cn megatron_t5_config_finetune_glue_xnli \
  trainer.devices=1 \
  trainer.accelerator=gpu \
  trainer.log_every_n_steps=1 \
  trainer.val_check_interval=1 \
  +trainer.limit_val_batches=2 \
  +trainer.limit_test_batches=2 \
  trainer.accumulate_grad_batches=1 \
  trainer.max_steps=2 \
  trainer.precision=16 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_xnli_results \
  model.restore_from_path=/home/TestData/nlp/megatron_t5/8m/megatron_t5_8m-refactor.nemo \
  model.pipeline_model_parallel_size=1 \
  model.pipeline_model_parallel_split_rank=0 \
  model.data.train_ds.global_batch_size=4 \
  model.data.train_ds.micro_batch_size=2 \
  model.data.validation_ds.global_batch_size=2 \
  model.data.validation_ds.micro_batch_size=2 \
  model.data.test_ds.global_batch_size=2 \
  model.data.test_ds.micro_batch_size=2 \
  model.data.train_ds.task_name=rte \
  model.data.train_ds.file_path=/home/TestData/nlp/megatron_t5/data/train_ci.tsv \
  model.data.validation_ds.task_name=xnli \
  model.data.validation_ds.file_path=/home/TestData/nlp/megatron_t5/data/xnli_dev_ci.tsv \
  model.data.test_ds.task_name=xnli \
  model.data.test_ds.file_path=/home/TestData/nlp/megatron_t5/data/xnli_dev_ci.tsv

  rm -rf examples/nlp/language_modeling/t5_xnli_results
L2_Megatron_T5_PEFT_Lora_TP2: |
  rm -rf /home/TestData/nlp/t5_lora_tuning_tp2

  python examples/nlp/language_modeling/tuning/megatron_t5_finetuning.py \
  trainer.devices=2 \
  trainer.log_every_n_steps=1 \
  trainer.max_epochs=9999 \
  trainer.max_steps=3 \
  trainer.val_check_interval=3 \
  ++trainer.limit_val_batches=2 \
  trainer.precision=16 \
  exp_manager.exp_dir=/home/TestData/nlp/t5_lora_tuning_tp2 \
  model.pipeline_model_parallel_size=1 \
  model.tensor_model_parallel_size=2 \
  model.restore_from_path=/home/TestData/nlp/megatron_t5/8m/megatron_t5_8m_tp2.nemo \
  model.peft.peft_scheme=lora \
  model.answer_only_loss=True \
  model.micro_batch_size=1 \
  model.global_batch_size=1 \
  model.data.train_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.train_ds.concat_sampling_probabilities=[1.0] \
  model.data.train_ds.num_workers=0 \
  model.data.validation_ds.num_workers=0 \
  model.data.validation_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel.jsonl] \
  model.data.validation_ds.names=[quarel]

  python examples/nlp/language_modeling/tuning/megatron_t5_generate.py \
  model.restore_from_path=/home/TestData/nlp/megatron_t5/8m/megatron_t5_8m_tp2.nemo \
  model.peft.restore_from_path=/home/TestData/nlp/t5_lora_tuning_tp2/megatron_t5_peft_lora_tuning/checkpoints/megatron_t5_peft_lora_tuning.nemo \
  model.peft.restore_from_ckpt_name=null \
  model.peft.restore_from_hparams_path=null \
  model.tensor_model_parallel_size=2 \
  trainer.devices=2 \
  model.data.test_ds.file_names=[/home/TestData/nlp/megatron_sft/quarel_4.jsonl] \
  model.data.test_ds.names=[quarel4] \
  model.global_batch_size=2 \
  model.micro_batch_size=1 \
  model.data.test_ds.tokens_to_generate=10 \
  model.data.test_ds.write_predictions_to_file=True \
  model.data.test_ds.output_file_path_prefix=/home/TestData/nlp/t5_lora_tuning_tp2/out \
  inference.greedy=True \
  inference.repetition_penalty=1.0 \
  inference.outfile_path=/home/TestData/nlp/t5_lora_tuning_tp2/out.jsonl

  rm -rf /home/TestData/nlp/t5_lora_tuning_tp2
L2_Megatron_Mock_Data_Generation_MockGPTDataset: |
  python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
      trainer.max_steps=10 \
      trainer.limit_val_batches=7 \
      trainer.val_check_interval=10 \
      exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
      model.mcore_gpt=True \
      model.data.data_impl=mock \
      model.data.data_prefix=[]
L2_Megatron_Mock_Data_Generation_MockT5Dataset: |
  python examples/nlp/language_modeling/megatron_t5_pretraining.py \
  trainer.max_steps=10 \
  trainer.limit_val_batches=3 \
  trainer.val_check_interval=10 \
  exp_manager.exp_dir=examples/nlp/language_modeling/t5_pretrain_results \
  model.data.data_impl=mock \
  model.data.data_prefix=[]

  rm -rf examples/nlp/language_modeling/t5_pretrain_results
L2_TTS_Fast_dev_runs_1_Tacotron_2: |
  python examples/tts/tacotron2.py \
  train_dataset=/home/TestData/an4_dataset/an4_train.json \
  validation_datasets=/home/TestData/an4_dataset/an4_val.json \
  trainer.devices=1 \
  trainer.accelerator="gpu" \
  +trainer.limit_train_batches=1 +trainer.limit_val_batches=1 trainer.max_epochs=1 \
  trainer.strategy=auto \
  model.decoder.decoder_rnn_dim=256 \
  model.decoder.attention_rnn_dim=1024 \
  model.decoder.prenet_dim=128 \
  model.postnet.postnet_n_convolutions=3 \
  model.train_ds.dataloader_params.batch_size=4 \
  model.train_ds.dataloader_params.num_workers=0 \
  model.validation_ds.dataloader_params.batch_size=4 \
  model.validation_ds.dataloader_params.num_workers=0 \
  ~model.text_normalizer \
  ~model.text_normalizer_call_kwargs \
  ~trainer.check_val_every_n_epoch
L2_TTS_Fast_dev_runs_1_WaveGlow: |
  python examples/tts/waveglow.py \
  train_dataset=/home/TestData/an4_dataset/an4_train.json \
  validation_datasets=/home/TestData/an4_dataset/an4_val.json \
  trainer.devices="[0]" \
  +trainer.limit_train_batches=1 +trainer.limit_val_batches=1 trainer.max_epochs=1 \
  trainer.strategy=auto \
  model.train_ds.dataloader_params.batch_size=4 \
  model.train_ds.dataloader_params.num_workers=0 \
  model.validation_ds.dataloader_params.batch_size=4 \
  model.validation_ds.dataloader_params.num_workers=0 \
  model.waveglow.n_flows=4 \
  model.waveglow.n_wn_layers=2 \
  model.waveglow.n_wn_channels=32 \
  ~trainer.check_val_every_n_epoch
L2_TTS_Fast_dev_runs_1_FastPitch: |
  python examples/tts/fastpitch.py \
  --config-name fastpitch_align_v1.05 \
  train_dataset=/home/TestData/an4_dataset/an4_train.json \
  validation_datasets=/home/TestData/an4_dataset/an4_val.json \
  sup_data_path=/home/TestData/an4_dataset/beta_priors \
  trainer.devices="[0]" \
  +trainer.limit_train_batches=1 \
  +trainer.limit_val_batches=1 \
  trainer.max_epochs=1 \
  trainer.strategy=auto \
  model.pitch_mean=212.35873413085938 \
  model.pitch_std=68.52806091308594 \
  model.train_ds.dataloader_params.batch_size=4 \
  model.train_ds.dataloader_params.num_workers=0 \
  model.validation_ds.dataloader_params.batch_size=4 \
  model.validation_ds.dataloader_params.num_workers=0 \
  model.symbols_embedding_dim=64 \
  model.input_fft.d_inner=384 \
  model.input_fft.n_layer=2 \
  model.output_fft.d_inner=384 \
  model.output_fft.n_layer=2 \
  ~trainer.check_val_every_n_epoch \
  ~model.text_normalizer \
  ~model.text_normalizer_call_kwargs
L2_TTS_Fast_dev_runs_1_RADTTS: |
  python examples/tts/radtts.py \
  train_dataset=/home/TestData/an4_dataset/an4_train.json \
  validation_datasets=/home/TestData/an4_dataset/an4_val.json \
  sup_data_path=/home/TestData/an4_dataset/radtts_beta_priors \
  trainer.devices="[0]" \
  +trainer.limit_train_batches=1 \
  +trainer.limit_val_batches=1 \
  trainer.max_epochs=1 \
  trainer.strategy=auto \
  model.pitch_mean=212.35873413085938 \
  model.pitch_std=68.52806091308594 \
  model.train_ds.dataloader_params.batch_size=4 \
  model.train_ds.dataloader_params.num_workers=0 \
  model.validation_ds.dataloader_params.batch_size=4 \
  model.validation_ds.dataloader_params.num_workers=0 \
  export_dir=/home/TestData/radtts_test \
  model.optim.lr=0.0001 \
  model.modelConfig.decoder_use_partial_padding=True \
  ~trainer.check_val_every_n_epoch \
  ~model.text_normalizer \
  ~model.text_normalizer_call_kwargs
L2_TTS_Fast_dev_runs_1_Mixer-TTS: |
  python examples/tts/mixer_tts.py \
  train_dataset=/home/TestData/an4_dataset/an4_train.json \
  validation_datasets=/home/TestData/an4_dataset/an4_val.json \
  sup_data_path=/home/TestData/an4_dataset/sup_data \
  trainer.devices="[0]" \
  +trainer.limit_train_batches=1 \
  +trainer.limit_val_batches=1 \
  trainer.max_epochs=1 \
  trainer.strategy=auto \
  model.pitch_mean=212.35873413085938 \
  model.pitch_std=68.52806091308594 \
  model.train_ds.dataloader_params.batch_size=4 \
  model.train_ds.dataloader_params.num_workers=0 \
  model.validation_ds.dataloader_params.batch_size=4 \
  model.validation_ds.dataloader_params.num_workers=0 \
  ~trainer.check_val_every_n_epoch \
  ~model.text_normalizer \
  ~model.text_normalizer_call_kwargs
L2_TTS_Fast_dev_runs_1_Hifigan: |
  python examples/tts/hifigan.py \
  train_dataset=/home/TestData/an4_dataset/an4_train.json \
  validation_datasets=/home/TestData/an4_dataset/an4_val.json \
  trainer.devices="[0]" \
  +trainer.limit_train_batches=1 \
  +trainer.limit_val_batches=1 \
  +trainer.max_epochs=1 \
  trainer.strategy=auto \
  model.train_ds.dataloader_params.batch_size=4 \
  model.train_ds.dataloader_params.num_workers=0 \
  model.validation_ds.dataloader_params.batch_size=4 \
  model.validation_ds.dataloader_params.num_workers=0 \
  model.generator.upsample_initial_channel=64 \
  +model.debug=true \
  ~trainer.check_val_every_n_epoch
