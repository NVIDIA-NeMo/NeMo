# Copyright (c) 2020-2021, NVIDIA CORPORATION.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
name: "CICD NeMo"

on:
  pull_request:
    branches: [ "main" ]
    types: [ labeled ]

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  gpu-test:
    runs-on: self-hosted-azure
    if: ${{ github.event.label.name == 'Run CICD' }}
    steps:
    - name: Run nvidia-smi test
      run: |
        whoami
        nvidia-smi

  cicd-cluster-clean:
    runs-on: self-hosted-azure-builder
    if: ${{ github.event.label.name == 'Run CICD' }}
    steps:
    - name: Clean server from old files
      run: |
        docker container prune --filter "until=24h" --force
        docker image prune -a --filter "until=24h" --force

#  checkout-repository:
#    runs-on: self-hosted-azure
#    container:
#      image: nvcr.io/nvidia/pytorch:24.02-py3
#      volumes:
#        - ${{ github.workspace }}:/workspace
#    steps:
#    - name: Checkout repository
#      uses: actions/checkout@v4
#      with:
#        path: ${{ github.run_id }}

  cicd-test-container-setup:
    needs: [cicd-cluster-clean]
    runs-on: self-hosted-azure-builder
    if: ${{ github.event.label.name == 'Run CICD' }}
    # uses: actions/cache@v2
    #container:
#      image: nvcr.io/nvidia/pytorch:24.02-py3
#      options: 
#        # --user 0:128
#        --device=/dev/nvidia0
#        --gpus all
#        --shm-size=8g 
#        --env TRANSFORMERS_OFFLINE=0
#        --env HYDRA_FULL_ERROR=1
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        path: ${{ github.run_id }}

    - name: Container setup
      run: |
        # Pull base PyTorch container
        docker pull nvcr.io/nvidia/pytorch:24.02-py3
        docker run --device=/dev/nvidia0 --gpus all --shm-size=8g --env TRANSFORMERS_OFFLINE=0 --env HYDRA_FULL_ERROR=1 --env PYTHONUNBUFFERED=1 --volume ${{ github.workspace }}/${{ github.run_id }}:/workspace --volume /mnt/datadrive/TestData:/home/TestData nvcr.io/nvidia/pytorch:24.02-py3 /bin/bash -c '
            set -x

            # PyTorch version
            python -c "import torch; print(torch.__version__)"
            python -c "import torchvision; print(torchvision.__version__)"

            # Install test requirements
            apt-get update && apt-get install -y bc && pip install -r requirements/requirements_test.txt && pip install -r requirements/requirements_lightning.txt

            # Code formatting checks
            python setup.py style

            # Copyright Headers check
            python tests/check_copyright_header.py --dir .

            # NeMo Installation
            ./reinstall.sh release

            # Transformer Engine installation
            git clone https://github.com/NVIDIA/TransformerEngine.git && \
                pushd TransformerEngine && \
                git fetch origin bfe21c3d68b0a9951e5716fb520045db53419c5e && \
                git checkout FETCH_HEAD && \
                git submodule init && git submodule update && \
                NVTE_FRAMEWORK=pytorch NVTE_WITH_USERBUFFERS=1 MPI_HOME=/usr/local/mpi pip install .  && \
                popd

            # Apex installation
            git clone https://github.com/NVIDIA/apex.git && \
                pushd apex && \
                git checkout 810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c && \
                cp -R apex /usr/local/lib/python3.10/dist-packages && \
                popd

            # pip package should be working with main, if not we can update the commit here
            # until the pip package is updated
            # Megatron Core installation
            git clone https://github.com/NVIDIA/Megatron-LM.git && \
                pushd Megatron-LM && \
                git checkout 709472117364eed93b6a767e2ac343e229d3aa89 && \
                pip install . && \
                  pushd megatron/core/datasets && \
                  make && \
                  popd && \
                popd
            export PYTHONPATH="${PYTHONPATH}:/workspace/Megatron-LM"

            # Install only for test: L2: Segmentation Tool
            pushd tools/ctc_segmentation && \
                pip install -r requirements.txt && \
                apt-get update && apt-get install libsox-fmt-all -y && \
                popd

            # AMMO installation
            pip install nvidia-ammo~=0.9.0 --extra-index-url https://pypi.nvidia.com --no-cache-dir

            # PyTorch Lightning version
            python -c "import pytorch_lightning; print(pytorch_lightning.__version__)"

            # PyTorch Lightning DDP Checks
            CUDA_VISIBLE_DEVICES="0,1" python "tests/core_ptl/check_for_ranks.py"

            # Basic Import Checks
            python -c "import nemo.collections.asr as nemo_asr"
            python -c "import nemo.collections.nlp as nemo_nlp"
            python -c "import nemo.collections.tts as nemo_tts"

            # set permission
            chmod 777 -R /workspace
            '
            ### \'\'

    - name: Push container to registry for future use
      run: |
        # Push container
        echo "Docker: List containers" && docker ps -a
        DOCKER_COMMIT=$(docker ps --latest --quiet)  # latest container
        docker commit $DOCKER_COMMIT nemoci.azurecr.io/nemo_container_${{ github.run_id }}
        docker tag nemoci.azurecr.io/nemo_container_${{ github.run_id }} nemoci.azurecr.io/nemo_container_${{ github.run_id }}
        docker push nemoci.azurecr.io/nemo_container_${{ github.run_id }}

    # - name: Build and push to local registry
    #   uses: docker/build-push-action@v5
    #   with:
    #       context: .
    #       push: true
    #       tags: nemoci.azurecr.io/name/app:latest

    # - name: Inspect
    #   run: |
    #     docker buildx imagetools inspect nemoci.azurecr.io/name/app:latest

    #- name: Post-workflow execution
    #  uses: gacts/run-and-post-run@v1
    #  with:
    #    post: |
    #      chmod -R 777 .

##     - name: L2: Multimodal Imagen Train

  # L2: Community LLM Checkpoints tests

  # this test is using a 7B model which is too large for GitHub CI
  # replace the model in this test with a toy model or move the test
  # to the nightly CI
  # L2_Community_LLM_Checkpoints_tests_Baichuan2:
  #   needs: [cicd-test-container-setup]
  #   runs-on: self-hosted-azure
  #   container:
  #     image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
  #     options: 
  #       # --user 0:128
  #       --device=/dev/nvidia0
  #       --gpus all
  #       --shm-size=8g
  #       --env TRANSFORMERS_OFFLINE=0 
  #       --env HYDRA_FULL_ERROR=1
  #       --volume /mnt/datadrive/TestData:/home/TestData
  #   steps:
  #       - name: Checkout repository
  #         uses: actions/checkout@v4
  #       - run: |
  #           python scripts/checkpoint_converters/convert_baichuan2_hf_to_nemo.py \
  #           --input_name_or_path=/home/TestData/nlp/megatron_gpt/Baichuan2-7B-Base \
  #           --output_path=/home/TestData/nlp/megatron_gpt/Baichuan2-7B-Base/ci.nemo
  #           rm -f /home/TestData/nlp/megatron_gpt/Baichuan2-7B-Base/ci.nemo
  #       - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
  #         if: "failure()"

  #L2_PTQ_Llama2_INT4_AWQ:
  #  needs: [cicd-test-container-setup]
  #  runs-on: self-hosted-azure
  #  timeout-minutes: 15
  #  container:
  #    image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
  #    options:
  #      # --user 0:128
  #      --device=/dev/nvidia0
  #      --gpus all
  #      --shm-size=8g
  #      --env TRANSFORMERS_OFFLINE=0
  #      --env HYDRA_FULL_ERROR=1
  #      --volume /mnt/datadrive/TestData:/home/TestData
  #  steps:
  #      - name: Checkout repository
  #        uses: actions/checkout@v4
  #      - run: |
  #          python examples/nlp/language_modeling/megatron_llama_quantization.py \
  #          model_file=/home/TestData/nlp/megatron_llama/llama_ci.nemo \
  #          tensor_model_parallel_size=1 \
  #          trainer.devices=1 \
  #          quantization.calib_dataset=/home/TestData/nlp/test_quantization/test.json \
  #          quantization.algorithm=int4_awq \
  #          quantization.num_calib_size=8 \
  #          inference.batch_size=2 \
  #          model_save=/home/TestData/nlp/megatron_llama/ci_int4_awq.qnemo
  #
  #          rm -rf /home/TestData/nlp/megatron_llama/ci_int4_awq.qnemo
        #- uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
        #  if: "failure()"

  # L2: ASR dev run
  
  # L2_Speech_to_Text_AED:
  #   needs: [cicd-test-container-setup]
  #   runs-on: self-hosted-azure-gpus-1
  #   container:
  #     image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
  #     options: 
  #       # --user 0:128
  #       --device=/dev/nvidia0
  #       --gpus all
  #       --shm-size=8g 
  #       --env TRANSFORMERS_OFFLINE=0 
  #       --env HYDRA_FULL_ERROR=1
  #       --volume /mnt/datadrive/TestData:/home/TestData
  #   steps:
  #       - name: Checkout repository
  #         uses: actions/checkout@v4
  #       - run: |
  #           python examples/asr/speech_multitask/speech_to_text_aed.py \
  #           model.prompt_format=canary \
  #           model.model_defaults.asr_enc_hidden=256 \
  #           model.model_defaults.lm_dec_hidden=256 \
  #           model.encoder.n_layers=12 \
  #           model.transf_encoder.num_layers=0 \
  #           model.transf_decoder.config_dict.num_layers=12 \
  #           model.train_ds.manifest_filepath=/home/TestData/asr/manifests/canary/an4_canary_train.json \
  #           ++model.train_ds.is_tarred=false \
  #           model.train_ds.batch_duration=60 \
  #           +model.train_ds.text_field="answer" \
  #           +model.train_ds.lang_field="target_lang" \
  #           model.validation_ds.manifest_filepath=/home/TestData/asr/manifests/canary/an4_canary_val.json \
  #           +model.validation_ds.text_field="answer" \
  #           +model.validation_ds.lang_field="target_lang" \
  #           model.test_ds.manifest_filepath=/home/TestData/asr/manifests/canary/an4_canary_val.json \
  #           +model.test_ds.text_field="answer" \
  #           +model.test_ds.lang_field="target_lang" \
  #           model.tokenizer.langs.spl_tokens.dir=/home/TestData/asr_tokenizers/canary/canary_spl_tokenizer_v32 \
  #           model.tokenizer.langs.spl_tokens.type="bpe" \
  #           model.tokenizer.langs.en.dir=/home/TestData/asr_tokenizers/canary/en/tokenizer_spe_bpe_v1024_max_4 \
  #           model.tokenizer.langs.en.type=bpe \
  #           ++model.tokenizer.langs.es.dir=/home/TestData/asr_tokenizers/canary/es/tokenizer_spe_bpe_v1024_max_4 \
  #           ++model.tokenizer.langs.es.type=bpe \
  #           trainer.devices=1 \
  #           trainer.accelerator="gpu" \
  #           +trainer.use_distributed_sampler=false \
  #           +trainer.fast_dev_run=True \
  #           exp_manager.exp_dir=examples/asr/speech_to_text_aed_results
  #           rm -rf examples/asr/speech_to_text_results

    # TODO: pleasefixme @redoctopus
    # - name: ByT5G2P training, evaluation and inference
    #   run: |
    #     cd examples/tts/g2p && \
    #         TIME=`date +"%Y-%m-%d-%T"` && OUTPUT_DIR_T5=output_byt5_${TIME} && \
    #         python g2p_train_and_evaluate.py \
    #             train_manifest=/home/TestData/g2p/g2p.json \
    #             validation_manifest=/home/TestData/g2p/g2p.json \
    #             model.test_ds.manifest_filepath=/home/TestData/g2p/g2p.json \
    #             trainer.max_epochs=1 \
    #             model.max_source_len=64 \
    #             trainer.devices=1 \
    #             do_training=True \
    #             do_testing=True \
    #             exp_manager.exp_dir=${OUTPUT_DIR_T5} \
    #             +exp_manager.use_datetime_version=False\
    #             +exp_manager.version=test && \
    #         python g2p_inference.py \
    #             pretrained_model=${OUTPUT_DIR_T5}/T5G2P/test/checkpoints/T5G2P.nemo \
    #             manifest_filepath=/home/TestData/g2p/g2p.json \
    #             phoneme_field=text
    #   }
    # }
    # - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
    # if: "failure()"

  # L2: Dialogue Classification

  # TODO: pleasefixme
  # L2_Dialogue_Classification_Dialogue_Intent_and_slot_classification_using_GPT:
  #   needs: [cicd-test-container-setup]
  #   runs-on: self-hosted-azure-gpus-1
  #   container:
  #     image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
  #     options: 
  #       # --user 0:128
  #       --device=/dev/nvidia0
  #       --gpus all
  #       --shm-size=8g 
  #       --env TRANSFORMERS_OFFLINE=0 
  #       --env HYDRA_FULL_ERROR=1
  #       --volume /mnt/datadrive/TestData:/home/TestData
  #   steps:
  #       - name: Checkout repository
  #         uses: actions/checkout@v4
  #       - run: |
  #           cd examples/nlp/dialogue && \
  #           python dialogue.py \
  #           model.dataset.data_dir=/home/TestData/nlp/sgd_small \
  #           model.language_model.lm_checkpoint=/home/TestData/nlp/gpt2/pytorch_model.bin\
  #           model.tokenizer.vocab_file=/home/TestData/nlp/gpt2/vocab.json\
  #           model.dataset.dialogues_example_dir=sgd_gen_outputs \
  #           model.dataset.task_name=debug_sample \
  #           trainer.max_steps=1 \
  #           trainer.max_epochs=1 \
  #           model.train_ds.batch_size=2 \
  #           model.validation_ds.batch_size=2 \
  #           model.test_ds.batch_size=2 \
  #           model.nemo_path=null \
  #           trainer.val_check_interval=0.0 \
  #           trainer.devices=1 \
  #           model.dataset.use_cache=false \
  #           model.tokenizer.special_tokens={pad_token:"endoftext"} \
  #           model.tokenizer.tokenizer_name=gpt2 \
  #           model.tokenizer.vocab_file=/home/TestData/nlp/gpt2/vocab.json\
  #           model.language_model.pretrained_model_name=/home/TestData/nlp/gpt2 \
  #           trainer.accelerator=gpu \
  #           exp_manager=null  && \
  #           rm -rf sgd_gen_outputs

#     - name: L2: Dialogue Generation Part 2
#       when {
#         anyOf {
#           branch main
#           changeRequest target: main
#         }
#       }
#       failFast true
#       parallel {
#         - name: Dialogue: Answer Extender using DialogueGPTGenerationModel
#           - run: |
#             cd examples/nlp/dialogue && \
#             python dialogue.py \
#             do_training=False \
#             model.dataset.data_dir=/home/TestData/nlp/ms-marco-qa \
#             model.dataset.dialogues_example_dir=answer_extender \
#             model.library=huggingface \
#             model.dataset.task=ms_marco \
#             model.dataset.debug_mode=True \
#             trainer.val_check_interval=0.0 \
#             trainer.devices=1 \
#             model.dataset.use_cache=false \
#             model.language_model.pretrained_model_name=gpt2 \
#             trainer.accelerator=gpu \
#             exp_manager=null  && \
#             rm -rf answer_extender
#           }
#         }
#       }
#     }

# Runs out of memory on the 12G TITAN V (GPU 0 on main CI)
# TODO: add when megatron bert is supported again in NeMo
# - name: L2: MegaBERT Token Classification
#   when {
#     anyOf {
#       branch main
#       changeRequest target: main
#     }
#   }
#   failFast true
#   - run: |
#     cd examples/nlp/token_classification && \
#     python token_classification_train.py \
#     model.dataset.data_dir=/home/TestData/nlp/token_classification_punctuation/ \
#     model.language_model.pretrained_model_name=megatron-bert-345m-uncased \
#     model.train_ds.batch_size=10 \
#     model.dataset.max_seq_length=50 \
#     model.dataset.use_cache=false \
#     trainer.accelerator=gpu \
#     trainer.strategy=ddp \
#     trainer.precision=16 \
#     trainer.devices=1 \
#     trainer.accelerator="gpu" \
#     +trainer.fast_dev_run=true \
#     exp_manager=null
#   }
# }

    # TODO: add when megatron-bert is supported again
    # stage('L2: Model Parallel Size 2 Megatron Text Classification') {
    #   when {
    #     anyOf{
    #       branch 'main'
    #       changeRequest target: 'main'
    #     }
    #   }
    #   failFast true
    #   steps{
    #     cd examples/nlp/text_classification && \
    #     python text_classification_with_bert.py \
    #     trainer.devices=[0,1] \
    #     trainer.accelerator="gpu" \
    #     trainer.num_nodes=1 \
    #     trainer.precision=16 \
    #     trainer.gradient_clip_val=1.0 \
    #     +trainer.fast_dev_run=true \
    #     model.dataset.num_classes=6 \
    #     model.train_ds.file_path=/home/TestData/nlp/retail_text_classification/train.tsv \
    #     model.train_ds.batch_size=4 \
    #     model.language_model.pretrained_model_name=megatron-bert-uncased \
    #     model.language_model.config_file=/home/TestData/nlp/mp_2_bert_toy/config.json \
    #     model.language_model.lm_checkpoint=/home/TestData/nlp/mp_2_bert_toy/iter_2000000 \
    #     model.nemo_path=null \
    #     ~model.infer_samples \
    #     exp_manager=null
    #   }
    # }

    # stage('L2: Model Parallel Size 2 Megatron Autoresume') {
    #   when {
    #     anyOf{
    #       branch 'main'
    #       changeRequest target: 'main'
    #     }
    #   }
    #   failFast true
    #   steps{
    #     cd examples/nlp/text_classification && \
    #     python text_classification_with_bert.py \
    #     trainer.devices=[0,1] \
    #     trainer.accelerator="gpu" \
    #     trainer.num_nodes=1 \
    #     trainer.precision=16 \
    #     trainer.gradient_clip_val=1.0 \
    #     trainer.max_epochs=1 \
    #     +trainer.fast_dev_run=true \
    #     model.dataset.num_classes=6 \
    #     model.train_ds.file_path=/home/TestData/nlp/retail_text_classification/train.tsv \
    #     model.train_ds.batch_size=4 \
    #     model.language_model.pretrained_model_name=megatron-bert-uncased \
    #     model.language_model.config_file=/home/TestData/nlp/mp_2_bert_toy/config.json \
    #     model.language_model.lm_checkpoint=/home/TestData/nlp/mp_2_bert_toy/iter_2000000 \
    #     model.nemo_path=null \
    #     ~model.infer_samples \
    #     +exp_manager.explicit_log_dir=/home/TestData/nlp/mp_autoresume \
    #     +exp_manager.resume_if_exists=true
    #   }
    # }

    # stage('L2: Model Parallel Size 2 Megatron Evaluation from .nemo') {
    #   when {
    #     anyOf{
    #       branch 'main'
    #       changeRequest target: 'main'
    #     }
    #   }
    #   failFast true
    #   steps{
    #     cd examples/nlp/text_classification && \
    #     python model_parallel_text_classification_evaluation.py \
    #     trainer.devices=[0,1] \
    #     trainer.accelerator="gpu" \
    #     trainer.num_nodes=1 \
    #     model.dataset.num_classes=6 \
    #     model.test_ds.file_path=/home/TestData/nlp/retail_text_classification/dev.tsv \
    #     model.nemo_path=/home/TestData/nlp/mp_2_nemo/retail_text_class_350M.nemo \
    #     exp_manager=null
    #   }
    # }

    # stage('L2: Model Parallel Size 2 Megatron Train from .nemo') {
    #   when {
    #     anyOf{
    #       branch 'main'
    #       changeRequest target: 'main'
    #     }
    #   }
    #   failFast true
    #   steps{
    #     cd examples/nlp/token_classification && \
    #     python token_classification_train.py \
    #     pretrained_model=/home/TestData/nlp/mp_2_nemo/ner_350M.nemo \
    #     model.dataset.data_dir=/home/TestData/nlp/ner/ \
    #     model.train_ds.batch_size=2 \
    #     model.dataset.use_cache=false \
    #     trainer.devices=[0,1] \
    #     trainer.accelerator="gpu" \
    #     +trainer.fast_dev_run=true \
    #     model.dataset.class_balancing="weighted_loss" \
    #     exp_manager=null
    #   }
    # }

  # Punctuation & Capitalization tarred dataset:
  
  cicd-test-cases-setup:
    needs: [cicd-cluster-clean]
    runs-on: ubuntu-22.04
    if: ${{ github.event.label.name == 'Run CICD' }}
    outputs:
      L0: ${{ steps.test-cases.outputs.L0 }}
      L2: ${{ steps.test-cases.outputs.L2 }}
      ASR: ${{ steps.test-cases.outputs.ASR }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          path: ${{ github.run_id }}
  
      - name: Parse CICD test cases
        shell: bash
        id: test-cases
        run: |
          echo "L0=$(cat ${{ github.run_id }}/.github/assets/test_cases/L0.yaml | yq 'to_entries' -I0 -o json)" >> $GITHUB_OUTPUT

          echo "L2=$(cat ${{ github.run_id }}/.github/assets/test_cases/L2.yaml | yq 'to_entries' -I0 -o json)" >> $GITHUB_OUTPUT

          echo "ASR=$(cat ${{ github.run_id }}/.github/assets/test_cases/ASR.yaml | yq 'to_entries' -I0 -o json)" >> $GITHUB_OUTPUT
  L0:
    needs: [cicd-test-cases-setup, cicd-test-container-setup]
    runs-on: self-hosted-azure
    name: ${{ matrix.key }}
    strategy:
      matrix: 
        include: ${{ fromJSON(needs.cicd-test-cases-setup.outputs.L0) }}
    container:
      image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
      options:
        # --user 0:128
        --device=/dev/nvidia0
        --gpus all
        --shm-size=8g
        --env TRANSFORMERS_OFFLINE=0 
        --env HYDRA_FULL_ERROR=1
        --volume /mnt/datadrive/TestData:/home/TestData
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    - name: Run test
      run: ${{ matrix.value }}
    - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
      if: "failure()"

  L2:
    needs: [cicd-test-cases-setup, cicd-test-container-setup]
    runs-on: self-hosted-azure
    name: ${{ matrix.key }}
    strategy:
      matrix: 
        include: ${{ fromJSON(needs.cicd-test-cases-setup.outputs.L2) }}
    container:
      image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
      options: 
        # --user 0:128
        --device=/dev/nvidia0
        --gpus all
        --shm-size=8g
        --env TRANSFORMERS_OFFLINE=0 
        --env HYDRA_FULL_ERROR=1
        --volume /mnt/datadrive/TestData:/home/TestData
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    - name: Run test
      run: ${{ matrix.value }}
    - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
      if: "failure()"

  ASR:
    needs: [cicd-test-cases-setup, cicd-test-container-setup]
    runs-on: self-hosted-azure
    name: ${{ matrix.key }}
    strategy:
      matrix: 
        include: ${{ fromJSON(needs.cicd-test-cases-setup.outputs.ASR) }}
    container:
      image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
      options: 
        # --user 0:128
        --device=/dev/nvidia0
        --gpus all
        --shm-size=8g
        --env TRANSFORMERS_OFFLINE=0 
        --env HYDRA_FULL_ERROR=1
        --volume /mnt/datadrive/TestData:/home/TestData
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    - name: Run test
      run: ${{ matrix.value }}
    - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
      if: "failure()"

  Punctuation_Capitalization_tarred_dataset_create_and_use_tarred_dataset:
    needs: [cicd-test-container-setup]
    runs-on: self-hosted-azure
    container:
      image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
      options: 
        # --user 0:128
        --device=/dev/nvidia0
        --gpus all
        --shm-size=8g
        --env TRANSFORMERS_OFFLINE=0 
        --env HYDRA_FULL_ERROR=1
        --volume /mnt/datadrive/TestData:/home/TestData
    steps:
        - name: Checkout repository
          uses: actions/checkout@v4
        - run: |
            data_dir="$(mktemp -d -p "$(pwd)")" && \
            cp -r /home/TestData/nlp/token_classification_punctuation/*.txt \
              /home/TestData/nlp/token_classification_punctuation/wmt_wiki_10000 \
              "${data_dir}"/ && \
            usual_data=${data_dir}/wmt_wiki_10000 && \
            output_dir="$(mktemp -d -p "$(pwd)")" && \
            tarred_data=${output_dir}/train_tarred && \
            tokens_in_batch=2000 && \
            max_seq_length=512 && \
            lm_model=distilbert-base-uncased && \
            python examples/nlp/token_classification/data/create_punctuation_capitalization_tarred_dataset.py \
              --text ${usual_data}/input.txt \
              --labels ${usual_data}/labels.txt \
              --output_dir ${tarred_data} \
              --tokens_in_batch ${tokens_in_batch} \
              --max_seq_length 512 \
              --lines_per_dataset_fragment 2000 \
              --num_batches_per_tarfile 5 \
              --tar_file_prefix punctuation_capitalization \
              --tokenizer_name ${lm_model} \
              --use_fast_tokenizer \
              --pad_label O \
              --n_jobs 3 && \
            echo "Number of tarred files in dataset:" && \
            ls ${tarred_data}/*.tar | wc -l && \
            echo "Label id files in dataset:" && \
            ls ${tarred_data}/*.csv && \
            metadata_file=${tarred_data}/metadata.punctuation_capitalization.tokens${tokens_in_batch}.max_seq_length${max_seq_length}.${lm_model}.json && \
            python examples/nlp/token_classification/punctuation_capitalization_train_evaluate.py \
              model.validation_ds.ds_item="${data_dir}" \
              model.test_ds.ds_item="${data_dir}" \
              model.train_ds.ds_item=${tarred_data} \
              model.language_model.pretrained_model_name=${lm_model} \
              model.train_ds.use_tarred_dataset=true \
              model.train_ds.tar_metadata_file=${metadata_file} \
              +model.train_ds.use_cache=false \
              +model.validation_ds.use_cache=false \
              +model.test_ds.use_cache=false \
              trainer.devices=[0,1] \
              trainer.accelerator="gpu" \
              trainer.strategy=ddp \
              trainer.max_epochs=1 \
              +exp_manager.explicit_log_dir=${output_dir}/output && \
            rm -rf "${output_dir}" "${data_dir}"
        - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
          if: "failure()"

  # Punctuation_Capitalization_Different_ways_of_passing_labels_to_model
  Punctuation_Capitalization_Using_model-common_datasets_parameters-label_vocab_dir:
    needs: [cicd-test-container-setup]
    runs-on: self-hosted-azure
    container:
      image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
      options: 
        # --user 0:128
        --device=/dev/nvidia0
        --gpus all
        --shm-size=8g
        --env TRANSFORMERS_OFFLINE=0 
        --env HYDRA_FULL_ERROR=1
        --volume /mnt/datadrive/TestData:/home/TestData
    steps:
        - name: Checkout repository
          uses: actions/checkout@v4
        - run: |
            cd examples/nlp/token_classification && \
            work_dir="$(mktemp -d -p "$(pwd)")" && \
            label_vocab_dir="${work_dir}/labels" && \
            mkdir -p ${label_vocab_dir} && \
            data_dir="${work_dir}/data" && \
            mkdir -p "${data_dir}" && \
            cp /home/TestData/nlp/token_classification_punctuation/*.txt "${data_dir}" && \
            output_dir="${work_dir}/output" && \
            mkdir -p "${output_dir}" && \
            punct_label_vocab="${label_vocab_dir}/punct_label_vocab.csv" && \
            capit_label_vocab="${label_vocab_dir}/capit_label_vocab.csv" && \
            printf "O\n,\n.\n?\n" > "${punct_label_vocab}" && \
            printf "O\nU\n" > "${capit_label_vocab}" && \
            python punctuation_capitalization_train_evaluate.py \
              model.train_ds.use_tarred_dataset=false \
              model.train_ds.ds_item="${data_dir}" \
              model.validation_ds.ds_item="${data_dir}" \
              model.test_ds.ds_item="${data_dir}" \
              model.language_model.pretrained_model_name=distilbert-base-uncased \
              model.common_dataset_parameters.label_vocab_dir="${label_vocab_dir}" \
              model.class_labels.punct_labels_file="$(basename "${punct_label_vocab}")" \
              model.class_labels.capit_labels_file="$(basename "${capit_label_vocab}")" \
              +model.train_ds.use_cache=false \
              +model.validation_ds.use_cache=false \
              +model.test_ds.use_cache=false \
              trainer.devices=[0,1] \
              trainer.strategy=ddp \
              trainer.max_epochs=1 \
              +exp_manager.explicit_log_dir="${output_dir}" \
              +do_testing=false && \
            python punctuation_capitalization_train_evaluate.py \
              +do_training=false \
              +do_testing=true \
              ~model.train_ds \
              ~model.validation_ds \
              model.test_ds.ds_item="${data_dir}" \
              pretrained_model="${output_dir}/checkpoints/Punctuation_and_Capitalization.nemo" \
              +model.train_ds.use_cache=false \
              +model.validation_ds.use_cache=false \
              +model.test_ds.use_cache=false \
              trainer.devices=[0,1] \
              trainer.strategy=ddp \
              trainer.max_epochs=1 \
              exp_manager=null && \
            rm -rf "${work_dir}"
  # TODO: pleasefixme
  # Punctuation_Capitalization_Using_model-common_datasets_parameters-punct-capit-_label_ids:
  #   needs: [cicd-test-container-setup]
  #   runs-on: self-hosted-azure
  #   container:
  #     image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
  #     options: 
  #       # --user 0:128
  #       --device=/dev/nvidia0
  #       --gpus all
  #       --shm-size=8g 
  #       --env TRANSFORMERS_OFFLINE=0 
  #       --env HYDRA_FULL_ERROR=1
  #       --volume /mnt/datadrive/TestData:/home/TestData
  #   steps:
  #       - name: Checkout repository
  #         uses: actions/checkout@v4
  #       - run: |
  #           cd examples/nlp/token_classification && \
  #           work_dir="$(mktemp -d -p "$(pwd)")" && \
  #           output_dir="${work_dir}/output" && \
  #           mkdir -p "${output_dir}" && \
  #           data_dir="${work_dir}/data" && \
  #           mkdir -p "${data_dir}" && \
  #           cp /home/TestData/nlp/token_classification_punctuation/*.txt "${data_dir}" && \
  #           conf_name=punctuation_capitalization_config_with_ids && \
  #           cp conf/punctuation_capitalization_config.yaml "${work_dir}/${conf_name}.yaml" && \
  #           sed -i $\'s/punct_label_ids: null/punct_label_ids: {O: 0, \\\',\\\': 1, .: 2, \\\'?\\\': 3}/\' \
  #             "${work_dir}/${conf_name}.yaml" && \
  #           sed -i $\'s/capit_label_ids: null/capit_label_ids: {O: 0, U: 1}/\' \
  #             "${work_dir}/${conf_name}.yaml" && \
  #           python punctuation_capitalization_train_evaluate.py \
  #             --config-path "${work_dir}" \
  #             --config-name "${conf_name}" \
  #             model.train_ds.use_tarred_dataset=false \
  #             model.train_ds.ds_item="${data_dir}" \
  #             model.validation_ds.ds_item="${data_dir}" \
  #             model.test_ds.ds_item="${data_dir}" \
  #             model.language_model.pretrained_model_name=distilbert-base-uncased \
  #             +model.train_ds.use_cache=false \
  #             +model.validation_ds.use_cache=false \
  #             +model.test_ds.use_cache=false \
  #             trainer.devices=[0,1] \
  #             trainer.strategy=ddp \
  #             trainer.max_epochs=1 \
  #             +exp_manager.explicit_log_dir="${output_dir}" \
  #             +do_testing=false && \
  #           python punctuation_capitalization_train_evaluate.py \
  #             +do_training=false \
  #             +do_testing=true \
  #             ~model.train_ds \
  #             ~model.validation_ds \
  #             model.test_ds.ds_item="${data_dir}" \
  #             pretrained_model="${output_dir}/checkpoints/Punctuation_and_Capitalization.nemo" \
  #             +model.train_ds.use_cache=false \
  #             +model.validation_ds.use_cache=false \
  #             +model.test_ds.use_cache=false \
  #             trainer.devices=[0,1] \
  #             trainer.strategy=ddp \
  #             trainer.max_epochs=1 \
  #             exp_manager=null && \
  #           rm -rf "${work_dir}"

  # Punctuation & Capitalization inference      
  Punctuation_Capitalization_inference_Restore_punctuation_and_capitalization_in_long_text:
    needs: [cicd-test-container-setup]
    runs-on: self-hosted-azure
    container:
      image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
      options: 
        # --user 0:128
        --device=/dev/nvidia0
        --gpus all
        --shm-size=8g
        --env TRANSFORMERS_OFFLINE=0 
        --env HYDRA_FULL_ERROR=1
        --volume /mnt/datadrive/TestData:/home/TestData
    steps:
        - name: Checkout repository
          uses: actions/checkout@v4
        - run: |
            output_dir="$(mktemp -d -p "$(pwd)")" && \
            python examples/nlp/token_classification/punctuate_capitalize_infer.py \
              --input_manifest /home/TestData/nlp/token_classification_punctuation/iwslt_tst2019.manifest \
              --output_text "${output_dir}/iwslt_inference_result.txt" \
              --max_seq_length 92 \
              --step 8 \
              --margin 16 \
              --pretrained_name punctuation_en_bert \
              --batch_size 32 && \
            rm -rf "${output_dir}"
        - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
          if: "failure()"
  
    # stage('L2: NMT Bottleneck Fallback') {
    #   when {
    #     anyOf {
    #       branch 'main'
    #       changeRequest target: 'main'
    #     }
    #   }
    #   failFast true
    #   parallel {
    #     stage('L2: seq2seq (no bottleneck)') {
    #         steps {
    #           cd examples/nlp/machine_translation && \
    #           enc_dec_nmt-bottleneck.py \
    #           --config-path=conf \
    #           --config-name=aayn_bottleneck \
    #           do_testing=true \
    #           model.model_type=nll \
    #           model.encoder.arch=seq2seq \
    #           model.encoder.hidden_steps=1 \
    #           model.encoder.hidden_blocks=1 \
    #           model.encoder.hidden_init_method=params \
    #           model.encoder.hidden_size=64 \
    #           model.encoder.inner_size=128 \
    #           model.encoder.num_attention_heads=2 \
    #           model.encoder.num_layers=2 \
    #           model.decoder.hidden_size=64 \
    #           model.decoder.inner_size=128 \
    #           model.decoder.num_attention_heads=2 \
    #           model.decoder.num_layers=2 \
    #           model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-en-de.src \
    #           model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-en-de.ref \
    #           model.validation_ds.src_file_name=[/home/TestData/nlp/nmt/toy_data/wmt13-en-de.src,/home/TestData/nlp/nmt/toy_data/wmt14-en-de.src] \
    #           model.validation_ds.tgt_file_name=[/home/TestData/nlp/nmt/toy_data/wmt13-en-de.ref,/home/TestData/nlp/nmt/toy_data/wmt14-en-de.ref] \
    #           model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt13-en-de.src \
    #           model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt13-en-de.ref \
    #           model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           trainer.devices=1 \
    #           trainer.accelerator="gpu" \
    #           +trainer.fast_dev_run=true \
    #           +trainer.limit_test_batches=2 \
    #           exp_manager=null \
    #         }
    #     }
    #   }
    # }
    # stage('L2: NMT Bottleneck Architecture') {
    #   when {
    #     anyOf {
    #       branch 'main'
    #       changeRequest target: 'main'
    #     }
    #   }
    #   failFast true
    #   parallel {
    #     stage('Bridge Encoder (identity)') {
    #         steps {
    #           cd examples/nlp/machine_translation && \
    #           enc_dec_nmt-bottleneck.py \
    #           --config-path=conf \
    #           --config-name=aayn_bottleneck \
    #           do_testing=true \
    #           model.model_type=nll \
    #           model.encoder.arch=bridge \
    #           model.encoder.hidden_steps=1 \
    #           model.encoder.hidden_blocks=1 \
    #           model.encoder.hidden_init_method=identity \
    #           model.encoder.hidden_size=64 \
    #           model.encoder.inner_size=128 \
    #           model.encoder.num_attention_heads=2 \
    #           model.encoder.num_layers=2 \
    #           model.decoder.hidden_size=64 \
    #           model.decoder.inner_size=128 \
    #           model.decoder.num_attention_heads=2 \
    #           model.decoder.num_layers=2 \
    #           model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
    #           model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           trainer.devices=1 \
    #           trainer.accelerator="gpu" \
    #           +trainer.fast_dev_run=true \
    #           +trainer.limit_test_batches=2 \
    #           exp_manager=null
    #         }
    #     }
    #     stage('Perceiver Encoder (params)') {
    #         steps {
    #           cd examples/nlp/machine_translation && \
    #           enc_dec_nmt-bottleneck.py \
    #           --config-path=conf \
    #           --config-name=aayn_bottleneck \
    #           do_testing=true \
    #           model.model_type=nll \
    #           model.encoder.arch=perceiver \
    #           model.encoder.hidden_steps=1 \
    #           model.encoder.hidden_blocks=1 \
    #           model.encoder.hidden_init_method=params \
    #           model.encoder.hidden_size=64 \
    #           model.encoder.inner_size=128 \
    #           model.encoder.num_attention_heads=2 \
    #           model.encoder.num_layers=2 \
    #           model.decoder.hidden_size=64 \
    #           model.decoder.inner_size=128 \
    #           model.decoder.num_attention_heads=2 \
    #           model.decoder.num_layers=2 \
    #           model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
    #           model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           trainer.devices=1 \
    #           trainer.accelerator="gpu" \
    #           +trainer.fast_dev_run=true \
    #           +trainer.limit_test_batches=2 \
    #           exp_manager=null
    #         }
    #     }
    #   }
    # }
    # stage('L2: NMT Bottleneck LVM') {
    #   when {
    #     anyOf {
    #       branch 'main'
    #       changeRequest target: 'main'
    #     }
    #   }
    #   failFast true
    #   parallel {
    #     stage('VAE') {
    #         steps {
    #           cd examples/nlp/machine_translation && \
    #           enc_dec_nmt-bottleneck.py \
    #           --config-path=conf \
    #           --config-name=aayn_bottleneck \
    #           do_testing=true \
    #           model.model_type=vae \
    #           model.encoder.arch=perceiver \
    #           model.encoder.hidden_steps=1 \
    #           model.encoder.hidden_blocks=1 \
    #           model.encoder.hidden_init_method=params \
    #           model.encoder.hidden_size=64 \
    #           model.encoder.inner_size=128 \
    #           model.encoder.num_attention_heads=2 \
    #           model.encoder.num_layers=2 \
    #           model.decoder.hidden_size=64 \
    #           model.decoder.inner_size=128 \
    #           model.decoder.num_attention_heads=2 \
    #           model.decoder.num_layers=2 \
    #           model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
    #           model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           trainer.devices=1 \
    #           trainer.accelerator="gpu" \
    #           +trainer.fast_dev_run=true \
    #           +trainer.limit_test_batches=2 \
    #           exp_manager=null
    #         }
    #     }
    #     stage('MIM') {
    #         steps {
    #           cd examples/nlp/machine_translation && \
    #           enc_dec_nmt-bottleneck.py \
    #           --config-path=conf \
    #           --config-name=aayn_bottleneck \
    #           do_testing=true \
    #           model.model_type=mim \
    #           model.encoder.arch=perceiver \
    #           model.encoder.hidden_steps=1 \
    #           model.encoder.hidden_blocks=1 \
    #           model.encoder.hidden_init_method=params \
    #           model.encoder.hidden_size=64 \
    #           model.encoder.inner_size=128 \
    #           model.encoder.num_attention_heads=2 \
    #           model.encoder.num_layers=2 \
    #           model.decoder.hidden_size=64 \
    #           model.decoder.inner_size=128 \
    #           model.decoder.num_attention_heads=2 \
    #           model.decoder.num_layers=2 \
    #           model.train_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.train_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.ref \
    #           model.validation_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.validation_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.src_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.test_ds.tgt_file_name=/home/TestData/nlp/nmt/toy_data/wmt14-de-en.src \
    #           model.encoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           model.decoder_tokenizer.tokenizer_model=/home/TestData/nlp/nmt/toy_data/tt_tokenizer.BPE.4096.model \
    #           trainer.devices=1 \
    #           trainer.accelerator="gpu" \
    #           +trainer.fast_dev_run=true \
    #           +trainer.limit_test_batches=2 \
    #           exp_manager=null
    #         }
    #     }
    #   }
    # }
        
  # L2_Megatron_RETRO_muTransfer_Pretraining_Performance:
  #   needs: [cicd-test-container-setup]
  #   runs-on: self-hosted-azure
  #   container:
  #     image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
  #     options: 
  #       # --user 0:128
  #       --device=/dev/nvidia0
  #       --gpus all
  #       --shm-size=8g 
  #       --env TRANSFORMERS_OFFLINE=0 
  #       --env HYDRA_FULL_ERROR=1
  #       --volume /mnt/datadrive/TestData:/home/TestData
  #   steps:
  #       - name: Checkout repository
  #         uses: actions/checkout@v4
  #       - run: |
  #           python examples/nlp/language_modeling/megatron_retro_mutransfer_pretrain.py \
  #               trainer.devices=2 \
  #               trainer.num_nodes=1 \
  #               trainer.accelerator=gpu \
  #               trainer.accumulate_grad_batches=1 \
  #               trainer.max_steps=100 \
  #               trainer.log_every_n_steps=1 \
  #               trainer.precision=16 \
  #               trainer.val_check_interval=100 \
  #               trainer.limit_val_batches=0 \
  #               trainer.gradient_clip_val=1.0 \
  #               +trainer.num_sanity_val_steps=0 \
  #               exp_manager.exp_dir=examples/nlp/language_modeling/retro_results/ \
  #               +exp_manager.version=smalltest \
  #               model.data.neighbors=2 \
  #               model.megatron_amp_O2=False \
  #               model.apply_query_key_layer_scaling=False \
  #               model.tensor_model_parallel_size=1 \
  #               model.optim.name=muadamw \
  #               model.optim.weight_decay=0.1 \
  #               model.optim.betas=[0.9,0.95] \
  #               model.optim.lr=6e-4 \
  #               model.optim.sched.warmup_steps=1000 \
  #               model.optim.sched.constant_steps=0 \
  #               model.optim.sched.min_lr=6e-5 \
  #               model.add_position_embedding=False \
  #               model.enc_num_layers=2 \
  #               model.dec_num_layers=6 \
  #               model.enc_cross_attention=[0] \
  #               model.dec_cross_attention=[3,5] \
  #               model.hidden_size=96 \
  #               model.ffn_hidden_size=384 \
  #               model.init_method_std=0.023 \
  #               model.num_attention_heads=12 \
  #               model.max_position_embeddings=1024 \
  #               model.encoder_seq_length=1024 \
  #               model.tokenizer.library=megatron \
  #               model.tokenizer.type=GPT2BPETokenizer \
  #               model.tokenizer.merge_file=/home/TestData/nlp/megatron_retro/gpt2-merges.txt \
  #               model.tokenizer.vocab_file=/home/TestData/nlp/megatron_retro/gpt2-vocab.json \
  #               model.data.data_prefix=[/home/TestData/nlp/megatron_retro/retro_wiki_test_text_document] \
  #               model.data.knn_index=[/home/TestData/nlp/megatron_retro/knn2_map_wiki_test.idx] \
  #               model.data.retrieval_prefix=/home/TestData/nlp/megatron_retro/retro_wiki_test_text_document \
  #               model.data.index_mapping_dir=/home/TestData/nlp/megatron_retro \
  #               model.data.num_workers=8 \
  #               model.micro_batch_size=8 \
  #               model.normalization=rmsnorm \
  #               model.transformer_block_type=pre_ln \
  #               model.bias_activation_fusion=True \
  #               model.bias_dropout_add_fusion=False \
  #               model.masked_softmax_fusion=True \
  #               model.hidden_dropout=0 \
  #               model.attention_dropout=0 \
  #               model.fp32_residual_connection=True \
  #               model.shape_file=/home/TestData/nlp/megatron_retro/o1_rel_shape_info_tiny.yaml

  #               python -c "import pandas as pd
  #               import pathlib
  #               from pandas.testing import assert_frame_equal
  #               from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
  #               import torch
  #               if not (torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name()):
  #                   import sys
  #                   sys.exit(0)
  #               event_file = list(pathlib.Path('examples/nlp/language_modeling/retro_results/megatron_retro/smalltest').glob('events.out.tfevents*'))[0]
  #               ea = EventAccumulator(str(event_file)).Reload()
  #               vals = []
  #               for i in ea.Scalars('reduced_train_loss'):
  #                   vals.append(i.value)
  #               training_curve = pd.DataFrame({'loss': vals})
  #               gt_curve = pd.read_csv('/home/TestData/nlp/megatron_retro/expected_learning_curve.csv')
  #               assert_frame_equal(training_curve, gt_curve, rtol=1e-3, atol=1e-3)"

  #               rm -rf examples/nlp/language_modeling/retro_results
  #       - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
  #         if: "failure()"

    #  This test requires Ampere but some of the test GPUs are Volta
    #  Need to add a check for compute capability before uncommenting this test
    #  - name: L2: Megatron GPT with Rope Pretraining using Flash Attention and Resume Training TP=2
    #    when {
    #      anyOf {
    #        branch main
    #        changeRequest target: main
    #      }
    #    }
    #    failFast true
    #    - run: |
    #      python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
    #      trainer.devices=2 \
    #      trainer.accelerator=gpu \
    #      trainer.log_every_n_steps=1 \
    #      trainer.val_check_interval=2 \
    #      trainer.limit_val_batches=2 \
    #      trainer.accumulate_grad_batches=1 \
    #      trainer.max_steps=3 \
    #      trainer.precision=16 \
    #      trainer.gradient_clip_val=1.0 \
    #      exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
    #      model.tensor_model_parallel_size=2 \
    #      model.optim.name=fused_adam \
    #      model.optim.lr=2e-4 \
    #      model.optim.sched.warmup_steps=1 \
    #      model.optim.sched.constant_steps=1 \
    #      model.optim.sched.min_lr=8e-5 \
    #      model.max_position_embeddings=128 \
    #      model.encoder_seq_length=128 \
    #      model.data.seq_length=128 \
    #      model.position_embedding_type=rope \
    #      model.rotary_percentage=0.5 \
    #      model.normalization=rmsnorm \
    #      model.bias=False \
    #      model.bias_activation_fusion=False \
    #      model.bias_dropout_add_fusion=False \
    #      model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
    #      model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
    #      model.num_layers=8 \
    #      model.hidden_size=256 \
    #      model.num_attention_heads=8 \
    #      model.activations_checkpoint_method=block \
    #      model.activations_checkpoint_granularity=full \
    #      model.activations_checkpoint_num_layers=1 \
    #      model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
    #      model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings \
    #      model.use_flash_attention=True "
    #      #  commented out to save time on github ci @adithyare
    #      # python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
    #      # trainer.devices=2 \
    #      # trainer.accelerator=gpu \
    #      # trainer.log_every_n_steps=1 \
    #      # trainer.val_check_interval=2 \
    #      # trainer.limit_val_batches=1 \
    #      # trainer.accumulate_grad_batches=1 \
    #      # trainer.max_steps=6 \
    #      # trainer.precision=16 \
    #      # trainer.gradient_clip_val=1.0 \
    #      # exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
    #      # exp_manager.resume_if_exists=True \
    #      # model.tensor_model_parallel_size=2 \
    #      # model.optim.name=fused_adam \
    #      # model.optim.lr=2e-4 \
    #      # model.optim.sched.warmup_steps=2 \
    #      # model.optim.sched.constant_steps=2 \
    #      # model.optim.sched.min_lr=8e-5 \
    #      # model.max_position_embeddings=128 \
    #      # model.encoder_seq_length=128 \
    #      # model.data.seq_length=128 \
    #      # model.position_embedding_type=rope \
    #      # model.rotary_percentage=0.5 \
    #      # model.normalization=rmsnorm \
    #      # model.bias=False \
    #      # model.bias_activation_fusion=False \
    #      # model.bias_dropout_add_fusion=False \
    #      # model.tokenizer.vocab_file=/home/TestData/nlp/megatron_gpt/data/gpt/vocab.json \
    #      # model.tokenizer.merge_file=/home/TestData/nlp/megatron_gpt/data/gpt/merges.txt \
    #      # model.num_layers=8 \
    #      # model.hidden_size=256 \
    #      # model.num_attention_heads=8 \
    #      # model.activations_checkpoint_method=block \
    #      # model.activations_checkpoint_granularity=full \
    #      # model.activations_checkpoint_num_layers=1 \
    #      # model.data.data_prefix=[.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document,.5,/home/TestData/nlp/megatron_gpt/data/gpt/simple_wiki_gpt_preproc_text_document] \
    #      # model.data.index_mapping_dir=examples/nlp/language_modeling/gpt_index_mappings \
    #      # model.use_flash_attention=True"
    #      rm -rf examples/nlp/language_modeling/gpt_pretrain_results"
    #      rm -rf examples/nlp/language_modeling/gpt_index_mappings"
    #    }
    #  }

    # TODO: Add this test back. Test was failing on CI machines due to HW error
    # - name: L2: Megatron GPT Convert from Megatron-LM checkpoing and Eval
    #   when {
    #     anyOf {
    #       branch main
    #       changeRequest target: main
    #     }
    #   }
    #   failFast true
    #   - run: |
    #     python -m torch.distributed.launch --nproc_per_node=2 \
    #     examples/nlp/language_modeling/megatron_lm_ckpt_to_nemo.py \
    #     --checkpoint_folder=/home/TestData/nlp/megatron_gpt/data/gpt/iter_0008700 \
    #     --checkpoint_name=model_optim_rng.pt \
    #     --hparams_file=/home/TestData/nlp/megatron_gpt/data/gpt/iter_0008700/hparams.yaml \
    #     --nemo_file_path=examples/nlp/language_modeling/small_gpt.nemo \
    #     --model_type=gpt \
    #     --pipeline_model_parallel_size=1 \
    #     --gpus_per_node=2 \
    #     --tensor_model_parallel_size=2"
    #     python examples/nlp/language_modeling/megatron_gpt_eval.py \
    #     --gpt_model_file=examples/nlp/language_modeling/small_gpt.nemo \
    #     --tokens_to_generate=32 \
    #     --tensor_model_parallel_size=2 \
    #     --prompt=This is a test.
    #     rm examples/nlp/language_modeling/small_gpt.nemo
  
  # L2: NeRF
  # L2_NeRF_DreamFusion:
  #   needs: [cicd-test-container-setup]
  #   runs-on: self-hosted-azure
  #   container:
  #     image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
  #     options: 
  #       # --user 0:128
  #       --device=/dev/nvidia0
  #       --gpus all
  #       --shm-size=8g 
  #       --env TRANSFORMERS_OFFLINE=0 
  #       --env HYDRA_FULL_ERROR=1
  #       --volume /mnt/datadrive/TestData:/home/TestData
  #   steps:
  #       - name: Checkout repository
  #         uses: actions/checkout@v4
  #       - run: |
  #           python examples/multimodal/text_to_image/nerf/main.py \
  #           trainer.num_nodes=1 \
  #           trainer.devices="[0]" \
  #           trainer.max_steps=1000 \
  #           model.prompt="a DSLR photo of a delicious hamburger" \
  #           exp_manager.exp_dir=examples/multimodal/text_to_image/nerf/dreamfusion_results
  #
  #           rm -rf examples/multimodal/text_to_image/nerf/dreamfusion_results
  #       - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
  #         if: "failure()"

  Speech_Checkpoints_tests:
    needs: [cicd-test-container-setup]
    runs-on: self-hosted-azure
    container:
      image: nemoci.azurecr.io/nemo_container_${{ github.run_id }}
      options: 
        # --user 0:128
        --device=/dev/nvidia0
        --gpus all
        --shm-size=8g
        --env TRANSFORMERS_OFFLINE=0 
        --env HYDRA_FULL_ERROR=1
        --volume /mnt/datadrive/TestData:/home/TestData
    steps:
        - name: Checkout repository
          uses: actions/checkout@v4
        - run: |
            CUDA_VISIBLE_DEVICES=0 python examples/asr/speech_to_text_eval.py \
                pretrained_name=QuartzNet15x5Base-En  \
                dataset_manifest=/home/TestData/librispeech/librivox-dev-other.json \
                batch_size=64 \
                tolerance=0.1012
            rm -f examples/asr/evaluation_transcripts.json
        - uses: "NVIDIA/NeMo/.github/actions/cancel-workflow@main"
          if: "failure()"


  # Nemo_CICD_Test:
  #   needs:
  #     - L0_Unit_Tests_GPU
  #     - L0_Unit_Tests_CPU
  #     - L2_Community_LLM_Checkpoints_tests_Llama
  #     - L2_Community_LLM_Checkpoints_tests_StarCoder
  #     - L2_Community_LLM_Checkpoints_tests_Falcon
  #     #- L2_Community_LLM_Checkpoints_tests_Baichuan2
  #     - ASR_dev_run_Speech_to_Text
  #     - ASR_dev_run_Speech_to_Text_WPE_-_CitriNet
  #     - ASR_dev_run_Speech_Pre-training_-_CitriNet
  #     - ASR_dev_run_Speech_To_Text_Finetuning
  #     - ASR_dev_run_Speech_To_Text_HF_Finetuning
  #     - ASR_dev_run_Speech_to_Text_WPE_-_Conformer
  #     - ASR_dev_run-part_two_Speech_to_Text_WPE_-_Squeezeformer
  #     - L2_Speech_to_Text_EMA
  #     - L2_Speaker_dev_run_Speaker_Recognition
  #     - L2_Speaker_dev_run_Speaker_Diarization
  #     - L2_Speaker_dev_run_Speech_to_Label
  #     - L2_Speaker_dev_run_Speaker_Diarization_with_ASR_Inference
  #     - L2_Speaker_dev_run_Clustering_Diarizer_Inference
  #     - L2_Speaker_dev_run_Neural_Diarizer_Inference
  #     - L2_Speaker_dev_run_Multispeaker_ASR_Data_Simulation
  #     - L2_ASR_Multi-dataloader_dev_run_Speech_to_Text_multi-dataloader
  #     - L2_ASR_Multi-dataloader_dev_run_Speech_to_Label_multi-dataloader
  #     - L2_ASR_Adapters_Linear_Adapters
  #     - L2_ASR_Adapters_RelPos_MHA_Adapters
  #     - L2_Speech_Transcription_Speech_to_Text_Transcribe
  #     - L2_Transducer_alignment_Running_pytest
  #     - L2_Segmentation_Tool_Parallel_ctc_segmentation_test_L2_Eng_CitriNet_with_wav
  #     - L2_Segmentation_Tool_Parallel_ctc_segmentation_test_L2_Ru_QN_with_mp3
  #     - L2_G2P_Models_G2P_Conformer_training_evaluation_and_inference
  #     - L2_G2P_Models_HeteronymClassificationModel_training_evaluation_and_inference
  #     - L2_Dialogue_Classification_Intent_and_slot_classification_using_SGDQA
  #     - L2_Dialogue_Classification_Intent_and_slot_classification_using_IntentSlotClassificationModel
  #     - L2_Dialogue_Classification_Intent_classification_using_ZeroShotIntentModel
  #     - L2_Dialogue_Classification_Design_Intent_classification_using_ZeroShotIntentModel
  #     - L2_Dialogue_Classification_Design_Intent_classification_using_ZeroShotIntentModel_BART_Classifier
  #     - L2_Dialogue_Classification_Design_Intent_classification_using_DialogueNearestNeighbourModel
  #     - L2_Dialogue_Generation_Dialogue_Answer_Extender_using_DialogueS2SGenerationModel
  #     - L2_Dialogue_Generation_Dialogue_SGD_Based_Answer_Extender_using_DialogueS2SGenerationModel
  #     - L2_COPY_Dialogue_Answer_Extender_using_DialogueGPTGenerationModel
  #     - L2_Duplex_Text_Normalization_with_Tarred_dataset
  #     - L2_BERT_Text_Classification_with_BERT_Test
  #     - L2_Parallel_BERT_Question-Answering_SQUAD_v1_1
  #     - L2_Parallel_BERT_Question-Answering_SQUAD_v2_0
  #     - L2_Parallel_BART_Question-Answering_SQUAD_v1_1
  #     - L2_Parallel_BART_Question-Answering_SQUAD_v2_0
  #     - L2_Parallel_GPT2_Question-Answering_SQUAD_v1_1
  #     - L2_Parallel_GPT2_Question-Answering_SQUAD_v2_0
  #     - L2_Intent_and_Slot_Classification_Tasks_Intent_and_Slot_Classification
  #     - L2_Intent_and_Slot_Classification_Tasks_Multi-Label_Intent_and_Slot_Classification
  #     - L2_Parallel_NLP_Examples2_NER_finetuning_from_pretrained_Test
  #     - L2_Parallel_NLP_Examples2_Punctuation_and_capitalization_finetuning_from_pretrained_test
  #     - L2_Parallel_NLP_Examples2_NER_with_TurkuNLP__bert-base-finnish-cased-v1
  #     - L2_Parallel_NLP_Examples2_Evaluation_script_for_Token_Classification
  #     - L2_Parallel_NLP_Examples2_Evaluation_script_for_Punctuation
  #     - L2_Parallel_NLP_Examples2_Punctuation_Capitalization_2GPUs_with_DistilBERT_Finetuning_on_other_data
  #     - Punctuation_Capitalization_tarred_dataset_create_and_use_tarred_dataset
  #     - Punctuation_Capitalization_Using_model-common_datasets_parameters-label_vocab_dir
  #     - Punctuation_Capitalization_inference_Restore_punctuation_and_capitalization_in_long_text
  #     - L2_Pretraining_BERT_pretraining_from_Text
  #     - L2_Pretraining_BERT_from_Preprocessed
  #     - L2_Entity_Linking_Self_Alignment_Pretraining_BERT
  #     - L2_NMT_Attention_is_All_You_Need_Training_NMT_Training_Post-LN
  #     - L2_NMT_Attention_is_All_You_Need_Training_NMT_Training_Pre-LN
  #     - L2_NMT_Attention_is_All_You_Need_Training_NMT_Multi-Validation
  #     - L2_NMT_Attention_is_All_You_Need_Inference
  #     - L2_NMT_Attention_is_All_You_Need_Finetuning
  #     - L2_NMT_Tarred_Dataset_Creation_Auto_Tarred_Dataset_Creation
  #     - L2_NMT_Tarred_Dataset_Creation_Script_Tarred_Dataset_Creation
  #     - L2_Megatron_NMT_Training_TP2
  #     - L2_Megatron_BART_Perceiver_MIM_Training_TP2
  #     - L2_Megatron_Bert_Pretraining_and_Resume_Training_with_Pipeline_Parallelism
  #     - L2_Megatron_Bert_Pretraining_and_Resume_Training
  #     - L2_Megatron_Core_Bert_Pretraining_and_Resume_Training
  #     - L2_Legacy_Megatron_RETRO_Pretraining_and_Resume_Training
  #     - L2_Megatron_RETRO_Pretraining_and_Resume_Training
  #     - L2_BioMegatron_Bert_NER_Task
  #     - L2_Megatron_GPT_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_GPT_with_Rope_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_GPT_with_ALiBi_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_GPT_with_KERPLE_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_GPT_Pretraining_and_Resume_Training_PP2
  #     - L2_Megatron_GPT_Finetuning_PP2
  #     - L2_Megatron_GPT_Finetuning_StarCoder_PP1
  #     - L2_Megatron_GPT_Embedding 
  #     - L2_Megatron_GPT_PEFT_Lora_PP2
  #     - L2_Megatron_GPT_PEFT_Lora_TP2
  #     - L2_Megatron_GPT_Eval
  #     - L2_Megatron_GPT_Eval_PP2
  #     - L2_Megatron_GPT_SFT_Eval_inference_seq_len_greaterThan_training_seq_len
  #     - L2_Megatron_Change_Partitions_Reduce_TP_Num_Partitions_-2_to_1-_and_PP_Num_Partitions_-1_to_2
  #     - L2_Megatron_Change_Partitions_Increase_TP_Num_Partitions_-2_to_4-_and_PP_Num_Partitions_-1_to_2
  #     - L2_Megatron_T5_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_T5_with_ALiBi_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_T5_with_KERPLE_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_T5_Pretraining_and_Resume_Training_PP2
  #     - L2_Megatron_T5_w_Mixture_of_Expert_Pretraining
  #     - L2_Megatron_UL2_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_T5_Eval
  #     - L2_Megatron_BART_Pretraining_and_Resume_Training_TP2
  #     - L2_Megatron_BART_Pretraining_and_Resume_Training_PP2
  #     - L2_Megatron_T5_GLUE_RTE
  #     - L2_Megatron_T5_GLUE_XNLI
  #     - L2_Megatron_T5_PEFT_Lora_TP2
  #     - L2_Megatron_Mock_Data_Generation_MockGPTDataset
  #     - L2_Megatron_Mock_Data_Generation_MockT5Dataset
  #     - L2_TTS_Fast_dev_runs_1_Tacotron_2
  #     - L2_TTS_Fast_dev_runs_1_WaveGlow
  #     - L2_TTS_Fast_dev_runs_1_FastPitch
  #     #- L2_TTS_Fast_dev_runs_1_RADTTS
  #     - L2_TTS_Fast_dev_runs_1_Mixer-TTS
  #     - L2_TTS_Fast_dev_runs_1_Hifigan
  #     - Speech_Checkpoints_tests

  #   runs-on: ubuntu-latest
  #   steps:
  #       # This should depend on all the tests so we block/unblock based on all tests passing
  #     - run: exit 0

