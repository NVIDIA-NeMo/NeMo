# Copyright (c) 2020-2021, NVIDIA CORPORATION.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
name: CICD NeMo
on:
  schedule:
    - cron: 0 0 * * *
  pull_request:
    branches:
      - main
      - r**
      - weekly-bump
    types: [labeled]
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      test_to_run:
        required: false
        default: all
        type: string
        description: Comma-separated list of tests to run. Use "all" to run the full test suite.

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-${{ github.event.label.name || 'main' }}-${{ github.event_name }}
  cancel-in-progress: true

jobs:
  pre-flight:
    runs-on: ubuntu-latest
    outputs:
      test_to_run: ${{ steps.test_to_run.outputs.main }}
      build_args: ${{ steps.manifest.outputs.BUILD_ARGS }}
      cache-from: ${{ steps.cache_from.outputs.LAST_PRS }}
    env:
      TESTS_TO_RUN: ${{ inputs.test_to_run }}
      EVENT_NAME: ${{ github.event_name }}
      HAS_LABEL: ${{ github.event.label.name == 'Run CICD' }}
    steps:
      - name: Checkout branch
        uses: actions/checkout@v4

      - name: Select tests to run
        id: test_to_run
        run: |
          # For manual dispatch, we replace `all` with the actual job names          
          if [[ "$EVENT_NAME" == "workflow_dispatch" ]]; then
            TESTS_TO_RUN=$TESTS_TO_RUN

          # For correctly labeled PR, we replace `all` with the actual job names          
          elif [[ "$EVENT_NAME" == "pull_request" && "$HAS_LABEL" == "true" ]]; then
            TESTS_TO_RUN=all

          # For incorrectly labeled PR, run no tests
          elif [[ "$EVENT_NAME" == "pull_request" && "$HAS_LABEL" != "true" ]]; then
            TESTS_TO_RUN=""
            
          # For push events, run all tests. This is so that we can generate coverage
          # on branch `main`.
          elif [[ "$EVENT_NAME" == "push" ]]; then
            TESTS_TO_RUN=all

          else
            echo "Unsupported event_name $EVENT_NAME provided".
            exit 1
          fi

          parsed_string=$(echo "$TESTS_TO_RUN" | jq -c --raw-input 'split(",")')
          echo "main=${parsed_string}" | tee -a "$GITHUB_OUTPUT"

      - name: Parse manifest.json
        id: manifest
        run: |
          BUILD_ARGS=$(cat << EOF
          BASE_IMAGE=$(cat requirements/manifest.json | jq -r '."ngc-pytorch"')
          TRTLLM_REPO=$(cat requirements/manifest.json | jq -r '."vcs-dependencies"."trt-llm".repo')
          TRTLLM_TAG=$(cat requirements/manifest.json | jq -r '."vcs-dependencies"."trt-llm".ref')
          MLM_REPO=$(cat requirements/manifest.json | jq -r '."vcs-dependencies"."megatron-lm".repo')
          MLM_TAG=$(cat requirements/manifest.json | jq -r '."vcs-dependencies"."megatron-lm".ref')
          TE_REPO=$(cat requirements/manifest.json | jq -r '."vcs-dependencies".transformer_engine.repo')
          TE_TAG=$(cat requirements/manifest.json | jq -r '."vcs-dependencies".transformer_engine.ref')
          APEX_REPO=$(cat requirements/manifest.json | jq -r '."vcs-dependencies".apex.repo')
          APEX_TAG=$(cat requirements/manifest.json | jq -r '."vcs-dependencies".apex.ref')
          EOF
          )

          echo "BUILD_ARGS<<EOF" >> $GITHUB_OUTPUT
          echo "$BUILD_ARGS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Get last merged PR
        id: cache_from
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          LAST_PRS=$(gh api graphql -f query='
            query {
              repository(owner: "NVIDIA", name: "NeMo") {
                pullRequests(states: MERGED, first: 10, orderBy: {field: UPDATED_AT, direction: DESC}) {
                  nodes {
                    number
                  }
                }
              }
            }' | jq -r '.data.repository.pullRequests.nodes[].number' | while read -r number; do
              echo "nemoci.azurecr.io/nemo_container-buildcache:$number"
            done)

          echo "LAST_PRS<<EOF" >> $GITHUB_OUTPUT
          echo "$LAST_PRS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

  code-linting:
    if: ${{ needs.pre-flight.outputs.test_to_run != '[]' }}
    needs: [pre-flight]
    uses: ./.github/workflows/code-linting.yml

  cicd-test-container-build:
    if: ${{ needs.pre-flight.outputs.test_to_run != '[]' }}
    uses: NVIDIA/NeMo-FW-CI-templates/.github/workflows/_build_container.yml@v0.27.0
    needs: [pre-flight, code-linting]
    with:
      image-name: nemo_container
      dockerfile: Dockerfile.ci
      image-label: nemo-core
      build-args: |
        IMAGE_LABEL=nemo-core
        NEMO_TAG=${{ github.sha }}
        NEMO_REPO=https://github.com/NVIDIA/NeMo
        ${{ needs.pre-flight.outputs.BUILD_ARGS }}
      prune-filter-timerange: 24h
      use-inline-cache: false
      cache-from: |
        nemoci.azurecr.io/nemo_container-buildcache:main
        nemoci.azurecr.io/nemo_container-buildcache:${{ github.event.pull_request.number || 0 }}
        ${{ needs.pre-flight.outputs.cache-from }}

  cicd-import-tests:
    if: ${{ needs.pre-flight.outputs.test_to_run != '[]' }}
    needs: [cicd-test-container-build, pre-flight]
    runs-on: self-hosted-azure-gpus-1
    steps:
      - name: Create UUID
        id: uuid
        run: |
          echo "id=$(uuidgen)" >> "$GITHUB_OUTPUT"

      - name: Checkout NeMo
        uses: actions/checkout@v2
        with:
          repository: NVIDIA/NeMo
          path: ${{ github.run_id }}/${{steps.uuid.outputs.id }}/NeMo

      - name: Run some checks
        run: |
          docker run \
              --rm \
            --device=/dev/nvidia0 \
            --gpus all \
            --shm-size=8g \
            --volume $(pwd)/${{ github.run_id }}/${{steps.uuid.outputs.id }}/NeMo:/workspace \
            --env TRANSFORMERS_OFFLINE=0 \
            --env HYDRA_FULL_ERROR=1 --env PYTHONUNBUFFERED=1 nemoci.azurecr.io/nemo_container:${{ github.run_id }} bash -c '\
            # PyTorch Lightning version
            python -c "import lightning.pytorch; print(lightning.pytorch.__version__)"

            # PyTorch Lightning DDP Checks
            CUDA_VISIBLE_DEVICES="0,1" python "tests/core_ptl/check_for_ranks.py"

            # Basic Import Checks
            python tests/core_ptl/check_imports.py --domain asr
            python tests/core_ptl/check_imports.py --domain nlp
            python tests/core_ptl/check_imports.py --domain tts
          '

  L0_Setup_Test_Data_And_Models:
    needs: [pre-flight, cicd-test-container-build]
    uses: ./.github/workflows/_test_template.yml
    with:
      RUNNER: self-hosted-azure
      SCRIPT: L0_Setup_Test_Data_And_Models
      TESTS_TO_RUN: '["L0_Setup_Test_Data_And_Models"]'

  cicd-main-unit-tests:
    needs: [pre-flight, cicd-test-container-build]
    uses: ./.github/workflows/cicd-main-unit-tests.yml
    with:
      test_to_run: ${{ needs.pre-flight.outputs.test_to_run }}

  cicd-main-e2e-tests:
    needs: [pre-flight, cicd-test-container-build, cicd-main-unit-tests]
    uses: ./.github/workflows/cicd-main-e2e-tests.yml
    with:
      test_to_run: ${{ needs.pre-flight.outputs.test_to_run }}

  Nemo_CICD_Test:
    needs:
      - pre-flight
      - cicd-test-container-build
      - cicd-import-tests
      - L0_Setup_Test_Data_And_Models
      - cicd-main-unit-tests
      - cicd-main-e2e-tests

      - L0_Unit_Tests_GPU_ASR
      - L0_Unit_Tests_GPU_Audio
      - L0_Unit_Tests_GPU_Common
      - L0_Unit_Tests_GPU_LLM
      - L0_Unit_Tests_GPU_VLM
      - L0_Unit_Tests_GPU_Multimodal
      - L0_Unit_Tests_GPU_TTS
      - L0_Unit_Tests_GPU_Core
      - L0_Unit_Tests_GPU_Hydra
      - L0_Unit_Tests_GPU_Lightning
      - L0_Unit_Tests_GPU_Others

      - L0_Unit_Tests_CPU_ASR
      - L0_Unit_Tests_CPU_Audio
      - L0_Unit_Tests_CPU_Common
      - L0_Unit_Tests_CPU_LLM
      - L0_Unit_Tests_CPU_VLM
      - L0_Unit_Tests_CPU_Multimodal
      - L0_Unit_Tests_CPU_TTS
      - L0_Unit_Tests_CPU_Core
      - L0_Unit_Tests_CPU_Hydra
      - L0_Unit_Tests_CPU_Lightning
      - L0_Unit_Tests_CPU_Others
      - L0_Unit_Tests_Eval

      # Nemo2 Conversion Tests
      - L2_NeMo_2_Conversion_Test_Baichuan2
      - L2_NeMo_2_Conversion_Test_ChatGLM
      - L2_NeMo_2_Conversion_Test_DeepSeek
      - L2_NeMo_2_Conversion_Test_Gemma
      - L2_NeMo_2_Conversion_Test_Gemma2
      - L2_NeMo_2_Conversion_Test_Llama
      - L2_NeMo_2_Conversion_Test_Llama_Embedding
      - L2_NeMo_2_Conversion_Test_Mistral
      - L2_NeMo_2_Conversion_Test_Nemotron
      - L2_NeMo_2_Conversion_Test_Phi3Mini
      - L2_NeMo_2_Conversion_Test_Qwen2
      - L2_NeMo_2_Conversion_Test_Starcoder
      - L2_NeMo_2_Conversion_Test_Starcoder2
      - L2_NeMo_2_Conversion_Test_T5
      - L2_NeMo_2_Conversion_Test_BERT

      - ASR_dev_run_Speech_to_Text
      - ASR_dev_run_Speech_to_Text_WPE_CitriNet
      - ASR_dev_run_Speech_Pre-training_-_CitriNet
      - Optional_ASR_dev_run_Speech_To_Text_Finetuning
      - Optional_ASR_dev_run_Speech_To_Text_HF_Finetuning
      - ASR_dev_run_Speech_to_Text_WPE_-_Conformer
      - ASR_dev_run-part_two_Speech_to_Text_WPE_-_Squeezeformer
      - L2_Speech_to_Text_EMA
      - L2_Speaker_dev_run_Speaker_Recognition
      - L2_Speaker_dev_run_Speaker_Diarization
      - L2_Speaker_dev_run_EndtoEnd_Speaker_Diarization_Sortformer
      - L2_Speaker_dev_run_EndtoEnd_Diarizer_Inference
      - L2_Speaker_dev_run_Speech_to_Label
      - L2_Speaker_dev_run_Speaker_Diarization_with_ASR_Inference
      - L2_Speaker_dev_run_Clustering_Diarizer_Inference
      - L2_Speaker_dev_run_Neural_Diarizer_Inference
      - L2_Speaker_dev_run_Multispeaker_ASR_Data_Simulation
      - L2_ASR_Multi-dataloader_dev_run_Speech_to_Text_multi-dataloader
      - L2_ASR_Multi-dataloader_dev_run_Speech_to_Label_multi-dataloader
      - L2_ASR_Adapters_Linear_Adapters
      - L2_ASR_Adapters_RelPos_MHA_Adapters
      - L2_Speech_Transcription_Speech_to_Text_Transcribe
      - L2_Segmentation_Tool_Parallel_ctc_segmentation_test_L2_Eng_CitriNet_with_wav
      - L2_Segmentation_Tool_Parallel_ctc_segmentation_test_L2_Ru_QN_with_mp3
      - L2_G2P_Models_G2P_Conformer_training_evaluation_and_inference
      - L2_G2P_Models_HeteronymClassificationModel_training_evaluation_and_inference
      - L2_NMT_Attention_is_All_You_Need_Training_NMT_Training_Post-LN
      - L2_NMT_Attention_is_All_You_Need_Training_NMT_Training_Pre-LN
      - L2_NMT_Attention_is_All_You_Need_Training_NMT_Multi-Validation
      - L2_NMT_Attention_is_All_You_Need_Inference
      - L2_NMT_Attention_is_All_You_Need_Finetuning
      - L2_NMT_Tarred_Dataset_Creation_Auto_Tarred_Dataset_Creation
      - L2_NMT_Tarred_Dataset_Creation_Script_Tarred_Dataset_Creation
      - L2_Megatron_NMT_Training_TP2
      - L2_TTS_Fast_dev_runs_1_Tacotron_2
      - L2_TTS_Fast_dev_runs_1_WaveGlow
      - L2_TTS_Fast_dev_runs_1_FastPitch
        #- OPTIONAL_L2_TTS_Fast_dev_runs_1_RADTTS
      - L2_TTS_Fast_dev_runs_1_Hifigan
      - Speech_Checkpoints_tests
      - L2_Stable_Diffusion_Training
      - L2_NeMo_2_NEVA_MOCK_PRETRAIN_TP2
      - L2_NeMo_2_NEVA_MOCK_PRETRAIN_PP2
      - L2_NeMo_2_NEVA_MOCK_PRETRAIN_CP2
      - L2_NeMo_2_NEVA_MOCK_FINETUNE_TP2
      - L2_NeMo_2_NEVA_MOCK_FINETUNE_PP2
      - L2_NeMo_2_NEVA_MOCK_FINETUNE_CP2
      - OPTIONAL_L2_NeMo_2_NEVA_LOAD_GENERATE
      - Optional_L2_NeMo_2_MLLAMA_MOCK_FINETUNE_TP2
      - L2_NeMo_2_MLLAMA_PRELOADED_FINETUNE_TP2
      - Optional_L2_NEMO_2_MLLAMA_Inference
      - L2_NeMo_2_NEVA_PRELOADED_FINETUNE_PP2_SEQPACK_PAD
      - L2_NeMo_2_NEVA_PRELOADED_FINETUNE_PP2_SEQPACK_TRUNC
      - L2_NeMo_2_NEVA_ENERGON_FINETUNE_TP2
      - L2_NeMo_2_MLLAMA_ENERGON_FINETUNE_TP2
      - L2_NeMo_2_LLAVA_IMPORT
      - L2_NeMo_2_MLLAMA_IMPORT
      - L2_NeMo_2_GPT_Pretraining_no_transformer_engine
      - L2_NeMo_2_GPT_DDP_Param_Parity_check
      - L2_NeMo_2_HF_MODEL_IMPORT
      - L2_NeMo_2_llama3_pretraining_recipe
      - L2_NeMo_2_llama3_fault_tolerance_plugin
      - L2_NeMo_2_llama3_straggler_detection
      - L2_HF_Transformer_PEFT_notebook
      - L2_HF_Transformer_PEFT
      - L2_HF_Transformer_PEFT_nemorun
      - L2_HF_Transformer_PEFT_2gpu
      - L2_HF_Transformer_PEFT_2gpu_FSDP2
      - L2_HF_Transformer_PEFT_2gpu_FSDP2_liger
      - L2_HF_Transformer_PEFT_2gpu_FSDP2_fp8
      - L2_HF_Transformer_PEFT_2gpu_nemorun
      - L2_HF_Transformer_SFT_notebook
      - L2_HF_Transformer_SFT
      - L2_HF_Transformer_SFT_nemorun
      - L2_HF_Transformer_SFT_2gpu
      - L2_HF_Transformer_SFT_2gpu_FSDP2
      - L2_HF_Transformer_SFT_2gpu_FSDP2_fp8
      - L2_VLM_HF_Transformer_PEFT
      - L2_VLM_HF_Transformer_PEFT_FSDP2
      - L2_VLM_HF_Transformer_PEFT_4bit
      - L2_VLM_HF_Transformer_SFT_FSDP2
      - L2_HF_Transformer_SFT_2gpu_nemorun
      - L2_HF_Transformer_SFT_TE_Acceleration
      - L2_HF_Transformer_PT
      - L2_HF_Transformer_PT_nemorun
      - L2_HF_Transformer_PT_2gpu
      - L2_HF_Transformer_PT_2gpu_nemorun
      - L2_HF_Transformer_PT_TE_Acceleration
      - L2_HF_Transformer_SpeechLM_SFT_2gpu
      - Optional_L2_NeMo_2_SSM_Pretraining
      - Optional_L2_NeMo_2_SSM_Finetuning
      - L2_NeMo_2_Hyena_Conversion_from_HF
      - L2_NeMo_2_Hyena_DDP_Pretraining_Test
      - L2_NeMo_2_T5_Pretraining
      - L2_NeMo_2_T5_Finetuning
      - L2_NeMo_2_T5_Squad
      - L2_NeMo_2_T5_LoRA
      - L2_NeMo_2_T5_MockData_Pretraining
      - L2_NeMo_2_BERT_Pretraining_Megatron
      - L2_NeMo_2_BERT_Pretraining_HuggingFace
      - L2_NeMo_2_GPT_SFT_TP1PP1_MBS1
      - L2_NeMo_2_GPT_SFT_TP1PP1_MBS2
      - L2_NeMo_2_GPT_SFT_TP1PP2_MBS2
      - L2_NeMo_2_GPT_SFT_TP2PP1_MBS2
      - L2_NeMo_2_GPT_SFT_TP1PP1_MBS1_PACKED
      - L2_NeMo_2_GPT_LoRA_TP1PP1_MBS1
      - L2_NeMo_2_GPT_LoRA_TP1PP1_MBS2
      - L2_NeMo_2_GPT_LoRA_TP1PP2_MBS2
      - L2_NeMo_2_GPT_LoRA_TP2PP1_MBS2
      - L2_NeMo_2_GPT_LoRA_TP1PP1_MBS1_Chat
      - L2_NeMo_2_GPT_LoRA_TP1PP1_MBS1_PACKED
      - L2_NeMo_2_GPT_DoRA_TP1PP1_MBS1_PACKED
      - L2_NeMo_2_GPT_CLoRA_TP1PP1_MBS1_PACKED
      - L2_NeMo_2_Mixtral_LoRA_EP2PP1_MBS2
      - L2_NeMo_2_Mixtral_LoRA_TP1PP1_MBS1
      - L2_NeMo_2_Mixtral_LoRA_TP2PP1_MBS1
      - L2_NeMo_2_Mistral_LoRA_TP1PP1_MBS1
      - L2_NeMo_2_Mistral_LoRA_TP2PP1_MBS1
      - L2_NeMo_2_Mistral_LoRA_TP1PP1_MBS1_exclude
      - L2_NeMo_2_Mixtral_LoRA_EP2PP1_MBS2_exclude
      - L2_NEMO_2_LoRA_MERGE
      - L2_NEMO_2_LoRA_Export
      - L2_NEMO_2_LoRA_Inference
      - L2_NeMo_2_Mixtral_Pretraining
      - L2_NeMo_2_Auto_Configurator_llama_TP1_PP1_MBS124
      - L2_NeMo_2_Auto_Configurator_bert_TP1_PP1_MBS124
      - L2_NeMo_2_Auto_Configurator_t5_TP1_PP1_MBS124
      - L2_Speech_to_Text_AED
      - L2_Speech_Estimate_Duration_Bins
      - L2_Speech_Batch_Size_OOMptimizer
        # - Optional_L2_Speech_Batch_Size_OOMptimizer_Canary
      - L2_Speech_Transcription_Canary_Transcribe_Full_Manifest
      - L2_Speech_Transcription_Canary_Transcribe_With_Prompt
      - L2_Speech_Transcription_Canary_Transcribe_Audio_Dir
      - L2_Longform_Speech_Transcription_Canary_Chunked_Infer_from_Audio_Dir
      - L2_Longform_Speech_Transcription_with_TimeStamps_Canary_Chunked_Infer_from_Audio_Dir
      - L2_Longform_Speech_Transcription_with_TimeStamps_Canary_Chunked_Infer_from_Manifest
      - L2_NeMo_2_NeMo_Mcore_Mixtral_bitexact
      - L2_NeMo_2_Automodel_PTQ_trtllm
      - L2_NeMo_2_Automodel_PTQ_hf
      - L2_NeMo_2_PTQ_Llama2_FP8_trtllm
      - L2_NeMo_2_PTQ_Llama2_FP8_nemo
      - L2_NeMo_2_Distill_Llama3_TP1PP2
      - L2_NeMo_2_Prune_Llama_TP1PP2
      - L2_NeMo_2_Export_In_Framework
      - L2_NeMo_2_jit_callback
      - L2_NeMo_2_LLAVA_NEXT_MOCK_TRAINING
      - L2_NeMo_2_VLLM_VISION
      - L2_NeMo_2_LLAVA_NEXT_HF_CONVERSION
      - L2_NeMo_2_LLAVA_NEXT_ENERGON_TRAIN
      - L2_NeMo_2_LLAVA_NEXT_ENERGON_PACKED_TRAIN
      - L2_NeMo_2_CLIP_PRETRAIN
      - L2_NeMo_2_CLIP_INFER
      - L2_HF_Transformer_SFT_FSDP2_2gpu
      - L2_HF_Transformer_SFT_2gpu_nemorun_fsdp2
      - L2_NeMo_2_VLLM_EXPORT
      - Optional_L2_NeMo_2_EVAL
      - L2_SpeechLM_LoRA_TP1PP1_MBS2
      - L2_NeMo_2_Export_HF_TRT_LLM
      - L2_Tron_Llama1B_Train

    if: always() && github.event != 'push'
    runs-on: ubuntu-latest
    permissions: write-all
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Get workflow result
        id: result
        env:
          GH_TOKEN: ${{ github.token }}
          RUN_ID: ${{ github.run_id }}
        run: |
          # Get workflow run details and check job conclusions
          NUM_FAILED=$(gh run view $RUN_ID --json jobs -q '[.jobs[] | select(.conclusion == "failure") | .name] | length')
          NUM_CANCELLED=$(gh run view $RUN_ID --json jobs -q '[.jobs[] | select(.conclusion == "cancelled") | .name] | length')
          NUM_SKIPPED=$(gh run view $RUN_ID --json jobs -q '[.jobs[] | select(.conclusion == "skipped") | .name] | length')
            
          if [[ $NUM_FAILED -eq 0 && $NUM_CANCELLED -eq 0 && $NUM_SKIPPED -eq 0 ]]; then
            RESULT="success"
          elif [[ $NUM_FAILED -eq 0 && $NUM_CANCELLED -gt 0 && $NUM_SKIPPED -eq 0 ]]; then
            RESULT="cancelled"
          else
            RESULT="failure"
          fi

          # Output the final status
          echo "code=$RESULT" | tee -a $GITHUB_OUTPUT

      - name: Checkout for GH CLI
        uses: actions/checkout@v4

      - name: Remove label if not cancelled
        if: ${{ steps.result.outputs.code != 'cancelled' && github.event.label.name == 'Run CICD' && github.event.pull_request.head.repo.full_name == github.repository }}
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ github.event.number }}
        run: gh pr edit "$PR_NUMBER" --remove-label "Run CICD"

      - name: Pipeline successful, add PR comment
        if: ${{ always() && steps.result.outputs.code == 'success' && github.event_name == 'pull_request' && env.SLACK_WEBHOOK != '' }}
        uses: peter-evans/create-or-update-comment@v4
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          REPOSITORY: ${{ github.repository }}
          RUN_ID: ${{ github.run_id }}
        with:
          issue-number: ${{ github.event.number }}
          body: |
            [🤖]: Hi @${{ github.event.pull_request.user.login }} 👋,

            We wanted to let you know that a [CICD pipeline](https://github.com/${{ env.REPOSITORY }}/actions/runs/${{ env.RUN_ID }}) for this PR just finished successfully

            So it might be time to merge this PR or get some approvals

            I'm just a bot so I'll leave it you what to do next.

            //cc @pablo-garay @ko3n1g

      - name: "Pipeline not successful and not cancelled: Send Slack alert & create step summary"
        if: ${{ always() && steps.result.outputs.code == 'failure' && env.SLACK_WEBHOOK != '' }}
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPOSITORY: ${{ github.repository }}
          RUN_ID: ${{ github.run_id }}
          PR_NUMBER: ${{ github.event.number }}
          SERVER_URL: ${{ github.server_url }}
        run: |
          set -x
          pip install PyGithub
          python .github/scripts/notify.py

      - name: Exit
        if: ${{ always() }}
        env:
          RESULT: ${{ steps.result.outputs.code }}
        run: |
          if [ $RESULT == "success" ]; then
            exit 0
          else
            exit 1
          fi

  Coverage:
    runs-on: ubuntu-latest
    needs: [Nemo_CICD_Test]
    strategy:
      matrix:
        flag: [unit-test, e2e]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download coverage reports of current branch
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-${{ matrix.flag }}-*

      - name: Get total coverage of current branch
        shell: bash -x -e -u -o pipefail {0}
        if: always()
        run: |
          pip install coverage

          ls -al .
          ls -al coverage-*/
          coverage combine --keep $(ls coverage-*/.coverage)
          coverage report -i
          rm -rf coverage-*
          ls -al

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          verbose: true
          flags: ${{ matrix.flag }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.flag }}-aggregated
          path: |
            .coverage
          include-hidden-files: true
