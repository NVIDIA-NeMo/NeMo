{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ccdc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import MagpieTTSDecoderModel\n",
    "from nemo.collections.tts.data.text_to_speech_dataset import MagpieTTSDataset, DatasetSample\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5798ac",
   "metadata": {},
   "source": [
    "### Checkpoint Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = \"/datap/misc/experiment_checkpoints/magpie_decoder/Debug2_hparams.yaml\"\n",
    "checkpoint_file = \"/datap/misc/experiment_checkpoints/magpie_decoder/Debug2_epoch13.ckpt\"\n",
    "# codecmodel_path = \"/datap/misc/checkpoints/AudioCodec_21Hz_no_eliz.nemo\"\n",
    "codecmodel_path = \"/datap/misc/checkpoints/21fps_causal_codecmodel.nemo\"\n",
    "\n",
    "\n",
    "# Temp out dir for saving audios\n",
    "out_dir = \"/datap/misc/t5tts_inference_notebook_samples\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf2a16",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf66f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_cfg = OmegaConf.load(hparams_file).cfg\n",
    "\n",
    "with open_dict(model_cfg):\n",
    "    model_cfg.codecmodel_path = codecmodel_path\n",
    "    model_cfg.train_ds = None\n",
    "    model_cfg.validation_ds = None\n",
    "\n",
    "\n",
    "model = MagpieTTSDecoderModel(cfg=model_cfg)\n",
    "print(\"Loading weights from checkpoint\")\n",
    "ckpt = torch.load(checkpoint_file, weights_only=False)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "print(\"Loaded weights.\")\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b5711",
   "metadata": {},
   "source": [
    "### Initialize Dataset class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MagpieTTSDataset(\n",
    "    dataset_meta={},\n",
    "    sample_rate=model.sample_rate,\n",
    "    min_duration=0.5,\n",
    "    max_duration=20,\n",
    "    codec_model_samples_per_frame=model.codec_model_samples_per_frame,\n",
    "    bos_id=None,\n",
    "    eos_id=model.eos_id,\n",
    "    context_audio_bos_id=model.context_audio_bos_id,\n",
    "    context_audio_eos_id=model.context_audio_eos_id,\n",
    "    audio_bos_id=model.audio_bos_id,\n",
    "    audio_eos_id=model.audio_eos_id,\n",
    "    num_audio_codebooks=model.num_audio_codebooks,\n",
    "    prior_scaling_factor=None,\n",
    "    load_cached_codes_if_available=True,\n",
    "    dataset_type='test',\n",
    "    tokenizer_config=None,\n",
    "    load_16khz_audio=False,\n",
    "    use_text_conditioning_tokenizer=True,\n",
    "    pad_context_text_to_max_duration=model.pad_context_text_to_max_duration,\n",
    "    context_duration_min=model.cfg.get('context_duration_min', 5.0),\n",
    "    context_duration_max=model.cfg.get('context_duration_max', 5.0),\n",
    ")\n",
    "test_dataset.text_tokenizer, test_dataset.text_conditioning_tokenizer = model.tokenizer, model.tokenizer.first_tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    with sf.SoundFile(file_path) as audio_file:\n",
    "        # Calculate the duration\n",
    "        duration = len(audio_file) / audio_file.samplerate\n",
    "        return duration\n",
    "\n",
    "def create_record(text, context_audio_filepath=None, context_text=None, ):\n",
    "    dummy_audio_fp = os.path.join(out_dir, \"dummy_audio.wav\")\n",
    "    dummy_audio = sf.write(dummy_audio_fp, np.zeros(22050 * 3), 22050)  # 3 seconds of silence\n",
    "    record = {\n",
    "        'audio_filepath' : \"/datap/misc/LibriTTSfromNemo/LibriTTS/dev-clean/2428/83705/2428_83705_000002_000000.wav\",\n",
    "        'duration': 6.23,\n",
    "        'text': text,\n",
    "        'speaker': \"dummy\",\n",
    "    }\n",
    "    if context_text is not None:\n",
    "        assert context_audio_filepath is None\n",
    "        record['context_text'] = context_text\n",
    "    else:\n",
    "        assert context_audio_filepath is not None\n",
    "        record['context_audio_filepath'] = context_audio_filepath\n",
    "        record['context_audio_duration'] = get_audio_duration(context_audio_filepath)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa7a5a",
   "metadata": {},
   "source": [
    "### Set transcript and context pairs to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7374d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# Change sample text and prompt audio/text here\n",
    "audio_base_dir = \"/\"\n",
    "test_entries = [\n",
    "    create_record(\n",
    "        text=\"Alrighty, this thing seems to be all good.\",\n",
    "        context_audio_filepath=\"/datap/misc/LibriTTSfromNemo/LibriTTS/dev-clean/2428/83705/2428_83705_000002_000000.wav\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "data_samples = []\n",
    "for entry in test_entries:\n",
    "    dataset_sample = DatasetSample(\n",
    "        dataset_name=\"sample\",\n",
    "        manifest_entry=entry,\n",
    "        audio_dir=audio_base_dir,\n",
    "        feature_dir=audio_base_dir,\n",
    "        text=entry['text'],\n",
    "        speaker=None,\n",
    "        speaker_index=0,\n",
    "        tokenizer_names=[\"qwen\"], # Change this for multilingual: \"english_phoneme\", \"spanish_phoneme\", \"english_chartokenizer\", \"german_chartokenizer\".. \n",
    "    )\n",
    "    data_samples.append(dataset_sample)\n",
    "    \n",
    "test_dataset.data_samples = data_samples\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9866b",
   "metadata": {},
   "source": [
    "### Generation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "item_idx = 0\n",
    "for bidx, batch in enumerate(test_data_loader):\n",
    "    batch_cuda ={}\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch_cuda[key] = batch[key].cuda()\n",
    "        else:\n",
    "            batch_cuda[key] = batch[key]\n",
    "    # process_batch_out = model.process_batch(batch_cuda)\n",
    "    predicted_audio, predicted_audio_lens,_,_ = model.infer_batch(batch_cuda,temperature=0.6,max_decoder_steps=430, use_local_transformer_for_inference=True)\n",
    "    for idx in range(predicted_audio.size(0)):\n",
    "        predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "        predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "        audio_path = os.path.join(out_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "        sf.write(audio_path, predicted_audio_np, model.cfg.sample_rate)\n",
    "        print(test_entries[bidx]['text'])\n",
    "        display(Audio(audio_path))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ad924-b862-4a3e-801f-3b9f1faf091f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
