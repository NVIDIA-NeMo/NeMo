{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a87650-d45f-4f90-8f24-23733ce5753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db53ae2c-d91a-4819-be52-69e52c89bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_config = AutoConfig.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3117a637-263c-482a-a272-8fd31828ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = AutoModel.from_config(backend_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e3796e1-fde6-420b-852b-701f413f5af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiModel(\n",
       "  (embed_tokens): Embedding(51200, 2560)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "      )\n",
       "      (mlp): PhiMLP(\n",
       "        (activation_fn): NewGELUActivation()\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (rotary_emb): PhiRotaryEmbedding()\n",
       "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fea1b7b-3ccf-4dcd-8150-119e2a7ee51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module transformers.models.phi.modeling_phi:\n",
      "\n",
      "forward(input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[transformers.cache_utils.Cache] = None, inputs_embeds: Optional[torch.FloatTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, cache_position: Optional[torch.LongTensor] = None, **flash_attn_kwargs: Unpack[transformers.modeling_flash_attention_utils.FlashAttentionKwargs]) -> Union[Tuple, transformers.modeling_outputs.BaseModelOutputWithPast] method of transformers.models.phi.modeling_phi.PhiModel instance\n",
      "    The [`PhiModel`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "\n",
      "    </Tip>\n",
      "\n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
      "            it.\n",
      "\n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "\n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "\n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n",
      "            `past_key_values`).\n",
      "\n",
      "            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n",
      "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
      "            information on the default strategy.\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "            config.n_positions - 1]`.\n",
      "\n",
      "            [What are position IDs?](../glossary#position-ids)\n",
      "        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n",
      "            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n",
      "            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n",
      "\n",
      "            Two formats are allowed:\n",
      "            - a [`~cache_utils.Cache`] instance, see our\n",
      "            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n",
      "            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
      "            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n",
      "            cache format.\n",
      "\n",
      "            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n",
      "            legacy cache format will be returned.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n",
      "            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n",
      "            of shape `(batch_size, sequence_length)`.\n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
      "            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n",
      "            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n",
      "            the complete sequence length.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(transformer.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d96cbb-732d-475e-a20d-2fd5234bd36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████████████████████████████████████████████████| 2/2 [00:50<00:00, 25.31s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████| 2/2 [00:02<00:00,  1.02s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       for j in range(2, i):\n",
      "           if i % j == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(i)\n",
      "   ```\n",
      "\n",
      "2. Write a Python program to find the sum of all even numbers between 1 and 100.\n",
      "\n",
      "   Ideas: Use a for loop to iterate over all numbers between 1 and 100. Use an if statement to check if the number is even. If it is, add it to a running total.\n",
      "\n",
      "   ```python\n",
      "   total = 0\n",
      "   for i in range(1, 101):\n",
      "       if i % 2 == 0:\n",
      "           total += i\n",
      "   print(total)\n",
      "   ```\n",
      "\n",
      "3. Write a Python program to find the largest number in a list.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc60083-ffe5-40ed-981b-aba291595395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(m):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and m\n",
      "   \"\"\"\n",
      "   for i in range(2, m+1):\n",
      "      if is_prime(i):\n",
      "         print(i)\n",
      "\n",
      "print_prime(100)\n",
      "```\n",
      "\n",
      "## Exercises\n",
      "\n",
      "1. Write a Python function to find the nth prime number.\n",
      "\n",
      "Hints:\n",
      "- You can use the `is_prime` function defined above.\n",
      "- You can use a while loop to keep finding primes until you have found n of them.\n",
      "\n",
      "Solution:\n",
      "\n",
      "```python\n",
      "def nth_prime(n):\n",
      "   \"\"\"\n",
      "   Find the nth prime number\n",
      "   \"\"\"\n",
      "   count = 0\n",
      "   num = 2\n",
      "   while count < n:\n",
      "      if is_prime(num):\n",
      "         count += 1\n",
      "      num += 1\n",
      "   return num - 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''def print_prime(m):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and m\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61749e7d-16f9-476a-acb0-9c40bb633324",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a4e6965-112d-48c6-8714-840c79d001e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module transformers.models.phi.modeling_phi:\n",
      "\n",
      "forward(input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Union[transformers.cache_utils.Cache, List[torch.FloatTensor], NoneType] = None, inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, cache_position: Optional[torch.LongTensor] = None, num_logits_to_keep: int = 0, **kwargs: Unpack[transformers.models.phi.modeling_phi.KwargsForCausalLM]) -> Union[Tuple, transformers.modeling_outputs.CausalLMOutputWithPast] method of transformers.models.phi.modeling_phi.PhiForCausalLM instance\n",
      "    The [`PhiForCausalLM`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "\n",
      "    </Tip>\n",
      "\n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
      "            it.\n",
      "\n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "\n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "\n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n",
      "            `past_key_values`).\n",
      "\n",
      "            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n",
      "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
      "            information on the default strategy.\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "            config.n_positions - 1]`.\n",
      "\n",
      "            [What are position IDs?](../glossary#position-ids)\n",
      "        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n",
      "            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n",
      "            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n",
      "\n",
      "            Two formats are allowed:\n",
      "            - a [`~cache_utils.Cache`] instance, see our\n",
      "            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n",
      "            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
      "            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n",
      "            cache format.\n",
      "\n",
      "            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n",
      "            legacy cache format will be returned.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n",
      "            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n",
      "            of shape `(batch_size, sequence_length)`.\n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
      "            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n",
      "            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n",
      "            the complete sequence length.\n",
      "\n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "            num_logits_to_keep (`int`, *optional*):\n",
      "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
      "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
      "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
      "\n",
      "\n",
      "        Returns:\n",
      "            [`transformers.modeling_outputs.CausalLMOutputWithPast`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.CausalLMOutputWithPast`] or a tuple of\n",
      "            `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "            elements depending on the configuration ([`PhiConfig`]) and inputs.\n",
      "\n",
      "            - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss (for next-token prediction).\n",
      "            - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      "            - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "              `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
      "\n",
      "              Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
      "              `past_key_values` input) to speed up sequential decoding.\n",
      "            - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "              one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "              Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "            - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "              sequence_length)`.\n",
      "\n",
      "              Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "              heads.\n",
      "\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, PhiForCausalLM\n",
      "\n",
      "        >>> model = PhiForCausalLM.from_pretrained(\"meta-phi/Phi-2-7b-hf\")\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-phi/Phi-2-7b-hf\")\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ef5426-07ad-4b91-a85b-79a827e6488b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module torch.nn.modules.sparse:\n",
      "\n",
      "forward(input: torch.Tensor) -> torch.Tensor method of torch.nn.modules.sparse.Embedding instance\n",
      "    Define the computation performed at every call.\n",
      "\n",
      "    Should be overridden by all subclasses.\n",
      "\n",
      "    .. note::\n",
      "        Although the recipe for forward pass needs to be defined within\n",
      "        this function, one should call the :class:`Module` instance afterwards\n",
      "        instead of this since the former takes care of running the\n",
      "        registered hooks while the latter silently ignores them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(embedding_layer.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427fdbe0-5772-4ee9-931d-e1e897dcbb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "103fe577-27e2-4352-a1a4-7ef302e50968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ġconflict': 5358,\n",
       " 'ĠMaj': 12390,\n",
       " 'gar': 4563,\n",
       " 'Ġmap': 3975,\n",
       " 'Ġperme': 29298,\n",
       " 'Ġspecializes': 29786,\n",
       " 'Ø³': 45692,\n",
       " 'Ġceased': 24368,\n",
       " 'Ġchickens': 25972,\n",
       " 'Ġselection': 6356,\n",
       " 'abytes': 38346,\n",
       " 'duration': 32257,\n",
       " 'Ġstone': 7815,\n",
       " 'Ġglean': 41881,\n",
       " 'Ġtransit': 11168,\n",
       " 'Ġwond': 3135,\n",
       " 'Ġdiscriminated': 41117,\n",
       " 'ager': 3536,\n",
       " 'omin': 6351,\n",
       " 'square': 23415,\n",
       " 'Ġimprovement': 9025,\n",
       " 'Ġopenings': 28431,\n",
       " 'Ġmessenger': 31228,\n",
       " 'ĠFi': 23238,\n",
       " '802': 30863,\n",
       " 'rentice': 20098,\n",
       " 'ĠBuild': 10934,\n",
       " 'Ban': 30457,\n",
       " 'ĠDise': 14865,\n",
       " 'ĠTrails': 40076,\n",
       " 'ĠJSON': 19449,\n",
       " 'Leaks': 17874,\n",
       " 'ggie': 23571,\n",
       " 'ĠTours': 42998,\n",
       " 'ĠInform': 45255,\n",
       " 'ogether': 8236,\n",
       " 'ĠSle': 19498,\n",
       " 'Ġgum': 27142,\n",
       " 'Ma': 21467,\n",
       " 'buster': 24899,\n",
       " 'ĠDove': 48595,\n",
       " 'wan': 8149,\n",
       " 'ĠResist': 36136,\n",
       " 'Ġacquired': 9477,\n",
       " 'opening': 29443,\n",
       " 'Ġauthorization': 19601,\n",
       " 'Subject': 19776,\n",
       " 'wei': 42990,\n",
       " 'ĠBarcelona': 15142,\n",
       " 'ĠStartup': 40472,\n",
       " 'Ġperception': 11202,\n",
       " 'Ġmaintain': 5529,\n",
       " 'Ġlevels': 2974,\n",
       " 'Ġaveraging': 20430,\n",
       " 'Ġfuzzy': 34669,\n",
       " 'Ġshortfall': 41688,\n",
       " 'Ġold': 1468,\n",
       " 'hus': 7537,\n",
       " 'ĠThumbnails': 28924,\n",
       " 'nan': 12647,\n",
       " 'gu': 5162,\n",
       " '565': 47372,\n",
       " 'INC': 30158,\n",
       " 'ophys': 39665,\n",
       " 'ĠDevil': 10489,\n",
       " 'ĠLug': 31541,\n",
       " 'ĠMonroe': 26249,\n",
       " 'Ġignorance': 17010,\n",
       " 'đ': 205,\n",
       " 'Ġlearners': 46184,\n",
       " 'Ġoverhaul': 18708,\n",
       " 'Ġredesigned': 36240,\n",
       " 'ĠBow': 9740,\n",
       " 'ogeneity': 37477,\n",
       " '44': 2598,\n",
       " 'ramid': 20255,\n",
       " 'Ġhence': 12891,\n",
       " 'ĠGh': 11972,\n",
       " 'Ġmonth': 1227,\n",
       " 'Ġtub': 12202,\n",
       " 'Mic': 25437,\n",
       " 'Ġrepr': 41575,\n",
       " 'ĠSpirits': 30141,\n",
       " 'lie': 14485,\n",
       " 'Die': 32423,\n",
       " 'stay': 31712,\n",
       " 'Ġcaffeine': 22511,\n",
       " 'Ġcervical': 36012,\n",
       " 'Ġmined': 35959,\n",
       " 'Ġsprites': 42866,\n",
       " 'Ġminds': 9017,\n",
       " 'ĠSteve': 6542,\n",
       " 'idel': 5943,\n",
       " 'Ġliberated': 35112,\n",
       " 'Ġthr': 5636,\n",
       " 'Ġuncertain': 8627,\n",
       " 'ĠLegends': 14270,\n",
       " 'OSS': 18420,\n",
       " 'ĠUSPS': 44640,\n",
       " 'Ns': 47503,\n",
       " 'Ġalarm': 10436,\n",
       " 'Ġpenis': 16360,\n",
       " 'Ġwarn': 9828,\n",
       " 'Ġwelf': 9468,\n",
       " '$.': 35307,\n",
       " 'seless': 10950,\n",
       " 'ifact': 29660,\n",
       " 'Ġassemb': 11156,\n",
       " 'essional': 12743,\n",
       " '                ': 50272,\n",
       " '\\\\-': 41441,\n",
       " 'ĠSiber': 28394,\n",
       " 'ĠMHz': 19805,\n",
       " 'Ġfluent': 43472,\n",
       " 'Ġinv': 800,\n",
       " 'Ġundergraduate': 22952,\n",
       " 'Ġperce': 36974,\n",
       " 'azeera': 28535,\n",
       " 'ĠImport': 17267,\n",
       " 'Ġcheek': 19353,\n",
       " 'California': 25284,\n",
       " 'Ġinconsistencies': 40467,\n",
       " 'ĠLady': 11182,\n",
       " 'Ġlib': 9195,\n",
       " 'ĠKuro': 25796,\n",
       " 'ĠTie': 36286,\n",
       " 'Ġoxidative': 37736,\n",
       " 'ĠSole': 37965,\n",
       " 'Ġspaced': 38980,\n",
       " 'Ġsuspicion': 15123,\n",
       " 'Ġagendas': 37611,\n",
       " 'ĠWrestling': 29662,\n",
       " 'currently': 41745,\n",
       " 'keye': 34929,\n",
       " 'ĠHayward': 41960,\n",
       " 'ĠWind': 3086,\n",
       " 'Ġarmed': 6936,\n",
       " 'Ġdotted': 38745,\n",
       " 'illes': 21718,\n",
       " 'ĠConcent': 37613,\n",
       " 'Ġfer': 11354,\n",
       " 'ĠDNS': 18538,\n",
       " 'Ġtouchscreen': 39965,\n",
       " 'Ġanalytical': 30063,\n",
       " 'ĠLima': 41578,\n",
       " 'aled': 3021,\n",
       " 'Ġcom': 401,\n",
       " 'que': 4188,\n",
       " 'Ġexposures': 32185,\n",
       " 'Ġneurotrans': 39307,\n",
       " 'jc': 48055,\n",
       " 'Ġtemper': 4124,\n",
       " 'Ġres': 581,\n",
       " 'ĠHammond': 26649,\n",
       " 'Ġgrids': 50000,\n",
       " 'Ġwoo': 36440,\n",
       " '163': 24136,\n",
       " 'ĠCommands': 49505,\n",
       " 'ĠMemorial': 14861,\n",
       " 'Ġanalys': 11090,\n",
       " 'Ġsympathy': 20242,\n",
       " 'Qaeda': 19058,\n",
       " 'ĠLime': 43503,\n",
       " 'Ġinsidious': 43732,\n",
       " 'Ġconference': 4495,\n",
       " 'Ġpatched': 39378,\n",
       " '=[': 41888,\n",
       " 'ality': 1483,\n",
       " 'mods': 24122,\n",
       " 'Ġascertain': 35520,\n",
       " 'Ġdepressive': 36568,\n",
       " 'enfranch': 39827,\n",
       " 'Ġfares': 33164,\n",
       " 'Ġmodify': 13096,\n",
       " 'ĠCors': 26978,\n",
       " 'ĠJudah': 49931,\n",
       " 'ĠKazakhstan': 34474,\n",
       " 'Ġadvances': 14901,\n",
       " 'adders': 45940,\n",
       " 'Ġmicrowave': 27000,\n",
       " 'ĠHamas': 15767,\n",
       " 'ĠNarr': 28390,\n",
       " 'Ġplanned': 6027,\n",
       " 'Ġberth': 45015,\n",
       " 'ession': 2521,\n",
       " 'ographs': 33492,\n",
       " 'Length': 24539,\n",
       " 'biology': 43592,\n",
       " 'Ġbuffet': 44703,\n",
       " 'Ġsa': 473,\n",
       " 'Ġsmack': 41014,\n",
       " 'Ġsusceptibility': 43304,\n",
       " 'bb': 11848,\n",
       " 'dim': 27740,\n",
       " 'Ġvicious': 17686,\n",
       " 'ãĥ¼ãĥĨãĤ£': 44686,\n",
       " 'endez': 41913,\n",
       " 'Ġ321': 39595,\n",
       " 'Ġreceive': 3328,\n",
       " 'ĠMISS': 49684,\n",
       " 'ĠReloaded': 33114,\n",
       " 'Ġdisruptive': 28094,\n",
       " 'Ġwine': 8237,\n",
       " 'ground': 2833,\n",
       " 'Ġaffecting': 13891,\n",
       " 'covered': 32111,\n",
       " 'SK': 18831,\n",
       " 'itted': 2175,\n",
       " 'ird': 1447,\n",
       " 'ĠGor': 19097,\n",
       " 'Ġpainted': 13055,\n",
       " 'INA': 28893,\n",
       " 'IENT': 28495,\n",
       " 'âĢ¦': 1399,\n",
       " 'ĠEsp': 20386,\n",
       " 'Ġfeaturing': 9593,\n",
       " 'olester': 15764,\n",
       " 'ĠWAS': 21725,\n",
       " 'ya': 3972,\n",
       " '602': 31418,\n",
       " 'ĠTome': 46403,\n",
       " 'Ġconvoluted': 47370,\n",
       " 'Ġparked': 19584,\n",
       " 'calling': 44714,\n",
       " 'Ġtopp': 23126,\n",
       " 'Ġvector': 15879,\n",
       " 'orous': 9610,\n",
       " 'ĠPersonally': 31342,\n",
       " 'Ġfunded': 10588,\n",
       " 'oggles': 48549,\n",
       " 'ĠJoseph': 7212,\n",
       " 'ĠYuk': 19760,\n",
       " 'Ġfences': 35771,\n",
       " 'ricting': 42870,\n",
       " 'ĠByrne': 38245,\n",
       " 'YP': 48232,\n",
       " 'ĠBeaut': 13711,\n",
       " 'ĠOmni': 44251,\n",
       " 'ge': 469,\n",
       " 'Ġfourteen': 29167,\n",
       " 'utical': 14224,\n",
       " 'Ġconscious': 6921,\n",
       " 'Rick': 33048,\n",
       " 'vv': 25093,\n",
       " 'Muslims': 36452,\n",
       " 'ĠHuang': 31663,\n",
       " 'Ġirresistible': 45420,\n",
       " 'Ġjan': 42897,\n",
       " 'Ġreductions': 20691,\n",
       " 'ĠLeaves': 46597,\n",
       " 'ĠEssentially': 34039,\n",
       " 'SM': 12310,\n",
       " 'Ġknots': 33620,\n",
       " 'Ġpotential': 2785,\n",
       " 'Ġstagnant': 41391,\n",
       " 'South': 14942,\n",
       " 'ĠUzbek': 42619,\n",
       " 'ĠWedding': 36679,\n",
       " 'Ġstays': 14768,\n",
       " 'Ġrescind': 39091,\n",
       " 'Availability': 29841,\n",
       " 'ĠBugs': 44991,\n",
       " 'Ġherds': 50040,\n",
       " 'Ġtruck': 7779,\n",
       " 'cost': 15805,\n",
       " 'ĠLock': 13656,\n",
       " 'ĠMF': 32850,\n",
       " 'Ġfriendly': 8030,\n",
       " 'Ġpermit': 8749,\n",
       " 'ag': 363,\n",
       " 'Ġlegalized': 33071,\n",
       " 'yrus': 21180,\n",
       " 'ĠHip': 29437,\n",
       " 'ousands': 19983,\n",
       " 'Ġwondered': 14028,\n",
       " 'abet': 8380,\n",
       " 'Ġorient': 11367,\n",
       " 'Ġprovisions': 8617,\n",
       " 'Ġtorch': 28034,\n",
       " '>.': 28401,\n",
       " '000000': 10535,\n",
       " 'ĠKaepernick': 30112,\n",
       " 'ĠShock': 22763,\n",
       " 'ĠCharlottesville': 24756,\n",
       " 'ĠYORK': 28154,\n",
       " 'oli': 11106,\n",
       " 'Ġ86': 9849,\n",
       " 'ĠMidnight': 27960,\n",
       " 'Ġassemble': 25432,\n",
       " 'ĠShed': 49340,\n",
       " 'phys': 34411,\n",
       " 'ĠKinect': 48879,\n",
       " 'ldom': 23826,\n",
       " 'ĠConspiracy': 38786,\n",
       " 'ĠNickel': 38398,\n",
       " 'ĠWheat': 34744,\n",
       " 'ĠRNC': 37570,\n",
       " 'North': 14157,\n",
       " 'ĠChem': 12870,\n",
       " 'Ġcane': 33009,\n",
       " 'ĠZucker': 27629,\n",
       " 'ãĤ§': 24806,\n",
       " 'ĠKee': 19799,\n",
       " 'Ġclandestine': 39903,\n",
       " 'hover': 43753,\n",
       " 'Ġdisco': 4655,\n",
       " 'ĠBars': 39924,\n",
       " 'ĠIn': 554,\n",
       " 'wing': 5469,\n",
       " 'Ġmanner': 5642,\n",
       " 'Ġpinpoint': 30534,\n",
       " 'Ġproblem': 1917,\n",
       " 'Ġskeleton': 18328,\n",
       " 'Ø§': 12919,\n",
       " 'Ġentertain': 8204,\n",
       " 'Ġvalve': 22580,\n",
       " 'amed': 2434,\n",
       " 'Ġvibrations': 38071,\n",
       " 'ĠSpeedway': 46000,\n",
       " 'RNA': 27204,\n",
       " 'rain': 3201,\n",
       " 'Ġassoci': 2570,\n",
       " 'Ġunderstatement': 46196,\n",
       " 'ĠHab': 19654,\n",
       " 'Ġcalculated': 10488,\n",
       " 'Ġem': 795,\n",
       " 'Ġliabilities': 25333,\n",
       " 'Ġ`': 4600,\n",
       " 'unt': 2797,\n",
       " 'Ġwere': 547,\n",
       " 'Ġwave': 6769,\n",
       " '              ': 50274,\n",
       " 'rounded': 39262,\n",
       " 'Ġfinding': 4917,\n",
       " 'Ġlockdown': 47955,\n",
       " '370': 20167,\n",
       " 'ILE': 41119,\n",
       " 'ĠFei': 39575,\n",
       " '803': 43564,\n",
       " 'ĠNotification': 42808,\n",
       " 'ĠSpread': 31843,\n",
       " 'Ġlegendary': 13273,\n",
       " 'nir': 32986,\n",
       " 'Ġcelestial': 33258,\n",
       " 'Ġbroke': 6265,\n",
       " 'Compan': 41309,\n",
       " 'ĠUsually': 19672,\n",
       " 'ĠFaw': 46081,\n",
       " 'ĠPerhaps': 8673,\n",
       " 'Ġhorrific': 19447,\n",
       " 'Ġtrillion': 12989,\n",
       " 'Wa': 33484,\n",
       " 'ĠOsiris': 40925,\n",
       " 'ĠPRE': 22814,\n",
       " 'ceptive': 25867,\n",
       " 'Ġfn': 24714,\n",
       " 'Ġresponsible': 4497,\n",
       " 'separ': 25512,\n",
       " 'ĠLok': 36861,\n",
       " 'PA': 4537,\n",
       " 'ĠBale': 43248,\n",
       " 'udence': 42581,\n",
       " 'urdue': 30345,\n",
       " 'ĠHulu': 39739,\n",
       " 'þ': 186,\n",
       " 'Ġantibiotic': 29883,\n",
       " 'ias': 4448,\n",
       " 'ĠFigures': 36574,\n",
       " 'ĠMali': 29599,\n",
       " 'Ġcapacity': 5339,\n",
       " 'Ġendorse': 11438,\n",
       " 'CD': 8610,\n",
       " 'Ġenz': 26365,\n",
       " 'ses': 8448,\n",
       " 'Ġbiology': 17219,\n",
       " 'Ġcoined': 33115,\n",
       " 'Ġglance': 16086,\n",
       " 'Ġoffic': 1163,\n",
       " 'Ġounce': 25799,\n",
       " 'Ġmanag': 2358,\n",
       " 'Ġsi': 33721,\n",
       " 'aspx': 31740,\n",
       " 'ARM': 33456,\n",
       " 'ĠNETWORK': 49791,\n",
       " 'Ġserves': 9179,\n",
       " 'Ġconce': 8571,\n",
       " 'Ġus': 514,\n",
       " 'ě': 215,\n",
       " 'Afee': 44314,\n",
       " 'CHAPTER': 41481,\n",
       " 'Ġadjustable': 28138,\n",
       " 'ONSORED': 36406,\n",
       " 'proxy': 36436,\n",
       " 'note': 11295,\n",
       " 'Ġrap': 4095,\n",
       " '379': 29088,\n",
       " 'ĠBroadcasting': 32250,\n",
       " 'ĠSing': 5573,\n",
       " 'Ġoccasionally': 10491,\n",
       " 'etheless': 12845,\n",
       " 'Ġpl': 458,\n",
       " 'aspers': 49412,\n",
       " 'Ġdisciple': 35567,\n",
       " 'ĠKara': 34856,\n",
       " 'ĠResults': 15691,\n",
       " '!,': 28265,\n",
       " 'eta': 17167,\n",
       " 'ĠDong': 28831,\n",
       " 'Ġacquaintance': 35552,\n",
       " 'iths': 47252,\n",
       " 'Pal': 11531,\n",
       " 'ĠBo': 3248,\n",
       " 'antly': 3875,\n",
       " 'feeding': 22824,\n",
       " 'ĠLimbaugh': 43306,\n",
       " 'ĠNotes': 11822,\n",
       " 'ĠProxy': 38027,\n",
       " 'ĠRichardson': 21679,\n",
       " 'cern': 30903,\n",
       " 'ĠBaal': 39648,\n",
       " 'ĠISPs': 28677,\n",
       " 'Ġclip': 10651,\n",
       " 'vm': 14761,\n",
       " 'umen': 20080,\n",
       " 'ĠKeane': 46160,\n",
       " 'ĠMish': 39523,\n",
       " '458': 29334,\n",
       " 'ĠEllie': 44801,\n",
       " 'Ġconstituent': 39384,\n",
       " 'Ġentrenched': 32472,\n",
       " 'Ġpropensity': 41121,\n",
       " 'Ġreproductive': 18391,\n",
       " 'Ġsorrow': 24140,\n",
       " 'iery': 23012,\n",
       " 'Ġthinker': 45206,\n",
       " 'Ġanalyzed': 15475,\n",
       " 'fix': 13049,\n",
       " 'Never': 12295,\n",
       " 'Unfortunately': 13898,\n",
       " 'ĠCorrection': 35074,\n",
       " 'ĠProtesters': 46574,\n",
       " 'Ġgood': 922,\n",
       " 'ĠCJ': 27731,\n",
       " 'Broad': 30507,\n",
       " 'Ġchase': 15505,\n",
       " 'Ġradius': 16874,\n",
       " 'Ġauditor': 30625,\n",
       " 'Ġmunicipality': 27264,\n",
       " 'izers': 11341,\n",
       " 'mund': 20125,\n",
       " 'Ġcolonial': 17091,\n",
       " 'åĲ': 28938,\n",
       " 'inki': 38799,\n",
       " 'aciously': 45289,\n",
       " 'ĠGust': 26657,\n",
       " 'Ġofferings': 18369,\n",
       " 'umble': 10344,\n",
       " 'Ġstray': 28583,\n",
       " 'Ġdive': 15647,\n",
       " 'Inc': 25517,\n",
       " 'Ġacquitted': 35497,\n",
       " 'APS': 44580,\n",
       " 'aire': 7626,\n",
       " 'Ġgameplay': 11327,\n",
       " 'Ġio': 33245,\n",
       " 'Ġdissolved': 26306,\n",
       " 'Ġomission': 35725,\n",
       " 'ĠPHP': 19599,\n",
       " 'Ġimg': 33705,\n",
       " 'Ġpleasing': 28790,\n",
       " 'Ġtargeting': 10822,\n",
       " 'lesh': 29730,\n",
       " 'Ġanecdotal': 41666,\n",
       " 'Ġgoblins': 46220,\n",
       " 'ĠShaman': 34805,\n",
       " 'Ġwasteland': 46045,\n",
       " 'Ġdisorder': 8967,\n",
       " 'arian': 3699,\n",
       " 'ĠMercenary': 49969,\n",
       " 'bt': 18347,\n",
       " 'enged': 47422,\n",
       " 'ĠAppropri': 29857,\n",
       " 'ĠSparrow': 49745,\n",
       " 'Ġcircumstance': 25179,\n",
       " 'ĠSwamp': 29114,\n",
       " 'Ġminimizing': 41366,\n",
       " 'Ġtwe': 4184,\n",
       " 'Fort': 21926,\n",
       " 'ĠChest': 25544,\n",
       " 'ĠCongratulations': 49185,\n",
       " 'Ġdb': 20613,\n",
       " 'Ġcondensed': 38784,\n",
       " 'Ġhurried': 31662,\n",
       " 'sonian': 35202,\n",
       " 'ĠSatisf': 48168,\n",
       " 'Ġnails': 23361,\n",
       " 'ĠISP': 33086,\n",
       " 'ĠRudd': 36131,\n",
       " 'iquette': 40387,\n",
       " 'Ġcontraception': 27503,\n",
       " 'Yu': 40728,\n",
       " 'Ġinvitations': 42851,\n",
       " 'Ġrug': 14477,\n",
       " 'Ġreach': 3151,\n",
       " 'Ġgenetic': 8513,\n",
       " 'ania': 5411,\n",
       " 'selage': 45217,\n",
       " 'uish': 32091,\n",
       " 'ĠOverse': 46864,\n",
       " 'Ġaddressed': 9469,\n",
       " 'Ġembryos': 39966,\n",
       " 'Ġkillings': 17709,\n",
       " 'pak': 41091,\n",
       " 'Ġme': 502,\n",
       " 'Ġought': 10783,\n",
       " 'Ġverifying': 45505,\n",
       " 'ĠADV': 43685,\n",
       " 'Ġvex': 41548,\n",
       " 'ĠâĢº': 37855,\n",
       " 'Ġheroes': 10281,\n",
       " 'Ġfurnace': 42227,\n",
       " 'arr': 3258,\n",
       " 'Ġ\"-': 27444,\n",
       " 'Ġheater': 39844,\n",
       " 'aptop': 45007,\n",
       " 'Ġloaded': 9639,\n",
       " 'ĠEnhanced': 22104,\n",
       " 'Item': 7449,\n",
       " 'ĠKent': 8758,\n",
       " 'Ġnighttime': 45324,\n",
       " 'Under': 9203,\n",
       " 'train': 27432,\n",
       " 'Ġpartially': 12387,\n",
       " 'onnaissance': 31539,\n",
       " 'killing': 43764,\n",
       " 'usercontent': 43667,\n",
       " 'Ġsets': 5621,\n",
       " 'ĠAMC': 36239,\n",
       " 'Ġcov': 39849,\n",
       " 'izations': 4582,\n",
       " 'Ġsignify': 44078,\n",
       " 'Ġties': 8470,\n",
       " 'Ġmedals': 28057,\n",
       " 'elist': 46331,\n",
       " 'ĠEarlier': 20635,\n",
       " 'dem': 9536,\n",
       " 'ĠPetition': 43723,\n",
       " 'nu': 28803,\n",
       " 'Ġbasket': 7988,\n",
       " 'Ġarcane': 34362,\n",
       " 'ĠCaller': 10244,\n",
       " 'Ġhur': 6990,\n",
       " 'ĠHarmon': 39712,\n",
       " 'Abyss': 49073,\n",
       " 'ience': 1240,\n",
       " 'Ġmoist': 13394,\n",
       " 'Ġmigrated': 40227,\n",
       " 'ĠShepherd': 30890,\n",
       " 'Ġoffensive': 5859,\n",
       " 'Ġrepayment': 36411,\n",
       " 'eely': 45269,\n",
       " 'ĠMasonic': 47237,\n",
       " 'Ġattendants': 46337,\n",
       " 'Ġfuturistic': 36701,\n",
       " 'highly': 47444,\n",
       " 'Ġcook': 4255,\n",
       " 'inflamm': 29639,\n",
       " 'Ġstandalone': 27669,\n",
       " 'ku': 23063,\n",
       " 'Ġbaseball': 9283,\n",
       " 'Ġtopping': 34366,\n",
       " 'ĠCanada': 3340,\n",
       " 'adows': 9797,\n",
       " 'charges': 34948,\n",
       " 'Ġnegligent': 42837,\n",
       " 'ĠBulletin': 30551,\n",
       " 'ĠChanges': 19179,\n",
       " 'popular': 47568,\n",
       " 'Ġclimax': 30032,\n",
       " 'Peter': 19727,\n",
       " 'ĠZeit': 47447,\n",
       " 'ĠSamsung': 10397,\n",
       " 'Ġleads': 5983,\n",
       " 'iership': 36689,\n",
       " 'ĠZealand': 8936,\n",
       " 'take': 20657,\n",
       " 'tt': 926,\n",
       " 'Or': 5574,\n",
       " 'ĠCaribbean': 18020,\n",
       " 'Cru': 27535,\n",
       " 'Planet': 41801,\n",
       " '×Ķ': 38269,\n",
       " 'Ġ($)': 45491,\n",
       " 'Ġoverl': 12893,\n",
       " 'Beer': 49802,\n",
       " 'iscover': 29392,\n",
       " 'ĠRugby': 26244,\n",
       " 'ĠSporting': 31790,\n",
       " 'Ġrubber': 14239,\n",
       " 'Hell': 28254,\n",
       " 'Ġforms': 5107,\n",
       " 'CAN': 44565,\n",
       " 'alez': 22149,\n",
       " 'oris': 37279,\n",
       " 'ĠPresident': 1992,\n",
       " 'thren': 25941,\n",
       " 'gone': 21260,\n",
       " 'Davis': 36462,\n",
       " 'ctrl': 44755,\n",
       " 'izarre': 12474,\n",
       " 'ĠIm': 1846,\n",
       " 'ĠNoon': 41035,\n",
       " 'Creating': 32071,\n",
       " 'oved': 2668,\n",
       " 'ĠIntern': 2445,\n",
       " 'ĠJoker': 23582,\n",
       " 'aha': 12236,\n",
       " 'wald': 21667,\n",
       " 'ĠProf': 4415,\n",
       " 'ĠTues': 48496,\n",
       " 'ĠWow': 24755,\n",
       " 'Ġsavior': 47921,\n",
       " 'ĠGeorge': 4502,\n",
       " 'ĠHowever': 2102,\n",
       " 'Ġinterviews': 9299,\n",
       " 'Ġtiers': 33355,\n",
       " 'ymm': 26621,\n",
       " 'ĠAmmo': 31073,\n",
       " 'ĠAnk': 24792,\n",
       " 'Ġvictories': 19017,\n",
       " 'Ġexplosive': 13835,\n",
       " 'axis': 22704,\n",
       " 'Ġdiabetes': 12593,\n",
       " '616': 44214,\n",
       " 'gged': 11178,\n",
       " 'ĠAman': 42614,\n",
       " 'Ġapproves': 43770,\n",
       " 'Version': 14815,\n",
       " 'Ġcube': 23441,\n",
       " 'ĠHue': 31788,\n",
       " 'ĠDistrict': 5665,\n",
       " 'mology': 29126,\n",
       " 'ĠWarlock': 34787,\n",
       " 'ĠWhis': 28424,\n",
       " 'ĠGets': 29620,\n",
       " 'Ġscandals': 28449,\n",
       " 'ipples': 27844,\n",
       " 'Ġtheor': 18765,\n",
       " 'Buff': 36474,\n",
       " 'Ġtorture': 11543,\n",
       " 'ustainable': 24196,\n",
       " 'Ġextreme': 3257,\n",
       " 'Oil': 44142,\n",
       " 'actionDate': 31538,\n",
       " 'Ġstretch': 7539,\n",
       " 'iries': 18561,\n",
       " 'Ġspinach': 39129,\n",
       " 'ĠShares': 23997,\n",
       " 'ĠCoco': 48222,\n",
       " 'Ġvivid': 21002,\n",
       " 'ulia': 43640,\n",
       " 'ĠEmerson': 41886,\n",
       " 'Ġcountering': 50043,\n",
       " 'Ġlooting': 39473,\n",
       " 'ĠPDF': 12960,\n",
       " 'ĠAssociate': 22669,\n",
       " 'isp': 8802,\n",
       " 'Ġimagery': 19506,\n",
       " 'Ġmedication': 14103,\n",
       " 'ĠSUR': 41016,\n",
       " 'ĠURLs': 32336,\n",
       " 'cise': 37561,\n",
       " 'Ġquotations': 50056,\n",
       " 'Ġcheating': 21608,\n",
       " 'RAM': 24115,\n",
       " 'Ġafflicted': 39785,\n",
       " 'Laun': 46182,\n",
       " 'Myth': 41444,\n",
       " 'ĠMeredith': 40378,\n",
       " 'ĠYak': 30254,\n",
       " 'Ġbulb': 28287,\n",
       " 'From': 4863,\n",
       " 'Ġadvancement': 27647,\n",
       " 'cery': 12757,\n",
       " 'ĠBlaster': 33406,\n",
       " 'ĠLH': 49730,\n",
       " 'Ġcents': 16059,\n",
       " 'Ġfreshmen': 45208,\n",
       " 'Ġselfie': 38704,\n",
       " 'Ġsite': 2524,\n",
       " 'Ġstable': 8245,\n",
       " 'ĠDoc': 14432,\n",
       " 'Ġswim': 9422,\n",
       " 'oubtedly': 16423,\n",
       " 'Ġbeverages': 24173,\n",
       " 'Ġfram': 5346,\n",
       " 'Ġpriceless': 49083,\n",
       " 'ĠSagan': 49381,\n",
       " 'sb': 36299,\n",
       " 'planned': 36800,\n",
       " 'alam': 44949,\n",
       " 'ĠBeast': 10984,\n",
       " 'hedral': 21962,\n",
       " 'Ġinfiltrated': 43862,\n",
       " 'Ġlic': 3476,\n",
       " 'Ġoffensively': 41774,\n",
       " 'Ġphysicists': 37898,\n",
       " 'Ġspectacular': 15013,\n",
       " 'Ġcommenters': 43807,\n",
       " 'Ġespresso': 48247,\n",
       " 'uren': 23532,\n",
       " 'Ġsuper': 2208,\n",
       " 'Ġmountain': 8598,\n",
       " 'classic': 49421,\n",
       " 'gate': 10494,\n",
       " 'Ġrooting': 40105,\n",
       " 'Alert': 36420,\n",
       " 'Ġbillionaires': 34740,\n",
       " 'Ġgeneral': 2276,\n",
       " 'iture': 8089,\n",
       " 'ĠReese': 39929,\n",
       " 'ĠMai': 36709,\n",
       " 'Ġlun': 14678,\n",
       " 'Ġimitation': 40260,\n",
       " 'Ġlearnt': 26338,\n",
       " 'E': 36,\n",
       " 'Ġobjects': 5563,\n",
       " 'Root': 30016,\n",
       " 'cycles': 32503,\n",
       " ';;': 7665,\n",
       " 'ef': 891,\n",
       " 'idd': 1638,\n",
       " 'Ġmoved': 3888,\n",
       " 'Ġpaintings': 21641,\n",
       " 'Bird': 42562,\n",
       " 'bsp': 24145,\n",
       " 'oxic': 18047,\n",
       " 'astrous': 20168,\n",
       " 'mer': 647,\n",
       " 'ĠWool': 24759,\n",
       " 'oxin': 39366,\n",
       " 'azing': 4070,\n",
       " 'ĠSoc': 3345,\n",
       " 'Ġdin': 16278,\n",
       " 'arkin': 39027,\n",
       " 'ĠOkin': 41020,\n",
       " 'lia': 24660,\n",
       " 'odynam': 24319,\n",
       " 'ĠPremiership': 47435,\n",
       " 'Ġanim': 2355,\n",
       " 'hibition': 24108,\n",
       " 'Ġbitterly': 40608,\n",
       " 'Ġenacted': 17814,\n",
       " 'Ġguaranteed': 11462,\n",
       " 'Ġmonks': 27646,\n",
       " 'Ġamid': 10371,\n",
       " 'Ġrats': 13623,\n",
       " 'ĠStew': 12194,\n",
       " 'Movie': 25097,\n",
       " 'Ġbids': 27837,\n",
       " 'Ġcontribute': 8676,\n",
       " 'Ġremembering': 24865,\n",
       " 'vik': 28930,\n",
       " 'ĠAugustine': 36158,\n",
       " 'ITNESS': 46144,\n",
       " 'ĠGuardians': 19752,\n",
       " 'ĠYuri': 38450,\n",
       " 'ENSE': 24290,\n",
       " 'Ġinvestments': 11115,\n",
       " 'Ġultraviolet': 49961,\n",
       " 'Ġreluctantly': 35462,\n",
       " 'Ġdestruct': 15256,\n",
       " 'Ġupstairs': 26148,\n",
       " 'hea': 21632,\n",
       " 'portion': 16864,\n",
       " 'Ġmer': 4017,\n",
       " 'Ġpreservation': 22459,\n",
       " 'Wood': 22911,\n",
       " 'ĠDiscussion': 27766,\n",
       " 'Ġscene': 3715,\n",
       " 'Ġo': 267,\n",
       " 'ICAL': 20151,\n",
       " 'Ġ}}': 34949,\n",
       " 'common': 11321,\n",
       " 'Ġasserted': 21635,\n",
       " 'Ġlighting': 12019,\n",
       " 'Ġfragrance': 36860,\n",
       " 'Ġmarked': 7498,\n",
       " 'ĠAnim': 11586,\n",
       " 'Ġshatter': 45131,\n",
       " 'ĠPF': 28223,\n",
       " 'uably': 14632,\n",
       " 'ĠSta': 44919,\n",
       " 'Ġhazard': 15834,\n",
       " 'Ġresults': 2482,\n",
       " 'Ġsorcery': 47815,\n",
       " 'ĠAviv': 28890,\n",
       " 'Ġstared': 18484,\n",
       " 'abb': 6485,\n",
       " 'Ġstay': 2652,\n",
       " 'ĠViolence': 20908,\n",
       " 'Assuming': 48142,\n",
       " 'Ġtaps': 34531,\n",
       " 'ĠReleased': 28728,\n",
       " 'ĠEat': 27574,\n",
       " '154': 21526,\n",
       " 'ĠEarn': 22535,\n",
       " 'Ġdetach': 48224,\n",
       " 'Result': 23004,\n",
       " 'ĠATI': 38344,\n",
       " 'Ġmah': 42768,\n",
       " 'wrote': 42910,\n",
       " 'Ġforces': 3386,\n",
       " 'Stay': 25681,\n",
       " '231': 25667,\n",
       " 'Ġdialect': 23637,\n",
       " 'ĠStores': 41835,\n",
       " 'Recommend': 41248,\n",
       " '994': 42691,\n",
       " 'ismo': 44126,\n",
       " 'Ġsmooth': 7209,\n",
       " 'Ġspared': 31348,\n",
       " 'Ġentitle': 44594,\n",
       " 'ĠGhana': 32825,\n",
       " 'Ġsuggests': 5644,\n",
       " 'Ġtariff': 36427,\n",
       " 'Ġpositives': 38548,\n",
       " 'Disk': 40961,\n",
       " 'ĠMetallic': 38037,\n",
       " 'Ġsequest': 46314,\n",
       " 'apy': 12826,\n",
       " 'Interestingly': 33092,\n",
       " 'female': 24724,\n",
       " 'ĠWW': 13505,\n",
       " 'ottenham': 21889,\n",
       " 'Rose': 31087,\n",
       " 'Ġpeaceful': 12309,\n",
       " 'Ġpsychological': 10590,\n",
       " 'ĠKik': 40571,\n",
       " 'ĠJ': 449,\n",
       " 'Spons': 43522,\n",
       " 'ĠRai': 42954,\n",
       " 'Ġbuilt': 3170,\n",
       " 'Ġfidelity': 37744,\n",
       " 'ÃĥÃĤÃĥÃĤ': 5815,\n",
       " 'Ġattempted': 7482,\n",
       " 'idently': 46046,\n",
       " 'ĠGAM': 49965,\n",
       " 'Ġabsolute': 4112,\n",
       " 'ĠFrankenstein': 45738,\n",
       " 'Ġresemble': 22464,\n",
       " 'Ġparaly': 26901,\n",
       " 'Full': 13295,\n",
       " 'Ġridge': 32525,\n",
       " 'ANC': 20940,\n",
       " 'ĠSn': 5489,\n",
       " 'Ġelegance': 49198,\n",
       " 'ĠWelch': 41524,\n",
       " 'Ġlingu': 20280,\n",
       " 'ĠItal': 33857,\n",
       " 'ĠMosque': 42825,\n",
       " 'Ġmid': 3095,\n",
       " 'Ġtavern': 42488,\n",
       " 'eter': 2357,\n",
       " 'Ġcharities': 23980,\n",
       " 'Ġefficiently': 18306,\n",
       " 'Ġ322': 38831,\n",
       " 'anes': 7305,\n",
       " 'Ġcapped': 28490,\n",
       " 'Ġmysteriously': 42529,\n",
       " 'Ġrightfully': 41802,\n",
       " 'ĠII': 2873,\n",
       " 'Ġhoax': 25943,\n",
       " 'Ġjson': 33918,\n",
       " 'Ġexchanged': 22112,\n",
       " 'Ġmedical': 3315,\n",
       " 'Lib': 25835,\n",
       " 'ates': 689,\n",
       " 'ĠCor': 2744,\n",
       " 'Ġdisplacement': 29358,\n",
       " 'Ġ300': 5867,\n",
       " 'Ġvari': 5553,\n",
       " 'ciation': 17269,\n",
       " 'ĠBlessed': 33398,\n",
       " 'ucing': 25648,\n",
       " 'ISA': 22312,\n",
       " '³': 111,\n",
       " 'ĠSanctuary': 27036,\n",
       " 'Ġapplic': 2161,\n",
       " 'Ġhostile': 12524,\n",
       " 'ĠStorage': 20514,\n",
       " 'hare': 43466,\n",
       " 'ĠTrop': 25491,\n",
       " 'addle': 37382,\n",
       " 'energy': 22554,\n",
       " 'verts': 24040,\n",
       " 'ãĤ¹': 8943,\n",
       " 'ĠSamuel': 17100,\n",
       " 'Ġbarn': 25203,\n",
       " 'ĠUnicode': 34371,\n",
       " 'Ġsteroid': 40762,\n",
       " 'ĠImplement': 48282,\n",
       " 'Ġnin': 13462,\n",
       " 'Ġorigin': 8159,\n",
       " 'Ġtang': 13875,\n",
       " 'ĠScheme': 32448,\n",
       " 'Ġlesions': 35258,\n",
       " 'elsius': 32495,\n",
       " 'Ġshone': 44193,\n",
       " 'Ġpersuaded': 25562,\n",
       " 'human': 10734,\n",
       " 'ĠMaher': 38137,\n",
       " 'ĠEd': 1717,\n",
       " 'Ġstruggling': 9648,\n",
       " 'uggest': 29212,\n",
       " 'EN': 1677,\n",
       " 'ĠBud': 10370,\n",
       " 'Ġcra': 15671,\n",
       " 'ĠFamous': 43261,\n",
       " 'Ġpineapple': 45540,\n",
       " 'iber': 1856,\n",
       " 'ĠGoff': 44572,\n",
       " 'Ġdefic': 12630,\n",
       " 'Ġcout': 42304,\n",
       " 'Ġinfused': 37681,\n",
       " 'ĠReve': 31091,\n",
       " 'Ġhallucinations': 40371,\n",
       " 'alis': 27315,\n",
       " 'ĠScarborough': 36126,\n",
       " 'Ġculinary': 35956,\n",
       " 'Ġinsanely': 40848,\n",
       " 'Ġnext': 1306,\n",
       " 'Ġparted': 37813,\n",
       " 'budget': 37315,\n",
       " 'Ġpounding': 33919,\n",
       " 'Ġpriest': 11503,\n",
       " 'cker': 15280,\n",
       " 'Ġbarb': 42577,\n",
       " 'Ġsegregation': 26718,\n",
       " '_-': 22955,\n",
       " 'ĠRally': 27752,\n",
       " 'Ġappearances': 11057,\n",
       " 'ĠGym': 31187,\n",
       " 'jobs': 43863,\n",
       " 'Ġunfolds': 45995,\n",
       " 'Sl': 11122,\n",
       " 'owship': 23473,\n",
       " 'ĠEugene': 24532,\n",
       " 'Ġlabou': 45161,\n",
       " 'ĠShooter': 35334,\n",
       " 'Ġstrongh': 26733,\n",
       " 'rieved': 28130,\n",
       " 'ĠABS': 29950,\n",
       " 'Ġcat': 3797,\n",
       " 'Ġeverybody': 7288,\n",
       " 'Ġepile': 29790,\n",
       " 'Join': 18234,\n",
       " 'Rail': 44631,\n",
       " 'lines': 6615,\n",
       " 'Ġimpulse': 25278,\n",
       " 'Ġtreating': 13622,\n",
       " 'Ġvelvet': 47750,\n",
       " 'ĠHubbard': 34342,\n",
       " 'Ġfascination': 35556,\n",
       " 'ĠUber': 12024,\n",
       " 'Ġmasturbation': 41054,\n",
       " 'ITS': 29722,\n",
       " 'Veter': 45182,\n",
       " '172': 23628,\n",
       " 'AF': 8579,\n",
       " 'Ur': 16692,\n",
       " 'ĠFem': 31149,\n",
       " 'Ġimaging': 19560,\n",
       " 'ĠGreater': 18169,\n",
       " 'ĠAngola': 48968,\n",
       " 'Ô': 144,\n",
       " 'give': 26535,\n",
       " 'ĠAnnual': 16328,\n",
       " 'ĠTsarnaev': 43760,\n",
       " 'Ġafterward': 20875,\n",
       " 'Ġarrogant': 28868,\n",
       " 'struct': 7249,\n",
       " 'Shares': 43566,\n",
       " 'Ġbracelet': 42893,\n",
       " 'Ġcavity': 31643,\n",
       " 'orians': 45825,\n",
       " 'anc': 1192,\n",
       " 'older': 19892,\n",
       " 'sembly': 5997,\n",
       " 'Ġdigs': 46561,\n",
       " 'daily': 29468,\n",
       " 'ĠPhoenix': 9643,\n",
       " 'ANK': 15154,\n",
       " 'Ġgmaxwell': 31886,\n",
       " 'Ġinterpol': 39555,\n",
       " 'pert': 11766,\n",
       " 'ĠHans': 13071,\n",
       " 'Shot': 28512,\n",
       " 'ATA': 13563,\n",
       " 'Ġmor': 2146,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32e77a92-d347-4b35-ab6b-6a019d52b6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1856b1d4-eae4-414b-bd0c-f6746a497bfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2516\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.\u001b[39;00m\n\u001b[1;32m   2447\u001b[0m \n\u001b[1;32m   2448\u001b[0m \u001b[38;5;124;03mThis module is often used to retrieve word embeddings using indices.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;124;03m             [ 0.6262,  0.2438,  0.7471]]])\u001b[39;00m\n\u001b[1;32m   2514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[0;32m-> 2516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2517\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2518\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/overrides.py:1719\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1719\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py:104\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "embedding_layer(torch.tensor("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d8f82-3bef-45cb-b7f6-2c9fac5a50d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
