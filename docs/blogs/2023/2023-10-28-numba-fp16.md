---
title: Training NeMo RNN-T Models Efficiently with Numba FP16 Support
author: [Somshubra Majumdar, Graham Markall]
author_gh_user: [titu1994, gmarkall]
read_time: 5 minutes
publish_date: 10/28/2023

# Optional: OpenGraph metadata
# og_title: Title of the blog post for Rich URL previews
og_image: https://numba.pydata.org/_static/numba-blue-horizontal-rgb.svg
# og_image_type: Image type (e.g. image/png). Defaults to image/png.
# page_path: Relative path to the image from the website root (e.g. /assets/images/)
description: NeMo RNNT Training with Numba FP16 Support

# DO NOT CHANGE BELOW
template: blog.html
---

# Training NeMo RNN-T Models Efficiently with Numba FP16 Support

In the field of Automatic Speech Recognition research, [RNN Transducer (RNN-T)](https://arxiv.org/abs/1211.3711) is a type of sequence-to-sequence model that has been shown to achieve state-of-the-art results for accurate, streaming speech recognition. RNN-Ts are also able to handle longer sequences than they were trained on, as well as out-of-vocabulary words, which is a common problem in speech recognition. This makes them a good choice for speech recognition applications that require real-time performance, such as on-device speech recognition for smartphones.

<figure markdown>
  ![RNN-Transducer architecture](https://github.com/NVIDIA/NeMo/releases/download/v1.20.0/asset-post-2023-10-28-numba-fp16-rnnt_joint.png)
  <figcaption><b>Figure 1.</b> <i>The RNN-Transducer architecture. The audio sequence x is passed through the encoder, the text sequence u to the prediction network, and combined by the joint network.</i></figcaption>
</figure>

A significant drawback of the Transducer architecture is the vast GPU memory required during training. As discussed in [Sequence-to-sequence learning with Transducers](https://lorenlugosch.github.io/posts/2020/11/transducer/), the output of the joint network in the transducer is a 4-dimensional tensor which occupies significant amounts of memory. The cost of this matrix can be easily calculated as follows : 

\\[Joint = \: B \times T \times U \times V \times 2 \times 4 \,\, \textit{bytes}\\] 

Here, B is the batch size, T is the audio sequence length, U is the text sequence length and V is the vocabulary size, we multiply by 2 for the activation as well as gradient, and assume fp32 datatype (4 bytes). Audio frames are commonly sampled at 100 Hz Mel-Spectrogram frames, which means each second of audio corresponds to 100 audio frames. 

For a single 20-second audio clip with about 100 subwords in its transcript, and a vocabulary of 1024 subword tokens, it would require (B=1) x (T=20s * 100) x (U=100) x (V=1024) x 2 x 4 bytes ~ **1.6 Gigabytes** for a single audio sample. If we use a larger batch size of 10 for training, we will quickly run out of memory even on 16 GB GPUs ! Also, remember, this is just for the joint, there is additional memory required to keep the model in memory, and to calculate the activation and gradients of the rest of the network!

<h4><i>Enter Numba support for FP16 dtype</i></h4>

As of [Numba 0.57](https://numba.readthedocs.io/en/stable/release-notes.html#version-0-57-0-1-may-2023) release, FP16 datatype format is now supported natively! Using this, we can effectively halve the memory requirement of the above joint and support larger batch sizes with almost no changes required!

NeMo utilizes [Numba's](https://numba.readthedocs.io/en/stable/index.html) [Just-in-time compile CUDA kernels](https://numba.pydata.org/numba-doc/latest/cuda/kernels.html) written in Python in order to efficiently compute the RNN-T loss. This allows a user to simply have Numba installed on their system, and without explicit compilation of C++ / CUDA code, they can train their RNN-T models easily. Furthermore, since the kernels are written in Python, it allows for simple modifications by researchers to develop advanced features such as [FastEmit](https://arxiv.org/abs/2010.11148), and even other extensions to the Transducer loss - such as [Token-and-Duration Transducers](https://arxiv.org/abs/2304.06795).


## Prerequisites

* [Pytorch](https://pytorch.org/) 1.13.1+
* [Nvidia NeMo](https://github.com/NVIDIA/NeMo) 1.20.0+
* [Numba](https://github.com/numba/numba) 0.57+ (conda install numba=0.57.1 -c conda-forge)
* [CUDA Python](https://nvidia.github.io/cuda-python/install.html)
* CUDA 11.8 (installed as part of cudatoolkit)
* It is preferable to install these libraries in a Conda environment (Python 3.10) for correct dependency resolution.

The following snippet can be used to install the requirements - 

```shell
conda create -n nemo -c pytorch -c nvidia -c conda-forge python=3.9 numba=0.57.1 cudatoolkit=11.8 cuda-python=11.8 pytorch torchvision torchaudio pytorch-cuda=11.8 cython
conda activate nemo
pip install nemo-toolkit[all]>=1.20.0
```

## Enabling Numba FP16 Support in NeMo

* Set the Numba environment variable: `export NUMBA_CUDA_USE_NVIDIA_BINDING=1`
* Set the NeMo environment variable: `export STRICT_NUMBA_COMPAT_CHECK=0`
* Check if installation is successful by using the following snippet: 

```python
from nemo.core.utils import numba_utils

# Should be True
print(numba_utils.numba_cuda_is_supported(numba_utils.__NUMBA_MINIMUM_VERSION_FP16_SUPPORTED__))

# Should also be True. 
print(numba_utils.is_numba_cuda_fp16_supported())
```

## Train a Transducer ASR model with FP16

With the above environment flags set, and the latest Numba version installed, NeMo supports training with FP16 loss out of the box. For a tutorial on how to setup and train a Transducer ASR model, please refer to the following tutorial - [ASR With Transducers](https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_with_Transducers.ipynb)

The only change necessary to use the FP16 loss is to pass “precision=16” to the PyTorch Lightning Trainer in place of 32.

## Measuring Memory and Compute Improvements

We devise a simple benchmarking script that computes the RNN-T loss with gradients enabled for various combinations of inputs which are common during training on the Librispeech Speech Recognition academic dataset. The benchmark script for measuring memory [can be found in this Gist](https://gist.github.com/titu1994/e786fbd1efccd81f412bf76df5ff41c7).

We assume that we are training a [Conformer](https://arxiv.org/abs/2005.08100) or [Fast Conformer](https://arxiv.org/abs/2305.05084) Transducer model, which performs 4x or 8x audio signal reduction respectively. For Librispeech, the longest audio file is approximately 17 seconds, which becomes approximately 200-time steps after 8x reduction. We check memory consumption for both Character tokenization (V=28) and Subword Tokenization (V=1024). Due to the tokenization, the transcript text may be between 80 to 250 tokens but we take a conservative limit of 100 to 200 tokens. Previously, we did not measure the memory consumption of the intermediate activation of the Joint network (which has a size of BxTxUxH), so here we keep H=640 which is commonly used in literature.


<h4><i>Results</i></h4>


<figure markdown>
  ![RNN-Transducer memory under fp16 vs fp32](https://github.com/NVIDIA/NeMo/releases/download/v1.20.0/asset-post-2023-10-28-numba-fp16-memory_joint.png)
  <figcaption><b>Figure 2.</b> <i>Plot of GPU Memory usage for a given combination of Batch size (B), Timesteps (T), Text length (U), Vocabulary size (V) and the hidden dimension of the RNN-Transducer Joint. When using larger batch sizes, memory is rapidly exhausted simply to compute the RNN-T Joint, while Numba FP16 effectively halves this memory cost.</i></figcaption>
</figure>

Plot of GPU Memory usage for a given combination of Batch size (B), Timesteps (T), Text length (U), Vocabulary size (V) and the hidden dimension of the RNN-Transducer Joint. When using larger batch sizes, memory is rapidly exhausted simply to compute the RNN-T Joint, while Numba FP16 effectively halves this memory cost.

It is to be noted that NVIDIA NeMo has several other mechanisms to significantly reduce peak memory consumption, such as [Batch Splitting](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/configs.html#effect-of-batch-splitting-fused-batch-step). When combined with FP16 support in Numba, this allows for the training of even larger ASR models with the Transducer loss. 

# Conclusion

Numba FP16 support alleviates one of the crucial issues of RNN-Transducer training - memory usage. This unlocks efficient streaming speech recognition model training for a wider audience of researchers and developers. With a simple installation step, users are empowered to [train](https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_with_Transducers.ipynb) and [fine-tune](https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_CTC_Language_Finetuning.ipynb) their own speech recognition solutions on commonly available GPUs.

Users can learn more about Numba and how to leverage it for high-performance computing using Python in their [5-minute guide](https://numba.readthedocs.io/en/stable/user/index.html). Furthermore, NeMo users can read up on how to perform speech recognition with many models and losses in the [NeMo ASR documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/intro.html).

