# NeMo ASR+VAD Inference

This example aims to provide a pipeline for integrating Voice Activity Detection (VAD) results in ASR inference. 

The `audio_to_feature_rttm.py` file takes in an audio manifest, extracts and saves audio features, then performs VAD on features and saves the results as RTTM files.

The `feature_rttm_to_text.py` file takes the output RTTM files as well as audio features generated by `audio_to_feature_rttm.py`, then masks the non-speech audio frames with "zero-signal" values, then feed the masked frames into the chosen ASR model. 

## Data

The input must be a manifest json file, where each line is a Python dictionary. The fields ["audio_filepath", "offset", "duration",  "text"] are required. An example of a manifest file is:
```
{"audio_filepath": "/path/to/audio_file1", "offset": 0, "duration": null,  "text": "a b c d e"}
{"audio_filepath": "/path/to/audio_file2", "offset": 0, "duration": null,  "text": "f g h i j"}
```

## Usage

To run with the default setting:

1. Extract audio features and perform VAD:
```bash
python audio_to_rttm.py \
    --config-path=./configs --config-name=vad_inference_postprocess \
    manifest_filepath=/PATH/TO/MANIFEST.json output_dir=./vad_output
```

2. Then run ASR with VAD outputs:
```bash
python feature_rttm_to_text.py \
    pretrained_name="stt_en_conformer_ctc_large" \
    dataset_manifest=./vad_output/manifest_vad_feat_rttm.json \
    num_workers=8 \
    normalize=post_norm \
    use_pure_noise=False \
    use_rttm=True \
    use_feature=True
```
