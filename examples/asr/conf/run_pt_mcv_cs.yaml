# The scri to be run.
script: ???
script_config: ???

exp_name: null  # populated by exp_manager.name if not provided
results_dir: ???  # Where to store the results of the run

# Optional arguments
num_runs: 3
num_gpus: 8
num_tasks_per_node: 8
max_runtime: "00:03:45:00"

########################################################################################################################

executor: slurm
ipl_training:
  inference_config: conf/inference_config_cs.yaml
  p_cache: 0.2
  num_ipl_epochs: 100
  prefix: mcv_mls_test_15

USER: ntadevosyan
ssh_tunnel:
  host: cs-oci-ord-login-01.nvidia.com
  # ------------------------------- Fill this up! -------------------------------
  user: "${USER}"  # your username; or resolved from ${USER} environment variable ; or can be null which resolved from ${USER} environment variable
  job_dir: "//lustre/fsw/portfolios/convai/users/${USER}/nemo-run/"
  identity: "${NEMO_OCI_IAD_SSH_IDENTITY}"
  # -----------------------------------------------------------------------------

account: convai_convaird_nemo-speech
partition: polar,polar3 
job_name_prefix: "convai_convaird_nemo-speech-pt"

containers:
  # asr: /lustre/fsw/portfolios/llmservice/users/kpuvvada/local_containers/nemo_dev_20240717_aistore.sqsh
  asr: nvcr.io/nvidian/ac-aiapps/nemo_ntad:ipl

env_vars:
  - 'TOKENIZERS_PARALLELISM=false'
  - 'AIS_ENDPOINT="http://asr.iad.oci.aistore.nvidia.com:51080"'
  - 'LHOTSE_AUDIO_DURATION_MISMATCH_TOLERANCE=0.3'
  - 'TORCH_CUDNN_V8_API_ENABLED=1'
  - 'PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True'
  - 'HYDRA_FULL_ERROR=1'

required_env_vars:
  - 'HF_TOKEN'
  - 'WANDB_KEY'

mounts:
  # Replace with your own paths in your cluster config
  - /lustre/fsw/:/lustre/fsw/

timeouts:
  polar,polar3: 04:00:00
  interactive: 04:00:00
  interactive_singlenode: 04:00:00
