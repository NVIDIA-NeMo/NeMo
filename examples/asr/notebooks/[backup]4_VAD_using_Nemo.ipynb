{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "!pip install wget\n",
    "!pip install git+https://github.com/NVIDIA/apex.git\n",
    "!pip install nemo-toolkit\n",
    "!pip install nemo-asr\n",
    "!pip install unidecode\n",
    "\n",
    "!mkdir configs\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/master/examples/asr/configs/quartznet_speech_commands_3x1_v1.yaml\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/master/examples/asr/configs/quartznet_speech_commands_3x1_v2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some necessary libraries\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from ruamel.yaml import YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Speech Command recognition tutorial is based on the QuartzNet model from the paper \"[QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions](https://arxiv.org/pdf/1910.10261.pdf)\" with a modified decoder head to suit classification tasks.\n",
    "\n",
    "The notebook will follow the steps below:\n",
    "\n",
    " - Dataset preparation: Preparing Google Speech Commands dataset\n",
    "\n",
    " - Audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)\n",
    "\n",
    " - Data augmentation using SpecAugment \"[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)\" to increase number of data samples.\n",
    " \n",
    " - Develop a small Neural classification model which can be trained efficiently.\n",
    " \n",
    " - Model training on the Google Speech Commands dataset in NeMo.\n",
    " \n",
    " - Evaluation of error cases of the model by audibly hearing the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where the Google Speech Commands directory will be placed.\n",
    "# Change this if you don't want the data to be extracted in the current directory.\n",
    "# Select the version of the dataset required as well (can be 1 or 2)\n",
    "DATASET_VER = 2\n",
    "data_dir = './google_dataset_v{0}/'.format(DATASET_VER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/fjia/data/freesound_resampled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "We will be using the open source Google Speech Commands Dataset (we will use V1 of the dataset for the tutorial, but require very minor changes to support V2 dataset). These scripts below will download the dataset and convert it to a format suitable for use with nemo_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "The dataset must be prepared using the scripts provided under the `{NeMo root directory}/scripts` sub-directory. \n",
    "\n",
    "Run the following command below to download the training script and execute it.\n",
    "\n",
    "**NOTE**: You should have at least 4GB of disk space available if youâ€™ve used --data_version=1; and at least 6GB if you used --data_version=2. Also, it will take some time to download and process, so go grab a coffee.\n",
    "\n",
    "**NOTE**: You may additionally pass a `--rebalance` flag at the end of the `process_speech_commands_data.py` script to rebalance the class samples in the manifest."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# !wget https://raw.githubusercontent.com/NVIDIA/NeMo/master/scripts/process_speech_commands_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!mkdir {data_dir}\n",
    "!python process_speech_commands_data.py --data_root={data_dir} --data_version={DATASET_VER}\n",
    "print(\"Dataset ready !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the path to manifest files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtaset_path = 'google_speech_recognition_v{0}'.format(DATASET_VER)\n",
    "dataset_basedir = os.path.join(data_dir, dtaset_path)\n",
    "\n",
    "train_dataset = os.path.join(dataset_basedir, 'train_manifest.json')\n",
    "val_dataset = os.path.join(dataset_basedir, 'validation_manifest.json')\n",
    "test_dataset = os.path.join(dataset_basedir, 'validation_manifest.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBO 2 \n",
    "## Background + Speech Command 57k, 7k, 7k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_basedir = data_dir\n",
    "\n",
    "train_dataset = './manifest/background_training_manifest.json,./manifest/2balanced_sc_train_manifest.json'\n",
    "# test_dataset  = './manifest/background_testing_manifest.json,./manifest/2balanced_sc_test_manifest.json'\n",
    "test_dataset  = './manifest/all_test.json'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "dataset_basedir = data_dir\n",
    "\n",
    "train_dataset = './manifest/3balanced_background_training_manifest.json,./manifest/3balanced_sc_train_manifest.json,./manifest/speech_training_manifest.json'\n",
    "val_dataset   = './manifest/3balanced_background_validation_manifest.json,./manifest/3balanced_sc_validation_manifest.json,./manifest/speech_validation_manifest.json'\n",
    "test_dataset  = './manifest/3balanced_background_testing_manifest.json,./manifest/3balanced_sc_test_manifest.json,./manifest/speech_testing_manifest.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a few rows of the manifest file \n",
    "\n",
    "Manifest files are the data structure used by NeMo to declare a few important details about the data :\n",
    "\n",
    "1) `audio_filepath`: Refers to the path to the raw audio file <br>\n",
    "2) `command`: The class label (or speech command) of this sample <br>\n",
    "3) `duration`: The length of the audio file, in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/dfdabe19_nohash_0.wav\", \"duration\": 0.810625, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/37fc5d97_nohash_0.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/964c7c9e_nohash_0.wav\", \"duration\": 0.97525, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/cd85758f_nohash_0.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/8494fba8_nohash_2.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/8494fba8_nohash_0.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/1f3bece8_nohash_1.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/837a0f64_nohash_0.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/f0ae7203_nohash_0.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n",
      "{\"audio_filepath\": \"./google_dataset_v2/google_speech_recognition_v2/happy/9a7c1f83_nohash_0.wav\", \"duration\": 1.0, \"label\": \"commands\"}\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 10 {test_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - Preparation\n",
    "\n",
    "We will be training a QuartzNet model from the paper \"[QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions](https://arxiv.org/pdf/1910.10261.pdf)\". The benefit of QuartzNet over JASPER models is that they use Separable Convolutions, which greatly reduce the number of parameters required to get good model accuracy.\n",
    "\n",
    "QuartzNet models generally follow the model definition pattern QuartzNet-[BxR], where B is the number of blocks and R is the number of convolutional sub-blocks. Each sub-block contains a 1-D masked convolution, batch normalization, ReLU, and dropout:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VER = 1\n",
    "COMBO_VERSION = 'combo_balanced_sc_bg_combine_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the config file for the QuartzNet 3x1 model\n",
    "# Here we will be using separable convolutions\n",
    "# with 3 blocks (k=3 repeated once r=1 from the picture above)\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open(\"configs/quartznet_vad_3x1_v{0}.yaml\".format(DATASET_VER)) as f:\n",
    "    jasper_params = yaml.load(f)\n",
    "\n",
    "# Pre-define a set of labels that this model must learn to predict\n",
    "labels = jasper_params['labels']\n",
    "\n",
    "# Get the sampling rate of the data\n",
    "sample_rate = jasper_params['sample_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "/home/fjia/anaconda3/envs/vad/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/fjia/anaconda3/envs/vad/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/fjia/anaconda3/envs/vad/lib/python3.7/site-packages/nemo/collections/asr/audio_preprocessing.py:48: UserWarning: Could not import torchaudio. Some features might not work.\n",
      "  warnings.warn('Could not import torchaudio. Some features might not work.')\n"
     ]
    }
   ],
   "source": [
    "# Import NeMo core functionality\n",
    "# NeMo's \"core\" package\n",
    "import nemo\n",
    "# NeMo's ASR collection\n",
    "import nemo.collections.asr as nemo_asr\n",
    "# NeMo's learning rate policy\n",
    "from nemo.utils.lr_policies import CosineAnnealing\n",
    "from nemo.collections.asr.helpers import (\n",
    "    monitor_classification_training_progress,\n",
    "    process_classification_evaluation_batch,\n",
    "    process_classification_evaluation_epoch,\n",
    ")\n",
    "from nemo.collections.asr.metrics import classification_accuracy\n",
    "\n",
    "logging = nemo.logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define some hyper parameters\n",
    "lr = 0.05\n",
    "num_epochs = 5 #5\n",
    "batch_size = 128\n",
    "weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the NeMo components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural Factory\n",
    "# It creates log files and tensorboard writers for us among other functions\n",
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    log_dir='./{0}/quartznet-3x1-v{1}'.format(dataset_basedir, COMBO_VERSION),\n",
    "    create_tb_writer=True)\n",
    "tb_writer = neural_factory.tb_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:23:50 collections:222] Filtered duration for loading collection is 7.351812.\n",
      "[NeMo I 2020-04-29 10:23:51 collections:222] Filtered duration for loading collection is 1.173313.\n",
      "[NeMo I 2020-04-29 10:23:51 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-29 10:23:51 features:152] STFT using conv\n",
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-10-242cb97ccf7d>:34] Steps per epoch : 894\n",
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-10-242cb97ccf7d>:35] Have 114200 examples to train on.\n"
     ]
    }
   ],
   "source": [
    "# Check if data augmentation such as white noise and time shift augmentation should be used\n",
    "audio_augmentor = jasper_params.get('AudioAugmentor', None)\n",
    "\n",
    "# Build the input data layer and the preprocessing layers for the train set\n",
    "train_data_layer = nemo_asr.AudioToSpeechLabelDataLayer(\n",
    "    manifest_filepath=train_dataset,\n",
    "    labels=labels,\n",
    "    sample_rate=sample_rate,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=os.cpu_count(),\n",
    "    augmentor=audio_augmentor,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    " # Build the input data layer and the preprocessing layers for the test set\n",
    "eval_data_layer = nemo_asr.AudioToSpeechLabelDataLayer(\n",
    "    manifest_filepath=test_dataset,\n",
    "    sample_rate=sample_rate,\n",
    "    labels=labels,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=os.cpu_count(),\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# We will convert the raw audio data into MelSpectrogram Features to feed as input to our model\n",
    "data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor(\n",
    "    sample_rate=sample_rate, **jasper_params[\"AudioToMelSpectrogramPreprocessor\"],\n",
    ")\n",
    "\n",
    "# Compute the total number of samples and the number of training steps per epoch\n",
    "N = len(train_data_layer)\n",
    "steps_per_epoch = math.ceil(N / float(batch_size) + 1)\n",
    "\n",
    "logging.info(\"Steps per epoch : {0}\".format(steps_per_epoch))\n",
    "logging.info('Have {0} examples to train on.'.format(N))\n",
    "\n",
    "# Here we begin defining all of the augmentations we want\n",
    "# We will pad the preprocessed spectrogram image to have a certain number of timesteps\n",
    "# This centers the generated spectrogram and adds black boundaries to either side\n",
    "# of the padded image.\n",
    "crop_pad_augmentation = nemo_asr.CropOrPadSpectrogramAugmentation(audio_length=128)\n",
    "\n",
    "# We also optionally add `SpecAugment` augmentations based on the config file\n",
    "# SpecAugment has various possible augmentations to the generated spectrogram\n",
    "# 1) Frequency band masking\n",
    "# 2) Time band masking\n",
    "# 3) Rectangular cutout\n",
    "spectr_augment_config = jasper_params.get('SpectrogramAugmentation', None)\n",
    "if spectr_augment_config:\n",
    "    data_spectr_augmentation = nemo_asr.SpectrogramAugmentation(**spectr_augment_config)\n",
    "\n",
    "# Build the QuartzNet Encoder model\n",
    "# The config defines the layers as a list of dictionaries\n",
    "# The first and last two blocks are not considered when we say QuartzNet-[BxR]\n",
    "# B is counted as the number of blocks after the first layer and before the penultimate layer.\n",
    "# R is defined as the number of repetitions of each block in B.\n",
    "# Note: We can scale the convolution kernels size by the float parameter `kernel_size_factor`\n",
    "jasper_encoder = nemo_asr.JasperEncoder(**jasper_params[\"JasperEncoder\"])\n",
    "\n",
    "# We then define the QuartzNet decoder.\n",
    "# This decoder head is specialized for the task for classification, such that it\n",
    "# accepts a set of `N-feat` per timestep of the model, and averages these features\n",
    "# over all the timesteps, before passing a Linear classification layer on those features.\n",
    "jasper_decoder = nemo_asr.JasperDecoderForClassification(\n",
    "    feat_in=jasper_params[\"JasperEncoder\"][\"jasper\"][-1][\"filters\"],\n",
    "    num_classes=len(labels),\n",
    "    **jasper_params['JasperDecoderForClassification'],\n",
    ")\n",
    "\n",
    "# We can easily apply cross entropy loss to train this model\n",
    "ce_loss = nemo_asr.CrossEntropyLossNM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-11-6805b5462cf6>:2] ================================\n",
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-11-6805b5462cf6>:3] Number of parameters in encoder: 73344\n",
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-11-6805b5462cf6>:4] Number of parameters in decoder: 258\n",
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-11-6805b5462cf6>:6] Total number of parameters in model: 73602\n",
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-11-6805b5462cf6>:8] ================================\n"
     ]
    }
   ],
   "source": [
    "# Lets print out the number of parameters of this model\n",
    "logging.info('================================')\n",
    "logging.info(f\"Number of parameters in encoder: {jasper_encoder.num_weights}\")\n",
    "logging.info(f\"Number of parameters in decoder: {jasper_decoder.num_weights}\")\n",
    "logging.info(\n",
    "    f\"Total number of parameters in model: \" f\"{jasper_decoder.num_weights + jasper_encoder.num_weights}\"\n",
    ")\n",
    "logging.info('================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Training Graph for NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have all of the components that are required to build the NeMo execution graph!\n",
    "## Build the training data loaders and preprocessors first\n",
    "audio_signal, audio_signal_len, labels, label_len = train_data_layer()\n",
    "processed_signal, processed_signal_len = data_preprocessor(input_signal=audio_signal, length=audio_signal_len)\n",
    "processed_signal, processed_signal_len = crop_pad_augmentation(\n",
    "    input_signal=processed_signal,\n",
    "    length=audio_signal_len\n",
    ")\n",
    "\n",
    "## Augment the dataset for training\n",
    "if spectr_augment_config:\n",
    "    processed_signal = data_spectr_augmentation(input_spec=processed_signal)\n",
    "\n",
    "## Define the model\n",
    "encoded, encoded_len = jasper_encoder(audio_signal=processed_signal, length=processed_signal_len)\n",
    "decoded = jasper_decoder(encoder_output=encoded)\n",
    "\n",
    "## Obtain the train loss\n",
    "train_loss = ce_loss(logits=decoded, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Test Graph for NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build the test graph in a similar way, reusing the above components\n",
    "## Build the test data loader and preprocess same way as train graph\n",
    "## But note, we do not add the spectrogram augmentation to the test graph !\n",
    "test_audio_signal, test_audio_signal_len, test_labels, test_label_len = eval_data_layer()\n",
    "test_processed_signal, test_processed_signal_len = data_preprocessor(\n",
    "    input_signal=test_audio_signal, length=test_audio_signal_len\n",
    ")\n",
    "test_processed_signal, test_processed_signal_len = crop_pad_augmentation(\n",
    "    input_signal=test_processed_signal, length=test_processed_signal_len\n",
    ")\n",
    "\n",
    "# Pass the test data through the model encoder and decoder\n",
    "test_encoded, test_encoded_len = jasper_encoder(\n",
    "    audio_signal=test_processed_signal, length=test_processed_signal_len\n",
    ")\n",
    "test_decoded = jasper_decoder(encoder_output=test_encoded)\n",
    "\n",
    "# Compute test loss for visualization\n",
    "test_loss = ce_loss(logits=test_decoded, labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up callbacks for training and test set evaluation, and checkpoint saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that we have our training and evaluation graphs built,\n",
    "# we can focus on a few callbacks to help us save the model checkpoints\n",
    "# during training, as well as display train and test metrics\n",
    "\n",
    "# Callbacks needed to print train info to console and Tensorboard\n",
    "train_callback = nemo.core.SimpleLossLoggerCallback(\n",
    "    # Notice that we pass in loss, predictions, and the labels.\n",
    "    # Of course we would like to see our training loss, but we need the\n",
    "    # other arguments to calculate the accuracy.\n",
    "    tensors=[train_loss, decoded, labels],\n",
    "    # The print_func defines what gets printed.\n",
    "    print_func=partial(monitor_classification_training_progress, eval_metric=None),\n",
    "    get_tb_values=lambda x: [(\"loss\", x[0])],\n",
    "    tb_writer=neural_factory.tb_writer,\n",
    ")\n",
    "\n",
    "# Callbacks needed to print test info to console and Tensorboard\n",
    "tagname = 'TestSet'\n",
    "eval_callback = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[test_loss, test_decoded, test_labels],\n",
    "    user_iter_callback=partial(process_classification_evaluation_batch, top_k=1),\n",
    "    user_epochs_done_callback=partial(process_classification_evaluation_epoch, eval_metric=1, tag=tagname),\n",
    "    eval_step=200,  # How often we evaluate the model on the test set\n",
    "    tb_writer=neural_factory.tb_writer,\n",
    ")\n",
    "\n",
    "# Callback to save model checkpoints\n",
    "chpt_callback = nemo.core.CheckpointCallback(\n",
    "    folder=neural_factory.checkpoint_dir,\n",
    "    step_freq=1000,\n",
    ")\n",
    "\n",
    "# Prepare a list of checkpoints to pass to the engine\n",
    "callbacks = [train_callback, eval_callback, chpt_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "Even with such a small model (77k parameters), and just 5 epochs (should take just a few minutes to train), you should be able to get a test set accuracy score in the range 85 - 90%. Not bad for a 30 (v1) or 35 (v2) way classification problem !\n",
    "\n",
    "Experiment with increasing the number of epochs or with batch size to see how much you can improve the score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:23:53 <ipython-input-16-689768946717>:13] Using `<nemo.utils.lr_policies.CosineAnnealing object at 0x7fbbd0259050>` Learning Rate Scheduler\n",
      "[NeMo I 2020-04-29 10:23:53 callbacks:187] Starting .....\n",
      "[NeMo I 2020-04-29 10:23:53 callbacks:359] Found 2 modules with weights:\n",
      "[NeMo I 2020-04-29 10:23:53 callbacks:361] JasperDecoderForClassification\n",
      "[NeMo I 2020-04-29 10:23:53 callbacks:361] JasperEncoder\n",
      "[NeMo I 2020-04-29 10:23:53 callbacks:362] Total model parameters: 73602\n",
      "[NeMo I 2020-04-29 10:23:53 callbacks:311] Found checkpoint folder .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints. Will attempt to restore checkpoints from it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-04-29 10:23:53 callbacks:328] For module JasperDecoderForClassification, no file matches  in .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints\n",
      "[NeMo W 2020-04-29 10:23:53 callbacks:330] Checkpoint folder .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints was present but nothing was restored. Continuing training from random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:23:53 callbacks:199] Starting epoch 0\n",
      "[NeMo I 2020-04-29 10:23:54 callbacks:224] Step: 0\n",
      "[NeMo I 2020-04-29 10:23:54 helpers:104] Loss: 0.7197867035865784\n",
      "[NeMo I 2020-04-29 10:23:54 helpers:110] training_batch_top@1:  47.6562\n",
      "[NeMo I 2020-04-29 10:23:54 callbacks:239] Step time: 0.20113658905029297 seconds\n",
      "[NeMo I 2020-04-29 10:23:54 callbacks:445] Doing Evaluation ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:23:56 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.6934638023376465\n",
      "[NeMo I 2020-04-29 10:23:56 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 49.4914\n",
      "[NeMo I 2020-04-29 10:23:56 callbacks:450] Evaluation time: 2.7741281986236572 seconds\n",
      "[NeMo I 2020-04-29 10:23:58 callbacks:224] Step: 25\n",
      "[NeMo I 2020-04-29 10:23:58 helpers:104] Loss: 0.14671579003334045\n",
      "[NeMo I 2020-04-29 10:23:58 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:23:58 callbacks:239] Step time: 0.0547490119934082 seconds\n",
      "[NeMo I 2020-04-29 10:23:59 callbacks:224] Step: 50\n",
      "[NeMo I 2020-04-29 10:23:59 helpers:104] Loss: 0.13893286883831024\n",
      "[NeMo I 2020-04-29 10:23:59 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:23:59 callbacks:239] Step time: 0.054253339767456055 seconds\n",
      "[NeMo I 2020-04-29 10:24:00 callbacks:224] Step: 75\n",
      "[NeMo I 2020-04-29 10:24:00 helpers:104] Loss: 0.2059304118156433\n",
      "[NeMo I 2020-04-29 10:24:00 helpers:110] training_batch_top@1:  92.9688\n",
      "[NeMo I 2020-04-29 10:24:00 callbacks:239] Step time: 0.05444645881652832 seconds\n",
      "[NeMo I 2020-04-29 10:24:02 callbacks:224] Step: 100\n",
      "[NeMo I 2020-04-29 10:24:02 helpers:104] Loss: 0.13474002480506897\n",
      "[NeMo I 2020-04-29 10:24:02 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:24:02 callbacks:239] Step time: 0.054471492767333984 seconds\n",
      "[NeMo I 2020-04-29 10:24:03 callbacks:224] Step: 125\n",
      "[NeMo I 2020-04-29 10:24:03 helpers:104] Loss: 0.14790311455726624\n",
      "[NeMo I 2020-04-29 10:24:03 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:03 callbacks:239] Step time: 0.054013729095458984 seconds\n",
      "[NeMo I 2020-04-29 10:24:04 callbacks:224] Step: 150\n",
      "[NeMo I 2020-04-29 10:24:04 helpers:104] Loss: 0.15698421001434326\n",
      "[NeMo I 2020-04-29 10:24:04 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:24:04 callbacks:239] Step time: 0.05069160461425781 seconds\n",
      "[NeMo I 2020-04-29 10:24:06 callbacks:224] Step: 175\n",
      "[NeMo I 2020-04-29 10:24:06 helpers:104] Loss: 0.12286563962697983\n",
      "[NeMo I 2020-04-29 10:24:06 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:06 callbacks:239] Step time: 0.0540003776550293 seconds\n",
      "[NeMo I 2020-04-29 10:24:07 callbacks:224] Step: 200\n",
      "[NeMo I 2020-04-29 10:24:07 helpers:104] Loss: 0.10698571801185608\n",
      "[NeMo I 2020-04-29 10:24:07 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:24:07 callbacks:239] Step time: 0.05445575714111328 seconds\n",
      "[NeMo I 2020-04-29 10:24:07 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:24:10 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.268638551235199\n",
      "[NeMo I 2020-04-29 10:24:10 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 90.6067\n",
      "[NeMo I 2020-04-29 10:24:10 callbacks:450] Evaluation time: 2.659414291381836 seconds\n",
      "[NeMo I 2020-04-29 10:24:11 callbacks:224] Step: 225\n",
      "[NeMo I 2020-04-29 10:24:11 helpers:104] Loss: 0.11761205643415451\n",
      "[NeMo I 2020-04-29 10:24:11 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:24:11 callbacks:239] Step time: 0.05470895767211914 seconds\n",
      "[NeMo I 2020-04-29 10:24:12 callbacks:224] Step: 250\n",
      "[NeMo I 2020-04-29 10:24:12 helpers:104] Loss: 0.19874082505702972\n",
      "[NeMo I 2020-04-29 10:24:12 helpers:110] training_batch_top@1:  92.1875\n",
      "[NeMo I 2020-04-29 10:24:12 callbacks:239] Step time: 0.05421900749206543 seconds\n",
      "[NeMo I 2020-04-29 10:24:14 callbacks:224] Step: 275\n",
      "[NeMo I 2020-04-29 10:24:14 helpers:104] Loss: 0.11698894202709198\n",
      "[NeMo I 2020-04-29 10:24:14 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:24:14 callbacks:239] Step time: 0.05382513999938965 seconds\n",
      "[NeMo I 2020-04-29 10:24:15 callbacks:224] Step: 300\n",
      "[NeMo I 2020-04-29 10:24:15 helpers:104] Loss: 0.0930037572979927\n",
      "[NeMo I 2020-04-29 10:24:15 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:24:15 callbacks:239] Step time: 0.051102399826049805 seconds\n",
      "[NeMo I 2020-04-29 10:24:16 callbacks:224] Step: 325\n",
      "[NeMo I 2020-04-29 10:24:16 helpers:104] Loss: 0.10345859080553055\n",
      "[NeMo I 2020-04-29 10:24:16 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:24:16 callbacks:239] Step time: 0.05122542381286621 seconds\n",
      "[NeMo I 2020-04-29 10:24:18 callbacks:224] Step: 350\n",
      "[NeMo I 2020-04-29 10:24:18 helpers:104] Loss: 0.24996128678321838\n",
      "[NeMo I 2020-04-29 10:24:18 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:18 callbacks:239] Step time: 0.050994873046875 seconds\n",
      "[NeMo I 2020-04-29 10:24:19 callbacks:224] Step: 375\n",
      "[NeMo I 2020-04-29 10:24:19 helpers:104] Loss: 0.04796969145536423\n",
      "[NeMo I 2020-04-29 10:24:19 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:24:19 callbacks:239] Step time: 0.05430769920349121 seconds\n",
      "[NeMo I 2020-04-29 10:24:20 callbacks:224] Step: 400\n",
      "[NeMo I 2020-04-29 10:24:20 helpers:104] Loss: 0.09714093059301376\n",
      "[NeMo I 2020-04-29 10:24:20 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:24:20 callbacks:239] Step time: 0.05428457260131836 seconds\n",
      "[NeMo I 2020-04-29 10:24:20 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:24:23 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.25824999809265137\n",
      "[NeMo I 2020-04-29 10:24:23 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 92.4464\n",
      "[NeMo I 2020-04-29 10:24:23 callbacks:450] Evaluation time: 2.69571590423584 seconds\n",
      "[NeMo I 2020-04-29 10:24:24 callbacks:224] Step: 425\n",
      "[NeMo I 2020-04-29 10:24:24 helpers:104] Loss: 0.13755367696285248\n",
      "[NeMo I 2020-04-29 10:24:24 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:24 callbacks:239] Step time: 0.05434393882751465 seconds\n",
      "[NeMo I 2020-04-29 10:24:26 callbacks:224] Step: 450\n",
      "[NeMo I 2020-04-29 10:24:26 helpers:104] Loss: 0.1505688726902008\n",
      "[NeMo I 2020-04-29 10:24:26 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:26 callbacks:239] Step time: 0.051306962966918945 seconds\n",
      "[NeMo I 2020-04-29 10:24:27 callbacks:224] Step: 475\n",
      "[NeMo I 2020-04-29 10:24:27 helpers:104] Loss: 0.04824719950556755\n",
      "[NeMo I 2020-04-29 10:24:27 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:24:27 callbacks:239] Step time: 0.05091238021850586 seconds\n",
      "[NeMo I 2020-04-29 10:24:28 callbacks:224] Step: 500\n",
      "[NeMo I 2020-04-29 10:24:28 helpers:104] Loss: 0.17073294520378113\n",
      "[NeMo I 2020-04-29 10:24:28 helpers:110] training_batch_top@1:  92.1875\n",
      "[NeMo I 2020-04-29 10:24:28 callbacks:239] Step time: 0.054322242736816406 seconds\n",
      "[NeMo I 2020-04-29 10:24:30 callbacks:224] Step: 525\n",
      "[NeMo I 2020-04-29 10:24:30 helpers:104] Loss: 0.19398383796215057\n",
      "[NeMo I 2020-04-29 10:24:30 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:30 callbacks:239] Step time: 0.05135035514831543 seconds\n",
      "[NeMo I 2020-04-29 10:24:31 callbacks:224] Step: 550\n",
      "[NeMo I 2020-04-29 10:24:31 helpers:104] Loss: 0.14686818420886993\n",
      "[NeMo I 2020-04-29 10:24:31 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:31 callbacks:239] Step time: 0.05068635940551758 seconds\n",
      "[NeMo I 2020-04-29 10:24:32 callbacks:224] Step: 575\n",
      "[NeMo I 2020-04-29 10:24:32 helpers:104] Loss: 0.12007243931293488\n",
      "[NeMo I 2020-04-29 10:24:32 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:32 callbacks:239] Step time: 0.05433773994445801 seconds\n",
      "[NeMo I 2020-04-29 10:24:34 callbacks:224] Step: 600\n",
      "[NeMo I 2020-04-29 10:24:34 helpers:104] Loss: 0.09587641805410385\n",
      "[NeMo I 2020-04-29 10:24:34 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:24:34 callbacks:239] Step time: 0.05449175834655762 seconds\n",
      "[NeMo I 2020-04-29 10:24:34 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:24:37 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.2446853518486023\n",
      "[NeMo I 2020-04-29 10:24:37 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 94.0841\n",
      "[NeMo I 2020-04-29 10:24:37 callbacks:450] Evaluation time: 2.7068910598754883 seconds\n",
      "[NeMo I 2020-04-29 10:24:38 callbacks:224] Step: 625\n",
      "[NeMo I 2020-04-29 10:24:38 helpers:104] Loss: 0.19146953523159027\n",
      "[NeMo I 2020-04-29 10:24:38 helpers:110] training_batch_top@1:  92.1875\n",
      "[NeMo I 2020-04-29 10:24:38 callbacks:239] Step time: 0.051160573959350586 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:24:39 callbacks:224] Step: 650\n",
      "[NeMo I 2020-04-29 10:24:39 helpers:104] Loss: 0.09678792953491211\n",
      "[NeMo I 2020-04-29 10:24:39 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:24:39 callbacks:239] Step time: 0.05098295211791992 seconds\n",
      "[NeMo I 2020-04-29 10:24:41 callbacks:224] Step: 675\n",
      "[NeMo I 2020-04-29 10:24:41 helpers:104] Loss: 0.08741825073957443\n",
      "[NeMo I 2020-04-29 10:24:41 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:24:41 callbacks:239] Step time: 0.05129122734069824 seconds\n",
      "[NeMo I 2020-04-29 10:24:42 callbacks:224] Step: 700\n",
      "[NeMo I 2020-04-29 10:24:42 helpers:104] Loss: 0.12505391240119934\n",
      "[NeMo I 2020-04-29 10:24:42 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:42 callbacks:239] Step time: 0.0515749454498291 seconds\n",
      "[NeMo I 2020-04-29 10:24:43 callbacks:224] Step: 725\n",
      "[NeMo I 2020-04-29 10:24:43 helpers:104] Loss: 0.06837981194257736\n",
      "[NeMo I 2020-04-29 10:24:43 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:24:43 callbacks:239] Step time: 0.054160356521606445 seconds\n",
      "[NeMo I 2020-04-29 10:24:45 callbacks:224] Step: 750\n",
      "[NeMo I 2020-04-29 10:24:45 helpers:104] Loss: 0.242524653673172\n",
      "[NeMo I 2020-04-29 10:24:45 helpers:110] training_batch_top@1:  91.4062\n",
      "[NeMo I 2020-04-29 10:24:45 callbacks:239] Step time: 0.050282955169677734 seconds\n",
      "[NeMo I 2020-04-29 10:24:46 callbacks:224] Step: 775\n",
      "[NeMo I 2020-04-29 10:24:46 helpers:104] Loss: 0.08137307316064835\n",
      "[NeMo I 2020-04-29 10:24:46 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:24:46 callbacks:239] Step time: 0.054169654846191406 seconds\n",
      "[NeMo I 2020-04-29 10:24:47 callbacks:224] Step: 800\n",
      "[NeMo I 2020-04-29 10:24:47 helpers:104] Loss: 0.20472973585128784\n",
      "[NeMo I 2020-04-29 10:24:47 helpers:110] training_batch_top@1:  92.1875\n",
      "[NeMo I 2020-04-29 10:24:47 callbacks:239] Step time: 0.05435347557067871 seconds\n",
      "[NeMo I 2020-04-29 10:24:47 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:24:50 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.15307392179965973\n",
      "[NeMo I 2020-04-29 10:24:50 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 94.2717\n",
      "[NeMo I 2020-04-29 10:24:50 callbacks:450] Evaluation time: 2.726698398590088 seconds\n",
      "[NeMo I 2020-04-29 10:24:51 callbacks:224] Step: 825\n",
      "[NeMo I 2020-04-29 10:24:51 helpers:104] Loss: 0.06799434870481491\n",
      "[NeMo I 2020-04-29 10:24:51 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:24:51 callbacks:239] Step time: 0.054123878479003906 seconds\n",
      "[NeMo I 2020-04-29 10:24:53 callbacks:224] Step: 850\n",
      "[NeMo I 2020-04-29 10:24:53 helpers:104] Loss: 0.3067556917667389\n",
      "[NeMo I 2020-04-29 10:24:53 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:24:53 callbacks:239] Step time: 0.054026126861572266 seconds\n",
      "[NeMo I 2020-04-29 10:24:54 callbacks:224] Step: 875\n",
      "[NeMo I 2020-04-29 10:24:54 helpers:104] Loss: 0.1047072485089302\n",
      "[NeMo I 2020-04-29 10:24:54 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:24:54 callbacks:239] Step time: 0.049286603927612305 seconds\n",
      "[NeMo I 2020-04-29 10:24:55 callbacks:207] Finished epoch 0 in 0:01:02.145563\n",
      "[NeMo I 2020-04-29 10:24:55 callbacks:199] Starting epoch 1\n",
      "[NeMo I 2020-04-29 10:24:56 callbacks:224] Step: 900\n",
      "[NeMo I 2020-04-29 10:24:56 helpers:104] Loss: 0.15606461465358734\n",
      "[NeMo I 2020-04-29 10:24:56 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:24:56 callbacks:239] Step time: 0.0544428825378418 seconds\n",
      "[NeMo I 2020-04-29 10:24:57 callbacks:224] Step: 925\n",
      "[NeMo I 2020-04-29 10:24:57 helpers:104] Loss: 0.09901610016822815\n",
      "[NeMo I 2020-04-29 10:24:57 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:24:57 callbacks:239] Step time: 0.056966543197631836 seconds\n",
      "[NeMo I 2020-04-29 10:24:59 callbacks:224] Step: 950\n",
      "[NeMo I 2020-04-29 10:24:59 helpers:104] Loss: 0.07501468062400818\n",
      "[NeMo I 2020-04-29 10:24:59 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:24:59 callbacks:239] Step time: 0.051447391510009766 seconds\n",
      "[NeMo I 2020-04-29 10:25:00 callbacks:224] Step: 975\n",
      "[NeMo I 2020-04-29 10:25:00 helpers:104] Loss: 0.09158161282539368\n",
      "[NeMo I 2020-04-29 10:25:00 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:00 callbacks:239] Step time: 0.05409669876098633 seconds\n",
      "[NeMo I 2020-04-29 10:25:01 callbacks:224] Step: 1000\n",
      "[NeMo I 2020-04-29 10:25:01 helpers:104] Loss: 0.16732896864414215\n",
      "[NeMo I 2020-04-29 10:25:01 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:25:01 callbacks:239] Step time: 0.05393338203430176 seconds\n",
      "[NeMo I 2020-04-29 10:25:01 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:25:04 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.23699040710926056\n",
      "[NeMo I 2020-04-29 10:25:04 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 93.1030\n",
      "[NeMo I 2020-04-29 10:25:04 callbacks:450] Evaluation time: 2.6756174564361572 seconds\n",
      "[NeMo I 2020-04-29 10:25:04 callbacks:303] Saved checkpoint: .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints/trainer-STEP-1000.pt\n",
      "[NeMo I 2020-04-29 10:25:05 callbacks:224] Step: 1025\n",
      "[NeMo I 2020-04-29 10:25:05 helpers:104] Loss: 0.16383390128612518\n",
      "[NeMo I 2020-04-29 10:25:05 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:05 callbacks:239] Step time: 0.05456209182739258 seconds\n",
      "[NeMo I 2020-04-29 10:25:07 callbacks:224] Step: 1050\n",
      "[NeMo I 2020-04-29 10:25:07 helpers:104] Loss: 0.10370954871177673\n",
      "[NeMo I 2020-04-29 10:25:07 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:07 callbacks:239] Step time: 0.05689573287963867 seconds\n",
      "[NeMo I 2020-04-29 10:25:08 callbacks:224] Step: 1075\n",
      "[NeMo I 2020-04-29 10:25:08 helpers:104] Loss: 0.1393769383430481\n",
      "[NeMo I 2020-04-29 10:25:08 helpers:110] training_batch_top@1:  93.7500\n",
      "[NeMo I 2020-04-29 10:25:08 callbacks:239] Step time: 0.05098843574523926 seconds\n",
      "[NeMo I 2020-04-29 10:25:09 callbacks:224] Step: 1100\n",
      "[NeMo I 2020-04-29 10:25:09 helpers:104] Loss: 0.09659826755523682\n",
      "[NeMo I 2020-04-29 10:25:09 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:25:09 callbacks:239] Step time: 0.051363229751586914 seconds\n",
      "[NeMo I 2020-04-29 10:25:11 callbacks:224] Step: 1125\n",
      "[NeMo I 2020-04-29 10:25:11 helpers:104] Loss: 0.13238662481307983\n",
      "[NeMo I 2020-04-29 10:25:11 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:11 callbacks:239] Step time: 0.05423617362976074 seconds\n",
      "[NeMo I 2020-04-29 10:25:12 callbacks:224] Step: 1150\n",
      "[NeMo I 2020-04-29 10:25:12 helpers:104] Loss: 0.07536077499389648\n",
      "[NeMo I 2020-04-29 10:25:12 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:12 callbacks:239] Step time: 0.06415581703186035 seconds\n",
      "[NeMo I 2020-04-29 10:25:14 callbacks:224] Step: 1175\n",
      "[NeMo I 2020-04-29 10:25:14 helpers:104] Loss: 0.13640744984149933\n",
      "[NeMo I 2020-04-29 10:25:14 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:25:14 callbacks:239] Step time: 0.05440163612365723 seconds\n",
      "[NeMo I 2020-04-29 10:25:15 callbacks:224] Step: 1200\n",
      "[NeMo I 2020-04-29 10:25:15 helpers:104] Loss: 0.06271614879369736\n",
      "[NeMo I 2020-04-29 10:25:15 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:25:15 callbacks:239] Step time: 0.05481982231140137 seconds\n",
      "[NeMo I 2020-04-29 10:25:15 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:25:18 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.1299056112766266\n",
      "[NeMo I 2020-04-29 10:25:18 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 96.0753\n",
      "[NeMo I 2020-04-29 10:25:18 callbacks:450] Evaluation time: 2.7040677070617676 seconds\n",
      "[NeMo I 2020-04-29 10:25:19 callbacks:224] Step: 1225\n",
      "[NeMo I 2020-04-29 10:25:19 helpers:104] Loss: 0.10957513004541397\n",
      "[NeMo I 2020-04-29 10:25:19 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:25:19 callbacks:239] Step time: 0.05114316940307617 seconds\n",
      "[NeMo I 2020-04-29 10:25:20 callbacks:224] Step: 1250\n",
      "[NeMo I 2020-04-29 10:25:20 helpers:104] Loss: 0.19712671637535095\n",
      "[NeMo I 2020-04-29 10:25:20 helpers:110] training_batch_top@1:  92.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:25:20 callbacks:239] Step time: 0.05465888977050781 seconds\n",
      "[NeMo I 2020-04-29 10:25:22 callbacks:224] Step: 1275\n",
      "[NeMo I 2020-04-29 10:25:22 helpers:104] Loss: 0.11587455123662949\n",
      "[NeMo I 2020-04-29 10:25:22 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:25:22 callbacks:239] Step time: 0.051351308822631836 seconds\n",
      "[NeMo I 2020-04-29 10:25:23 callbacks:224] Step: 1300\n",
      "[NeMo I 2020-04-29 10:25:23 helpers:104] Loss: 0.08585667610168457\n",
      "[NeMo I 2020-04-29 10:25:23 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:25:23 callbacks:239] Step time: 0.059935808181762695 seconds\n",
      "[NeMo I 2020-04-29 10:25:24 callbacks:224] Step: 1325\n",
      "[NeMo I 2020-04-29 10:25:24 helpers:104] Loss: 0.12452378869056702\n",
      "[NeMo I 2020-04-29 10:25:24 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:25:24 callbacks:239] Step time: 0.05432558059692383 seconds\n",
      "[NeMo I 2020-04-29 10:25:26 callbacks:224] Step: 1350\n",
      "[NeMo I 2020-04-29 10:25:26 helpers:104] Loss: 0.16563795506954193\n",
      "[NeMo I 2020-04-29 10:25:26 helpers:110] training_batch_top@1:  93.7500\n",
      "[NeMo I 2020-04-29 10:25:26 callbacks:239] Step time: 0.055264949798583984 seconds\n",
      "[NeMo I 2020-04-29 10:25:27 callbacks:224] Step: 1375\n",
      "[NeMo I 2020-04-29 10:25:27 helpers:104] Loss: 0.1641942858695984\n",
      "[NeMo I 2020-04-29 10:25:27 helpers:110] training_batch_top@1:  93.7500\n",
      "[NeMo I 2020-04-29 10:25:27 callbacks:239] Step time: 0.054015159606933594 seconds\n",
      "[NeMo I 2020-04-29 10:25:29 callbacks:224] Step: 1400\n",
      "[NeMo I 2020-04-29 10:25:29 helpers:104] Loss: 0.13210783898830414\n",
      "[NeMo I 2020-04-29 10:25:29 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:25:29 callbacks:239] Step time: 0.050873756408691406 seconds\n",
      "[NeMo I 2020-04-29 10:25:29 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:25:31 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.1916007250547409\n",
      "[NeMo I 2020-04-29 10:25:31 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 94.0120\n",
      "[NeMo I 2020-04-29 10:25:31 callbacks:450] Evaluation time: 2.7001900672912598 seconds\n",
      "[NeMo I 2020-04-29 10:25:33 callbacks:224] Step: 1425\n",
      "[NeMo I 2020-04-29 10:25:33 helpers:104] Loss: 0.15310782194137573\n",
      "[NeMo I 2020-04-29 10:25:33 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:33 callbacks:239] Step time: 0.05074667930603027 seconds\n",
      "[NeMo I 2020-04-29 10:25:34 callbacks:224] Step: 1450\n",
      "[NeMo I 2020-04-29 10:25:34 helpers:104] Loss: 0.07181796431541443\n",
      "[NeMo I 2020-04-29 10:25:34 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:25:34 callbacks:239] Step time: 0.05113935470581055 seconds\n",
      "[NeMo I 2020-04-29 10:25:35 callbacks:224] Step: 1475\n",
      "[NeMo I 2020-04-29 10:25:35 helpers:104] Loss: 0.06372704356908798\n",
      "[NeMo I 2020-04-29 10:25:35 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:25:35 callbacks:239] Step time: 0.05232572555541992 seconds\n",
      "[NeMo I 2020-04-29 10:25:37 callbacks:224] Step: 1500\n",
      "[NeMo I 2020-04-29 10:25:37 helpers:104] Loss: 0.0454741045832634\n",
      "[NeMo I 2020-04-29 10:25:37 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:25:37 callbacks:239] Step time: 0.054206132888793945 seconds\n",
      "[NeMo I 2020-04-29 10:25:38 callbacks:224] Step: 1525\n",
      "[NeMo I 2020-04-29 10:25:38 helpers:104] Loss: 0.15243205428123474\n",
      "[NeMo I 2020-04-29 10:25:38 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:25:38 callbacks:239] Step time: 0.05459165573120117 seconds\n",
      "[NeMo I 2020-04-29 10:25:39 callbacks:224] Step: 1550\n",
      "[NeMo I 2020-04-29 10:25:39 helpers:104] Loss: 0.18411023914813995\n",
      "[NeMo I 2020-04-29 10:25:39 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:25:39 callbacks:239] Step time: 0.05834531784057617 seconds\n",
      "[NeMo I 2020-04-29 10:25:41 callbacks:224] Step: 1575\n",
      "[NeMo I 2020-04-29 10:25:41 helpers:104] Loss: 0.32086366415023804\n",
      "[NeMo I 2020-04-29 10:25:41 helpers:110] training_batch_top@1:  90.6250\n",
      "[NeMo I 2020-04-29 10:25:41 callbacks:239] Step time: 0.05756378173828125 seconds\n",
      "[NeMo I 2020-04-29 10:25:42 callbacks:224] Step: 1600\n",
      "[NeMo I 2020-04-29 10:25:42 helpers:104] Loss: 0.06269247084856033\n",
      "[NeMo I 2020-04-29 10:25:42 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:42 callbacks:239] Step time: 0.05416059494018555 seconds\n",
      "[NeMo I 2020-04-29 10:25:42 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:25:45 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.11640175431966782\n",
      "[NeMo I 2020-04-29 10:25:45 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 96.3062\n",
      "[NeMo I 2020-04-29 10:25:45 callbacks:450] Evaluation time: 2.7217233180999756 seconds\n",
      "[NeMo I 2020-04-29 10:25:46 callbacks:224] Step: 1625\n",
      "[NeMo I 2020-04-29 10:25:46 helpers:104] Loss: 0.11102479696273804\n",
      "[NeMo I 2020-04-29 10:25:46 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:25:46 callbacks:239] Step time: 0.05141711235046387 seconds\n",
      "[NeMo I 2020-04-29 10:25:47 callbacks:224] Step: 1650\n",
      "[NeMo I 2020-04-29 10:25:47 helpers:104] Loss: 0.0876203179359436\n",
      "[NeMo I 2020-04-29 10:25:47 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:47 callbacks:239] Step time: 0.05455160140991211 seconds\n",
      "[NeMo I 2020-04-29 10:25:49 callbacks:224] Step: 1675\n",
      "[NeMo I 2020-04-29 10:25:49 helpers:104] Loss: 0.14863801002502441\n",
      "[NeMo I 2020-04-29 10:25:49 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:25:49 callbacks:239] Step time: 0.050975799560546875 seconds\n",
      "[NeMo I 2020-04-29 10:25:50 callbacks:224] Step: 1700\n",
      "[NeMo I 2020-04-29 10:25:50 helpers:104] Loss: 0.11857340484857559\n",
      "[NeMo I 2020-04-29 10:25:50 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:25:50 callbacks:239] Step time: 0.059668540954589844 seconds\n",
      "[NeMo I 2020-04-29 10:25:51 callbacks:224] Step: 1725\n",
      "[NeMo I 2020-04-29 10:25:51 helpers:104] Loss: 0.13183265924453735\n",
      "[NeMo I 2020-04-29 10:25:51 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:25:51 callbacks:239] Step time: 0.05576133728027344 seconds\n",
      "[NeMo I 2020-04-29 10:25:53 callbacks:224] Step: 1750\n",
      "[NeMo I 2020-04-29 10:25:53 helpers:104] Loss: 0.10089176148176193\n",
      "[NeMo I 2020-04-29 10:25:53 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:25:53 callbacks:239] Step time: 0.055162906646728516 seconds\n",
      "[NeMo I 2020-04-29 10:25:54 callbacks:224] Step: 1775\n",
      "[NeMo I 2020-04-29 10:25:54 helpers:104] Loss: 0.21443317830562592\n",
      "[NeMo I 2020-04-29 10:25:54 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:25:54 callbacks:239] Step time: 0.049506187438964844 seconds\n",
      "[NeMo I 2020-04-29 10:25:55 callbacks:207] Finished epoch 1 in 0:00:59.710315\n",
      "[NeMo I 2020-04-29 10:25:55 callbacks:199] Starting epoch 2\n",
      "[NeMo I 2020-04-29 10:25:56 callbacks:224] Step: 1800\n",
      "[NeMo I 2020-04-29 10:25:56 helpers:104] Loss: 0.11492376774549484\n",
      "[NeMo I 2020-04-29 10:25:56 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:25:56 callbacks:239] Step time: 0.05205559730529785 seconds\n",
      "[NeMo I 2020-04-29 10:25:56 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:25:59 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.12119320780038834\n",
      "[NeMo I 2020-04-29 10:25:59 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 95.9310\n",
      "[NeMo I 2020-04-29 10:25:59 callbacks:450] Evaluation time: 2.720149040222168 seconds\n",
      "[NeMo I 2020-04-29 10:26:00 callbacks:224] Step: 1825\n",
      "[NeMo I 2020-04-29 10:26:00 helpers:104] Loss: 0.16817261278629303\n",
      "[NeMo I 2020-04-29 10:26:00 helpers:110] training_batch_top@1:  93.7500\n",
      "[NeMo I 2020-04-29 10:26:00 callbacks:239] Step time: 0.05234694480895996 seconds\n",
      "[NeMo I 2020-04-29 10:26:02 callbacks:224] Step: 1850\n",
      "[NeMo I 2020-04-29 10:26:02 helpers:104] Loss: 0.08540299534797668\n",
      "[NeMo I 2020-04-29 10:26:02 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:26:02 callbacks:239] Step time: 0.0634150505065918 seconds\n",
      "[NeMo I 2020-04-29 10:26:03 callbacks:224] Step: 1875\n",
      "[NeMo I 2020-04-29 10:26:03 helpers:104] Loss: 0.08333276212215424\n",
      "[NeMo I 2020-04-29 10:26:03 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:26:03 callbacks:239] Step time: 0.061745643615722656 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:26:04 callbacks:224] Step: 1900\n",
      "[NeMo I 2020-04-29 10:26:04 helpers:104] Loss: 0.11703500896692276\n",
      "[NeMo I 2020-04-29 10:26:04 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:26:04 callbacks:239] Step time: 0.054168701171875 seconds\n",
      "[NeMo I 2020-04-29 10:26:06 callbacks:224] Step: 1925\n",
      "[NeMo I 2020-04-29 10:26:06 helpers:104] Loss: 0.06961838901042938\n",
      "[NeMo I 2020-04-29 10:26:06 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:26:06 callbacks:239] Step time: 0.05392885208129883 seconds\n",
      "[NeMo I 2020-04-29 10:26:07 callbacks:224] Step: 1950\n",
      "[NeMo I 2020-04-29 10:26:07 helpers:104] Loss: 0.06956960260868073\n",
      "[NeMo I 2020-04-29 10:26:07 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:26:07 callbacks:239] Step time: 0.05443620681762695 seconds\n",
      "[NeMo I 2020-04-29 10:26:08 callbacks:224] Step: 1975\n",
      "[NeMo I 2020-04-29 10:26:08 helpers:104] Loss: 0.12177383899688721\n",
      "[NeMo I 2020-04-29 10:26:08 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:26:08 callbacks:239] Step time: 0.05093121528625488 seconds\n",
      "[NeMo I 2020-04-29 10:26:10 callbacks:224] Step: 2000\n",
      "[NeMo I 2020-04-29 10:26:10 helpers:104] Loss: 0.2596765160560608\n",
      "[NeMo I 2020-04-29 10:26:10 helpers:110] training_batch_top@1:  92.9688\n",
      "[NeMo I 2020-04-29 10:26:10 callbacks:239] Step time: 0.054441213607788086 seconds\n",
      "[NeMo I 2020-04-29 10:26:10 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:26:12 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.07851748913526535\n",
      "[NeMo I 2020-04-29 10:26:12 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 97.2296\n",
      "[NeMo I 2020-04-29 10:26:12 callbacks:450] Evaluation time: 2.6686630249023438 seconds\n",
      "[NeMo I 2020-04-29 10:26:12 callbacks:303] Saved checkpoint: .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints/trainer-STEP-2000.pt\n",
      "[NeMo I 2020-04-29 10:26:14 callbacks:224] Step: 2025\n",
      "[NeMo I 2020-04-29 10:26:14 helpers:104] Loss: 0.1987808495759964\n",
      "[NeMo I 2020-04-29 10:26:14 helpers:110] training_batch_top@1:  92.9688\n",
      "[NeMo I 2020-04-29 10:26:14 callbacks:239] Step time: 0.054053544998168945 seconds\n",
      "[NeMo I 2020-04-29 10:26:15 callbacks:224] Step: 2050\n",
      "[NeMo I 2020-04-29 10:26:15 helpers:104] Loss: 0.11189454793930054\n",
      "[NeMo I 2020-04-29 10:26:15 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:26:15 callbacks:239] Step time: 0.05397439002990723 seconds\n",
      "[NeMo I 2020-04-29 10:26:16 callbacks:224] Step: 2075\n",
      "[NeMo I 2020-04-29 10:26:16 helpers:104] Loss: 0.12963388860225677\n",
      "[NeMo I 2020-04-29 10:26:16 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:26:16 callbacks:239] Step time: 0.052359819412231445 seconds\n",
      "[NeMo I 2020-04-29 10:26:18 callbacks:224] Step: 2100\n",
      "[NeMo I 2020-04-29 10:26:18 helpers:104] Loss: 0.10981930792331696\n",
      "[NeMo I 2020-04-29 10:26:18 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:26:18 callbacks:239] Step time: 0.05180239677429199 seconds\n",
      "[NeMo I 2020-04-29 10:26:19 callbacks:224] Step: 2125\n",
      "[NeMo I 2020-04-29 10:26:19 helpers:104] Loss: 0.09167241305112839\n",
      "[NeMo I 2020-04-29 10:26:19 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:26:19 callbacks:239] Step time: 0.05106377601623535 seconds\n",
      "[NeMo I 2020-04-29 10:26:20 callbacks:224] Step: 2150\n",
      "[NeMo I 2020-04-29 10:26:20 helpers:104] Loss: 0.11653026938438416\n",
      "[NeMo I 2020-04-29 10:26:20 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:26:20 callbacks:239] Step time: 0.05473685264587402 seconds\n",
      "[NeMo I 2020-04-29 10:26:22 callbacks:224] Step: 2175\n",
      "[NeMo I 2020-04-29 10:26:22 helpers:104] Loss: 0.0460287407040596\n",
      "[NeMo I 2020-04-29 10:26:22 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:26:22 callbacks:239] Step time: 0.05092144012451172 seconds\n",
      "[NeMo I 2020-04-29 10:26:23 callbacks:224] Step: 2200\n",
      "[NeMo I 2020-04-29 10:26:23 helpers:104] Loss: 0.04141818732023239\n",
      "[NeMo I 2020-04-29 10:26:23 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:26:23 callbacks:239] Step time: 0.05100679397583008 seconds\n",
      "[NeMo I 2020-04-29 10:26:23 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:26:26 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.0752493366599083\n",
      "[NeMo I 2020-04-29 10:26:26 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 97.5904\n",
      "[NeMo I 2020-04-29 10:26:26 callbacks:450] Evaluation time: 2.6593263149261475 seconds\n",
      "[NeMo I 2020-04-29 10:26:27 callbacks:224] Step: 2225\n",
      "[NeMo I 2020-04-29 10:26:27 helpers:104] Loss: 0.1498992145061493\n",
      "[NeMo I 2020-04-29 10:26:27 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:26:27 callbacks:239] Step time: 0.050589799880981445 seconds\n",
      "[NeMo I 2020-04-29 10:26:28 callbacks:224] Step: 2250\n",
      "[NeMo I 2020-04-29 10:26:28 helpers:104] Loss: 0.127058207988739\n",
      "[NeMo I 2020-04-29 10:26:28 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:26:28 callbacks:239] Step time: 0.05419182777404785 seconds\n",
      "[NeMo I 2020-04-29 10:26:30 callbacks:224] Step: 2275\n",
      "[NeMo I 2020-04-29 10:26:30 helpers:104] Loss: 0.1446111500263214\n",
      "[NeMo I 2020-04-29 10:26:30 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:26:30 callbacks:239] Step time: 0.0514683723449707 seconds\n",
      "[NeMo I 2020-04-29 10:26:31 callbacks:224] Step: 2300\n",
      "[NeMo I 2020-04-29 10:26:31 helpers:104] Loss: 0.11434227228164673\n",
      "[NeMo I 2020-04-29 10:26:31 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:26:31 callbacks:239] Step time: 0.05094647407531738 seconds\n",
      "[NeMo I 2020-04-29 10:26:32 callbacks:224] Step: 2325\n",
      "[NeMo I 2020-04-29 10:26:32 helpers:104] Loss: 0.059230610728263855\n",
      "[NeMo I 2020-04-29 10:26:32 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:26:32 callbacks:239] Step time: 0.06367635726928711 seconds\n",
      "[NeMo I 2020-04-29 10:26:34 callbacks:224] Step: 2350\n",
      "[NeMo I 2020-04-29 10:26:34 helpers:104] Loss: 0.1819426417350769\n",
      "[NeMo I 2020-04-29 10:26:34 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:26:34 callbacks:239] Step time: 0.05076026916503906 seconds\n",
      "[NeMo I 2020-04-29 10:26:35 callbacks:224] Step: 2375\n",
      "[NeMo I 2020-04-29 10:26:35 helpers:104] Loss: 0.036565639078617096\n",
      "[NeMo I 2020-04-29 10:26:35 helpers:110] training_batch_top@1:  100.0000\n",
      "[NeMo I 2020-04-29 10:26:35 callbacks:239] Step time: 0.05423784255981445 seconds\n",
      "[NeMo I 2020-04-29 10:26:36 callbacks:224] Step: 2400\n",
      "[NeMo I 2020-04-29 10:26:36 helpers:104] Loss: 0.18114691972732544\n",
      "[NeMo I 2020-04-29 10:26:36 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:26:36 callbacks:239] Step time: 0.0515437126159668 seconds\n",
      "[NeMo I 2020-04-29 10:26:36 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:26:39 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.06934050470590591\n",
      "[NeMo I 2020-04-29 10:26:39 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 98.1026\n",
      "[NeMo I 2020-04-29 10:26:39 callbacks:450] Evaluation time: 2.677745819091797 seconds\n",
      "[NeMo I 2020-04-29 10:26:40 callbacks:224] Step: 2425\n",
      "[NeMo I 2020-04-29 10:26:40 helpers:104] Loss: 0.10510102659463882\n",
      "[NeMo I 2020-04-29 10:26:40 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:26:40 callbacks:239] Step time: 0.05423092842102051 seconds\n",
      "[NeMo I 2020-04-29 10:26:42 callbacks:224] Step: 2450\n",
      "[NeMo I 2020-04-29 10:26:42 helpers:104] Loss: 0.19129152595996857\n",
      "[NeMo I 2020-04-29 10:26:42 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:26:42 callbacks:239] Step time: 0.05389070510864258 seconds\n",
      "[NeMo I 2020-04-29 10:26:43 callbacks:224] Step: 2475\n",
      "[NeMo I 2020-04-29 10:26:43 helpers:104] Loss: 0.10363103449344635\n",
      "[NeMo I 2020-04-29 10:26:43 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:26:43 callbacks:239] Step time: 0.05237746238708496 seconds\n",
      "[NeMo I 2020-04-29 10:26:44 callbacks:224] Step: 2500\n",
      "[NeMo I 2020-04-29 10:26:44 helpers:104] Loss: 0.05336526781320572\n",
      "[NeMo I 2020-04-29 10:26:44 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:26:44 callbacks:239] Step time: 0.051445960998535156 seconds\n",
      "[NeMo I 2020-04-29 10:26:46 callbacks:224] Step: 2525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:26:46 helpers:104] Loss: 0.21195298433303833\n",
      "[NeMo I 2020-04-29 10:26:46 helpers:110] training_batch_top@1:  90.6250\n",
      "[NeMo I 2020-04-29 10:26:46 callbacks:239] Step time: 0.0598597526550293 seconds\n",
      "[NeMo I 2020-04-29 10:26:47 callbacks:224] Step: 2550\n",
      "[NeMo I 2020-04-29 10:26:47 helpers:104] Loss: 0.08120717853307724\n",
      "[NeMo I 2020-04-29 10:26:47 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:26:47 callbacks:239] Step time: 0.05080556869506836 seconds\n",
      "[NeMo I 2020-04-29 10:26:48 callbacks:224] Step: 2575\n",
      "[NeMo I 2020-04-29 10:26:48 helpers:104] Loss: 0.15907777845859528\n",
      "[NeMo I 2020-04-29 10:26:48 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:26:48 callbacks:239] Step time: 0.05122995376586914 seconds\n",
      "[NeMo I 2020-04-29 10:26:50 callbacks:224] Step: 2600\n",
      "[NeMo I 2020-04-29 10:26:50 helpers:104] Loss: 0.0868486613035202\n",
      "[NeMo I 2020-04-29 10:26:50 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:26:50 callbacks:239] Step time: 0.05065178871154785 seconds\n",
      "[NeMo I 2020-04-29 10:26:50 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:26:52 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.08620739728212357\n",
      "[NeMo I 2020-04-29 10:26:52 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 96.7318\n",
      "[NeMo I 2020-04-29 10:26:52 callbacks:450] Evaluation time: 2.684065818786621 seconds\n",
      "[NeMo I 2020-04-29 10:26:54 callbacks:224] Step: 2625\n",
      "[NeMo I 2020-04-29 10:26:54 helpers:104] Loss: 0.07964996993541718\n",
      "[NeMo I 2020-04-29 10:26:54 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:26:54 callbacks:239] Step time: 0.054490089416503906 seconds\n",
      "[NeMo I 2020-04-29 10:26:55 callbacks:224] Step: 2650\n",
      "[NeMo I 2020-04-29 10:26:55 helpers:104] Loss: 0.13373196125030518\n",
      "[NeMo I 2020-04-29 10:26:55 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:26:55 callbacks:239] Step time: 0.04942131042480469 seconds\n",
      "[NeMo I 2020-04-29 10:26:56 callbacks:224] Step: 2675\n",
      "[NeMo I 2020-04-29 10:26:56 helpers:104] Loss: 0.1453908532857895\n",
      "[NeMo I 2020-04-29 10:26:56 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:26:56 callbacks:239] Step time: 0.049837589263916016 seconds\n",
      "[NeMo I 2020-04-29 10:26:57 callbacks:207] Finished epoch 2 in 0:01:02.006271\n",
      "[NeMo I 2020-04-29 10:26:57 callbacks:199] Starting epoch 3\n",
      "[NeMo I 2020-04-29 10:26:59 callbacks:224] Step: 2700\n",
      "[NeMo I 2020-04-29 10:26:59 helpers:104] Loss: 0.05811038613319397\n",
      "[NeMo I 2020-04-29 10:26:59 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:26:59 callbacks:239] Step time: 0.06290078163146973 seconds\n",
      "[NeMo I 2020-04-29 10:27:00 callbacks:224] Step: 2725\n",
      "[NeMo I 2020-04-29 10:27:00 helpers:104] Loss: 0.08540324866771698\n",
      "[NeMo I 2020-04-29 10:27:00 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:00 callbacks:239] Step time: 0.06734418869018555 seconds\n",
      "[NeMo I 2020-04-29 10:27:01 callbacks:224] Step: 2750\n",
      "[NeMo I 2020-04-29 10:27:01 helpers:104] Loss: 0.01888876035809517\n",
      "[NeMo I 2020-04-29 10:27:01 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:27:01 callbacks:239] Step time: 0.05295133590698242 seconds\n",
      "[NeMo I 2020-04-29 10:27:03 callbacks:224] Step: 2775\n",
      "[NeMo I 2020-04-29 10:27:03 helpers:104] Loss: 0.09198489785194397\n",
      "[NeMo I 2020-04-29 10:27:03 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:27:03 callbacks:239] Step time: 0.05500531196594238 seconds\n",
      "[NeMo I 2020-04-29 10:27:04 callbacks:224] Step: 2800\n",
      "[NeMo I 2020-04-29 10:27:04 helpers:104] Loss: 0.19772876799106598\n",
      "[NeMo I 2020-04-29 10:27:04 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:27:04 callbacks:239] Step time: 0.05517077445983887 seconds\n",
      "[NeMo I 2020-04-29 10:27:04 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:27:07 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.09441033750772476\n",
      "[NeMo I 2020-04-29 10:27:07 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 97.5471\n",
      "[NeMo I 2020-04-29 10:27:07 callbacks:450] Evaluation time: 2.6774306297302246 seconds\n",
      "[NeMo I 2020-04-29 10:27:08 callbacks:224] Step: 2825\n",
      "[NeMo I 2020-04-29 10:27:08 helpers:104] Loss: 0.02948656491935253\n",
      "[NeMo I 2020-04-29 10:27:08 helpers:110] training_batch_top@1:  100.0000\n",
      "[NeMo I 2020-04-29 10:27:08 callbacks:239] Step time: 0.05135989189147949 seconds\n",
      "[NeMo I 2020-04-29 10:27:09 callbacks:224] Step: 2850\n",
      "[NeMo I 2020-04-29 10:27:09 helpers:104] Loss: 0.09910288453102112\n",
      "[NeMo I 2020-04-29 10:27:09 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:27:09 callbacks:239] Step time: 0.057996511459350586 seconds\n",
      "[NeMo I 2020-04-29 10:27:11 callbacks:224] Step: 2875\n",
      "[NeMo I 2020-04-29 10:27:11 helpers:104] Loss: 0.16689275205135345\n",
      "[NeMo I 2020-04-29 10:27:11 helpers:110] training_batch_top@1:  93.7500\n",
      "[NeMo I 2020-04-29 10:27:11 callbacks:239] Step time: 0.05141305923461914 seconds\n",
      "[NeMo I 2020-04-29 10:27:12 callbacks:224] Step: 2900\n",
      "[NeMo I 2020-04-29 10:27:12 helpers:104] Loss: 0.1113920584321022\n",
      "[NeMo I 2020-04-29 10:27:12 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:12 callbacks:239] Step time: 0.06421637535095215 seconds\n",
      "[NeMo I 2020-04-29 10:27:13 callbacks:224] Step: 2925\n",
      "[NeMo I 2020-04-29 10:27:13 helpers:104] Loss: 0.1374468207359314\n",
      "[NeMo I 2020-04-29 10:27:13 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:27:13 callbacks:239] Step time: 0.054579973220825195 seconds\n",
      "[NeMo I 2020-04-29 10:27:15 callbacks:224] Step: 2950\n",
      "[NeMo I 2020-04-29 10:27:15 helpers:104] Loss: 0.09529829025268555\n",
      "[NeMo I 2020-04-29 10:27:15 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:27:15 callbacks:239] Step time: 0.05193352699279785 seconds\n",
      "[NeMo I 2020-04-29 10:27:16 callbacks:224] Step: 2975\n",
      "[NeMo I 2020-04-29 10:27:16 helpers:104] Loss: 0.11982230842113495\n",
      "[NeMo I 2020-04-29 10:27:16 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:27:16 callbacks:239] Step time: 0.05131649971008301 seconds\n",
      "[NeMo I 2020-04-29 10:27:17 callbacks:224] Step: 3000\n",
      "[NeMo I 2020-04-29 10:27:17 helpers:104] Loss: 0.10399133712053299\n",
      "[NeMo I 2020-04-29 10:27:17 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:27:17 callbacks:239] Step time: 0.051053762435913086 seconds\n",
      "[NeMo I 2020-04-29 10:27:17 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:27:20 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.09770986437797546\n",
      "[NeMo I 2020-04-29 10:27:20 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 96.6741\n",
      "[NeMo I 2020-04-29 10:27:20 callbacks:450] Evaluation time: 2.688720703125 seconds\n",
      "[NeMo I 2020-04-29 10:27:20 callbacks:303] Saved checkpoint: .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints/trainer-STEP-3000.pt\n",
      "[NeMo I 2020-04-29 10:27:21 callbacks:224] Step: 3025\n",
      "[NeMo I 2020-04-29 10:27:21 helpers:104] Loss: 0.058684542775154114\n",
      "[NeMo I 2020-04-29 10:27:21 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:27:21 callbacks:239] Step time: 0.052535057067871094 seconds\n",
      "[NeMo I 2020-04-29 10:27:23 callbacks:224] Step: 3050\n",
      "[NeMo I 2020-04-29 10:27:23 helpers:104] Loss: 0.12345895916223526\n",
      "[NeMo I 2020-04-29 10:27:23 helpers:110] training_batch_top@1:  93.7500\n",
      "[NeMo I 2020-04-29 10:27:23 callbacks:239] Step time: 0.05328226089477539 seconds\n",
      "[NeMo I 2020-04-29 10:27:24 callbacks:224] Step: 3075\n",
      "[NeMo I 2020-04-29 10:27:24 helpers:104] Loss: 0.16889356076717377\n",
      "[NeMo I 2020-04-29 10:27:24 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:27:24 callbacks:239] Step time: 0.055274248123168945 seconds\n",
      "[NeMo I 2020-04-29 10:27:26 callbacks:224] Step: 3100\n",
      "[NeMo I 2020-04-29 10:27:26 helpers:104] Loss: 0.11166384816169739\n",
      "[NeMo I 2020-04-29 10:27:26 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:26 callbacks:239] Step time: 0.05066561698913574 seconds\n",
      "[NeMo I 2020-04-29 10:27:27 callbacks:224] Step: 3125\n",
      "[NeMo I 2020-04-29 10:27:27 helpers:104] Loss: 0.1109837219119072\n",
      "[NeMo I 2020-04-29 10:27:27 helpers:110] training_batch_top@1:  96.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:27:27 callbacks:239] Step time: 0.05430293083190918 seconds\n",
      "[NeMo I 2020-04-29 10:27:28 callbacks:224] Step: 3150\n",
      "[NeMo I 2020-04-29 10:27:28 helpers:104] Loss: 0.1677408665418625\n",
      "[NeMo I 2020-04-29 10:27:28 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:27:28 callbacks:239] Step time: 0.05421328544616699 seconds\n",
      "[NeMo I 2020-04-29 10:27:30 callbacks:224] Step: 3175\n",
      "[NeMo I 2020-04-29 10:27:30 helpers:104] Loss: 0.1150323674082756\n",
      "[NeMo I 2020-04-29 10:27:30 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:27:30 callbacks:239] Step time: 0.050679922103881836 seconds\n",
      "[NeMo I 2020-04-29 10:27:31 callbacks:224] Step: 3200\n",
      "[NeMo I 2020-04-29 10:27:31 helpers:104] Loss: 0.09516698122024536\n",
      "[NeMo I 2020-04-29 10:27:31 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:27:31 callbacks:239] Step time: 0.054258108139038086 seconds\n",
      "[NeMo I 2020-04-29 10:27:31 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:27:34 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.07558991760015488\n",
      "[NeMo I 2020-04-29 10:27:34 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 97.9655\n",
      "[NeMo I 2020-04-29 10:27:34 callbacks:450] Evaluation time: 2.6642773151397705 seconds\n",
      "[NeMo I 2020-04-29 10:27:35 callbacks:224] Step: 3225\n",
      "[NeMo I 2020-04-29 10:27:35 helpers:104] Loss: 0.07138925790786743\n",
      "[NeMo I 2020-04-29 10:27:35 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:35 callbacks:239] Step time: 0.0508427619934082 seconds\n",
      "[NeMo I 2020-04-29 10:27:36 callbacks:224] Step: 3250\n",
      "[NeMo I 2020-04-29 10:27:36 helpers:104] Loss: 0.06202761083841324\n",
      "[NeMo I 2020-04-29 10:27:36 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:36 callbacks:239] Step time: 0.058791160583496094 seconds\n",
      "[NeMo I 2020-04-29 10:27:38 callbacks:224] Step: 3275\n",
      "[NeMo I 2020-04-29 10:27:38 helpers:104] Loss: 0.15228262543678284\n",
      "[NeMo I 2020-04-29 10:27:38 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:38 callbacks:239] Step time: 0.06398558616638184 seconds\n",
      "[NeMo I 2020-04-29 10:27:39 callbacks:224] Step: 3300\n",
      "[NeMo I 2020-04-29 10:27:39 helpers:104] Loss: 0.020921027287840843\n",
      "[NeMo I 2020-04-29 10:27:39 helpers:110] training_batch_top@1:  100.0000\n",
      "[NeMo I 2020-04-29 10:27:39 callbacks:239] Step time: 0.0508275032043457 seconds\n",
      "[NeMo I 2020-04-29 10:27:40 callbacks:224] Step: 3325\n",
      "[NeMo I 2020-04-29 10:27:40 helpers:104] Loss: 0.1367417871952057\n",
      "[NeMo I 2020-04-29 10:27:40 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:40 callbacks:239] Step time: 0.05087685585021973 seconds\n",
      "[NeMo I 2020-04-29 10:27:42 callbacks:224] Step: 3350\n",
      "[NeMo I 2020-04-29 10:27:42 helpers:104] Loss: 0.08524440228939056\n",
      "[NeMo I 2020-04-29 10:27:42 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:42 callbacks:239] Step time: 0.05313754081726074 seconds\n",
      "[NeMo I 2020-04-29 10:27:43 callbacks:224] Step: 3375\n",
      "[NeMo I 2020-04-29 10:27:43 helpers:104] Loss: 0.10380123555660248\n",
      "[NeMo I 2020-04-29 10:27:43 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:43 callbacks:239] Step time: 0.051484107971191406 seconds\n",
      "[NeMo I 2020-04-29 10:27:44 callbacks:224] Step: 3400\n",
      "[NeMo I 2020-04-29 10:27:44 helpers:104] Loss: 0.04569854959845543\n",
      "[NeMo I 2020-04-29 10:27:44 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:27:44 callbacks:239] Step time: 0.0516352653503418 seconds\n",
      "[NeMo I 2020-04-29 10:27:44 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:27:47 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.07136167585849762\n",
      "[NeMo I 2020-04-29 10:27:47 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 97.7058\n",
      "[NeMo I 2020-04-29 10:27:47 callbacks:450] Evaluation time: 2.721564769744873 seconds\n",
      "[NeMo I 2020-04-29 10:27:48 callbacks:224] Step: 3425\n",
      "[NeMo I 2020-04-29 10:27:48 helpers:104] Loss: 0.08646143972873688\n",
      "[NeMo I 2020-04-29 10:27:48 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:48 callbacks:239] Step time: 0.05431628227233887 seconds\n",
      "[NeMo I 2020-04-29 10:27:50 callbacks:224] Step: 3450\n",
      "[NeMo I 2020-04-29 10:27:50 helpers:104] Loss: 0.07351109385490417\n",
      "[NeMo I 2020-04-29 10:27:50 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:50 callbacks:239] Step time: 0.051152944564819336 seconds\n",
      "[NeMo I 2020-04-29 10:27:51 callbacks:224] Step: 3475\n",
      "[NeMo I 2020-04-29 10:27:51 helpers:104] Loss: 0.08159083127975464\n",
      "[NeMo I 2020-04-29 10:27:51 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:51 callbacks:239] Step time: 0.05102729797363281 seconds\n",
      "[NeMo I 2020-04-29 10:27:52 callbacks:224] Step: 3500\n",
      "[NeMo I 2020-04-29 10:27:52 helpers:104] Loss: 0.07011497765779495\n",
      "[NeMo I 2020-04-29 10:27:52 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:52 callbacks:239] Step time: 0.05512404441833496 seconds\n",
      "[NeMo I 2020-04-29 10:27:54 callbacks:224] Step: 3525\n",
      "[NeMo I 2020-04-29 10:27:54 helpers:104] Loss: 0.08482909202575684\n",
      "[NeMo I 2020-04-29 10:27:54 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:54 callbacks:239] Step time: 0.054119110107421875 seconds\n",
      "[NeMo I 2020-04-29 10:27:55 callbacks:224] Step: 3550\n",
      "[NeMo I 2020-04-29 10:27:55 helpers:104] Loss: 0.10511782765388489\n",
      "[NeMo I 2020-04-29 10:27:55 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:27:55 callbacks:239] Step time: 0.0492253303527832 seconds\n",
      "[NeMo I 2020-04-29 10:27:56 callbacks:207] Finished epoch 3 in 0:00:59.514953\n",
      "[NeMo I 2020-04-29 10:27:56 callbacks:199] Starting epoch 4\n",
      "[NeMo I 2020-04-29 10:27:57 callbacks:224] Step: 3575\n",
      "[NeMo I 2020-04-29 10:27:57 helpers:104] Loss: 0.1367793083190918\n",
      "[NeMo I 2020-04-29 10:27:57 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:57 callbacks:239] Step time: 0.057634592056274414 seconds\n",
      "[NeMo I 2020-04-29 10:27:58 callbacks:224] Step: 3600\n",
      "[NeMo I 2020-04-29 10:27:58 helpers:104] Loss: 0.10197967290878296\n",
      "[NeMo I 2020-04-29 10:27:58 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:27:58 callbacks:239] Step time: 0.05113482475280762 seconds\n",
      "[NeMo I 2020-04-29 10:27:58 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:28:01 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.048593901097774506\n",
      "[NeMo I 2020-04-29 10:28:01 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 98.4777\n",
      "[NeMo I 2020-04-29 10:28:01 callbacks:450] Evaluation time: 2.7132320404052734 seconds\n",
      "[NeMo I 2020-04-29 10:28:02 callbacks:224] Step: 3625\n",
      "[NeMo I 2020-04-29 10:28:02 helpers:104] Loss: 0.11267171055078506\n",
      "[NeMo I 2020-04-29 10:28:02 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:02 callbacks:239] Step time: 0.05115461349487305 seconds\n",
      "[NeMo I 2020-04-29 10:28:04 callbacks:224] Step: 3650\n",
      "[NeMo I 2020-04-29 10:28:04 helpers:104] Loss: 0.039293501526117325\n",
      "[NeMo I 2020-04-29 10:28:04 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:28:04 callbacks:239] Step time: 0.05127978324890137 seconds\n",
      "[NeMo I 2020-04-29 10:28:05 callbacks:224] Step: 3675\n",
      "[NeMo I 2020-04-29 10:28:05 helpers:104] Loss: 0.06883879005908966\n",
      "[NeMo I 2020-04-29 10:28:05 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:05 callbacks:239] Step time: 0.05458784103393555 seconds\n",
      "[NeMo I 2020-04-29 10:28:06 callbacks:224] Step: 3700\n",
      "[NeMo I 2020-04-29 10:28:06 helpers:104] Loss: 0.0750492513179779\n",
      "[NeMo I 2020-04-29 10:28:06 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:06 callbacks:239] Step time: 0.054225921630859375 seconds\n",
      "[NeMo I 2020-04-29 10:28:08 callbacks:224] Step: 3725\n",
      "[NeMo I 2020-04-29 10:28:08 helpers:104] Loss: 0.03267631307244301\n",
      "[NeMo I 2020-04-29 10:28:08 helpers:110] training_batch_top@1:  100.0000\n",
      "[NeMo I 2020-04-29 10:28:08 callbacks:239] Step time: 0.0543668270111084 seconds\n",
      "[NeMo I 2020-04-29 10:28:09 callbacks:224] Step: 3750\n",
      "[NeMo I 2020-04-29 10:28:09 helpers:104] Loss: 0.10222311317920685\n",
      "[NeMo I 2020-04-29 10:28:09 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:28:09 callbacks:239] Step time: 0.051474571228027344 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:28:11 callbacks:224] Step: 3775\n",
      "[NeMo I 2020-04-29 10:28:11 helpers:104] Loss: 0.09238077700138092\n",
      "[NeMo I 2020-04-29 10:28:11 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:28:11 callbacks:239] Step time: 0.05410361289978027 seconds\n",
      "[NeMo I 2020-04-29 10:28:12 callbacks:224] Step: 3800\n",
      "[NeMo I 2020-04-29 10:28:12 helpers:104] Loss: 0.07627440989017487\n",
      "[NeMo I 2020-04-29 10:28:12 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:28:12 callbacks:239] Step time: 0.052138328552246094 seconds\n",
      "[NeMo I 2020-04-29 10:28:12 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:28:15 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.049037039279937744\n",
      "[NeMo I 2020-04-29 10:28:15 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 98.6437\n",
      "[NeMo I 2020-04-29 10:28:15 callbacks:450] Evaluation time: 2.6959190368652344 seconds\n",
      "[NeMo I 2020-04-29 10:28:16 callbacks:224] Step: 3825\n",
      "[NeMo I 2020-04-29 10:28:16 helpers:104] Loss: 0.07330396771430969\n",
      "[NeMo I 2020-04-29 10:28:16 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:28:16 callbacks:239] Step time: 0.05430412292480469 seconds\n",
      "[NeMo I 2020-04-29 10:28:17 callbacks:224] Step: 3850\n",
      "[NeMo I 2020-04-29 10:28:17 helpers:104] Loss: 0.07658527046442032\n",
      "[NeMo I 2020-04-29 10:28:17 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:28:17 callbacks:239] Step time: 0.05412697792053223 seconds\n",
      "[NeMo I 2020-04-29 10:28:19 callbacks:224] Step: 3875\n",
      "[NeMo I 2020-04-29 10:28:19 helpers:104] Loss: 0.09705013036727905\n",
      "[NeMo I 2020-04-29 10:28:19 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:19 callbacks:239] Step time: 0.053887128829956055 seconds\n",
      "[NeMo I 2020-04-29 10:28:20 callbacks:224] Step: 3900\n",
      "[NeMo I 2020-04-29 10:28:20 helpers:104] Loss: 0.08430173993110657\n",
      "[NeMo I 2020-04-29 10:28:20 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:20 callbacks:239] Step time: 0.05408835411071777 seconds\n",
      "[NeMo I 2020-04-29 10:28:21 callbacks:224] Step: 3925\n",
      "[NeMo I 2020-04-29 10:28:21 helpers:104] Loss: 0.1438731551170349\n",
      "[NeMo I 2020-04-29 10:28:21 helpers:110] training_batch_top@1:  94.5312\n",
      "[NeMo I 2020-04-29 10:28:21 callbacks:239] Step time: 0.05128288269042969 seconds\n",
      "[NeMo I 2020-04-29 10:28:23 callbacks:224] Step: 3950\n",
      "[NeMo I 2020-04-29 10:28:23 helpers:104] Loss: 0.11641387641429901\n",
      "[NeMo I 2020-04-29 10:28:23 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:23 callbacks:239] Step time: 0.050872802734375 seconds\n",
      "[NeMo I 2020-04-29 10:28:24 callbacks:224] Step: 3975\n",
      "[NeMo I 2020-04-29 10:28:24 helpers:104] Loss: 0.11888992786407471\n",
      "[NeMo I 2020-04-29 10:28:24 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:28:24 callbacks:239] Step time: 0.05091428756713867 seconds\n",
      "[NeMo I 2020-04-29 10:28:25 callbacks:224] Step: 4000\n",
      "[NeMo I 2020-04-29 10:28:25 helpers:104] Loss: 0.08796387910842896\n",
      "[NeMo I 2020-04-29 10:28:25 helpers:110] training_batch_top@1:  96.0938\n",
      "[NeMo I 2020-04-29 10:28:25 callbacks:239] Step time: 0.05072808265686035 seconds\n",
      "[NeMo I 2020-04-29 10:28:25 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:28:28 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.04505020007491112\n",
      "[NeMo I 2020-04-29 10:28:28 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 98.5499\n",
      "[NeMo I 2020-04-29 10:28:28 callbacks:450] Evaluation time: 2.775146961212158 seconds\n",
      "[NeMo I 2020-04-29 10:28:28 callbacks:303] Saved checkpoint: .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints/trainer-STEP-4000.pt\n",
      "[NeMo I 2020-04-29 10:28:30 callbacks:224] Step: 4025\n",
      "[NeMo I 2020-04-29 10:28:30 helpers:104] Loss: 0.06865464150905609\n",
      "[NeMo I 2020-04-29 10:28:30 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:28:30 callbacks:239] Step time: 0.05118274688720703 seconds\n",
      "[NeMo I 2020-04-29 10:28:31 callbacks:224] Step: 4050\n",
      "[NeMo I 2020-04-29 10:28:31 helpers:104] Loss: 0.023320330306887627\n",
      "[NeMo I 2020-04-29 10:28:31 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:28:31 callbacks:239] Step time: 0.053954362869262695 seconds\n",
      "[NeMo I 2020-04-29 10:28:32 callbacks:224] Step: 4075\n",
      "[NeMo I 2020-04-29 10:28:32 helpers:104] Loss: 0.020520418882369995\n",
      "[NeMo I 2020-04-29 10:28:32 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:28:32 callbacks:239] Step time: 0.054556846618652344 seconds\n",
      "[NeMo I 2020-04-29 10:28:34 callbacks:224] Step: 4100\n",
      "[NeMo I 2020-04-29 10:28:34 helpers:104] Loss: 0.11030283570289612\n",
      "[NeMo I 2020-04-29 10:28:34 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:34 callbacks:239] Step time: 0.05418515205383301 seconds\n",
      "[NeMo I 2020-04-29 10:28:35 callbacks:224] Step: 4125\n",
      "[NeMo I 2020-04-29 10:28:35 helpers:104] Loss: 0.03240646421909332\n",
      "[NeMo I 2020-04-29 10:28:35 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:28:35 callbacks:239] Step time: 0.051256656646728516 seconds\n",
      "[NeMo I 2020-04-29 10:28:36 callbacks:224] Step: 4150\n",
      "[NeMo I 2020-04-29 10:28:36 helpers:104] Loss: 0.1022593304514885\n",
      "[NeMo I 2020-04-29 10:28:36 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:36 callbacks:239] Step time: 0.0542597770690918 seconds\n",
      "[NeMo I 2020-04-29 10:28:38 callbacks:224] Step: 4175\n",
      "[NeMo I 2020-04-29 10:28:38 helpers:104] Loss: 0.0832553580403328\n",
      "[NeMo I 2020-04-29 10:28:38 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:38 callbacks:239] Step time: 0.05864262580871582 seconds\n",
      "[NeMo I 2020-04-29 10:28:39 callbacks:224] Step: 4200\n",
      "[NeMo I 2020-04-29 10:28:39 helpers:104] Loss: 0.03643099218606949\n",
      "[NeMo I 2020-04-29 10:28:39 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:28:39 callbacks:239] Step time: 0.05430006980895996 seconds\n",
      "[NeMo I 2020-04-29 10:28:39 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:28:42 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.050162822008132935\n",
      "[NeMo I 2020-04-29 10:28:42 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 98.5643\n",
      "[NeMo I 2020-04-29 10:28:42 callbacks:450] Evaluation time: 2.6498334407806396 seconds\n",
      "[NeMo I 2020-04-29 10:28:43 callbacks:224] Step: 4225\n",
      "[NeMo I 2020-04-29 10:28:43 helpers:104] Loss: 0.0594276487827301\n",
      "[NeMo I 2020-04-29 10:28:43 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:28:43 callbacks:239] Step time: 0.05102729797363281 seconds\n",
      "[NeMo I 2020-04-29 10:28:44 callbacks:224] Step: 4250\n",
      "[NeMo I 2020-04-29 10:28:44 helpers:104] Loss: 0.055781226605176926\n",
      "[NeMo I 2020-04-29 10:28:44 helpers:110] training_batch_top@1:  97.6562\n",
      "[NeMo I 2020-04-29 10:28:44 callbacks:239] Step time: 0.05110597610473633 seconds\n",
      "[NeMo I 2020-04-29 10:28:46 callbacks:224] Step: 4275\n",
      "[NeMo I 2020-04-29 10:28:46 helpers:104] Loss: 0.06145215034484863\n",
      "[NeMo I 2020-04-29 10:28:46 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:28:46 callbacks:239] Step time: 0.050757408142089844 seconds\n",
      "[NeMo I 2020-04-29 10:28:47 callbacks:224] Step: 4300\n",
      "[NeMo I 2020-04-29 10:28:47 helpers:104] Loss: 0.06860387325286865\n",
      "[NeMo I 2020-04-29 10:28:47 helpers:110] training_batch_top@1:  96.8750\n",
      "[NeMo I 2020-04-29 10:28:47 callbacks:239] Step time: 0.05425691604614258 seconds\n",
      "[NeMo I 2020-04-29 10:28:48 callbacks:224] Step: 4325\n",
      "[NeMo I 2020-04-29 10:28:48 helpers:104] Loss: 0.03713781014084816\n",
      "[NeMo I 2020-04-29 10:28:48 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:28:48 callbacks:239] Step time: 0.05272984504699707 seconds\n",
      "[NeMo I 2020-04-29 10:28:50 callbacks:224] Step: 4350\n",
      "[NeMo I 2020-04-29 10:28:50 helpers:104] Loss: 0.020292792469263077\n",
      "[NeMo I 2020-04-29 10:28:50 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:28:50 callbacks:239] Step time: 0.05399727821350098 seconds\n",
      "[NeMo I 2020-04-29 10:28:51 callbacks:224] Step: 4375\n",
      "[NeMo I 2020-04-29 10:28:51 helpers:104] Loss: 0.03359495475888252\n",
      "[NeMo I 2020-04-29 10:28:51 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:28:51 callbacks:239] Step time: 0.05258941650390625 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:28:52 callbacks:224] Step: 4400\n",
      "[NeMo I 2020-04-29 10:28:52 helpers:104] Loss: 0.06088229641318321\n",
      "[NeMo I 2020-04-29 10:28:52 helpers:110] training_batch_top@1:  98.4375\n",
      "[NeMo I 2020-04-29 10:28:52 callbacks:239] Step time: 0.057554006576538086 seconds\n",
      "[NeMo I 2020-04-29 10:28:52 callbacks:445] Doing Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:28:55 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.044208068400621414\n",
      "[NeMo I 2020-04-29 10:28:55 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 98.7519\n",
      "[NeMo I 2020-04-29 10:28:55 callbacks:450] Evaluation time: 2.7680327892303467 seconds\n",
      "[NeMo I 2020-04-29 10:28:57 callbacks:224] Step: 4425\n",
      "[NeMo I 2020-04-29 10:28:57 helpers:104] Loss: 0.01991482637822628\n",
      "[NeMo I 2020-04-29 10:28:57 helpers:110] training_batch_top@1:  99.2188\n",
      "[NeMo I 2020-04-29 10:28:57 callbacks:239] Step time: 0.052809953689575195 seconds\n",
      "[NeMo I 2020-04-29 10:28:58 callbacks:224] Step: 4450\n",
      "[NeMo I 2020-04-29 10:28:58 helpers:104] Loss: 0.10388176143169403\n",
      "[NeMo I 2020-04-29 10:28:58 helpers:110] training_batch_top@1:  95.3125\n",
      "[NeMo I 2020-04-29 10:28:58 callbacks:239] Step time: 0.049547672271728516 seconds\n",
      "[NeMo I 2020-04-29 10:28:59 callbacks:207] Finished epoch 4 in 0:01:02.482966\n",
      "[NeMo I 2020-04-29 10:28:59 callbacks:195] Done in 0:05:05.876078\n",
      "[NeMo I 2020-04-29 10:28:59 callbacks:468] Final Evaluation ..............................\n",
      "[NeMo I 2020-04-29 10:29:01 helpers:273] ==========>>>>>>Evaluation Loss TestSet: 0.045099321752786636\n",
      "[NeMo I 2020-04-29 10:29:01 helpers:275] ==========>>>>>>Evaluation Accuracy Top@1 TestSet: 98.6581\n",
      "[NeMo I 2020-04-29 10:29:01 callbacks:473] Evaluation time: 2.691084146499634 seconds\n",
      "[NeMo I 2020-04-29 10:29:01 callbacks:303] Saved checkpoint: .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints/trainer-STEP-4465.pt\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Now we have all the components required to train the model\n",
    "# Lets define a learning rate schedule\n",
    "\n",
    "# Define a learning rate schedule\n",
    "lr_policy = CosineAnnealing(\n",
    "    total_steps=num_epochs * steps_per_epoch,\n",
    "    warmup_ratio=0.05,\n",
    "    min_lr=0.001,\n",
    ")\n",
    "\n",
    "logging.info(f\"Using `{lr_policy}` Learning Rate Scheduler\")\n",
    "\n",
    "# Finally, lets train this model !\n",
    "neural_factory.train(\n",
    "    tensors_to_optimize=[train_loss],\n",
    "    callbacks=callbacks,\n",
    "    lr_policy=lr_policy,\n",
    "    optimizer=\"novograd\",\n",
    "    optimization_params={\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"max_steps\": None,\n",
    "        \"lr\": lr,\n",
    "        \"momentum\": 0.95,\n",
    "        \"betas\": (0.98, 0.5),\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"grad_norm_clip\": None,\n",
    "    },\n",
    "    batches_per_step=1,\n",
    ")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.57925844192505"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of incorrectly predicted samples\n",
    "\n",
    "Given that we have a trained model, which performs reasonably well, lets try to listen to the samples where the model is least confident in its predictions.\n",
    "\n",
    "For this, we need support of the librosa library.\n",
    "\n",
    "**NOTE**: The following code depends on librosa. To install it, run the following code block first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add a path to the checkpoint dir\n",
    "model_path = neural_factory.checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the predictions from the model\n",
    "\n",
    "We want to possess the actual logits of the model instead of just the final evaluation score, so we use `NeuralFactory.infer(...)` to extract the logits per batch of samples provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:29:01 actions:1493] Restoring JasperEncoder from .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints/JasperEncoder-STEP-4465.pt\n",
      "[NeMo I 2020-04-29 10:29:01 actions:1493] Restoring JasperDecoderForClassification from .//home/fjia/data/freesound_resampled/quartznet-3x1-vcombo_balanced_sc_bg_combine_test/checkpoints/JasperDecoderForClassification-STEP-4465.pt\n",
      "[NeMo I 2020-04-29 10:29:02 actions:734] Evaluating batch 0 out of 109\n",
      "[NeMo I 2020-04-29 10:29:02 actions:734] Evaluating batch 10 out of 109\n",
      "[NeMo I 2020-04-29 10:29:02 actions:734] Evaluating batch 20 out of 109\n",
      "[NeMo I 2020-04-29 10:29:02 actions:734] Evaluating batch 30 out of 109\n",
      "[NeMo I 2020-04-29 10:29:03 actions:734] Evaluating batch 40 out of 109\n",
      "[NeMo I 2020-04-29 10:29:03 actions:734] Evaluating batch 50 out of 109\n",
      "[NeMo I 2020-04-29 10:29:03 actions:734] Evaluating batch 60 out of 109\n",
      "[NeMo I 2020-04-29 10:29:03 actions:734] Evaluating batch 70 out of 109\n",
      "[NeMo I 2020-04-29 10:29:03 actions:734] Evaluating batch 80 out of 109\n",
      "[NeMo I 2020-04-29 10:29:04 actions:734] Evaluating batch 90 out of 109\n",
      "[NeMo I 2020-04-29 10:29:04 actions:734] Evaluating batch 100 out of 109\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Only --- #\n",
    "# We've already built the inference DAG above, so all we need is to call infer().\n",
    "evaluated_tensors = neural_factory.infer(\n",
    "    # These are the tensors we want to get from the model.\n",
    "    tensors=[test_loss, test_decoded, test_labels],\n",
    "    # checkpoint_dir specifies where the model params are loaded from.\n",
    "    checkpoint_dir=model_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:29:04 <ipython-input-22-674fb7de9132>:19] Total correct / Total count : 13675 / 13861\n",
      "[NeMo I 2020-04-29 10:29:04 <ipython-input-22-674fb7de9132>:20] Final accuracy : 0.986581054757954\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for batch_idx, (logits, labels) in enumerate(zip(evaluated_tensors[1], evaluated_tensors[2])):\n",
    "    acc = classification_accuracy(\n",
    "        logits=logits,\n",
    "        targets=labels,\n",
    "        top_k=[1]\n",
    "    )\n",
    "\n",
    "    # Select top 1 accuracy only\n",
    "    acc = acc[0]\n",
    "\n",
    "    # Since accuracy here is \"per batch\", we simply denormalize it by multiplying\n",
    "    # by batch size to recover the count of correct samples.\n",
    "    correct_count += int(acc * logits.size(0))\n",
    "    total_count += logits.size(0)\n",
    "\n",
    "logging.info(f\"Total correct / Total count : {correct_count} / {total_count}\")\n",
    "logging.info(f\"Final accuracy : {correct_count / float(total_count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo test\n",
    "from typing import List, Optional\n",
    "def binary_classification_confusion_matrix(logits: torch.Tensor, targets: torch.Tensor, top_k: Optional[List[int]] = None) -> List[float]:\n",
    "    \"\"\"\n",
    "    ]\n",
    "    [TODO]\n",
    "    Computes the top-k classification accuracy provided with\n",
    "    un-normalized logits of a model and ground truth targets.\n",
    "    If top_k is not provided, defaults to top_1 accuracy.\n",
    "    If top_k is provided as a list, then the values are sorted\n",
    "    in ascending order.\n",
    "    Args:\n",
    "        logits: Un-normalized logits of a model. Softmax will be\n",
    "            applied to these logits prior to computation of accuracy.\n",
    "        targets: Vector of integers which represent indices of class\n",
    "            labels.\n",
    "        top_k: Optional list of integers in the range [1, max_classes].\n",
    "    Returns:\n",
    "        A list of length `top_k`, where each value represents top_i\n",
    "        accuracy (i in `top_k`).\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = [1]\n",
    "    max_k = max(top_k)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        _, predictions = logits.topk(max_k, dim=1, largest=True, sorted=True)\n",
    "        predictions = predictions.t().squeeze()\n",
    "\n",
    "        # speech(command) positive | background negative\n",
    "        \n",
    "        true_negative = 0\n",
    "        false_negative = 0\n",
    "        false_positive = 0\n",
    "        true_positive = 0\n",
    "        \n",
    "        for i in range(predictions.size(-1)):\n",
    "            pred = predictions[i]\n",
    "            targ = targets[i]\n",
    "#             print(pred, targ)\n",
    "            if pred == 0 and targ == 0:\n",
    "                true_negative += 1\n",
    "            elif pred == 0 and targ == 1:\n",
    "                false_negative += 1\n",
    "            elif pred == 1 and targ == 0:\n",
    "                false_positive += 1\n",
    "            elif pred == 1 and targ == 1:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                raise ValueError('Predictions or targets not in 0/1')\n",
    "               \n",
    "                \n",
    "#         correct = predictions.eq(targets.view(1, -1)).expand_as(predictions)\n",
    "#         print(correct)\n",
    "\n",
    "#         results = []\n",
    "#         for k in top_k:\n",
    "#             correct_k = correct[:k].view(-1).float().mean().to('cpu').numpy()\n",
    "#             results.append(correct_k)\n",
    "\n",
    "#     return results\n",
    "    return true_negative, false_negative , false_positive, true_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-29 10:42:14 <ipython-input-26-c91b90f9f69f>:22] Final Precision: 0.9842262327696462\n",
      "[NeMo I 2020-04-29 10:42:14 <ipython-input-26-c91b90f9f69f>:23] Final Recall : 0.9892872446793315\n",
      "[NeMo I 2020-04-29 10:42:14 <ipython-input-26-c91b90f9f69f>:24] Final F1 score : 0.9867502493232654\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "total_true_negative, total_false_negative , total_false_positive, total_true_positive = 0, 0, 0, 0\n",
    "for batch_idx, (logits, labels) in enumerate(zip(evaluated_tensors[1], evaluated_tensors[2])):\n",
    "    true_negative, false_negative , false_positive, true_positive = binary_classification_confusion_matrix(\n",
    "        logits=logits,\n",
    "        targets=labels,\n",
    "        top_k=[1]\n",
    "    )\n",
    "\n",
    "    total_true_negative += true_negative\n",
    "    total_false_negative += false_negative\n",
    "    total_false_positive += false_positive\n",
    "    total_true_positive  += true_positive\n",
    "    \n",
    "#     print(cc)\n",
    "\n",
    "precision = total_true_positive / (total_true_positive + total_false_positive)\n",
    "recall = total_true_positive / (total_true_positive + total_false_negative)\n",
    "f1_score =  2 * precision * recall / (precision + recall)\n",
    "logging.info(f\"Final Precision: {precision}\")\n",
    "logging.info(f\"Final Recall : {recall}\")\n",
    "logging.info(f\"Final F1 score : {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out incorrect samples\n",
    "Let us now filter out the incorrectly labeled samples from the total set of samples in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import json\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets create a utility class to remap the integer class labels to actual string label\n",
    "class ReverseMapLabel:\n",
    "    def __init__(self, data_layer: nemo_asr.AudioToSpeechLabelDataLayer):\n",
    "        self.label2id = dict(data_layer._dataset.label2id)\n",
    "        self.id2label = dict(data_layer._dataset.id2label)\n",
    "\n",
    "    def __call__(self, pred_idx, label_idx):\n",
    "        return self.id2label[pred_idx], self.id2label[label_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, lets get the indices of all the incorrectly labeled samples\n",
    "sample_idx = 0\n",
    "incorrect_preds = []\n",
    "rev_map = ReverseMapLabel(eval_data_layer)\n",
    "\n",
    "# Remember, evaluated_tensor = (loss, logits, labels)\n",
    "for batch_idx, (logits, labels) in enumerate(zip(evaluated_tensors[1], evaluated_tensors[2])):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    probas, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "    incorrect_ids = (preds != labels).nonzero()\n",
    "    for idx in incorrect_ids:\n",
    "        proba = float(probas[idx][0])\n",
    "        pred = int(preds[idx][0])\n",
    "        label = int(labels[idx][0])\n",
    "        idx = int(idx[0]) + sample_idx\n",
    "\n",
    "        incorrect_preds.append((idx, *rev_map(pred, label), proba))\n",
    "\n",
    "    sample_idx += labels.size(0)\n",
    "\n",
    "logging.info(f\"Num test samples : {total_count}\")\n",
    "logging.info(f\"Num errors : {len(incorrect_preds)}\")\n",
    "\n",
    "# First lets sort by confidence of prediction\n",
    "incorrect_preds = sorted(incorrect_preds, key=lambda x: x[-1], reverse=True) #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, lets get the indices of all the incorrectly labeled samples\n",
    "sample_idx = 0\n",
    "correct_preds = []\n",
    "rev_map = ReverseMapLabel(eval_data_layer)\n",
    "\n",
    "# Remember, evaluated_tensor = (loss, logits, labels)\n",
    "for batch_idx, (logits, labels) in enumerate(zip(evaluated_tensors[1], evaluated_tensors[2])):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    probas, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "    correct_ids = (preds == labels).nonzero()\n",
    "    for idx in correct_ids:\n",
    "        proba = float(probas[idx][0])\n",
    "        pred = int(preds[idx][0])\n",
    "        label = int(labels[idx][0])\n",
    "        idx = int(idx[0]) + sample_idx\n",
    "\n",
    "        correct_preds.append((idx, *rev_map(pred, label), proba))\n",
    "\n",
    "    sample_idx += labels.size(0)\n",
    "\n",
    "logging.info(f\"Num test samples : {total_count}\")\n",
    "logging.info(f\"Num correct : {len(correct_preds)}\")\n",
    "\n",
    "# First lets sort by confidence of prediction\n",
    "correct_preds = sorted(correct_preds, key=lambda x: x[-1], reverse=True) #False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine a subset of incorrect samples\n",
    "Lets print out the (test id, predicted label, ground truth label, confidence) tuple of first 20 incorrectly labeled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for incorrect_sample in incorrect_preds[:50]:\n",
    "    \n",
    "#     if incorrect_sample[2] == 'background':\n",
    "#         print(incorrect_sample)\n",
    "    logging.info(str(incorrect_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define a threshold below which we designate a model's prediction as \"low confidence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out how many such samples exist\n",
    "low_confidence_threshold = 0.55\n",
    "count_low_confidence = len(list(filter(lambda x: x[-1] <= low_confidence_threshold, incorrect_preds)))\n",
    "logging.info(f\"Number of low confidence predictions : {count_low_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out how many such samples exist\n",
    "high_confidence_threshold = 0.99\n",
    "count_high_confidence = len(list(filter(lambda x: x[-1] >= high_confidence_threshold, incorrect_preds)))\n",
    "logging.info(f\"Number of high confidence predictions : {count_high_confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets hear the samples which the model has least confidence in !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets create a helper function to parse the manifest files\n",
    "def parse_manifest(manifest):\n",
    "    data = []\n",
    "    for line in manifest:\n",
    "        line = json.loads(line)\n",
    "        data.append(line)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, lets create a helper function to actually listen to certain samples\n",
    "def listen_to_file(sample_id, pred=None, label=None, proba=None):\n",
    "    # Load the audio waveform using librosa\n",
    "    filepath = test_samples[sample_id]['audio_filepath']\n",
    "    if 'offset' in test_samples[sample_id]:\n",
    "        audio, sample_rate = librosa.load(filepath,\n",
    "                                          offset = test_samples[sample_id]['offset'],\n",
    "                                          duration = test_samples[sample_id]['duration'])\n",
    "    else:\n",
    "         audio, sample_rate = librosa.load(filepath)\n",
    "\n",
    "    if pred is not None and label is not None and proba is not None:\n",
    "        logging.info(f\"filepath: {filepath}, Sample : {sample_id} Prediction : {pred} Label : {label} Confidence = {proba: 0.4f}\")\n",
    "    else:\n",
    "        logging.info(f\"Sample : {sample_id}\")\n",
    "\n",
    "    return ipd.Audio(audio, rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets load the test manifest into memory\n",
    "\n",
    "all_test_samples = []\n",
    "for _ in test_dataset.split(','):\n",
    "    with open(_, 'r') as test_f:\n",
    "        test_samples = test_f.readlines()\n",
    "        print(_, len(test_samples))\n",
    "       \n",
    "        all_test_samples.extend(test_samples)\n",
    "        \n",
    "test_samples = parse_manifest(all_test_samples)\n",
    "print(len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, lets listen to all the audio samples where the model made a mistake\n",
    "# Note: This list of incorrect samples may be quite large, so you may choose to subsample `incorrect_preds`\n",
    "\n",
    "for sample_id, pred, label, proba in incorrect_preds[:200]:\n",
    "    filepath = test_samples[sample_id]['audio_filepath']\n",
    "    \n",
    "    print(test_samples[sample_id])\n",
    "#     if filepath not in exist:\n",
    "    ipd.display(listen_to_file(sample_id, pred=pred, label=label, proba=proba))\n",
    "    exist.add(filepath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Finally, lets listen to all the audio samples where the model made a mistake\n",
    "# Note: This list of incorrect samples may be quite large, so you may choose to subsample `incorrect_preds`\n",
    "for sample_id, pred, label, proba in incorrect_preds[:count_low_confidence]:\n",
    "    ipd.display(listen_to_file(sample_id, pred=pred, label=label, proba=proba))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'audio_filepath': '/home/fjia/data/freesound_resampled/Aircraft/id_437765 G04-01_P-51FighterBy.wav', 'duration': 1.0, 'label': 'background', 'text': '_', 'offset': 2.0}\n",
    "[NeMo I 2020-04-19 22:04:47 <ipython-input-41-3d463c444dc4>:13] filepaht: /home/fjia/data/freesound_resampled/Aircraft/id_437765 G04-01_P-51FighterBy.wav, Sample : 3353 Prediction : commands Label : background Confidence =  0.9999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
