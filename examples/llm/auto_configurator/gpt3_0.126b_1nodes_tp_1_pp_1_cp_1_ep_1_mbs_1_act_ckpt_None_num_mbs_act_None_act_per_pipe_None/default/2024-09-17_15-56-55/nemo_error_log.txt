[NeMo W 2024-09-17 15:56:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(ctx, input, weight, bias, allreduce_dgrad):
    
[NeMo W 2024-09-17 15:56:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2024-09-17 15:56:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(
    
[NeMo W 2024-09-17 15:56:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2024-09-17 15:56:52 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor
    
[NeMo W 2024-09-17 15:56:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
    
[NeMo W 2024-09-17 15:56:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, dout):
    
[NeMo W 2024-09-17 15:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/ops/triton/layer_norm.py:959: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(
    
[NeMo W 2024-09-17 15:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/ops/triton/layer_norm.py:1018: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, dout, *args):
    
[NeMo W 2024-09-17 15:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):
    
[NeMo W 2024-09-17 15:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2024-09-17 15:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/ops/triton/ssd_combined.py:736: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float("inf")), return_final_states=False, activation="silu",
    
[NeMo W 2024-09-17 15:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm-2.0.3-py3.10-linux-x86_64.egg/mamba_ssm/ops/triton/ssd_combined.py:814: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, dout, *args):
    
[NeMo W 2024-09-17 15:56:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:166: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
      quantize_op_abstract = torch.library.impl_abstract("tensorrt::quantize_op")(
    
[NeMo W 2024-09-17 15:56:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.
      cm = get_cmap("Set1")
    
[NeMo W 2024-09-17 15:56:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
      if get_gast_version() < LooseVersion("0.5"):
    
[NeMo W 2024-09-17 15:56:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
      other = LooseVersion(other)
    
[NeMo W 2024-09-17 15:56:55 nemo_logger:173] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to gpt3_0.126b_1nodes_tp_1_pp_1_cp_1_ep_1_mbs_1_act_ckpt_None_num_mbs_act_None_act_per_pipe_None/tb_logs
[NeMo W 2024-09-17 15:56:55 mixed_precision:219] Overwrote Config.bf16  False -> True
[NeMo W 2024-09-17 15:56:55 mixed_precision:219] Overwrote Config.autocast_dtype  None -> torch.bfloat16
[NeMo W 2024-09-17 15:56:55 mixed_precision:219] Overwrote GPTConfig126M.bf16  False -> True
[NeMo W 2024-09-17 15:56:55 mixed_precision:219] Overwrote GPTConfig126M.autocast_dtype  torch.float32 -> torch.bfloat16
[NeMo W 2024-09-17 15:56:55 mixed_precision:219] Overwrote DistributedDataParallelConfig.grad_reduce_in_fp32  False -> True
[NeMo W 2024-09-17 15:56:56 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.
      warnings.warn(
    
[NeMo W 2024-09-17 15:57:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.
      warnings.warn(
    
[NeMo W 2024-09-17 15:57:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('global_batch_size', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2024-09-17 15:57:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2024-09-17 15:57:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.
      warnings.warn(
    
[NeMo W 2024-09-17 15:57:01 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.
      warnings.warn(
    
[NeMo W 2024-09-17 15:57:02 nemo_logging:349] /opt/megatron-lm/megatron/core/distributed/param_and_grad_buffer.py:259: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
      torch.distributed._reduce_scatter_base(
    
[NeMo W 2024-09-17 15:57:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
      return func(*args, **kwargs)
    
[NeMo W 2024-09-17 15:57:14 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmptsrald2z'>
      _warnings.warn(warn_message, ResourceWarning)
    
