[NeMo I 2024-07-30 17:37:51 tokenizer_utils:216] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2024-07-30 17:37:51 tokenizer_utils:132] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo W 2024-07-30 17:37:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
      warnings.warn(
    
[NeMo I 2024-07-30 17:37:52 nemo_logger:124] Experiments will be logged at default/2024-07-30_17-37-52
[NeMo W 2024-07-30 17:37:52 nemo_logger:156] "update_logger_directory" is True. Overwriting logger "save_dir" to . and "name" to default
[NeMo I 2024-07-30 17:37:52 megatron_init:269] Rank 0 has data parallel group : [0]
[NeMo I 2024-07-30 17:37:52 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2024-07-30 17:37:52 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2024-07-30 17:37:52 megatron_init:283] Ranks 0 has data parallel rank: 0
[NeMo I 2024-07-30 17:37:52 megatron_init:291] Rank 0 has context parallel group: [0]
[NeMo I 2024-07-30 17:37:52 megatron_init:294] All context parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:37:52 megatron_init:295] Ranks 0 has context parallel rank: 0
[NeMo I 2024-07-30 17:37:52 megatron_init:302] Rank 0 has model parallel group: [0]
[NeMo I 2024-07-30 17:37:52 megatron_init:303] All model parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:37:52 megatron_init:312] Rank 0 has tensor model parallel group: [0]
[NeMo I 2024-07-30 17:37:52 megatron_init:316] All tensor model parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:37:52 megatron_init:317] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-07-30 17:37:52 megatron_init:337] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2024-07-30 17:37:52 megatron_init:349] Rank 0 has embedding group: [0]
[NeMo I 2024-07-30 17:37:52 megatron_init:355] All pipeline model parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:37:52 megatron_init:356] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-07-30 17:37:52 megatron_init:357] All embedding group ranks: [[0]]
[NeMo I 2024-07-30 17:37:52 megatron_init:358] Rank 0 has embedding rank: 0
[NeMo I 2024-07-30 17:37:52 utils:220] Let split_matrix = [(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)]
[NeMo I 2024-07-30 17:37:52 utils:220] Building dataset splits with cls=GPTDataset, sizes=[1600, 3264, 32], and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=[['[/home/data/test_text_document]'], None], blend_per_split=None, split='900,50,50', split_matrix=[(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x7d198a94dfc0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)
