[NeMo I 2024-07-30 17:38:48 tokenizer_utils:216] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2024-07-30 17:38:48 tokenizer_utils:132] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo W 2024-07-30 17:38:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
      warnings.warn(
    
[NeMo I 2024-07-30 17:38:48 nemo_logger:124] Experiments will be logged at default/2024-07-30_17-38-48
[NeMo W 2024-07-30 17:38:48 nemo_logger:156] "update_logger_directory" is True. Overwriting logger "save_dir" to . and "name" to default
[NeMo I 2024-07-30 17:38:48 megatron_init:269] Rank 0 has data parallel group : [0]
[NeMo I 2024-07-30 17:38:48 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2024-07-30 17:38:48 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2024-07-30 17:38:48 megatron_init:283] Ranks 0 has data parallel rank: 0
[NeMo I 2024-07-30 17:38:48 megatron_init:291] Rank 0 has context parallel group: [0]
[NeMo I 2024-07-30 17:38:48 megatron_init:294] All context parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:38:48 megatron_init:295] Ranks 0 has context parallel rank: 0
[NeMo I 2024-07-30 17:38:48 megatron_init:302] Rank 0 has model parallel group: [0]
[NeMo I 2024-07-30 17:38:48 megatron_init:303] All model parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:38:48 megatron_init:312] Rank 0 has tensor model parallel group: [0]
[NeMo I 2024-07-30 17:38:48 megatron_init:316] All tensor model parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:38:48 megatron_init:317] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-07-30 17:38:48 megatron_init:337] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2024-07-30 17:38:48 megatron_init:349] Rank 0 has embedding group: [0]
[NeMo I 2024-07-30 17:38:48 megatron_init:355] All pipeline model parallel group ranks: [[0]]
[NeMo I 2024-07-30 17:38:48 megatron_init:356] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-07-30 17:38:48 megatron_init:357] All embedding group ranks: [[0]]
[NeMo I 2024-07-30 17:38:48 megatron_init:358] Rank 0 has embedding rank: 0
[NeMo I 2024-07-30 17:38:49 utils:220] Let split_matrix = [(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)]
[NeMo I 2024-07-30 17:38:49 utils:220] Building dataset splits with cls=GPTDataset, sizes=[1600, 3264, 32], and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=[['/home/data/test_text_document'], None], blend_per_split=None, split='900,50,50', split_matrix=[(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x7ca4c65aa230>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)
[NeMo I 2024-07-30 17:38:49 utils:220] Load the _IndexReader from /home/data/test_text_document.idx
[NeMo I 2024-07-30 17:38:49 utils:220] 	Extract the sequence lengths
[NeMo I 2024-07-30 17:38:49 utils:220] 	Extract the sequence pointers
[NeMo I 2024-07-30 17:38:49 utils:220] 	Extract the document indices
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of sequences: 10042
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of documents: 10042
[NeMo I 2024-07-30 17:38:49 utils:220] Build and save the GPTDataset train indices
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of samples: 1723
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of epochs: 5
[NeMo I 2024-07-30 17:38:49 utils:220] Build and save the GPTDataset valid indices
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of samples: 3283
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of epochs: 137
[NeMo I 2024-07-30 17:38:49 utils:220] Build and save the GPTDataset test indices
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of samples: 47
[NeMo I 2024-07-30 17:38:49 utils:220] > total number of epochs: 2
[NeMo I 2024-07-30 17:38:49 base:30] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2024-07-30 17:38:49 num_microbatches_calculator:119] setting number of micro-batches to constant 8
[NeMo I 2024-07-30 17:38:49 megatron_parallel:462]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 125262336
[NeMo I 2024-07-30 17:38:49 utils:220] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False)
[NeMo I 2024-07-30 17:38:49 utils:241] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (125262336 elements):
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.bias
    	module.decoder.layers.0.self_attention.linear_proj.bias
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_proj.bias
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_qkv.bias
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.embedding.word_embeddings.weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.7.self_attention.linear_qkv.bias
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.2.mlp.linear_fc1.bias
    	module.decoder.layers.0.self_attention.linear_qkv.bias
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    	module.decoder.layers.8.self_attention.linear_qkv.bias
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.bias
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_qkv.bias
    	module.decoder.layers.1.mlp.linear_fc2.bias
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.bias
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.8.mlp.linear_fc2.bias
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.bias
    	module.decoder.layers.0.mlp.linear_fc1.bias
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.2.self_attention.linear_proj.bias
    	module.decoder.layers.6.mlp.linear_fc2.bias
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.bias
    	module.decoder.layers.5.self_attention.linear_qkv.bias
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.bias
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.10.self_attention.linear_qkv.bias
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.11.self_attention.linear_qkv.bias
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.embedding.position_embeddings.weight
    	module.decoder.layers.9.mlp.linear_fc1.bias
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_proj.bias
    	module.decoder.layers.10.mlp.linear_fc1.bias
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.bias
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.11.mlp.linear_fc1.bias
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.9.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.3.mlp.linear_fc2.bias
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.10.mlp.linear_fc2.bias
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_proj.bias
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.bias
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.10.self_attention.linear_proj.bias
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.5.mlp.linear_fc2.bias
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.final_layernorm.bias
    	module.decoder.layers.9.mlp.linear_fc2.bias
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.9.self_attention.linear_proj.bias
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.9.self_attention.linear_qkv.bias
    	module.decoder.layers.8.self_attention.linear_proj.bias
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.bias
    	module.decoder.layers.4.self_attention.linear_qkv.bias
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.2.self_attention.linear_qkv.bias
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.11.mlp.linear_fc2.bias
    	module.decoder.layers.9.self_attention.linear_qkv.weight
    	module.decoder.layers.6.mlp.linear_fc1.bias
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.bias
    	module.decoder.layers.4.mlp.linear_fc2.bias
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc2.bias
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_bias
    	module.decoder.layers.8.mlp.linear_fc1.bias
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.bias
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
    	module.decoder.layers.4.self_attention.linear_proj.bias
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
[NeMo I 2024-07-30 17:38:49 utils:220] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0006, min_lr=6e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None)
[NeMo W 2024-07-30 17:38:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2024-07-30 17:39:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...
    
