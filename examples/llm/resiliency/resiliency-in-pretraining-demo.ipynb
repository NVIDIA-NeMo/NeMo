{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63d934c-f709-4e6d-aa44-03f6b1926180",
   "metadata": {},
   "source": [
    "# Resilient LLM Training with NeMo Framework\n",
    "\n",
    "This notebook demonstrates how to use NeMo's resiliency features for robust LLM training. It covers:\n",
    "\n",
    "1. **Crash Recovery**: Using in-job restart capabilities to automatically recover from failures during training\n",
    "2. **Straggler Detection**: Identifying and handling slow/stuck processes in distributed training\n",
    "3. **Checkpointing**: Implementing asynchronous checkpointing for efficient model saving\n",
    "\n",
    "The demo uses a small LLaMA model and simulated crashes to showcase these features in action. We'll walk through:\n",
    "- Setting up a local executor with fault tolerance enabled\n",
    "- Configuring the straggler detection callbacks\n",
    "- Launching distributed training with resiliency features\n",
    "- Monitoring training progress and recovery from failures\n",
    "- Analyzing logs and checkpoints\n",
    "\n",
    "This demonstrates how NeMo makes LLM training more robust and production-ready by handling common failure modes automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfb9b8b-3359-4d00-88fa-abf0e24f7850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-03-03 21:33:58 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n",
    "\n",
    "# Required Libraries\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from functools import partial\n",
    "from typing import Any\n",
    "import torch\n",
    "\n",
    "import nemo_run as run\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm.recipes.callbacks.common import straggler_det_callback\n",
    "from nemo.lightning.run import plugins\n",
    "\n",
    "from crash_simulator import CrashSimulationCallback\n",
    "\n",
    "print(\"Required libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a19a6d-8df8-4930-bb50-d622b6b72af7",
   "metadata": {},
   "source": [
    "### Define the executor\n",
    "\n",
    "Define and initialize a local executor, which is used to manage distributed computing tasks. The executor encapsulates configurations for launching jobs (e.g. number of devices, environment variables, task distribution).\n",
    "\n",
    "The executor uses the (ft launcher)[https://github.com/NVIDIA/NeMo-Run/blob/main/docs/source/guides/execution.md#launchers] to enable fault tolerance capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8740b1b8-0f89-40a9-a361-a88a6371d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor setup complete.\n"
     ]
    }
   ],
   "source": [
    "def local_executor(devices: int = 8) -> run.LocalExecutor:\n",
    "    \"\"\"\n",
    "    Factory method for creating a LocalExecutor instance. \n",
    "    This sets up environment variables and configures the number of devices.\n",
    "\n",
    "    Args:\n",
    "        devices (int): Number of devices to be used per node.\n",
    "\n",
    "    Returns:\n",
    "        run.LocalExecutor: Configured local executor object.\n",
    "    \"\"\"\n",
    "    env_vars = {\n",
    "        \"TRANSFORMERS_OFFLINE\": \"1\",   # Run Transformer models offline\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",  # Optimize PyTorch NCCL\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",      # Experimental NCCL environment variable\n",
    "        \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\", \n",
    "        \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "    }\n",
    "    # Create LocalExecutor with the `ft` launcher\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"ft\", env_vars=env_vars)\n",
    "    return executor\n",
    "\n",
    "# Initialize the executor based on the arguments\n",
    "executor = local_executor(devices=8)\n",
    "\n",
    "print(\"Executor setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ffd38-5f97-4001-ad86-46b686edb0e8",
   "metadata": {},
   "source": [
    "### Model setup\n",
    "Load and configure a LLAMA pretrain recipe. We choose a small 54M parameter llama3 based model for faster execution. This model is obtained by reducing the sequence length, number of layers, hidden size and number of attention heads from the original llama3 8B model configuration as defined in the (Llama3Config8B class)[https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/gpt/model/llama.py]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52245a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small LLAMA3 model configuration\n",
    "def small_llama_cfg() -> llm.GPTConfig:\n",
    "    \"\"\"Small 54M parameter model\"\"\"\n",
    "    return run.Config(\n",
    "        llm.Llama3Config8B,\n",
    "        rotary_base=500_000,\n",
    "        seq_length=128,\n",
    "        num_layers=4,\n",
    "        hidden_size=768,\n",
    "        ffn_hidden_size=2688,\n",
    "        num_attention_heads=16,\n",
    "        init_method_std=0.023,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6988ce3",
   "metadata": {},
   "source": [
    "### Modify the training recipe\n",
    "`pretrain` is a partial function that takes in the experiment name and checkpoint directory, and returns a pretrain recipe. It is setup to use `num_nodes=1` and `num_gpus_per_node=8` by default but this can be changed by modifying the `num_nodes` and `num_gpus_per_node` arguments. This demo uses the llama3 8b pretrain recipe as defined in the `llama31_8b.pretrain_recipe` (module)[https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/recipes/llama3_8b.py]. This defaults to using a mock dataset: (MockDataModule)[https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/gpt/data/mock.py] but please refer to the [Llama3_8b recipe](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/recipes/llama3_8b.py) for instructions on how to use a custom dataset. Since we are using a mock dataset, we set the `max_steps` to 20 so we can run the experiment in a reasonable time.\n",
    "\n",
    "We also disable validation sanity checks to reduce startup time, and set tensor model parallel size to 2 and context parallel size to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c99ffc-3718-4383-b77a-161f387ce302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model recipe setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Experiment name\n",
    "exp_name = \"resiliency-in-pretraining-demo\"\n",
    "\n",
    "# Preliminary setup for the LLAMA pretrain recipe\n",
    "pretrain = partial(llm.llama31_8b.pretrain_recipe, num_nodes=1, num_gpus_per_node=8)(\n",
    "    name=exp_name, dir=\"/tmp/nemo_run/checkpoints\"\n",
    ")\n",
    "pretrain.model = run.Config(llm.LlamaModel, small_llama_cfg())\n",
    "pretrain.trainer.strategy.tensor_model_parallel_size = 2\n",
    "pretrain.trainer.strategy.context_parallel_size = 1\n",
    "pretrain.trainer.num_sanity_val_steps = 0\n",
    "pretrain.broadcast(max_steps=20)\n",
    "pretrain.trainer.limit_val_batches = 2\n",
    "pretrain.trainer.log_every_n_steps = 1\n",
    "pretrain.trainer.val_check_interval = 10\n",
    "print(\"Model recipe setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67639495",
   "metadata": {},
   "source": [
    "### TODO: Add info on Straggler Detection callback, Preemption Plugin, Fault Tolerance Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a1e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect and mitigate stragglers during training\n",
    "pretrain.trainer.callbacks.append(straggler_det_callback(straggler_report_time_interval=1))\n",
    "\n",
    "# Add runtime plugins (e.g., preemption and fault tolerance)\n",
    "run_plugins = [plugins.PreemptionPlugin()]\n",
    "run_plugins.append(plugins.FaultTolerancePlugin())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716602ab-7b1a-4a6a-a50b-15ba877c0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add info on what these env variables are for\n",
    "# Setup ENV\n",
    "os.environ[\"FAULT_TOL_CFG_PATH\"] = \"/tmp/sample_job_ft_cfg.yml\"\n",
    "os.environ[\"FAULT_TOL_FINISHED_FLAG_FILE\"] = \"/tmp/sample_job_finished_flag\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae75f7-1a91-4429-bfbf-3ebe62bce123",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "Run the entire pretraining experiment. Depending on the arguments passed:\n",
    "- If `dryrun` is True, it performs a dry run (to validate configurations).\n",
    "- Otherwise, it launches the actual training run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03887dd7-a23b-44c9-825a-311849729531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(exp_name, pretrain, executor, run_plugins, dryrun=False):\n",
    "    \"\"\"\n",
    "    Run the pretraining experiment either as a dry run or actual training.\n",
    "    \n",
    "    Args:\n",
    "        exp_name: Name of the experiment\n",
    "        pretrain: Pretrain configuration object\n",
    "        executor: Executor to run the experiment\n",
    "        run_plugins: List of runtime plugins\n",
    "        dryrun: Boolean flag to perform a dry run\n",
    "    \"\"\"\n",
    "    with run.Experiment(f\"{exp_name}\") as exp:\n",
    "        # Add the pretrain job to the experiment\n",
    "        exp.add(\n",
    "            pretrain,\n",
    "            executor=executor,\n",
    "            name=exp_name,\n",
    "            plugins=run_plugins,\n",
    "            tail_logs=True,\n",
    "        )\n",
    "\n",
    "        # Execute the experiment based on the dryrun flag\n",
    "        if dryrun:\n",
    "            print(\"Performing dry run ...\")\n",
    "            exp.dryrun()\n",
    "        else:\n",
    "            print(\"Launching training run ...\")\n",
    "            exp.run(sequential=True, detach=True)\n",
    "            print(\"Experiment executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7884ed07-d4eb-4015-b1a0-e2dfaf30476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# delete old checkpoints\n",
    "rm -rf /tmp/nemo_run/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2836df0e-3a43-4dfd-9534-4633f5aa2441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment resiliency-in-pretraining-demo with id: resiliency-in-pretraining-demo_1741037640</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────── \u001b[0m\u001b[1;35mEntering Experiment resiliency-in-pretraining-demo with id: resiliency-in-pretraining-demo_1741037640\u001b[0m\u001b[92m ──────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training run ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:34:00] </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Cannot detach from this experiment. Please keep it running until completion.</span>          <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#651\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">651</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:34:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31m Cannot detach from this experiment. Please keep it running until completion.\u001b[0m          \u001b]8;id=35655;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=515364;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#651\u001b\\\u001b[2m651\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job resiliency-in-pretraining-demo for experiment </span>                           <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#724\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">resiliency-in-pretraining-demo</span>                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job resiliency-in-pretraining-demo for experiment \u001b[0m                           \u001b]8;id=61586;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=716139;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#724\u001b\\\u001b[2m724\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mresiliency-in-pretraining-demo\u001b[0m                                                         \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo\n",
      "Launched app: local_persistent://nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd\n",
      "AppStatus:\n",
      "    State: RUNNING\n",
      "    Num Restarts: 0\n",
      "    Roles: \n",
      "    Msg: <NONE>\n",
      "    Structured Error Msg: <NONE>\n",
      "    UI URL: file:///root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment executed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment resiliency-in-pretraining-demo_1741037640 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment resiliency-in-pretraining-demo_1741037640 to finish\u001b[0m\u001b[92m ────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">resiliency-in-pretraining-demo_1741037640</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mresiliency-in-pretraining-demo_1741037640\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">resiliency-in-pretraining-demo</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: resiliency-in-pretraining-demo-vl040cxsvlrxrd\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mresiliency-in-pretraining-demo\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: resiliency-in-pretraining-demo-vl040cxsvlrxrd\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job resiliency-in-pretraining-demo-vl040cxsvlrxrd to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ining-demo/0 [2025-03-03 21:34:02,051] [WARNING] [ft_launcher@ec00c0d0158b] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
      "ining-demo/0 [2025-03-03 21:34:02,051] [WARNING] [ft_launcher@ec00c0d0158b] \n",
      "ining-demo/0 *****************************************\n",
      "ining-demo/0 Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "ining-demo/0 *****************************************\n",
      "ining-demo/0 [2025-03-03 21:34:02,055] [INFO] [ft_launcher@ec00c0d0158b] [default] starting workers for entrypoint: python\n",
      "ining-demo/0 [2025-03-03 21:34:02,055] [INFO] [ft_launcher@ec00c0d0158b] [default] Rendezvous'ing worker group\n",
      "ining-demo/0 [2025-03-03 21:34:02,297] [INFO] [ft_launcher@ec00c0d0158b] [default] Rendezvous complete for workers. Result:\n",
      "ining-demo/0   restart_count=0\n",
      "ining-demo/0   master_addr=ec00c0d0158b\n",
      "ining-demo/0   master_port=54883\n",
      "ining-demo/0   group_rank=0\n",
      "ining-demo/0   group_world_size=1\n",
      "ining-demo/0   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "ining-demo/0   global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "ining-demo/0 \n",
      "ining-demo/0 [2025-03-03 21:34:02,297] [INFO] [ft_launcher@ec00c0d0158b] [default] Starting worker group\n",
      "ining-demo/0 [2025-03-03 21:34:13,387] [INFO] [ft_launcher@ec00c0d0158b] Setting worker0 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/0/error.json\n",
      "ining-demo/0 [2025-03-03 21:34:13,387] [INFO] [ft_launcher@ec00c0d0158b] Setting worker1 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/1/error.json\n",
      "ining-demo/0 [2025-03-03 21:34:13,388] [INFO] [ft_launcher@ec00c0d0158b] Setting worker2 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/2/error.json\n",
      "ining-demo/0 [2025-03-03 21:34:13,388] [INFO] [ft_launcher@ec00c0d0158b] Setting worker3 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/3/error.json\n",
      "ining-demo/0 [2025-03-03 21:34:13,388] [INFO] [ft_launcher@ec00c0d0158b] Setting worker4 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/4/error.json\n",
      "ining-demo/0 [2025-03-03 21:34:13,388] [INFO] [ft_launcher@ec00c0d0158b] Setting worker5 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/5/error.json\n",
      "ining-demo/0 [2025-03-03 21:34:13,388] [INFO] [ft_launcher@ec00c0d0158b] Setting worker6 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/6/error.json\n",
      "ining-demo/0 [2025-03-03 21:34:13,388] [INFO] [ft_launcher@ec00c0d0158b] Setting worker7 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037640/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-vl040cxsvlrxrd/torchelastic/resiliency-in-pretraining-demo/2490_3byl8s62/attempt_0/7/error.json\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:22 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "ining-demo/0 [default0]:      cm = get_cmap(\"Set1\")\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:23 tokenizer_utils:224] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:23 megatron_utils:208] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json to /root/.cache/torch/megatron/megatron-gpt-345m_vocab\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:24 megatron_utils:208] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt to /root/.cache/torch/megatron/megatron-gpt-345m_merges\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:24 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:24 nemo_logger:145] Experiments will be logged at /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:24 megatron_strategy:315] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "ining-demo/0 [default0]:GPU available: True (cuda), used: True\n",
      "ining-demo/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "ining-demo/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:24 nemo_logger:123] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:24 nemo_logger:173] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/nemo_run/checkpoints/tb_logs\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:24 nemo_logger:189] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:24 nemo_logger:212] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 20. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:24 resume:228] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints. Training from scratch.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:426] Rank 0 has data parallel group : [0, 2, 4, 6]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:432] Rank 0 has combined group of data parallel and context parallel : [0, 2, 4, 6]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:437] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:440] Ranks 0 has data parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:448] Rank 0 has context parallel group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:451] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:452] Ranks 0 has context parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:459] Rank 0 has model parallel group: [0, 1]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:460] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:469] Rank 0 has tensor model parallel group: [0, 1]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:473] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:474] Rank 0 has tensor model parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:494] Rank 0 has pipeline model parallel group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:506] Rank 0 has embedding group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:512] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:513] Rank 0 has pipeline model parallel rank 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:514] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:26 megatron_init:515] Rank 0 has embedding rank: 0\n",
      "ining-demo/0 [default2]:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "ining-demo/0 [default6]:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "ining-demo/0 [default4]:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "ining-demo/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "ining-demo/0 [default5]:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "ining-demo/0 [default1]:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "ining-demo/0 [default3]:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "ining-demo/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "ining-demo/0 [default0]:distributed_backend=nccl\n",
      "ining-demo/0 [default0]:All distributed processes registered. Starting with 8 processes\n",
      "ining-demo/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "ining-demo/0 [default0]:\n",
      "ining-demo/0 [default7]:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:27 fault_tolerance_callback:311] [FaultToleranceCallback@rank0] Fault tolerance dir: /tmp/nemo_run/checkpoints\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:27 fault_tolerance_callback:311] [FaultToleranceCallback@rank0] Fault tolerance client initialized. Timeouts: HeartbeatTimeouts(initial=1800.00, subsequent=300.00, were_calculated=False)\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:28 base:44] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:28 megatron_strategy:327] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:28 num_microbatches_calculator:228] setting number of microbatches to constant 128\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:28 megatron_parallel:549]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 54663936\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:28 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=40000000, average_in_collective=True, fp8_param_gather=False)\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:28 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "ining-demo/0 [default0]:    Params for bucket 1 (54663936 elements):\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.embedding.word_embeddings.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.final_layernorm.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.output_layer.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:34:28 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "ining-demo/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default5]:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default6]:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default7]:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default4]:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:\n",
      "ining-demo/0 [default0]:  | Name   | Type | Params | Mode \n",
      "ining-demo/0 [default0]:----------------------------------------\n",
      "ining-demo/0 [default0]:0 | module | DDP  | 54.7 M | train\n",
      "ining-demo/0 [default0]:----------------------------------------\n",
      "ining-demo/0 [default0]:54.7 M    Trainable params\n",
      "ining-demo/0 [default0]:0         Non-trainable params\n",
      "ining-demo/0 [default0]:54.7 M    Total params\n",
      "ining-demo/0 [default0]:218.656   Total estimated model params size (MB)\n",
      "ining-demo/0 [default0]:91        Modules in train mode\n",
      "ining-demo/0 [default0]:0         Modules in eval mode\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:42 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:34:42 rerun_state_machine:211] RerunStateMachine initialized in mode disabled\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 0/19 | lr: 1.499e-07 | global_batch_size: 512 | global_step: 0 | reduced_train_loss: 11.03 | train_step_timing in s: 14.51\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 1/19 | lr: 2.999e-07 | global_batch_size: 512 | global_step: 1 | reduced_train_loss: 11.03 | train_step_timing in s: 7.585 | consumed_samples: 1024\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 2/19 | lr: 4.498e-07 | global_batch_size: 512 | global_step: 2 | reduced_train_loss: 11.03 | train_step_timing in s: 7.586 | consumed_samples: 1536\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 3/19 | lr: 5.997e-07 | global_batch_size: 512 | global_step: 3 | reduced_train_loss: 11.03 | train_step_timing in s: 7.59 | consumed_samples: 2048\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 4/19 | lr: 7.496e-07 | global_batch_size: 512 | global_step: 4 | reduced_train_loss: 11.03 | train_step_timing in s: 7.589 | consumed_samples: 2560\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 5/19 | lr: 8.996e-07 | global_batch_size: 512 | global_step: 5 | reduced_train_loss: 11.03 | train_step_timing in s: 7.601 | consumed_samples: 3072\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 6/19 | lr: 1.049e-06 | global_batch_size: 512 | global_step: 6 | reduced_train_loss: 11.03 | train_step_timing in s: 7.591 | consumed_samples: 3584\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 7/19 | lr: 1.199e-06 | global_batch_size: 512 | global_step: 7 | reduced_train_loss: 11.03 | train_step_timing in s: 7.603 | consumed_samples: 4096\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 8/19 | lr: 1.349e-06 | global_batch_size: 512 | global_step: 8 | reduced_train_loss: 11.03 | train_step_timing in s: 7.606 | consumed_samples: 4608\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 9/19 | lr: 1.499e-06 | global_batch_size: 512 | global_step: 9 | reduced_train_loss: 11.03 | train_step_timing in s: 7.602 | consumed_samples: 5120\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:35:52 validation:389] There is difference in the common state dict in different ranks. The differences are {2: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 3: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 4: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 5: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], [])}\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:35:55 model_checkpoint:497] Scheduled async checkpoint save for /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=9-consumed_samples=5120.0-last.ckpt\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 10/19 | lr: 1.649e-06 | global_batch_size: 512 | global_step: 10 | reduced_train_loss: 11.03 | train_step_timing in s: 7.603 | consumed_samples: 5632\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:36:02 model_checkpoint:522] Async checkpoint save for step 10 (/tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=9-consumed_samples=5120.0-last.ckpt) finalized successfully.\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 11/19 | lr: 1.799e-06 | global_batch_size: 512 | global_step: 11 | reduced_train_loss: 11.03 | train_step_timing in s: 7.601 | consumed_samples: 6144\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 12/19 | lr: 1.949e-06 | global_batch_size: 512 | global_step: 12 | reduced_train_loss: 11.03 | train_step_timing in s: 7.605 | consumed_samples: 6656\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 13/19 | lr: 2.099e-06 | global_batch_size: 512 | global_step: 13 | reduced_train_loss: 11.03 | train_step_timing in s: 7.611 | consumed_samples: 7168\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 14/19 | lr: 2.249e-06 | global_batch_size: 512 | global_step: 14 | reduced_train_loss: 11.03 | train_step_timing in s: 7.607 | consumed_samples: 7680\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 15/19 | lr: 2.399e-06 | global_batch_size: 512 | global_step: 15 | reduced_train_loss: 11.03 | train_step_timing in s: 7.61 | consumed_samples: 8192\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:36:48 straggler_det_callback:144] \n",
      "ining-demo/0 [default0]:    GPU relative performance:\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=0.96\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=0.97\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=0.97\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.97\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=0.97\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:36:48 straggler_det_callback:153] \n",
      "ining-demo/0 [default0]:    GPU individual performance:\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:36:48 straggler_det_callback:236] Straggler report processing time: 0.036 sec.\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 16/19 | lr: 2.549e-06 | global_batch_size: 512 | global_step: 16 | reduced_train_loss: 11.03 | train_step_timing in s: 7.613 | consumed_samples: 8704\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:36:56 straggler_det_callback:144] \n",
      "ining-demo/0 [default0]:    GPU relative performance:\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:36:56 straggler_det_callback:153] \n",
      "ining-demo/0 [default0]:    GPU individual performance:\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:36:56 straggler_det_callback:236] Straggler report processing time: 0.014 sec.\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 17/19 | lr: 2.699e-06 | global_batch_size: 512 | global_step: 17 | reduced_train_loss: 11.03 | train_step_timing in s: 7.612 | consumed_samples: 9216\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:03 straggler_det_callback:144] \n",
      "ining-demo/0 [default0]:    GPU relative performance:\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:03 straggler_det_callback:153] \n",
      "ining-demo/0 [default0]:    GPU individual performance:\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:03 straggler_det_callback:236] Straggler report processing time: 0.016 sec.\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 18/19 | lr: 2.849e-06 | global_batch_size: 512 | global_step: 18 | reduced_train_loss: 11.03 | train_step_timing in s: 7.622 | consumed_samples: 9728\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:37:04 validation:389] There is difference in the common state dict in different ranks. The differences are {2: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 3: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 4: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 5: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], [])}\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:04 model_checkpoint:497] Scheduled async checkpoint save for /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=18-consumed_samples=9728.0-last.ckpt\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:12 straggler_det_callback:144] \n",
      "ining-demo/0 [default0]:    GPU relative performance:\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:12 straggler_det_callback:153] \n",
      "ining-demo/0 [default0]:    GPU individual performance:\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:12 straggler_det_callback:236] Straggler report processing time: 0.017 sec.\n",
      "ining-demo/0 [default0]:Training epoch 1, iteration 0/19 | lr: 2.999e-06 | global_batch_size: 512 | global_step: 19 | reduced_train_loss: 11.03 | train_step_timing in s: 7.65 | consumed_samples: 10240\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:12 model_checkpoint:522] Async checkpoint save for step 19 (/tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=18-consumed_samples=9728.0-last.ckpt) finalized successfully.\n",
      "ining-demo/0 [default0]:`Trainer.fit` stopped: `max_steps=20` reached.\n",
      "ining-demo/0 [2025-03-03 21:37:18,797] [INFO] [ft_launcher@ec00c0d0158b] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "ining-demo/0 [2025-03-03 21:37:18,797] [INFO] [ft_launcher@ec00c0d0158b] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "ining-demo/0 [2025-03-03 21:37:18,798] [INFO] [ft_launcher@ec00c0d0158b] Done waiting for other agents. Elapsed: 0.00030350685119628906 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job resiliency-in-pretraining-demo-vl040cxsvlrxrd finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['resiliency-in-pretraining-demo']</span><span style=\"background-color: #272822\">                              </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"resiliency-in-pretraining-demo_1741037640\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"resiliency-in-pretraining-demo\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"resiliency-in-pretraining-demo\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['resiliency-in-pretraining-demo']\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresiliency-in-pretraining-demo_1741037640\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresiliency-in-pretraining-demo\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresiliency-in-pretraining-demo\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status resiliency-in-pretraining-demo_1741037640</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs resiliency-in-pretraining-demo_1741037640 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel resiliency-in-pretraining-demo_1741037640 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresiliency-in-pretraining-demo_1741037640\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresiliency-in-pretraining-demo_1741037640\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresiliency-in-pretraining-demo_1741037640\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run the experiment\n",
    "run_experiment(exp_name, pretrain, executor, run_plugins, dryrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b5f5e9-d073-4ade-865e-61156ee11b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# delete old checkpoints\n",
    "rm -rf /tmp/nemo_run/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88a1a5",
   "metadata": {},
   "source": [
    "### Demonstrate in-job restart with a crash simulator\n",
    "We use the `CrashSimulationCallback` to simulate a crash during training. This callback is configured to crash the process at step 17 if a crash has not already occurred.\n",
    "\n",
    "Expected workflow:\n",
    "- Start training: Trainer Step counter = 0\n",
    "- After 10 trainer steps: Trainer Step counter = 10 -> save checkpoint\n",
    "- After 17 trainer steps: Trainer Step counter = 17 -> crash simulated, set `has_simulated_crash_happened` to `True`\n",
    "- Automatic in-job restart from checkpoint at step 10: Trainer step counter = 10\n",
    "- After 17 trainer steps:Trainer Step counter = 17 -> no crash simulated as `has_simulated_crash_happened == True`\n",
    "- After 20 trainer steps: Trainer Step counter = 20 -> successfully completes training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2943a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment resiliency-in-pretraining-demo with id: resiliency-in-pretraining-demo_1741037843</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────── \u001b[0m\u001b[1;35mEntering Experiment resiliency-in-pretraining-demo with id: resiliency-in-pretraining-demo_1741037843\u001b[0m\u001b[92m ──────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training run ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:37:23] </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Cannot detach from this experiment. Please keep it running until completion.</span>          <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#651\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">651</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:37:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31m Cannot detach from this experiment. Please keep it running until completion.\u001b[0m          \u001b]8;id=330183;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=31362;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#651\u001b\\\u001b[2m651\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job resiliency-in-pretraining-demo for experiment </span>                           <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#724\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">resiliency-in-pretraining-demo</span>                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job resiliency-in-pretraining-demo for experiment \u001b[0m                           \u001b]8;id=222577;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=704399;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#724\u001b\\\u001b[2m724\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mresiliency-in-pretraining-demo\u001b[0m                                                         \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo\n",
      "Launched app: local_persistent://nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc\n",
      "AppStatus:\n",
      "    State: RUNNING\n",
      "    Num Restarts: 0\n",
      "    Roles: \n",
      "    Msg: <NONE>\n",
      "    Structured Error Msg: <NONE>\n",
      "    UI URL: file:///root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment executed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment resiliency-in-pretraining-demo_1741037843 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment resiliency-in-pretraining-demo_1741037843 to finish\u001b[0m\u001b[92m ────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">resiliency-in-pretraining-demo_1741037843</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mresiliency-in-pretraining-demo_1741037843\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">resiliency-in-pretraining-demo</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: resiliency-in-pretraining-demo-th70tx1qsh43kc\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mresiliency-in-pretraining-demo\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: resiliency-in-pretraining-demo-th70tx1qsh43kc\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job resiliency-in-pretraining-demo-th70tx1qsh43kc to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ining-demo/0 [2025-03-03 21:37:24,813] [WARNING] [ft_launcher@ec00c0d0158b] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
      "ining-demo/0 [2025-03-03 21:37:24,813] [WARNING] [ft_launcher@ec00c0d0158b] \n",
      "ining-demo/0 *****************************************\n",
      "ining-demo/0 Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "ining-demo/0 *****************************************\n",
      "ining-demo/0 [2025-03-03 21:37:24,817] [INFO] [ft_launcher@ec00c0d0158b] [default] starting workers for entrypoint: python\n",
      "ining-demo/0 [2025-03-03 21:37:24,817] [INFO] [ft_launcher@ec00c0d0158b] [default] Rendezvous'ing worker group\n",
      "ining-demo/0 [2025-03-03 21:37:25,045] [INFO] [ft_launcher@ec00c0d0158b] [default] Rendezvous complete for workers. Result:\n",
      "ining-demo/0   restart_count=0\n",
      "ining-demo/0   master_addr=ec00c0d0158b\n",
      "ining-demo/0   master_port=50919\n",
      "ining-demo/0   group_rank=0\n",
      "ining-demo/0   group_world_size=1\n",
      "ining-demo/0   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "ining-demo/0   global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "ining-demo/0 \n",
      "ining-demo/0 [2025-03-03 21:37:25,045] [INFO] [ft_launcher@ec00c0d0158b] [default] Starting worker group\n",
      "ining-demo/0 [2025-03-03 21:37:36,141] [INFO] [ft_launcher@ec00c0d0158b] Setting worker0 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/0/error.json\n",
      "ining-demo/0 [2025-03-03 21:37:36,141] [INFO] [ft_launcher@ec00c0d0158b] Setting worker1 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/1/error.json\n",
      "ining-demo/0 [2025-03-03 21:37:36,141] [INFO] [ft_launcher@ec00c0d0158b] Setting worker2 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/2/error.json\n",
      "ining-demo/0 [2025-03-03 21:37:36,141] [INFO] [ft_launcher@ec00c0d0158b] Setting worker3 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/3/error.json\n",
      "ining-demo/0 [2025-03-03 21:37:36,142] [INFO] [ft_launcher@ec00c0d0158b] Setting worker4 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/4/error.json\n",
      "ining-demo/0 [2025-03-03 21:37:36,142] [INFO] [ft_launcher@ec00c0d0158b] Setting worker5 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/5/error.json\n",
      "ining-demo/0 [2025-03-03 21:37:36,142] [INFO] [ft_launcher@ec00c0d0158b] Setting worker6 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/6/error.json\n",
      "ining-demo/0 [2025-03-03 21:37:36,142] [INFO] [ft_launcher@ec00c0d0158b] Setting worker7 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_0/7/error.json\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:37:45 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "ining-demo/0 [default0]:      cm = get_cmap(\"Set1\")\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default1]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:46 tokenizer_utils:224] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:46 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False\n",
      "ining-demo/0 [default0]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default0]:GPU available: True (cuda), used: True\n",
      "ining-demo/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "ining-demo/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:46 nemo_logger:145] Experiments will be logged at /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:46 megatron_strategy:315] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:37:46 nemo_logger:123] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:37:46 nemo_logger:173] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/nemo_run/checkpoints/tb_logs\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:37:46 nemo_logger:189] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:37:46 nemo_logger:212] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 20. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:37:46 resume:228] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints. Training from scratch.\n",
      "ining-demo/0 [default1]:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:426] Rank 0 has data parallel group : [0, 2, 4, 6]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:432] Rank 0 has combined group of data parallel and context parallel : [0, 2, 4, 6]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:437] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:440] Ranks 0 has data parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:448] Rank 0 has context parallel group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:451] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:452] Ranks 0 has context parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:459] Rank 0 has model parallel group: [0, 1]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:460] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:469] Rank 0 has tensor model parallel group: [0, 1]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:473] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:474] Rank 0 has tensor model parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:494] Rank 0 has pipeline model parallel group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:506] Rank 0 has embedding group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:512] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:513] Rank 0 has pipeline model parallel rank 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:514] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:47 megatron_init:515] Rank 0 has embedding rank: 0\n",
      "ining-demo/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "ining-demo/0 [default7]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default2]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default6]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default3]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default5]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default4]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default7]:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "ining-demo/0 [default5]:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "ining-demo/0 [default2]:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "ining-demo/0 [default6]:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "ining-demo/0 [default4]:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "ining-demo/0 [default3]:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "ining-demo/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "ining-demo/0 [default0]:distributed_backend=nccl\n",
      "ining-demo/0 [default0]:All distributed processes registered. Starting with 8 processes\n",
      "ining-demo/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "ining-demo/0 [default0]:\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 fault_tolerance_callback:311] [FaultToleranceCallback@rank0] Fault tolerance dir: /tmp/nemo_run/checkpoints\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 fault_tolerance_callback:311] [FaultToleranceCallback@rank0] Fault tolerance client initialized. Timeouts: HeartbeatTimeouts(initial=1800.00, subsequent=300.00, were_calculated=False)\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 base:44] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 megatron_strategy:327] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 num_microbatches_calculator:228] setting number of microbatches to constant 128\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 megatron_parallel:549]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 54663936\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=40000000, average_in_collective=True, fp8_param_gather=False)\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "ining-demo/0 [default0]:    Params for bucket 1 (54663936 elements):\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.output_layer.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.final_layernorm.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.embedding.word_embeddings.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:37:50 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "ining-demo/0 [default7]:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default4]:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default6]:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:\n",
      "ining-demo/0 [default0]:  | Name   | Type | Params | Mode \n",
      "ining-demo/0 [default0]:----------------------------------------\n",
      "ining-demo/0 [default0]:0 | module | DDP  | 54.7 M | train\n",
      "ining-demo/0 [default0]:----------------------------------------\n",
      "ining-demo/0 [default0]:54.7 M    Trainable params\n",
      "ining-demo/0 [default0]:0         Non-trainable params\n",
      "ining-demo/0 [default0]:54.7 M    Total params\n",
      "ining-demo/0 [default0]:218.656   Total estimated model params size (MB)\n",
      "ining-demo/0 [default0]:91        Modules in train mode\n",
      "ining-demo/0 [default0]:0         Modules in eval mode\n",
      "ining-demo/0 [default5]:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:38:01 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:38:01 rerun_state_machine:211] RerunStateMachine initialized in mode disabled\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 0/19 | lr: 1.499e-07 | global_batch_size: 512 | global_step: 0 | reduced_train_loss: 11.03 | train_step_timing in s: 10.69\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 1/19 | lr: 2.999e-07 | global_batch_size: 512 | global_step: 1 | reduced_train_loss: 11.03 | train_step_timing in s: 7.601 | consumed_samples: 1024\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 2/19 | lr: 4.498e-07 | global_batch_size: 512 | global_step: 2 | reduced_train_loss: 11.03 | train_step_timing in s: 7.604 | consumed_samples: 1536\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 3/19 | lr: 5.997e-07 | global_batch_size: 512 | global_step: 3 | reduced_train_loss: 11.03 | train_step_timing in s: 7.609 | consumed_samples: 2048\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 4/19 | lr: 7.496e-07 | global_batch_size: 512 | global_step: 4 | reduced_train_loss: 11.03 | train_step_timing in s: 7.614 | consumed_samples: 2560\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 5/19 | lr: 8.996e-07 | global_batch_size: 512 | global_step: 5 | reduced_train_loss: 11.03 | train_step_timing in s: 7.612 | consumed_samples: 3072\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 6/19 | lr: 1.049e-06 | global_batch_size: 512 | global_step: 6 | reduced_train_loss: 11.03 | train_step_timing in s: 7.611 | consumed_samples: 3584\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 7/19 | lr: 1.199e-06 | global_batch_size: 512 | global_step: 7 | reduced_train_loss: 11.03 | train_step_timing in s: 7.618 | consumed_samples: 4096\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 8/19 | lr: 1.349e-06 | global_batch_size: 512 | global_step: 8 | reduced_train_loss: 11.03 | train_step_timing in s: 7.618 | consumed_samples: 4608\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 9/19 | lr: 1.499e-06 | global_batch_size: 512 | global_step: 9 | reduced_train_loss: 11.03 | train_step_timing in s: 7.62 | consumed_samples: 5120\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:39:10 validation:389] There is difference in the common state dict in different ranks. The differences are {2: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 3: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 4: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], []), 5: ([], [('optimizer', 0, 'optimizer', 'param_groups', 1, 'step')], [])}\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:39:13 model_checkpoint:497] Scheduled async checkpoint save for /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=9-consumed_samples=5120.0-last.ckpt\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 10/19 | lr: 1.649e-06 | global_batch_size: 512 | global_step: 10 | reduced_train_loss: 11.03 | train_step_timing in s: 7.609 | consumed_samples: 5632\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:39:21 model_checkpoint:522] Async checkpoint save for step 10 (/tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=9-consumed_samples=5120.0-last.ckpt) finalized successfully.\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 11/19 | lr: 1.799e-06 | global_batch_size: 512 | global_step: 11 | reduced_train_loss: 11.03 | train_step_timing in s: 7.617 | consumed_samples: 6144\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 12/19 | lr: 1.949e-06 | global_batch_size: 512 | global_step: 12 | reduced_train_loss: 11.03 | train_step_timing in s: 7.614 | consumed_samples: 6656\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 13/19 | lr: 2.099e-06 | global_batch_size: 512 | global_step: 13 | reduced_train_loss: 11.03 | train_step_timing in s: 7.62 | consumed_samples: 7168\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 14/19 | lr: 2.249e-06 | global_batch_size: 512 | global_step: 14 | reduced_train_loss: 11.03 | train_step_timing in s: 7.614 | consumed_samples: 7680\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 15/19 | lr: 2.399e-06 | global_batch_size: 512 | global_step: 15 | reduced_train_loss: 11.03 | train_step_timing in s: 7.622 | consumed_samples: 8192\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:07 straggler_det_callback:144] \n",
      "ining-demo/0 [default0]:    GPU relative performance:\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=0.98\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=0.99\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:07 straggler_det_callback:153] \n",
      "ining-demo/0 [default0]:    GPU individual performance:\n",
      "ining-demo/0 [default0]:      Rank=0 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=1 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=2 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=3 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=4 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=5 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=6 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:      Rank=7 Node=ec00c0d0158b Score=1.00\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:07 straggler_det_callback:236] Straggler report processing time: 0.038 sec.\n",
      "ining-demo/0 [default3]:[rank3]: Traceback (most recent call last):\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default3]:[rank3]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default3]:[rank3]:     exec(code, run_globals)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default3]:[rank3]:     fdl_runner_app()\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default3]:[rank3]:     raise e\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default3]:[rank3]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default3]:[rank3]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default3]:[rank3]:     return _main(\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default3]:[rank3]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default3]:[rank3]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default3]:[rank3]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default3]:[rank3]:     return callback(**use_params)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default3]:[rank3]:     fdl_fn()\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default3]:[rank3]:     return train(\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default3]:[rank3]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default3]:[rank3]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default3]:[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default3]:[rank3]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default3]:[rank3]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default3]:[rank3]:     results = self._run_stage()\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default3]:[rank3]:     self.fit_loop.run()\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default3]:[rank3]:     self.advance()\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default3]:[rank3]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default3]:[rank3]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default3]:[rank3]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default3]:[rank3]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default3]:[rank3]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default3]:[rank3]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default3]:[rank3]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default3]:[rank3]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [default7]:[rank7]: Traceback (most recent call last):\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default7]:[rank7]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default7]:[rank7]:     exec(code, run_globals)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default7]:[rank7]:     fdl_runner_app()\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default7]:[rank7]:     raise e\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default7]:[rank7]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default7]:[rank7]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default7]:[rank7]:     return _main(\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default7]:[rank7]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default7]:[rank7]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default7]:[rank7]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default7]:[rank7]:     return callback(**use_params)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default7]:[rank7]:     fdl_fn()\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default7]:[rank7]:     return train(\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default7]:[rank7]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default7]:[rank7]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default7]:[rank7]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default7]:[rank7]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default7]:[rank7]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default7]:[rank7]:     results = self._run_stage()\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default7]:[rank7]:     self.fit_loop.run()\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default7]:[rank7]:     self.advance()\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default7]:[rank7]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default7]:[rank7]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default7]:[rank7]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default7]:[rank7]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default7]:[rank7]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default7]:[rank7]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default7]:[rank7]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default7]:[rank7]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [default2]:[rank2]: Traceback (most recent call last):\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default2]:[rank2]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default2]:[rank2]:     exec(code, run_globals)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default2]:[rank2]:     fdl_runner_app()\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default2]:[rank2]:     raise e\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default2]:[rank2]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default2]:[rank2]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default2]:[rank2]:     return _main(\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default2]:[rank2]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default0]:[rank0]: Traceback (most recent call last):\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default2]:[rank2]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default2]:[rank2]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default0]:[rank0]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default2]:[rank2]:     return callback(**use_params)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default2]:[rank2]:     fdl_fn()\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default2]:[rank2]:     return train(\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default0]:[rank0]:     exec(code, run_globals)\n",
      "ining-demo/0 [default2]:[rank2]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default2]:[rank2]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default0]:[rank0]:     fdl_runner_app()\n",
      "ining-demo/0 [default2]:[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default0]:[rank0]:     raise e\n",
      "ining-demo/0 [default2]:[rank2]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default0]:[rank0]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default2]:[rank2]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default0]:[rank0]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default2]:[rank2]:     results = self._run_stage()\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default0]:[rank0]:     return _main(\n",
      "ining-demo/0 [default2]:[rank2]:     self.fit_loop.run()\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default0]:[rank0]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default2]:[rank2]:     self.advance()\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default0]:[rank0]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default2]:[rank2]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default2]:[rank2]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default0]:[rank0]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default2]:[rank2]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default2]:[rank2]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default0]:[rank0]:     return callback(**use_params)\n",
      "ining-demo/0 [default2]:[rank2]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default2]:[rank2]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default2]:[rank2]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default2]:[rank2]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [default0]:[rank0]:     fdl_fn()\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default0]:[rank0]:     return train(\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default0]:[rank0]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default0]:[rank0]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default0]:[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default0]:[rank0]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default0]:[rank0]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default0]:[rank0]:     results = self._run_stage()\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default0]:[rank0]:     self.fit_loop.run()\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default0]:[rank0]:     self.advance()\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default0]:[rank0]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default0]:[rank0]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default0]:[rank0]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default0]:[rank0]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default0]:[rank0]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default0]:[rank0]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default0]:[rank0]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default0]:[rank0]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [default1]:[rank1]: Traceback (most recent call last):\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default1]:[rank1]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default1]:[rank1]:     exec(code, run_globals)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default1]:[rank1]:     fdl_runner_app()\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default1]:[rank1]:     raise e\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default1]:[rank1]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default1]:[rank1]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default1]:[rank1]:     return _main(\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default1]:[rank1]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default1]:[rank1]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default1]:[rank1]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default1]:[rank1]:     return callback(**use_params)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default1]:[rank1]:     fdl_fn()\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default1]:[rank1]:     return train(\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default1]:[rank1]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default1]:[rank1]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default1]:[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default1]:[rank1]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default1]:[rank1]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default1]:[rank1]:     results = self._run_stage()\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default1]:[rank1]:     self.fit_loop.run()\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default1]:[rank1]:     self.advance()\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default1]:[rank1]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default1]:[rank1]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default1]:[rank1]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default1]:[rank1]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default1]:[rank1]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default1]:[rank1]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default1]:[rank1]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default1]:[rank1]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [default6]:[rank6]: Traceback (most recent call last):\n",
      "ining-demo/0 [default5]:[rank5]: Traceback (most recent call last):\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default6]:[rank6]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default6]:[rank6]:     exec(code, run_globals)\n",
      "ining-demo/0 [default5]:[rank5]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default5]:[rank5]:     exec(code, run_globals)\n",
      "ining-demo/0 [default6]:[rank6]:     fdl_runner_app()\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default5]:[rank5]:     fdl_runner_app()\n",
      "ining-demo/0 [default6]:[rank6]:     raise e\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default5]:[rank5]:     raise e\n",
      "ining-demo/0 [default6]:[rank6]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default5]:[rank5]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default6]:[rank6]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default5]:[rank5]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default6]:[rank6]:     return _main(\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default5]:[rank5]:     return _main(\n",
      "ining-demo/0 [default6]:[rank6]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default5]:[rank5]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default6]:[rank6]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default5]:[rank5]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default6]:[rank6]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default5]:[rank5]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default6]:[rank6]:     return callback(**use_params)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default5]:[rank5]:     return callback(**use_params)\n",
      "ining-demo/0 [default6]:[rank6]:     fdl_fn()\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default5]:[rank5]:     fdl_fn()\n",
      "ining-demo/0 [default6]:[rank6]:     return train(\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default5]:[rank5]:     return train(\n",
      "ining-demo/0 [default6]:[rank6]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default5]:[rank5]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default6]:[rank6]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default5]:[rank5]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default6]:[rank6]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default5]:[rank5]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default6]:[rank6]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default5]:[rank5]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default6]:[rank6]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default5]:[rank5]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default6]:[rank6]:     results = self._run_stage()\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default5]:[rank5]:     results = self._run_stage()\n",
      "ining-demo/0 [default6]:[rank6]:     self.fit_loop.run()\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default5]:[rank5]:     self.fit_loop.run()\n",
      "ining-demo/0 [default6]:[rank6]:     self.advance()\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default5]:[rank5]:     self.advance()\n",
      "ining-demo/0 [default6]:[rank6]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default5]:[rank5]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default6]:[rank6]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default5]:[rank5]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default6]:[rank6]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default5]:[rank5]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default6]:[rank6]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default5]:[rank5]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default6]:[rank6]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default6]:[rank6]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default5]:[rank5]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default6]:[rank6]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default5]:[rank5]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default6]:[rank6]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [default5]:[rank5]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default5]:[rank5]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [default4]:[rank4]: Traceback (most recent call last):\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "ining-demo/0 [default4]:[rank4]:     return _run_code(code, main_globals, None,\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "ining-demo/0 [default4]:[rank4]:     exec(code, run_globals)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\n",
      "ining-demo/0 [default4]:[rank4]:     fdl_runner_app()\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\n",
      "ining-demo/0 [default4]:[rank4]:     raise e\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\n",
      "ining-demo/0 [default4]:[rank4]:     return get_command(self)(*args, **kwargs)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "ining-demo/0 [default4]:[rank4]:     return self.main(*args, **kwargs)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\n",
      "ining-demo/0 [default4]:[rank4]:     return _main(\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
      "ining-demo/0 [default4]:[rank4]:     rv = self.invoke(ctx)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "ining-demo/0 [default4]:[rank4]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "ining-demo/0 [default4]:[rank4]:     return __callback(*args, **kwargs)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
      "ining-demo/0 [default4]:[rank4]:     return callback(**use_params)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\n",
      "ining-demo/0 [default4]:[rank4]:     fdl_fn()\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\n",
      "ining-demo/0 [default4]:[rank4]:     return train(\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\n",
      "ining-demo/0 [default4]:[rank4]:     trainer.fit(model, data)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "ining-demo/0 [default4]:[rank4]:     call._call_and_handle_interrupt(\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "ining-demo/0 [default4]:[rank4]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "ining-demo/0 [default4]:[rank4]:     return function(*args, **kwargs)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "ining-demo/0 [default4]:[rank4]:     self._run(model, ckpt_path=ckpt_path)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "ining-demo/0 [default4]:[rank4]:     results = self._run_stage()\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "ining-demo/0 [default4]:[rank4]:     self.fit_loop.run()\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "ining-demo/0 [default4]:[rank4]:     self.advance()\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "ining-demo/0 [default4]:[rank4]:     self.epoch_loop.run(self._data_fetcher)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "ining-demo/0 [default4]:[rank4]:     self.advance(data_fetcher)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/opt/NeMo/nemo/lightning/pytorch/trainer.py\", line 47, in advance\n",
      "ining-demo/0 [default4]:[rank4]:     super().advance(data_fetcher)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 269, in advance\n",
      "ining-demo/0 [default4]:[rank4]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "ining-demo/0 [default4]:[rank4]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "ining-demo/0 [default4]:[rank4]:   File \"/gtc/NeMo/examples/llm/resiliency/crash_simulator.py\", line 26, in on_train_batch_end\n",
      "ining-demo/0 [default4]:[rank4]:     raise Exception(f\"Simulating a crash at step {self.crash_step}!\")\n",
      "ining-demo/0 [default4]:[rank4]: Exception: Simulating a crash at step 17!\n",
      "ining-demo/0 [2025-03-03 21:40:16,516] [ERROR] [ft_launcher@ec00c0d0158b] failed (exitcode: 1) local_rank: 0 (pid: 9580) of binary: /usr/bin/python\n",
      "ining-demo/0 [2025-03-03 21:40:16,516] [INFO] [ft_launcher@ec00c0d0158b] [default] Worker group FAILED. 3/3 attempts left; will restart worker group\n",
      "ining-demo/0 [2025-03-03 21:40:16,516] [INFO] [ft_launcher@ec00c0d0158b] [default] Stopping worker group\n",
      "ining-demo/0 [2025-03-03 21:40:16,525] [INFO] [ft_launcher@ec00c0d0158b] [default] Rendezvous'ing worker group\n",
      "ining-demo/0 [2025-03-03 21:40:16,644] [INFO] [ft_launcher@ec00c0d0158b] [default] Rendezvous complete for workers. Result:\n",
      "ining-demo/0   restart_count=1\n",
      "ining-demo/0   master_addr=ec00c0d0158b\n",
      "ining-demo/0   master_port=44357\n",
      "ining-demo/0   group_rank=0\n",
      "ining-demo/0   group_world_size=1\n",
      "ining-demo/0   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "ining-demo/0   role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "ining-demo/0   global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "ining-demo/0 \n",
      "ining-demo/0 [2025-03-03 21:40:16,644] [INFO] [ft_launcher@ec00c0d0158b] [default] Starting worker group\n",
      "ining-demo/0 [2025-03-03 21:40:16,644] [INFO] [ft_launcher@ec00c0d0158b] Setting worker0 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/0/error.json\n",
      "ining-demo/0 [2025-03-03 21:40:16,644] [INFO] [ft_launcher@ec00c0d0158b] Setting worker1 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/1/error.json\n",
      "ining-demo/0 [2025-03-03 21:40:16,644] [INFO] [ft_launcher@ec00c0d0158b] Setting worker2 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/2/error.json\n",
      "ining-demo/0 [2025-03-03 21:40:16,644] [INFO] [ft_launcher@ec00c0d0158b] Setting worker3 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/3/error.json\n",
      "ining-demo/0 [2025-03-03 21:40:16,644] [INFO] [ft_launcher@ec00c0d0158b] Setting worker4 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/4/error.json\n",
      "ining-demo/0 [2025-03-03 21:40:16,645] [INFO] [ft_launcher@ec00c0d0158b] Setting worker5 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/5/error.json\n",
      "ining-demo/0 [2025-03-03 21:40:16,645] [INFO] [ft_launcher@ec00c0d0158b] Setting worker6 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/6/error.json\n",
      "ining-demo/0 [2025-03-03 21:40:16,645] [INFO] [ft_launcher@ec00c0d0158b] Setting worker7 reply file to: /root/.nemo_run/experiments/resiliency-in-pretraining-demo/resiliency-in-pretraining-demo_1741037843/resiliency-in-pretraining-demo/nemo_run/resiliency-in-pretraining-demo-th70tx1qsh43kc/torchelastic/resiliency-in-pretraining-demo/3057_du64ipfr/attempt_1/7/error.json\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:26 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "ining-demo/0 [default0]:      cm = get_cmap(\"Set1\")\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default6]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default5]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default6]:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "ining-demo/0 [default4]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default5]:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "ining-demo/0 [default2]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default3]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:27 tokenizer_utils:224] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:27 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False\n",
      "ining-demo/0 [default0]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:27 nemo_logger:145] Experiments will be logged at /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:27 megatron_strategy:315] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "ining-demo/0 [default0]:GPU available: True (cuda), used: True\n",
      "ining-demo/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "ining-demo/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:27 nemo_logger:123] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:27 nemo_logger:173] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/nemo_run/checkpoints/tb_logs\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:27 nemo_logger:189] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:27 nemo_logger:212] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 20. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "ining-demo/0 [default7]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default1]:Setup to simulate a crash if step == 17\n",
      "ining-demo/0 [default4]:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:426] Rank 0 has data parallel group : [0, 2, 4, 6]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:432] Rank 0 has combined group of data parallel and context parallel : [0, 2, 4, 6]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:437] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:440] Ranks 0 has data parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:448] Rank 0 has context parallel group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:451] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:452] Ranks 0 has context parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:459] Rank 0 has model parallel group: [0, 1]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:460] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:469] Rank 0 has tensor model parallel group: [0, 1]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:473] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:474] Rank 0 has tensor model parallel rank: 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:494] Rank 0 has pipeline model parallel group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:506] Rank 0 has embedding group: [0]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:512] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:513] Rank 0 has pipeline model parallel rank 0\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:514] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:28 megatron_init:515] Rank 0 has embedding rank: 0\n",
      "ining-demo/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "ining-demo/0 [default3]:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "ining-demo/0 [default2]:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "ining-demo/0 [default1]:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "ining-demo/0 [default7]:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "ining-demo/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "ining-demo/0 [default0]:distributed_backend=nccl\n",
      "ining-demo/0 [default0]:All distributed processes registered. Starting with 8 processes\n",
      "ining-demo/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "ining-demo/0 [default0]:\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 fault_tolerance_callback:311] [FaultToleranceCallback@rank0] Fault tolerance dir: /tmp/nemo_run/checkpoints\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 fault_tolerance_callback:311] [FaultToleranceCallback@rank0] Fault tolerance client initialized. Timeouts: HeartbeatTimeouts(initial=1800.00, subsequent=300.00, were_calculated=False)\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 base:44] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 megatron_strategy:327] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 num_microbatches_calculator:228] setting number of microbatches to constant 128\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 megatron_parallel:549]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 54663936\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=40000000, average_in_collective=True, fp8_param_gather=False)\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "ining-demo/0 [default0]:    Params for bucket 1 (54663936 elements):\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.final_layernorm.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.output_layer.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.embedding.word_embeddings.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight\n",
      "ining-demo/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.weight\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:30 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "ining-demo/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:\n",
      "ining-demo/0 [default0]:  | Name   | Type | Params | Mode \n",
      "ining-demo/0 [default0]:----------------------------------------\n",
      "ining-demo/0 [default0]:0 | module | DDP  | 54.7 M | train\n",
      "ining-demo/0 [default0]:----------------------------------------\n",
      "ining-demo/0 [default0]:54.7 M    Trainable params\n",
      "ining-demo/0 [default0]:0         Non-trainable params\n",
      "ining-demo/0 [default0]:54.7 M    Total params\n",
      "ining-demo/0 [default0]:218.656   Total estimated model params size (MB)\n",
      "ining-demo/0 [default0]:91        Modules in train mode\n",
      "ining-demo/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default0]:0         Modules in eval mode\n",
      "ining-demo/0 [default0]:Restoring states from the checkpoint path at /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=9-consumed_samples=5120.0-last/weights\n",
      "ining-demo/0 [default7]:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default5]:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default6]:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default4]:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "ining-demo/0 [default2]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default3]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default4]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default7]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default6]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default5]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default1]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default0]:Resuming from checkpoint, setting has_simulated_crash_happened to True!\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:40:31 distrib_optimizer:705] Loading distributed optimizer sharded state of type fully_sharded_model_space\n",
      "ining-demo/0 [default0]:Restored all states from the checkpoint at /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=9-consumed_samples=5120.0-last/weights\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:31 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:161: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
      "ining-demo/0 [default0]:    \n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:41 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!\n",
      "ining-demo/0 [default0]:[NeMo W 2025-03-03 21:40:41 rerun_state_machine:211] RerunStateMachine initialized in mode disabled\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 10/19 | lr: 1.649e-06 | consumed_samples: 5632 | global_batch_size: 512 | global_step: 10 | reduced_train_loss: 11.03 | train_step_timing in s: 10.31\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 11/19 | lr: 1.799e-06 | consumed_samples: 6144 | global_batch_size: 512 | global_step: 11 | reduced_train_loss: 11.03 | train_step_timing in s: 7.624\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 12/19 | lr: 1.949e-06 | consumed_samples: 6656 | global_batch_size: 512 | global_step: 12 | reduced_train_loss: 11.03 | train_step_timing in s: 7.627\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 13/19 | lr: 2.099e-06 | consumed_samples: 7168 | global_batch_size: 512 | global_step: 13 | reduced_train_loss: 11.03 | train_step_timing in s: 7.633\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 14/19 | lr: 2.249e-06 | consumed_samples: 7680 | global_batch_size: 512 | global_step: 14 | reduced_train_loss: 11.03 | train_step_timing in s: 7.631\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 15/19 | lr: 2.399e-06 | consumed_samples: 8192 | global_batch_size: 512 | global_step: 15 | reduced_train_loss: 11.03 | train_step_timing in s: 7.636\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 16/19 | lr: 2.549e-06 | consumed_samples: 8704 | global_batch_size: 512 | global_step: 16 | reduced_train_loss: 11.03 | train_step_timing in s: 7.634\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 17/19 | lr: 2.699e-06 | consumed_samples: 9216 | global_batch_size: 512 | global_step: 17 | reduced_train_loss: 11.03 | train_step_timing in s: 7.63\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 18/19 | lr: 2.849e-06 | consumed_samples: 9728 | global_batch_size: 512 | global_step: 18 | reduced_train_loss: 11.03 | train_step_timing in s: 7.639\n",
      "ining-demo/0 [default0]:Training epoch 0, iteration 19/19 | lr: 2.999e-06 | consumed_samples: 10240 | global_batch_size: 512 | global_step: 19 | reduced_train_loss: 11.03 | train_step_timing in s: 7.638\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:41:54 model_checkpoint:497] Scheduled async checkpoint save for /tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=19-consumed_samples=10240.0-last.ckpt\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:41:54 dist_ckpt_io:174] Pending async checkpoint saves. Finalizing them synchronously now\n",
      "ining-demo/0 [default0]:`Trainer.fit` stopped: `max_steps=20` reached.\n",
      "ining-demo/0 [default0]:[NeMo I 2025-03-03 21:41:55 model_checkpoint:522] Async checkpoint save for step 20 (/tmp/nemo_run/checkpoints/resiliency-in-pretraining-demo/checkpoints/model_name=0--val_loss=0.00-step=19-consumed_samples=10240.0-last.ckpt) finalized successfully.\n",
      "ining-demo/0 [2025-03-03 21:42:01,959] [INFO] [ft_launcher@ec00c0d0158b] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "ining-demo/0 [2025-03-03 21:42:01,959] [INFO] [ft_launcher@ec00c0d0158b] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "ining-demo/0 [2025-03-03 21:42:01,959] [INFO] [ft_launcher@ec00c0d0158b] Done waiting for other agents. Elapsed: 0.00024008750915527344 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job resiliency-in-pretraining-demo-th70tx1qsh43kc finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['resiliency-in-pretraining-demo']</span><span style=\"background-color: #272822\">                              </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"resiliency-in-pretraining-demo_1741037843\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"resiliency-in-pretraining-demo\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"resiliency-in-pretraining-demo\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['resiliency-in-pretraining-demo']\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresiliency-in-pretraining-demo_1741037843\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresiliency-in-pretraining-demo\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresiliency-in-pretraining-demo\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status resiliency-in-pretraining-demo_1741037843</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs resiliency-in-pretraining-demo_1741037843 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel resiliency-in-pretraining-demo_1741037843 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresiliency-in-pretraining-demo_1741037843\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresiliency-in-pretraining-demo_1741037843\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresiliency-in-pretraining-demo_1741037843\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable a crash simulation callback\n",
    "pretrain.trainer.callbacks.append(run.Config(CrashSimulationCallback, crash_step=17))\n",
    "\n",
    "# run the experiment\n",
    "run_experiment(exp_name, pretrain, executor, run_plugins, dryrun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3acd19-c456-4a81-a933-12f11032bfb5",
   "metadata": {},
   "source": [
    "### Asynchronous Checkpointing\n",
    "Checkpointing is important for recovering from failures, but traditional checkpointing has drawbacks:\n",
    "\n",
    "1. Training pauses while saving checkpoints\n",
    "2. To minimize these pauses, checkpoints are usually only saved once per epoch\n",
    "3. If training fails between checkpoints, work must be redone from the last checkpoint\n",
    "\n",
    "For example, with:\n",
    "- 500 steps per epoch\n",
    "- 10 seconds per step\n",
    "- 3 epochs total\n",
    "\n",
    "Best case (no failures):\n",
    "- Training time = 15,000 seconds (500 steps × 10 seconds × 3 epochs)\n",
    "\n",
    "Worst case (failure at step 799):\n",
    "- Must redo nearly 2 full epochs\n",
    "- Training time = 20,000 seconds (nearly 5,000 seconds wasted)\n",
    "\n",
    "Asynchronous checkpointing solves these problems by:\n",
    "- Saving checkpoints without pausing training\n",
    "- Using fast distributed checkpointing via Megatron-Core\n",
    "- Allowing frequent checkpoints with minimal overhead\n",
    "\n",
    "This means you can checkpoint often to minimize lost work, without slowing down training.\n",
    "\n",
    "For more details, see:\n",
    "- [Megatron-Core distributed checkpointing](https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/dist_checkpointing.html)\n",
    "- [NeMo documentation](https://github.com/NVIDIA/NeMo/blob/main/docs/source/checkpoints/dist_ckpt.rst)\n",
    "\n",
    "Note: NeMo enables asynchronous and parallel checkpointing by default through MegatronStrategy's \n",
    "ckpt_async_save and ckpt_parallel_save options, so users automatically get these benefits\n",
    "without any additional configuration needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2b70f-c2a6-45ef-80a4-57b9f88e7879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
