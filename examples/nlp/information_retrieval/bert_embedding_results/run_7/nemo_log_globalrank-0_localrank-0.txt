[NeMo W 2024-04-08 16:25:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo I 2024-04-08 16:25:26 megatron_bert_embedding_finetuning:31] 
    
    ************** Experiment configuration ***********
[NeMo I 2024-04-08 16:25:26 megatron_bert_embedding_finetuning:32] 
    name: megatron_bert
    restore_from_path: /opt/NeMo/examples/nlp/language_modeling/bert_pretrain_results/megatron_bert/checkpoints/megatron_bert.nemo
    trainer:
      devices: 1
      num_nodes: 1
      accelerator: gpu
      precision: 16
      logger: false
      enable_checkpointing: false
      use_distributed_sampler: false
      max_epochs: 36
      max_steps: 12
      log_every_n_steps: 10
      val_check_interval: 4
      limit_val_batches: 50
      limit_test_batches: 500
      accumulate_grad_batches: 1
      gradient_clip_val: 1.0
      benchmark: false
      num_sanity_val_steps: 0
    exp_manager:
      explicit_log_dir: examples/nlp/information_retrieval/bert_embedding_results
      exp_dir: null
      name: megatron_bert
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: null
        name: null
      resume_if_exists: false
      resume_ignore_no_checkpoint: true
      create_checkpoint_callback: true
      checkpoint_callback_params:
        monitor: val_loss
        save_top_k: 10
        mode: min
        always_save_nemo: false
        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}
        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
    model:
      mcore_bert: true
      micro_batch_size: 1
      global_batch_size: 1
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      virtual_pipeline_model_parallel_size: null
      encoder_seq_length: 128
      max_position_embeddings: ${.encoder_seq_length}
      position_embedding_type: learned_absolute
      num_layers: 2
      hidden_size: 256
      ffn_hidden_size: 3072
      num_attention_heads: 8
      transformer_block_type: post_ln
      add_pooler: true
      add_lm_head: false
      init_method_std: 0.02
      hidden_dropout: 0.1
      kv_channels: null
      apply_query_key_layer_scaling: false
      normalization: layernorm
      layernorm_epsilon: 1.0e-12
      make_vocab_size_divisible_by: 128
      pre_process: true
      post_process: true
      bert_binary_head: true
      megatron_legacy: false
      tokenizer:
        library: huggingface
        type: intfloat/e5-large-unsupervised
        model: null
        vocab_file: null
        merge_file: null
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: false
      grad_allreduce_chunk_size_mb: 125
      grad_div_ar_fusion: false
      seed: 1234
      use_cpu_initialization: false
      onnx_safe: false
      gradient_as_bucket_view: true
      activations_checkpoint_granularity: null
      activations_checkpoint_method: null
      activations_checkpoint_num_layers: null
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: false
      data:
        data_train: /home/data/dev-v1.1_gpt.json
        data_validation: null
        hard_negatives_to_train: 4
        index_mapping_dir: null
        data_impl: mmap
        splits_string: 900,50,50
        seq_length: ${model.encoder_seq_length}
        skip_warmup: true
        num_workers: 0
        dataloader_type: single
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        masked_lm_prob: 0.15
        short_seq_prob: 0.1
      optim:
        name: fused_adam
        lr: 0.0005
        weight_decay: 0.01
        betas:
        - 0.9
        - 0.98
        sched:
          name: CosineAnnealing
          warmup_steps: 500
          constant_steps: 50000
          min_lr: 2.0e-05
    
[NeMo W 2024-04-08 16:25:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
    
[NeMo W 2024-04-08 16:25:27 exp_manager:708] Exp_manager is logging to examples/nlp/information_retrieval/bert_embedding_results, but it already exists.
[NeMo I 2024-04-08 16:25:27 exp_manager:396] Experiments will be logged at examples/nlp/information_retrieval/bert_embedding_results
[NeMo I 2024-04-08 16:25:27 exp_manager:856] TensorboardLogger has been set up
[NeMo W 2024-04-08 16:25:27 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 12. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.
[NeMo I 2024-04-08 16:25:27 megatron_bert_embedding_finetuning:47] Loading model from /opt/NeMo/examples/nlp/language_modeling/bert_pretrain_results/megatron_bert/checkpoints/megatron_bert.nemo
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-04-08 16:25:27 megatron_init:253] Rank 0 has data parallel group : [0]
[NeMo I 2024-04-08 16:25:27 megatron_init:259] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2024-04-08 16:25:27 megatron_init:264] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2024-04-08 16:25:27 megatron_init:267] Ranks 0 has data parallel rank: 0
[NeMo I 2024-04-08 16:25:27 megatron_init:284] Rank 0 has context parallel group: [0]
[NeMo I 2024-04-08 16:25:27 megatron_init:287] All context parallel group ranks: [[0]]
[NeMo I 2024-04-08 16:25:27 megatron_init:288] Ranks 0 has context parallel rank: 0
[NeMo I 2024-04-08 16:25:27 megatron_init:299] Rank 0 has model parallel group: [0]
[NeMo I 2024-04-08 16:25:27 megatron_init:300] All model parallel group ranks: [[0]]
[NeMo I 2024-04-08 16:25:27 megatron_init:310] Rank 0 has tensor model parallel group: [0]
[NeMo I 2024-04-08 16:25:27 megatron_init:314] All tensor model parallel group ranks: [[0]]
[NeMo I 2024-04-08 16:25:27 megatron_init:315] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-04-08 16:25:27 megatron_init:344] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2024-04-08 16:25:27 megatron_init:356] Rank 0 has embedding group: [0]
[NeMo I 2024-04-08 16:25:27 megatron_init:362] All pipeline model parallel group ranks: [[0]]
[NeMo I 2024-04-08 16:25:27 megatron_init:363] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-04-08 16:25:27 megatron_init:364] All embedding group ranks: [[0]]
[NeMo I 2024-04-08 16:25:27 megatron_init:365] Rank 0 has embedding rank: 0
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-04-08 16:25:27 tokenizer_utils:177] Getting HuggingFace AutoTokenizer with pretrained_model_name: intfloat/e5-large-unsupervised
[NeMo I 2024-04-08 16:25:27 megatron_base_model:584] Padded vocab_size: 30592, original vocab_size: 30522, dummy tokens: 70.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:1172] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: attention_dropout in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: persist_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-04-08 16:25:27 megatron_base_model:556] The model: MegatronBertEmbeddingModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
