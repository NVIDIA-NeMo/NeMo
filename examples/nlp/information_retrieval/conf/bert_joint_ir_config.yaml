# Config file for training left-to-right Transformer language model

trainer:
  gpus: 4 # the number of gpus, 0 for CPU
  num_nodes: 1
  max_epochs: 2
  max_steps: 40000 # precedence over max_epochs
  accumulate_grad_batches: 1 # accumulates grads every k batches
  amp_level: O2 # O1/O2 for mixed precision
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  distributed_backend: ddp
  checkpoint_callback: False  # Provided by exp_manager
  logger: False  # Provided by exp_manager
  row_log_interval: 1  # Interval of logging.
  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.

model:

  language_model:
    pretrained_model_name: bert-base-uncased
    hidden_size: 768
    dropout: 0.1

  dataset:
    num_workers: 1 # number of workers for data loaders
    drop_last: false # drops the last last batch if it is smaller than the batch size
    pin_memory: false # enables pin_memory feature of the data loaders

  train_ds:
    passages: /workspace/datasets/msmarco/trec_submit/collection.tsv
    queries: /workspace/datasets/msmarco/trec_submit/queries.train.tsv
    query_to_passages: /workspace/datasets/msmarco/trec_submit/triples.train.extra_tiny.tsv
    num_negatives: 11
    batch_size: 6
    shuffle: true
    num_samples: -1 # number of samples to be considered, -1 means all the dataset

  validation_ds:
    passages: /workspace/datasets/msmarco/trec_submit/collection.bm25.dev.small.tsv
    queries: /workspace/datasets/msmarco/trec_submit/queries.dev.small.tsv
    query_to_passages: /workspace/datasets/msmarco/trec_submit/top100.bm25.dev.small.tsv
    num_candidates: 50
    batch_size: 1
    shuffle: false
    num_samples: -1 # number of samples to be considered, -1 means all the dataset

  optim:
    name: adam
    lr: 1e-4
    betas: [0.9, 0.999]
    weight_decay: 0

    sched:
      name: WarmupAnnealing
      warmup_steps: null
      warmup_ratio: 0.05
      last_epoch: -1

      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

exp_manager:
  exp_dir: msmarco  # where to store logs and checkpoints
  name: BertJointIR  # The name of your model
  create_tensorboard_logger: True
  create_checkpoint_callback: True

hydra:
  run:
    dir: .
  job_logging:
    root:
      handlers: null
