defaults:
  - 126m_pile
  - _self_

# Not based on an official NeMo config, just a modified 126m config, highly un-optimized

run:
  name: gpt3_355m

model:
  num_layers: 24
  hidden_size: 1024
  ffn_hidden_size: ${multiply:4, ${.hidden_size}}  # Transformer FFN hidden size. 4 * hidden_size.
  num_attention_heads: 16

  optim:
    lr: 3e-4