defaults:
  - 153m_hyena_pile
  - _self_

# Not based on an official NeMo config, just a modified 126m config, highly un-optimized

run:
  name: gpt3_355m_hyena

model:
  num_layers: 36
  hidden_size: 1024
  ffn_hidden_size: ${multiply:2, ${.hidden_size}}  # Transformer FFN hidden size. 4 * hidden_size.

  optim:
    lr: 3e-4