defaults:
  - nemo_fw_gpt3_pretrain_126m
  - _self_

model:
  spec_overrides:
    # Uncomment layer norm override if not using fused TE classes in HyenaOperator
    # - submodule: 'submodules.input_layernorm'
    #   class: 'megatron.core.transformer.custom_layers.transformer_engine.TENorm'
    - submodule: 'submodules.self_attention'
      class: 'nemo.collections.nlp.modules.common.hyena.HyenaOperator'
      config_key: 'hyena'
    #    - submodule: 'submodules.mlp.submodules.linear_fc1'
    #      params:
    #        stride: 3

  hyena:
    # HyenaOperator parameters
    d_model: ${model.hidden_size}
    l_max: ${model.encoder_seq_length}
    order: 2
    filter_order: 64
    num_heads: ${model.num_attention_heads}
    inner_factor: 1
    num_blocks: 1
    fused_bias_fc: False
    outer_mixing: False
    dropout: 0.0
    filter_dropout: 0.0
    post_order_ffn: False
    jit_filter: False
    short_filter_order: 3
    activation: "identity"

    # HyenaFilter parameters
    emb_dim: 3 # dim of input to MLP, augments with positional encoding
    fused_fft_conv: false
    learn_pos_emb_z: 1e-5
    w: 1 # frequency of periodic activations
    bias: true
    normalized: False
    num_inner_mlps: 2

    # ExponentialModulation parameters
    fast_decay_pct: 0.3
    slow_decay_pct: 1.5
    target: 1e-2
    learn_modulation: False
    modulate: True
    shift: 0.0