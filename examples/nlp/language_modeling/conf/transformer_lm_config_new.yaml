# Config file for training left-to-right Transformer language model
name: &name TransformerLM

trainer:
  gpus: 4 # the number of gpus, 0 for CPU
  num_nodes: 1
  max_steps: 50000 # precedence over max_epochs
  accumulate_grad_batches: 1 # accumulates grads every k batches
  amp_level: O2 # O1/O2 for mixed precision
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  accelerator: ddp
  checkpoint_callback: false  # Provided by exp_manager
  logger: false  # Provided by exp_manager
  log_every_n_steps: 1  # Interval of logging.
  val_check_interval: 0.1  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.

model:

  language_model:
    tokenizer: yttm
    special_tokens:
        unk_token: '<UNK>'
        pad_token: '<PAD>'
        bos_token: '<BOS>'
        eos_token: '<EOS>'
    vocab_file: null
    tokenizer_model: /workspace/datasets/en_ja/ngc_dataset/tokenizer.decoder.32000.BPE.model
    hidden_size: 512
    num_layers: 6
    num_attn_heads: 8
    inner_size: 2048
    max_seq_length: 256
    embedding_dropout: 0
    ffn_dropout: 0
    attn_score_dropout: 0
    attn_layer_dropout: 0

  dataset:
    max_seq_length: 192
    num_workers: 2 # number of workers for data loaders
    drop_last: false # drops the last last batch if it is smaller than the batch size
    pin_memory: false # enables pin_memory feature of the data loaders
    predict_last_k: 64 # number of last tokens to predict on evaluation

  train_ds:
    use_tarred_dataset: true
    metadata_file: /workspace/datasets/en_ja/tarred300k/metadata.json # path to file with training data
    tar_files: /workspace/datasets/en_ja/tarred300k/mono-batches.tokens.4096._OP_1..13_CL_.tar
    shard_strategy: scatter
    tokens_in_batch: 4096
    tar_shuffle_n: 256

  validation_ds:
    file_name: /workspace/datasets/en_ja/ngc_dataset/test384x10.ja # path to file with validation data
    tokens_in_batch: 512
    shuffle: false

  optim:
    name: adam
    lr: 1e-4
    betas: [0.9, 0.999]
    weight_decay: 0

    sched:
      name: WarmupAnnealing
      warmup_steps: null
      warmup_ratio: 0.05
      last_epoch: -1

exp_manager:
  exp_dir: null  # where to store logs and checkpoints
  name: *name  # name of experiment
  create_tensorboard_logger: true
  create_checkpoint_callback: true

hydra:
  run:
    dir: .
  job_logging:
    root:
      handlers: null
