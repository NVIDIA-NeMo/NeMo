[NeMo W 2024-03-11 18:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo I 2024-03-11 18:56:53 megatron_gpt_pretraining:34] 
    
    ************** Experiment configuration ***********
[NeMo I 2024-03-11 18:56:53 megatron_gpt_pretraining:35] 
    name: megatron_gpt
    restore_from_path: null
    trainer:
      devices: 1
      num_nodes: 1
      accelerator: gpu
      precision: 16
      logger: false
      enable_checkpointing: false
      use_distributed_sampler: false
      max_epochs: -1
      max_steps: 3
      log_every_n_steps: 1
      val_check_interval: 2
      limit_val_batches: 2
      limit_test_batches: 500
      accumulate_grad_batches: 1
      gradient_clip_val: 1.0
      benchmark: false
      enable_model_summary: false
    exp_manager:
      explicit_log_dir: null
      exp_dir: examples/nlp/language_modeling/gpt_pretrain_results
      name: megatron_gpt
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: null
        name: null
      create_neptune_logger: false
      neptune_logger_kwargs:
        project: null
        name: null
        prefix: train
        log_model_checkpoints: false
        tags: null
        description: null
      resume_if_exists: true
      resume_ignore_no_checkpoint: true
      resume_from_checkpoint: ${model.resume_from_checkpoint}
      create_checkpoint_callback: true
      checkpoint_callback_params:
        monitor: val_loss
        save_top_k: 10
        mode: min
        always_save_nemo: false
        save_nemo_on_train_end: false
        filename: megatron_gpt--{val_loss:.2f}-{step}-{consumed_samples}
        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
    model:
      mcore_gpt: true
      micro_batch_size: 4
      global_batch_size: 8
      rampup_batch_size: null
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      virtual_pipeline_model_parallel_size: null
      encoder_seq_length: 128
      max_position_embeddings: 128
      num_layers: 8
      hidden_size: 256
      ffn_hidden_size: 3072
      num_attention_heads: 8
      init_method_std: 0.02
      use_scaled_init_method: true
      hidden_dropout: 0.1
      attention_dropout: 0.1
      ffn_dropout: 0.0
      kv_channels: null
      apply_query_key_layer_scaling: false
      normalization: layernorm1p
      layernorm_epsilon: 1.0e-05
      do_layer_norm_weight_decay: false
      make_vocab_size_divisible_by: 128
      pre_process: true
      post_process: true
      persist_layer_norm: true
      bias: true
      activation: gelu
      headscale: false
      transformer_block_type: pre_ln
      openai_gelu: false
      normalize_attention_scores: true
      position_embedding_type: learned_absolute
      rotary_percentage: 1.0
      attention_type: multihead
      share_embeddings_and_output_weights: true
      overlap_p2p_comm: false
      batch_p2p_comm: true
      seq_len_interpolation_factor: null
      num_query_groups: null
      tokenizer:
        library: megatron
        type: GPT2BPETokenizer
        model: null
        vocab_file: null
        merge_file: null
        delimiter: null
        sentencepiece_legacy: false
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      hysteresis: 2
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: false
      grad_allreduce_chunk_size_mb: 125
      grad_div_ar_fusion: true
      gradient_accumulation_fusion: false
      bias_activation_fusion: true
      bias_dropout_add_fusion: true
      masked_softmax_fusion: true
      get_attention_mask_from_fusion: true
      apply_rope_fusion: false
      seed: 1234
      resume_from_checkpoint: null
      use_cpu_initialization: false
      onnx_safe: false
      apex_transformer_log_level: 30
      gradient_as_bucket_view: true
      sync_batch_comm: false
      nccl_communicator_config_path: null
      fsdp: false
      fsdp_sharding_strategy: full
      fsdp_grad_reduce_dtype: fp32
      fsdp_sharded_checkpoint: false
      activations_checkpoint_granularity: null
      activations_checkpoint_method: null
      activations_checkpoint_num_layers: null
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: false
      transformer_engine: false
      fp8: false
      fp8_e4m3: false
      fp8_hybrid: true
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1024
      fp8_amax_compute_algo: max
      reduce_amax: true
      use_emha: false
      ub_tp_comm_overlap: false
      ub_tp_comm_overlap_cfg: null
      use_flash_attention: false
      cpu_offloading: false
      cpu_offloading_num_layers: ${sum:${.num_layers},-1}
      cpu_offloading_activations: true
      cpu_offloading_weights: true
      sharp: false
      enable_megatron_timers: false
      megatron_timer_kwargs:
        log_every_n_steps: 10
        log_mode: minmax
        barrier: false
      data:
        data_prefix:
        - 1.0
        - /home/data/test_text_document
        index_mapping_dir: null
        data_impl: mmap
        splits_string: 900,50,50
        seq_length: 128
        skip_warmup: true
        num_workers: 2
        dataloader_type: single
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        validation_drop_last: true
        no_seqlen_plus_one_input_tokens: false
        pad_samples_to_global_batch_size: false
        shuffle_documents: true
        exchange_indices_distributed: false
        mock_dataset: false
        add_fim: true
        fim:
          extra_tokens:
            prefix: fim_prefix
            middle: fim_middle
            suffix: fim_suffix
            pad: fim_pad
            eod: endoftext
      nsys_profile:
        enabled: false
        start_step: 10
        end_step: 10
        ranks:
        - 0
        gen_shape: false
      optim:
        name: fused_adam
        lr: 0.0002
        weight_decay: 0.01
        betas:
        - 0.9
        - 0.98
        sched:
          name: CosineAnnealing
          warmup_steps: 1
          constant_steps: 1
          min_lr: 8.0e-05
      gc_interval: 0
      name: megatron_gpt_full_te_layer_autocast
    
[NeMo W 2024-03-11 18:56:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
      rank_zero_warn(
    
[NeMo W 2024-03-11 18:56:53 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo W 2024-03-11 18:56:53 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :examples/nlp/language_modeling/gpt_pretrain_results/megatron_gpt/checkpoints. Training from scratch.
[NeMo I 2024-03-11 18:56:53 exp_manager:396] Experiments will be logged at examples/nlp/language_modeling/gpt_pretrain_results/megatron_gpt
[NeMo I 2024-03-11 18:56:53 exp_manager:856] TensorboardLogger has been set up
[NeMo W 2024-03-11 18:56:53 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 3. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-03-11 18:56:53 megatron_init:253] Rank 0 has data parallel group : [0]
[NeMo I 2024-03-11 18:56:53 megatron_init:259] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2024-03-11 18:56:53 megatron_init:264] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2024-03-11 18:56:53 megatron_init:267] Ranks 0 has data parallel rank: 0
[NeMo I 2024-03-11 18:56:53 megatron_init:284] Rank 0 has context parallel group: [0]
[NeMo I 2024-03-11 18:56:53 megatron_init:287] All context parallel group ranks: [[0]]
[NeMo I 2024-03-11 18:56:53 megatron_init:288] Ranks 0 has context parallel rank: 0
[NeMo I 2024-03-11 18:56:53 megatron_init:299] Rank 0 has model parallel group: [0]
[NeMo I 2024-03-11 18:56:53 megatron_init:300] All model parallel group ranks: [[0]]
[NeMo I 2024-03-11 18:56:53 megatron_init:310] Rank 0 has tensor model parallel group: [0]
[NeMo I 2024-03-11 18:56:53 megatron_init:314] All tensor model parallel group ranks: [[0]]
[NeMo I 2024-03-11 18:56:53 megatron_init:315] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-03-11 18:56:53 megatron_init:344] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2024-03-11 18:56:53 megatron_init:356] Rank 0 has embedding group: [0]
[NeMo I 2024-03-11 18:56:53 megatron_init:362] All pipeline model parallel group ranks: [[0]]
[NeMo I 2024-03-11 18:56:53 megatron_init:363] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-03-11 18:56:53 megatron_init:364] All embedding group ranks: [[0]]
[NeMo I 2024-03-11 18:56:53 megatron_init:365] Rank 0 has embedding rank: 0
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:53 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-03-11 18:56:54 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2024-03-11 18:56:54 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2024-03-11 18:56:54 megatron_base_model:574] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-03-11 18:56:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.
      rank_zero_warn(
    
[NeMo W 2024-03-11 18:56:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.
      rank_zero_warn(
    
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1342] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 2.76e+07. Total number of model parameters: 2.76e+07.
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1217] Building GPT datasets.
[NeMo I 2024-03-11 18:56:54 auto_tokenizer:174] 5 special tokens added, resize your model accordingly.
[NeMo I 2024-03-11 18:56:54 utils:47] mock = False
[NeMo I 2024-03-11 18:56:54 utils:47] Let split_matrix = [(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)]
[NeMo I 2024-03-11 18:56:54 utils:47] Load the _IndexReader from /home/data/test_text_document.idx
[NeMo I 2024-03-11 18:56:54 utils:47] 	Extract the sequence lengths
[NeMo I 2024-03-11 18:56:54 utils:47] 	Extract the sequence pointers
[NeMo I 2024-03-11 18:56:54 utils:47] 	Extract the document indices
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of sequences: 10042
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of documents: 10042
[NeMo I 2024-03-11 18:56:54 utils:47] Build and save the GPTFIMDataset train indices
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the document index to 8f58baf1cd732c41affaa52e0441a2c0-GPTFIMDataset-document_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the sample index to 8f58baf1cd732c41affaa52e0441a2c0-GPTFIMDataset-sample_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the shuffle index to 8f58baf1cd732c41affaa52e0441a2c0-GPTFIMDataset-shuffle_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of samples: 5513
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of epochs: 1
[NeMo I 2024-03-11 18:56:54 utils:47] Build and save the GPTFIMDataset valid indices
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the document index to c959220327434dc22be2e9bc39a156f7-GPTFIMDataset-document_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the sample index to c959220327434dc22be2e9bc39a156f7-GPTFIMDataset-sample_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the shuffle index to c959220327434dc22be2e9bc39a156f7-GPTFIMDataset-shuffle_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of samples: 383
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of epochs: 1
[NeMo I 2024-03-11 18:56:54 utils:47] Build and save the GPTFIMDataset test indices
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the document index to ebd155e1ee7c75ec086e834e311bc84c-GPTFIMDataset-document_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the sample index to ebd155e1ee7c75ec086e834e311bc84c-GPTFIMDataset-sample_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the shuffle index to ebd155e1ee7c75ec086e834e311bc84c-GPTFIMDataset-shuffle_index.npy
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of samples: 4140
[NeMo I 2024-03-11 18:56:54 utils:47] > total number of epochs: 11
[NeMo W 2024-03-11 18:56:54 utils:47] Building a BlendedDataset for a single MegatronDataset
[NeMo I 2024-03-11 18:56:54 utils:47] Build and save the BlendedDataset indices
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the dataset and dataset sample indexes
[NeMo W 2024-03-11 18:56:54 utils:47] Unable to save the indexes because path_to_cache is None
[NeMo I 2024-03-11 18:56:54 utils:47] > BlendedDataset length: 25
[NeMo W 2024-03-11 18:56:54 utils:47] Building a BlendedDataset for a single MegatronDataset
[NeMo I 2024-03-11 18:56:54 utils:47] Build and save the BlendedDataset indices
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the dataset and dataset sample indexes
[NeMo W 2024-03-11 18:56:54 utils:47] Unable to save the indexes because path_to_cache is None
[NeMo I 2024-03-11 18:56:54 utils:47] > BlendedDataset length: 33
[NeMo W 2024-03-11 18:56:54 utils:47] Building a BlendedDataset for a single MegatronDataset
[NeMo I 2024-03-11 18:56:54 utils:47] Build and save the BlendedDataset indices
[NeMo I 2024-03-11 18:56:54 utils:47] 	Build and save the dataset and dataset sample indexes
[NeMo W 2024-03-11 18:56:54 utils:47] Unable to save the indexes because path_to_cache is None
[NeMo I 2024-03-11 18:56:54 utils:47] > BlendedDataset length: 4020
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1280] Length of train dataset: 25
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1282] Length of val dataset: 33
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1284] Length of test dataset: 4020
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1285] Finished building GPT datasets.
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1384] Setting up train dataloader with len(len(self._train_ds)): 25 and consumed samples: 0
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1294] Building dataloader with consumed samples: 0
[NeMo I 2024-03-11 18:56:54 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 25 and consumed_samples: 0
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1392] Setting up validation dataloader with len(len(self._validation_ds)): 33 and consumed samples: 0
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1294] Building dataloader with consumed samples: 0
[NeMo I 2024-03-11 18:56:54 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 33 and consumed_samples: 0
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1412] Setting up test dataloader with len(len(self._test_ds)): 4020 and consumed samples: 0
[NeMo I 2024-03-11 18:56:54 megatron_gpt_model:1294] Building dataloader with consumed samples: 0
[NeMo I 2024-03-11 18:56:54 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 4020 and consumed_samples: 0
[NeMo I 2024-03-11 18:56:54 nlp_overrides:228] Configuring DDP for model parallelism.
[NeMo I 2024-03-11 18:56:54 modelPT:724] Optimizer config = FusedAdam (
    Parameter Group 0
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 0.0002
        weight_decay: 0.01
    
    Parameter Group 1
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        lr: 0.0002
        weight_decay: 0.0
    )
[NeMo I 2024-03-11 18:56:54 lr_scheduler:915] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f7f3d52d810>" 
    will be used during training (effective maximum steps = 3) - 
    Parameters : 
    (warmup_steps: 1
    constant_steps: 1
    min_lr: 8.0e-05
    max_steps: 3
    )
[NeMo W 2024-03-11 18:57:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      rank_zero_warn(
    
[NeMo W 2024-03-11 18:57:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.
      rank_zero_warn(
    
[NeMo W 2024-03-11 18:57:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
      warning_cache.warn(
    
[NeMo W 2024-03-11 18:57:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      rank_zero_warn(
    
[NeMo W 2024-03-11 18:57:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.
      rank_zero_warn(
    
[NeMo W 2024-03-11 18:57:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
      warning_cache.warn(
    
[NeMo W 2024-03-11 18:57:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
      warning_cache.warn(
    
[NeMo W 2024-03-11 18:57:29 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/megatron/clip_grads.py:86: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
      dummy_overflow_buf = torch.cuda.IntTensor([0])
    
[NeMo W 2024-03-11 18:57:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.
      rank_zero_warn(
    
