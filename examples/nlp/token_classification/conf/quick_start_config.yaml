# Quick start configuration for token classification testing
# This is a minimal configuration that should complete within 30 minutes

name: token_classification_quick_start
trainer:
  max_epochs: 2
  accelerator: auto
  devices: 1
  precision: 16
  enable_checkpointing: false
  logger: false
  log_every_n_steps: 10

model:
  train_ds:
    dataset: conll2003
    max_seq_length: 128
    batch_size: 16
    use_cache: true
    num_workers: 2
    pin_memory: true

  validation_ds:
    dataset: conll2003
    max_seq_length: 128
    batch_size: 16
    use_cache: true
    num_workers: 2
    pin_memory: true

  optim:
    name: adamw
    lr: 2e-5
    weight_decay: 0.01
    betas: [0.9, 0.999]
    sched:
      name: WarmupPolicy
      warmup_steps: 100
      min_lr: 0.0

  tokenizer:
    tokenizer_name: bert-base-uncased
    vocab_file: null
    do_lower_case: true

  language_model:
    pretrained_model_name: bert-base-uncased
    config:
      hidden_size: 768
      num_attention_heads: 12
      num_hidden_layers: 2  # Reduced for quick training
      intermediate_size: 3072
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 512
      type_vocab_size: 2
      initializer_range: 0.02
      layer_norm_eps: 1e-12
      pad_token_id: 0
      gradient_checkpointing: false
      position_embedding_type: absolute
      use_cache: true

  head:
    num_fc_layers: 2
    fc_dropout: 0.5
    activation: 'relu'
    use_transformer_init: true 