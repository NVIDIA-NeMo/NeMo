# This config contains the default values for training EnCodec
# If you want to train model on other dataset, you can change config values according to your dataset.
# Most dataset-specific arguments are in the head of the config file, see below.

name: EnCodec

defaults:
  - sample: ???
  - bottleneck: ???

max_epochs: ???
batch_size: 16
weighted_sampling_steps_per_epoch: null

train_ds_meta: ???
val_ds_meta: ???
log_ds_meta: ???

log_dir: ???

model:

  max_epochs: ${max_epochs}
  steps_per_epoch: ${weighted_sampling_steps_per_epoch}

  sample_rate: ${sample.sample_rate}
  disc_update_steps: 2
  encoder_noise_stdev: ${bottleneck.encoder_noise_stdev}

  mel_loss_resolutions: [
    [32, 8, 32], [64, 16, 64], [128, 32, 128], [256, 64, 256], [512, 128, 512], [1024, 256, 1024], [2048, 512, 2048]
  ]

  train_ds:
    dataset:
      _target_: nemo.collections.tts.data.vocoder_dataset.VocoderDataset
      weighted_sampling_steps_per_epoch: ${weighted_sampling_steps_per_epoch}
      sample_rate: ${sample.sample_rate}
      n_samples: ${sample.train_n_samples}
      min_duration: 1.01
      max_duration: null
      dataset_meta: ${train_ds_meta}

    dataloader_params:
      batch_size: ${batch_size}
      drop_last: true
      num_workers: 4

  validation_ds:
    dataset:
      _target_: nemo.collections.tts.data.vocoder_dataset.VocoderDataset
      sample_rate: ${sample.sample_rate}
      n_samples: null
      min_duration: null
      max_duration: null
      trunc_duration: 10.0
      dataset_meta: ${val_ds_meta}

    dataloader_params:
      batch_size: 8
      num_workers: 2

  log_config:
    log_dir: ${log_dir}
    log_epochs: [10, 50]
    epoch_frequency: 100
    log_tensorboard: false
    log_wandb: false

    generators:
      - _target_: nemo.collections.tts.parts.utils.callbacks.EnCodecArtifactGenerator
        log_audio: true
        log_encoding: true

    dataset:
      _target_: nemo.collections.tts.data.vocoder_dataset.VocoderDataset
      sample_rate: ${sample.sample_rate}
      n_samples: null
      min_duration: null
      max_duration: null
      trunc_duration: 15.0
      dataset_meta: ${log_ds_meta}

    dataloader_params:
      batch_size: 4
      num_workers: 2

  audio_encoder:
    _target_: nemo.collections.tts.modules.encodec_modules.SEANetEncoder
    down_sample_rates: ${sample.down_sample_rates}

  generator:
    _target_: nemo.collections.tts.modules.encodec_modules.SEANetDecoder
    up_sample_rates: ${sample.up_sample_rates}

  vector_quantizer: ${bottleneck.vector_quantizer}

  discriminator:
    _target_: nemo.collections.tts.modules.encodec_modules.MultiResolutionDiscriminatorSTFT
    resolutions: [[128, 32, 128], [256, 64, 256], [512, 128, 512], [1024, 256, 1024], [2048, 512, 2048]]

  optim:
    _target_: torch.optim.Adam
    lr: 3e-4
    betas: [0.5, 0.9]

    sched:
      name: ExponentialLR
      gamma: 0.999

trainer:
  num_nodes: 1
  devices: 1
  accelerator: gpu
  strategy: ddp
  precision: 32
  max_epochs: ${max_epochs}
  accumulate_grad_batches: 1
  enable_checkpointing: False  # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 100
  check_val_every_n_epoch: 5
  benchmark: false

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  create_wandb_logger: false
  checkpoint_callback_params:
    monitor: val_loss
  resume_if_exists: false
  resume_ignore_no_checkpoint: false
