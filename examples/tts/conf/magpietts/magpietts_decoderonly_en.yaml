name: Magpie-TTS-DecoderOnly-EN

max_epochs: ???
# Adjust batch size based on GPU memory
batch_size: 2
# When doing weighted sampling with multiple manifests, this defines how many training steps are in an epoch.
# If null, then weighted sampling is disabled.
weighted_sampling_steps_per_epoch: null

# Dataset metadata for each manifest
# https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/data/vocoder_dataset.py#L39-L41
train_ds_meta: ???
val_ds_meta: ???

model:
  transformer_hf_backend: "Qwen/Qwen2.5-1.5B"
  use_text_conditioning_encoder: true # If true, distilbert will be used to encode context_text if provided.
  context_duration_min: 3.0
  context_duration_max: 5.0
  load_cached_codes_if_available: true
  
  embedding_dim: 1536
  hidden_dim: 1536
  codecmodel_path: ???
  max_epochs: ${max_epochs}
  steps_per_epoch: ${weighted_sampling_steps_per_epoch}
  
  # Local transformer parameters for autoregressive codebook prediction within a frame
  local_transformer_type: "none" # "none", "autoregressive", "maskgit"
  # Below args are only relevant if use_local_transformer is autoregressive, maskgit
  local_transformer_loss_scale: 1.0
  local_transformer_n_layers: 3
  local_transformer_n_heads: 1
  local_transformer_hidden_dim: 256

  cfg_unconditional_prob: 0.1
  cfg_unk_token: "<|fim_prefix|>" # Any special token from the model's tokenizer, to be used as UNK token for unconditional CFG.
  # To get special_tokens of the tokenzer, you can do:
  # model.tokenizer.first_tokenizer.additional_special_tokens
  
  text_tokenizers: # Add more languages for multi-lingual TTS
    english_phoneme:
      _target_: nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer
      punct: true
      apostrophe: true
      pad_with_space: false
      g2p:
        _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
        phoneme_dict: "scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt"
        heteronyms: "scripts/tts_dataset_files/heteronyms-052722"
        phoneme_probability: 0.8
        ignore_ambiguous_words: false
        use_chars: true
        use_stresses: true

  train_ds:
    dataset:
      _target_: nemo.collections.tts.data.text_to_speech_dataset.MagpieTTSDataset
      dataset_meta: ${train_ds_meta}
      weighted_sampling_steps_per_epoch: ${weighted_sampling_steps_per_epoch}
      min_duration: 0.2
      max_duration: 20.0

    dataloader_params:
      batch_size: ${batch_size}
      num_workers: 4
      drop_last: true
      pin_memory: true

  validation_ds:
    dataset:
      _target_: nemo.collections.tts.data.text_to_speech_dataset.MagpieTTSDataset
      dataset_meta: ${val_ds_meta}
      min_duration: 0.2
      max_duration: 20.0

    dataloader_params:
      batch_size: ${batch_size}
      num_workers: 4
      pin_memory: true

  optim:
    _target_: torch.optim.AdamW
    lr: 1e-4

    sched:
      name: ExponentialLR
      gamma: 0.998

trainer:
  num_nodes: 1
  devices: -1
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  precision: bf16-mixed
  max_epochs: ${max_epochs}
  accumulate_grad_batches: 1
  enable_checkpointing: False # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 100
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  benchmark: false
  gradient_clip_val: 2.5

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: null
    project: null
    resume: true
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: val_loss
    mode: min
    save_top_k: 5
    save_best_model: true
    always_save_nemo: true
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
