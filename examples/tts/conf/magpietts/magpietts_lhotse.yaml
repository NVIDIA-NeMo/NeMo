name: T5TTS

max_steps: ???
limit_val_batches: ???
# Adjust batch size based on GPU memory
batch_size: 16
micro_batch_size: 16
batch_duration: ???
eval_batch_size: ???

# Dataset metadata for each manifest
# https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/data/vocoder_dataset.py#L39-L41
train_ds_meta: ???
val_ds_meta: ???

# Modify these values based on your sample rate
sample_rate: 22050

phoneme_dict_path: "scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt"
heteronyms_path: "scripts/tts_dataset_files/heteronyms-052722"
model:
  use_lhotse: true
  model_type: "multi_encoder_context_tts" # single_encoder_sv_tts, multi_encoder_context_tts, decoder_context_tts or decoder_pretrain_synthesizer
  use_text_conditioning_encoder: false # If true, distilbert will be used to encode context_text if provided.
  transcript_decoder_layers: [3,4,5,6,7] # ONLY used in multi_encoder_context_tts, Transcript goes to these layer ids, context goes to the rest. In single_encoder_sv_tts, all layers are used for transcript.
  context_decoder_layers: [8,9] # ONLY used in multi_encoder_context_tts
  context_duration_min: 3.0
  context_duration_max: 8.0
  speaker_emb_dim: 192 # Only used for single_encoder_sv_tts, ignored otherwise
  num_audio_codebooks: 8
  num_audio_tokens_per_codebook: 2048 # Keep atleast 2 extra for eos/bos ids
  codec_model_downsample_factor: 1024
  load_cached_codes_if_available: true
  prior_scaling_factor: 0.5
  prior_end_step: 12000
  prior_scaledown_start_step: 8000
  indefinite_prior_prob: 0.0 # If > 0, then prior will be applied after prior_end_step with this probability.
  alignment_loss_scale: 0.0
  embedding_dim: 768
  codecmodel_path: ???
  max_steps: ${max_steps}

  sample_rate: ${sample_rate}

  # Alignment encoder parameters, to binarize the prior
  # This is used for attention-constrained training and inference
  use_alignment_encoder: false
  # Below args are only relevant if use_alignment_encoder is true
  use_prior_for_aligner: true # Whether to use the beta-binomial prior to train the alignment encoder
  alignment_encoder_loss_scale: 1.0
  binarize_prior_after_step: 10000 # Switch from beta-binomial prior to binarized prior after this step.
  binarize_attn_method: "nemo_binarize" # nemo_binarize or argmax.
  prior_future_context: 2 # Future window of the binarized prior. 
  prior_past_context: 2 # Past window of the binarized prior.
  prior_future_decay: 0.8 # Decay factor for future context
  prior_past_decay: 0.5 # Decay factor for past context
  binarize_repeat_audio_factor: 2 # Temporally increase audio timesteps, for nemo_binarize to work better. Increase this for low frame rate codecs
  binarized_prior_epsilon: 0.0
  aligner_encoder_train_steps: 50000

  # Local transformer parameters for autoregressive codebook prediction within a frame
  use_local_transformer: false
  # Below args are only relevant if use_local_transformer is true
  local_transformer_loss_scale: 1.0
  local_transformer_n_layers: 1
  local_transformer_n_heads: 1
  local_transformer_hidden_dim: 256

  text_tokenizers:
    english_phoneme:
      _target_: nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer
      punct: true
      apostrophe: true
      pad_with_space: false
      g2p:
        _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
        phoneme_dict: ${phoneme_dict_path}
        heteronyms: ${heteronyms_path}
        phoneme_probability: 0.8
        ignore_ambiguous_words: false
        use_chars: true
        use_stresses: true

  train_ds:
    use_lhotse: ${model.use_lhotse}
    dataset:
      input_cfg:
      - type: lhotse_shar
        shar_path: /cluster_data/TTS/tts_lhotse_datasets/hifitts_v0/
        weight: 1.0
        tags:
          lang: en
          s2s: True
          tokenizer_names: ["english_phoneme"]

      - type: lhotse_shar
        shar_path: /cluster_data/TTS/tts_lhotse_datasets/libri100/
        weight: 1.0
        tags:
          lang: en
          s2s: True
          tokenizer_names: ["english_phoneme"]

      - type: lhotse_shar
        shar_path: /cluster_data/TTS/tts_lhotse_datasets/rivaLindyRodney/
        weight: 1.0
        tags:
          lang: en
          s2s: True
          tokenizer_names: ["english_phoneme"]

      - type: lhotse_shar
        shar_path: /cluster_data/TTS/tts_lhotse_datasets/libri360/
        weight: 1.0
        tags:
          lang: en
          s2s: True
          tokenizer_names: ["english_phoneme"]

      global_batch_size: ${batch_size}
      micro_batch_size: ${micro_batch_size}
      batch_size: null
      shuffle: True
      num_workers: 0
      pin_memory: True
      max_seq_length: 512
      min_seq_length: 1
      drop_last: True
      # Notably, the data weights are controlled by either bucketing_weights
      # or concat_sampling_probabilities depending on the dataset type (tar and
      # non-tar).
      # See audio_text_qa_dataset.py for details.
      concat_sampling_probabilities: null # When providing a list of datasets, this arg defines the sampling probabilities from each dataset when strategy='random'
      # ASR configs
      sample_rate: ${model.sample_rate}
      max_duration: 24 # it is set for LibriSpeech, you may need to update it for your dataset
      min_duration: 0.1
      # tarred datasets
      is_tarred: false
      # tarred_audio_filepaths: null
      shuffle_n: 2048
      # bucketing params
      bucketing_strategy: "fully_randomized"
      bucketing_batch_size: null
      use_bucketing: true
      use_lhotse: ${model.use_lhotse}
      text_field : "text"
      seed: 'trng'
      batch_duration : ${batch_duration}  # 0
      quadratic_duration : 20
      num_buckets : 31
      buffer_size : 10000
      shuffle_buffer_size : 10000
      num_cuts_for_bins_estimate: 10000
      duration_bins: [3.155,3.76,4.27,4.74,5.1935,5.64,6.096,6.588,7.14,7.81,8.28,8.664,9.072,9.57,10.14,10.7335,11.3735,12.09,12.78,13.41,14.01,14.62,15.253375,15.96875,16.71,17.45,18.1335,18.7735,19.4,20.0]
      # bucket_duration_bins: [3.155,3.76,4.27,4.74,5.1935,5.64,6.096,6.588,7.14,7.81,8.28,8.664,9.072,9.57,10.14,10.7335,11.3735,12.09,12.78,13.41,14.01,14.62,15.253375,15.96875,16.71,17.45,18.1335,18.7735,19.4,20.0]

  validation_ds:
    use_lhotse: ${model.use_lhotse}
    dataset:
      input_cfg:
      - type: lhotse_shar
        shar_path: /cluster_data/TTS/tts_lhotse_datasets/LibriTTS_dev_clean/
        weight: 1.0
        tags:
          lang: en
          s2s: True
          tokenizer_names: ["english_phoneme"]

      global_batch_size: ${batch_size}
      micro_batch_size: ${micro_batch_size}
      shuffle: False
      num_workers: 0
      pin_memory: True
      drop_last: False
      use_bucketing: false
      is_tarred: false
      batch_size: ${eval_batch_size}
  
  t5_encoder:
    n_layers: 6
    d_model: 768
    d_ffn: 3072
    sa_n_heads: 12
    kernel_size: 3
    p_dropout: 0.1
    p_dropout_out: 0.0
    has_xattn: false
    is_causal: False
    apply_norm_out: true
    max_length_causal_mask: 2048
    use_learnable_pos_emb: true
  
  context_encoder: # Only used for multi_encoder_context_tts, ignored otherwise
    n_layers: 3
    d_model: 768
    d_ffn: 3072
    sa_n_heads: 12
    kernel_size: 3
    p_dropout: 0.1
    p_dropout_out: 0.0
    has_xattn: false
    is_causal: false
    apply_norm_out: true
    max_length_causal_mask: 2048
    use_learnable_pos_emb: true

  t5_decoder:
    n_layers: 12
    d_model: 768
    d_ffn: 3072
    sa_n_heads: 12
    kernel_size: 3
    p_dropout: 0.1
    p_dropout_out: 0.0
    has_xattn: true
    xa_d_memory: 768
    xa_n_heads: 12
    is_causal: true
    apply_norm_to_cond: true
    apply_norm_out: true
    max_length_causal_mask: 2048
    use_learnable_pos_emb: true
    prior_eps: 1e-8

  optim:
    _target_: torch.optim.Adam
    lr: 2e-4
    betas: [0.8, 0.99]

    sched:
      name: ExponentialLR
      gamma: 0.998

trainer:
  num_nodes: 1
  devices: -1
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  precision: 32
  max_epochs: -1
  accumulate_grad_batches: 1
  enable_checkpointing: False # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 10
  val_check_interval: 500
  limit_train_batches: ${trainer.val_check_interval}
  # check_val_every_n_epoch: 10
  benchmark: false
  max_steps: ${max_steps}
  limit_val_batches: ${limit_val_batches}
  use_distributed_sampler: false

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: null
    project: null
  create_checkpoint_callback: true 
  checkpoint_callback_params:
    monitor: val_loss
    mode: min
    save_top_k: 5
    save_best_model: true
    always_save_nemo: true
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
