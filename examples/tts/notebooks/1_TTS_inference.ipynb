{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS Inference\n",
    "\n",
    "This notebook can be used to generate audio samples using either NeMo's pretrained models or after training a NeMo TTS model. This script currently uses a two step inference procedure. First, a model is used to text into a mel spectrogram. Second, a model is turn mel spectrograms into audio.\n",
    "\n",
    "Currently supported models are:\n",
    "Mel Spectrogram Generators:\n",
    "- Tacotron 2\n",
    "\n",
    "Audio Generators\n",
    "- Grifflin-Lim\n",
    "- WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "!pip install wget\n",
    "!pip install nemo_toolkit[tts]\n",
    "\n",
    "!mkdir configs\n",
    "# !wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/candidate/examples/tts/conf/tacotron2.yaml\n",
    "# !wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/candidate/examples/tts/conf/waveglow.yaml\n",
    "CONFIG_PATH = \"conf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try: CONFIG_PATH\n",
    "except NameError: CONFIG_PATH = Path(\"..\") / \"conf\"\n",
    "    \n",
    "# supported_spec_gen = [\"tacotron2\"]\n",
    "# supported_audio_gen = [\"grifflin-lim\", \"waveglow\"]\n",
    "# supported_audio_gen_req_checkpoint = [\"waveglow\"]\n",
    "\n",
    "# print(\"Choose one of the following spectrogram generators:\")\n",
    "# print([model for model in supported_spec_gen])\n",
    "# spectrogram_generator = input()\n",
    "# print(\"Choose one of the following audio generators:\")\n",
    "# print([model for model in supported_audio_gen])\n",
    "# audio_generator = input()\n",
    "\n",
    "# assert spectrogram_generator in supported_spec_gen\n",
    "# assert audio_generator in supported_audio_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pre-trained checkpoints\n",
    "\n",
    "TODO: Enable downloading pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram_generator_checkpoint_path = input(f\"Input the path to the {spectrogram_generator} checkpoint: \")\n",
    "# # if audio_generator in supported_audio_gen_req_checkpoint:\n",
    "# audio_generator_checkpoint_path = input(f\"Input the path to the {audio_generator} checkpoint: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def _check_model_old_version(model):\n",
    "    if \"waveglow.WN.0.res_layers\" in model or \"waveglow.WN.0.cond_layers\" in model:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _update_model_res_skip(old_model, new_model):\n",
    "    for idx in range(0, len(new_model.WN)):\n",
    "        wavenet = new_model.WN[idx]\n",
    "        n_channels = wavenet.n_channels\n",
    "        n_layers = wavenet.n_layers\n",
    "        wavenet.res_skip_layers = torch.nn.ModuleList()\n",
    "        for i in range(0, n_layers):\n",
    "            if i < n_layers - 1:\n",
    "                res_skip_channels = 2*n_channels\n",
    "            else:\n",
    "                res_skip_channels = n_channels\n",
    "            res_skip_layer = torch.nn.Conv1d(n_channels, res_skip_channels, 1)\n",
    "            skip_layer = torch.nn.utils.remove_weight_norm(wavenet.skip_layers[i])\n",
    "            if i < n_layers - 1:\n",
    "                res_layer = torch.nn.utils.remove_weight_norm(wavenet.res_layers[i])\n",
    "                res_skip_layer.weight = torch.nn.Parameter(torch.cat([res_layer.weight, skip_layer.weight]))\n",
    "                res_skip_layer.bias = torch.nn.Parameter(torch.cat([res_layer.bias, skip_layer.bias]))\n",
    "            else:\n",
    "                res_skip_layer.weight = torch.nn.Parameter(skip_layer.weight)\n",
    "                res_skip_layer.bias = torch.nn.Parameter(skip_layer.bias)\n",
    "            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "            wavenet.res_skip_layers.append(res_skip_layer)\n",
    "        del wavenet.res_layers\n",
    "        del wavenet.skip_layers\n",
    "\n",
    "def _update_model_cond(old_model, new_model):\n",
    "    for idx in range(0, len(new_model.WN)):\n",
    "        wavenet = new_model.WN[idx]\n",
    "        n_channels = wavenet.n_channels\n",
    "        n_layers = wavenet.n_layers\n",
    "        n_mel_channels = wavenet.cond_layers[0].weight.shape[1]\n",
    "        cond_layer = torch.nn.Conv1d(n_mel_channels, 2*n_channels*n_layers, 1)\n",
    "        cond_layer_weight = []\n",
    "        cond_layer_bias = []\n",
    "        for i in range(0, n_layers):\n",
    "            _cond_layer = torch.nn.utils.remove_weight_norm(wavenet.cond_layers[i])\n",
    "            cond_layer_weight.append(_cond_layer.weight)\n",
    "            cond_layer_bias.append(_cond_layer.bias)\n",
    "        cond_layer.weight = torch.nn.Parameter(torch.cat(cond_layer_weight))\n",
    "        cond_layer.bias = torch.nn.Parameter(torch.cat(cond_layer_bias))\n",
    "        cond_layer = torch.nn.utils.weight_norm(cond_layer, name='weight')\n",
    "        wavenet.cond_layer = cond_layer\n",
    "        del wavenet.cond_layers\n",
    "\n",
    "def update_model(old_model):\n",
    "#     if not _check_model_old_version(old_model):\n",
    "#         return old_model\n",
    "    new_model = copy.deepcopy(old_model)\n",
    "    if \"WN.0.res_layers\" in old_model:\n",
    "        print(\"update\")\n",
    "        _update_model_res_skip(old_model, new_model)\n",
    "    if \"WN.0.cond_layers\" in old_model:\n",
    "        print(\"update 2\")\n",
    "        _update_model_cond(old_model, new_model)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import Tacotron2Model, WaveGlowModel\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "def load_tacotron2_model(cfg):\n",
    "    with open(cfg) as file:\n",
    "        tacotron2_config = OmegaConf.load(file)\n",
    "    del tacotron2_config.model[\"train_ds\"]\n",
    "    del tacotron2_config.model[\"validation_ds\"]\n",
    "    del tacotron2_config.model[\"optim\"]\n",
    "    return Tacotron2Model(cfg=tacotron2_config.model), tacotron2_config.labels\n",
    "\n",
    "def load_waveglow_model(cfg):\n",
    "    with open(cfg) as file:\n",
    "        wave_config = OmegaConf.load(file)\n",
    "    del wave_config.model[\"train_ds\"]\n",
    "    del wave_config.model[\"validation_ds\"]\n",
    "    del wave_config.model[\"optim\"]\n",
    "    return WaveGlowModel(cfg=wave_config[\"model\"])\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "# tacotron2, labels = load_tacotron2_model(\"/home/jasoli/nemo/NeMo/examples/tts/conf/tacotron2.yaml\")\n",
    "waveglow = load_waveglow_model(\"/home/jasoli/nemo/NeMo/examples/tts/conf/waveglow_16khz.yaml\")\n",
    "\n",
    "# tacotron2.load_state_dict(torch.load(\"/home/jasoli/nemo/NeMo/examples/tts/experiments/1325283-Tacotron_O0_LJS_V1b/Tacotron 2/2020-07-24_21-39-14/checkpoints/Tacotron 2--last.ckpt\")[\"state_dict\"])\n",
    "waveglow_checkpoint = torch.load(\"/home/jasoli/nemo/NeMo/WaveGlow16--val_loss=-7.38-epoch=124.ckpt\")[\"state_dict\"]\n",
    "if \"waveglow.WN.0.in_layers.0.bias\" in waveglow_checkpoint:\n",
    "    keys_to_del = []\n",
    "    new_dict = {}\n",
    "    for key, val in waveglow_checkpoint.items():\n",
    "        if \"WN\" in key:\n",
    "            new_dict[key.replace(\".WN.\", \".wavenet.\")] = val\n",
    "    for key in keys_to_del:\n",
    "        del waveglow_checkpoint[key]\n",
    "    for key, val in new_dict.items():\n",
    "        waveglow_checkpoint[key] = new_dict[key]\n",
    "# waveglow_checkpoint = update_model(waveglow_checkpoint)\n",
    "waveglow.load_state_dict(waveglow_checkpoint, strict=False)\n",
    "\n",
    "# tacotron2 = tacotron2.cuda()\n",
    "waveglow = waveglow.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2_infer(model, input_string, labels):\n",
    "    model.eval()\n",
    "    labels_dict = { char:i for i, char in enumerate(labels)}\n",
    "    tokens = [len(labels)]\n",
    "    for char in input_string:\n",
    "        tokens.append(labels_dict[char])\n",
    "    tokens.append(len(labels)+1)\n",
    "    token_len = torch.tensor(len(tokens)).unsqueeze(0).cuda()\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0).cuda()\n",
    "    print(tokens.shape)\n",
    "    token_embedding = model.text_embedding(tokens).transpose(1, 2)\n",
    "    encoder_embedding = model.encoder(token_embedding=token_embedding, token_len=token_len)\n",
    "    spec_dec, gate, alignments, _ = model.decoder(memory=encoder_embedding, memory_lengths=token_len)\n",
    "    spec_postnet = model.postnet(mel_spec=spec_dec)\n",
    "    \n",
    "    return spec_postnet\n",
    "\n",
    "def wg_infer(model, spec):\n",
    "    model.eval()\n",
    "    model.mode = 2\n",
    "    model.waveglow.mode = 2\n",
    "    return model.waveglow(spect=spec, run_inverse=True, audio=None, sigma=1.0)\n",
    "\n",
    "# spec = t2_infer(tacotron2, \"this is a test of tacotron two and waveglow\", labels)\n",
    "import numpy as np\n",
    "import torch\n",
    "spec = np.load(\"results/spec_0.png.npy\")\n",
    "spec = torch.from_numpy(spec).cuda()\n",
    "audio = wg_infer(waveglow, spec)\n",
    "\n",
    "print(spec)\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Tacotron 2 + WaveGlow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveglow_checkpoint = torch.load(\"/home/jasoli/nemo/NeMo/examples/tts/experiments/1318094-Waveglow_O1_LJS_V1b/WaveGlow/2020-07-21_15-17-19/checkpoints/WaveGlow--last.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets hear the generated audio !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "ipd.Audio(audio.detach().cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "imshow(spec.detach().cpu().numpy()[0])\n",
    "# img = Image.fromarray(spec.detach().cpu().numpy()[0])\n",
    "# ipd.Image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
