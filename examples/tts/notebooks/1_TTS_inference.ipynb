{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS Inference\n",
    "\n",
    "This notebook can be used to generate audio samples using either NeMo's pretrained models or after training NeMo TTS models. This script currently uses a two step inference procedure. First, a model is used to generate a mel spectrogram from text. Second, a model is used to generate audio from a mel spectrogram.\n",
    "\n",
    "Currently supported models are:\n",
    "Mel Spectrogram Generators:\n",
    "- Tacotron 2\n",
    "- Glow-TTS\n",
    "\n",
    "Audio Generators\n",
    "- Grifflin-Lim\n",
    "- WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install unidecode\n",
    "!pip install nemo_toolkit[tts]\n",
    "\n",
    "!mkdir configs\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/candidate/examples/tts/conf/tacotron2.yaml\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/candidate/examples/tts/conf/waveglow.yaml\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/candidate/examples/tts/conf/glow_tts.yaml\n",
    "CONFIG_PATH = \"conf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try: CONFIG_PATH\n",
    "except NameError: CONFIG_PATH = Path(\"..\") / \"conf\"\n",
    "    \n",
    "# supported_spec_gen = [\"tacotron2\", \"glow-tts\"]\n",
    "# supported_audio_gen = [\"grifflin-lim\", \"waveglow\"]\n",
    "# supported_audio_gen_req_checkpoint = [\"waveglow\"]\n",
    "\n",
    "supported_spec_gen = [\"tacotron2\"]\n",
    "supported_audio_gen = [\"waveglow\"]\n",
    "supported_audio_gen_req_checkpoint = [\"waveglow\"]\n",
    "\n",
    "print(\"Choose one of the following spectrogram generators:\")\n",
    "print([model for model in supported_spec_gen])\n",
    "spectrogram_generator = input()\n",
    "print(\"Choose one of the following audio generators:\")\n",
    "print([model for model in supported_audio_gen])\n",
    "audio_generator = input()\n",
    "\n",
    "# # TODO\n",
    "# spectrogram_generator = \"tacotron2\"\n",
    "# audio_generator = \"waveglow\"\n",
    "\n",
    "assert spectrogram_generator in supported_spec_gen\n",
    "assert audio_generator in supported_audio_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pre-trained checkpoints\n",
    "\n",
    "TODO: Enable downloading pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram_generator_checkpoint_path = input(f\"Input the path to the {spectrogram_generator} checkpoint: \")\n",
    "# if audio_generator in supported_audio_gen_req_checkpoint:\n",
    "#     audio_generator_checkpoint_path = input(f\"Input the path to the {audio_generator} checkpoint: \")\n",
    "\n",
    "# TODO\n",
    "audio_generator_checkpoint_path = Path.home()/\"nemo/NeMo/examples/tts/experiments/1374354-Waveglow_O2_LJS_V1b/WaveGlow/2020-07-27_18-54-10/checkpoints/WaveGlow--last.ckpt\"\n",
    "spectrogram_generator_checkpoint_path = Path.home()/\"nemo/NeMo/examples/tts/experiments/1325283-Tacotron_O0_LJS_V1b/Tacotron 2/2020-07-24_21-39-14/checkpoints/Tacotron 2--last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from ruamel.yaml import YAML\n",
    "from nemo.collections.asr.parts import parsers\n",
    "\n",
    "def load_spectrogram_model():\n",
    "    if spectrogram_generator == \"tacotron2\":\n",
    "        from nemo.collections.tts.models import Tacotron2Model as SpecModel\n",
    "        cfg_file = Path(CONFIG_PATH) / \"tacotron2.yaml\"\n",
    "    elif spectrogram_generator == \"glow-tts\":\n",
    "        raise NotImplementedError\n",
    "        from nemo.collections.tts.models import GlowTTSModel as SpecModel\n",
    "        cfg_file = Path(CONFIG_PATH) / \"glow_tts.yaml\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    with open(cfg_file) as file:\n",
    "        cfg = OmegaConf.load(file)\n",
    "    del cfg.model[\"train_ds\"]\n",
    "    del cfg.model[\"validation_ds\"]\n",
    "    del cfg.model[\"optim\"]\n",
    "    return SpecModel(cfg=cfg.model)\n",
    "\n",
    "def load_vocoder_model():\n",
    "    if audio_generator == \"waveglow\":\n",
    "        from nemo.collections.tts.models import WaveGlowModel as VocoderModel\n",
    "        cfg_file = Path(CONFIG_PATH) / \"waveglow.yaml\"\n",
    "    elif audio_generator == \"grifflin-lim\":\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    with open(cfg_file) as file:\n",
    "        cfg = OmegaConf.load(file)\n",
    "    del cfg.model[\"train_ds\"]\n",
    "    del cfg.model[\"validation_ds\"]\n",
    "    del cfg.model[\"optim\"]\n",
    "    return VocoderModel(cfg=cfg.model)\n",
    "\n",
    "spec_gen = load_spectrogram_model()\n",
    "vocoder = load_vocoder_model()\n",
    "\n",
    "spec_gen.load_state_dict(torch.load(spectrogram_generator_checkpoint_path)[\"state_dict\"])\n",
    "vocoder.load_state_dict(torch.load(audio_generator_checkpoint_path)[\"state_dict\"])\n",
    "\n",
    "spec_gen = spec_gen.cuda()\n",
    "vocoder = vocoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocder_model, str_input):\n",
    "    parsed = spec_gen.parse(text_to_generate)\n",
    "    spectrogram = spec_gen.generate_spectrogram(tokens=parsed)\n",
    "    audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_generate = input(\"Input what you want the model to say: \")\n",
    "spec, audio = infer(spec_gen, vocoder, text_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "ipd.Audio(audio.detach().cpu().numpy(), rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "imshow(spec.detach().cpu().numpy()[0], origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
