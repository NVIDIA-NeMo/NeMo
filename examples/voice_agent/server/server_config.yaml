
# bot_prompt: /path/to/prompt.txt  # or use path to a txt file that contains a long prompt, for example in `../example_prompts/fast_bite.txt`
bot_prompt: "You are a helpful AI agent named Lisa. Start by greeting the user warmly and introducing yourself within one sentence. Your answer should be concise and to the point."


transport:
  audio_out_10ms_chunks: 8  # use 4 as websocket default, but increasing to a larger number might have less glitches in TTS output

vad:
  type: silero
  confidence: 0.6  # VAD threshold for detecting speech versus non-speech
  start_secs: 0.1  # min amout of speech to trigger UserStartSpeaking
  stop_secs: 0.8  # min about of silence to trigger UserStopSpeaking
  min_volume: 0.4  # Microphone volumn threshold for VAD

stt:
  type: nemo
  model: "stt_en_fastconformer_hybrid_large_streaming_80ms"
  device: "cuda"
  att_context_size: [70, 1]
  frame_len_in_secs: 0.08  # default for FastConformer, do not change

diar:
  type: nemo
  enabled: false # the checkpoint is under release process
  model: null  # the checkpoint is under release process
  device: "cuda"
  threshold: 0.4
  frame_len_in_secs: 0.08  # default for FastConformer, do not change

turn_taking:
  max_buffer_size: 2  # num of words more than this amount will interrupt the LLM immediately
  bot_stop_delay: 0.5  # in seconds, a delay between server and client audio output

llm:
  type: hf
  dtype: bfloat16  # torch.dtype for LLM
  model: "nvidia/Llama-3.1-Nemotron-Nano-8B-v1"  # model name for HF models, will be used via `AutoModelForCausalLM.from_pretrained()`
  device: "cuda"
  system_role: "system"  # role for system prompt, set it to `user` for models that do not support system prompt
  apply_chat_template_kwargs: null  # please refer to the model page of each HF LLM model to set them correctly, by default `tokenize=False` and `add_generation_prompt=True` are applied
  generation_kwargs:  # kwargs that will be passed into model.generate() function of HF models
    temperature: 0.7  # LLM sampling params
    top_p: 0.9  # LLM sampling params
    max_new_tokens: 128  # max num of output tokens from LLM
    do_sample: true # enable sampling

tts:
  type: nemo
  model: fastpitch-hifigan
  fastpitch_model: "nvidia/tts_en_fastpitch"
  hifigan_model: "nvidia/tts_hifigan"
  device: "cuda"
  extra_separator: null  # additional punctuations to chunk LLM response into segments for faster TTS output, e.g., ","
  think_tokens: ["<think>", "</think>"]  # specify them to avoid TTS for thinking process, set to `null` to allow thinking out loud
