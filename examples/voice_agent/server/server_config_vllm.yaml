# This is an example config for setting up a NeMo Voice Agent server.
# Please refer to https://github.com/NVIDIA-NeMo/NeMo/tree/main/examples/voice_agent/README.md for more details

transport:
  audio_out_10ms_chunks: 10  # use 4 as websocket default, but increasing to a larger number might have less glitches in TTS output

vad:
  type: silero
  confidence: 0.6  # VAD threshold for detecting speech versus non-speech
  start_secs: 0.1  # min amount of speech to trigger UserStartSpeaking
  stop_secs: 0.8  # min amount of silence to trigger UserStopSpeaking
  min_volume: 0.4  # Microphone volumn threshold for VAD

stt:
  type: nemo
  model: "stt_en_fastconformer_hybrid_large_streaming_80ms"
  device: "cuda"
  att_context_size: [70, 1] # left and right attention context sizes for streaming ASR
  frame_len_in_secs: 0.08  # default for FastConformer, do not change unless using other architechtures

diar:
  type: nemo
  enabled: true # set to false to disable
  model: "nvidia/diar_streaming_sortformer_4spk-v2"
  device: "cuda"
  threshold: 0.4  # threshold value used to determine if a speaker exists or not, setting it to a lower value will increaset the sensitivity of the model
  frame_len_in_secs: 0.08  # default for Sortformer, do not change unless using other architechtures

turn_taking:
  backchannel_phrases: "./server/backchannel_phrases.yaml"  # set it to the actual path of the file, or specify a list of backchannel phrases here
  max_buffer_size: 2  # num of words more than this amount will interrupt the LLM immediately if not backchannel phrases
  bot_stop_delay: 0.5  # a delay in seconds allowed between server and client audio output, so that the BotStopSpeaking signal is handled not too far away from the actual time that the user hears all audio output

llm:
  type: vllm
  model: "nvidia/NVIDIA-Nemotron-Nano-9B-v2"  # this is just for logging, please setup the vllm server manually before starting the voice agent server
  api_key: "EMPTY"
  base_url: "http://localhost:8000/v1"  # get this link from your vllm server first
  params:
    seed: 42
    temperature: 0.6
    top_k: null # currently ignored by openai client
    top_p: 0.9
    max_completion_tokens: 256  # max number of tokens to generate
    extra: null  # additional model specific params can be specified as dict format
  system_role: "system"  # role for system prompt, set it to `user` for models that do not support system prompt
  # `system_prompt` is used as the sytem prompt to the LLM, please refer to differnt LLM webpage for spcial functions like enabling/disabling thinking
  # system_prompt: /path/to/prompt.txt  # or use path to a txt file that contains a long prompt, for example in `../example_prompts/fast_bite.txt`
  system_prompt: "You are a helpful AI agent named Lisa. Start by greeting the user warmly and introducing yourself within one sentence. Your answer should be concise and to the point. You might also see speaker tags (<speaker_0>, <speaker_1>, etc.) in the user context. You should respond to the user based on the speaker tag and the context of that speaker. Do not include the speaker tags in your response, use them only to identify the speaker. If a speaker provides their name, use their name when addressing their requests. /no_think"

tts:
  type: nemo
  model: fastpitch-hifigan
  fastpitch_model: "nvidia/tts_en_fastpitch"
  hifigan_model: "nvidia/tts_hifigan"
  device: "cuda"
  extra_separator:  # a list of additional punctuations to chunk LLM response into segments for faster TTS output, e.g., ",". Set to `null` to use default behavior 
    - ","
    - "?"
    - "!"
  think_tokens: ["<think>", "</think>"]  # specify them to avoid TTS for thinking process, set to `null` to allow thinking out loud
