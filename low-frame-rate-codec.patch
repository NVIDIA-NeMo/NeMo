diff --git a/nemo/collections/tts/models/audio_codec.py b/nemo/collections/tts/models/audio_codec.py
index 230a24e36c..c3dc1027f6 100644
--- a/nemo/collections/tts/models/audio_codec.py
+++ b/nemo/collections/tts/models/audio_codec.py
@@ -32,6 +32,7 @@ from nemo.collections.tts.losses.audio_codec_loss import (
     SISDRLoss,
     TimeDomainLoss,
 )
+from nemo.collections.tts.modules.audio_codec_modules import PhonemeASR, ResNetSpeakerEncoder
 from nemo.collections.tts.modules.common import GaussianDropout
 from nemo.collections.tts.parts.utils.callbacks import LoggingCallback
 from nemo.collections.tts.parts.utils.helpers import get_batch_size, get_num_workers
@@ -43,6 +44,13 @@ from nemo.core.optim.lr_scheduler import compute_max_steps, prepare_lr_scheduler
 from nemo.utils import logging, model_utils
 from nemo.utils.decorators import experimental
 
+try:
+    import torchaudio
+
+    HAVE_TORCHAUDIO = True
+except ModuleNotFoundError:
+    HAVE_TORCHAUDIO = False
+
 
 @experimental
 class AudioCodecModel(ModelPT):
@@ -152,6 +160,27 @@ class AudioCodecModel(ModelPT):
         if self.commit_loss_scale > 0 and not self.vector_quantizer_has_commit_loss:
             raise ValueError('Commit loss is enabled but the quantizer does not support it.')
 
+        self.use_scl_loss = cfg.get("use_scl_loss", False)
+        self.scl_loss_scale = cfg.get("scl_loss_scale", False)
+        if self.use_scl_loss:
+            self.speaker_encoder = ResNetSpeakerEncoder()
+            # load pretrained model
+            # self.speaker_encoder.load_checkpoint("https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar")
+            self.speaker_encoder.load_checkpoint(
+                "https://huggingface.co/Edresson/Speaker_Encoder_H_ASP/resolve/main/pytorch_model.bin", strict=False
+            )
+            # freeze the pretrained speaker encoder
+            self.speaker_encoder.freeze()
+            print("Speaker encoder loaded and frozen !!")
+
+        self.use_asr_consitency_loss = cfg.get("use_asr_consitency_loss", False)
+        self.acl_loss_scale = cfg.get("acl_loss_scale", False)
+        if self.use_asr_consitency_loss:
+            self.phoneme_asr_model = PhonemeASR(input_sr=self.sample_rate)
+            self.phoneme_asr_model.freeze()
+            # self.acl_loss = CrossEntropyLoss()
+            print("Phoneme ASR model loaded and frozen !!")
+
         # Log setup
         self.log_config = cfg.get("log_config", None)
 
@@ -159,6 +188,51 @@ class AudioCodecModel(ModelPT):
         self.lr_schedule_interval = None
         self.automatic_optimization = False
 
+    def state_dict(self, destination=None, prefix='', keep_vars=False):
+        if hasattr(self, '_no_state_dict') and self._no_state_dict:
+            return {}
+        # Don't save the speaker verification and codec model in the state dict
+        state_dict = super().state_dict(destination, prefix, keep_vars)
+        for key in list(state_dict.keys()):
+            if self.use_scl_loss and "speaker_encoder." in key:
+                del state_dict[key]
+            if "discriminator" in key and ".slm_model.ssl_model." in key:
+                del state_dict[key]
+        return state_dict
+
+    def load_state_dict(self, state_dict, strict=True):
+        # Override to load all the keys except .speaker_encoder. and WavLM model
+        for key in list(state_dict.keys()):
+            if self.use_scl_loss and "speaker_encoder." in key:
+                del state_dict[key]
+            if "discriminator" in key and ".slm_model.ssl_model." in key:
+                del state_dict[key]
+
+        super().load_state_dict(state_dict, strict=False)
+
+    def get_speaker_embedding(self, audio, requires_grad=False):
+        if not requires_grad:
+            with torch.no_grad():
+                if HAVE_TORCHAUDIO:
+                    audio_resampled = torchaudio.functional.resample(
+                        audio, self.sample_rate, self.speaker_encoder.audio_config["sample_rate"]
+                    )
+                else:
+                    logging.error('Could not import torchaudio!')
+                    raise ModuleNotFoundError(f"torchaudio is not installed but is necessary to audio resample !!")
+                g = self.speaker_encoder(audio_resampled, l2_norm=True).unsqueeze(-1)
+        else:
+            if HAVE_TORCHAUDIO:
+                audio_resampled = torchaudio.functional.resample(
+                    audio, self.sample_rate, self.speaker_encoder.audio_config["sample_rate"]
+                )
+            else:
+                logging.error('Could not import torchaudio!')
+                raise ModuleNotFoundError(f"torchaudio is not installed but is necessary to audio resample !!")
+            g = self.speaker_encoder(audio_resampled, l2_norm=True).unsqueeze(-1)
+
+        return g
+
     @typecheck(
         input_types={
             "audio": NeuralType(('B', 'T_audio'), AudioSignal()),
@@ -470,6 +544,36 @@ class AudioCodecModel(ModelPT):
             metrics["g_loss_commit"] = commit_loss
             generator_losses.append(self.commit_loss_scale * commit_loss)
 
+        # compute embeddings for speaker consistency loss
+        if self.use_scl_loss:
+            # concate generated and GT waveforms
+            audios_batch = torch.cat((audio.squeeze(1), audio_gen.squeeze(1)), dim=0)
+
+            # get speaker embeddings with grads
+            pred_embs = self.get_speaker_embedding(audios_batch, requires_grad=True)
+
+            # split generated and GT speaker embeddings
+            gt_spk_emb, syn_spk_emb = torch.chunk(pred_embs, 2, dim=0)
+
+            # speaker consistency loss like YourTTS paper
+            loss_scl = -1 * torch.nn.functional.cosine_similarity(gt_spk_emb, syn_spk_emb).mean() * self.scl_loss_scale
+
+            metrics["g_loss_scl"] = loss_scl
+            generator_losses.append(metrics["g_loss_scl"])
+
+        if self.use_asr_consitency_loss:
+            # concate generated and GT waveforms
+            audios_batch = torch.cat((audio.squeeze(1), audio_gen.squeeze(1)), dim=0)
+
+            logits, _ = self.phoneme_asr_model(audios_batch)
+
+            logits_gt, logits_pred = torch.chunk(logits, 2, dim=0)
+            # labels_gt, labels_pred = torch.chunk(labels, 2, dim=0)
+
+            loss_acl = torch.nn.functional.mse_loss(logits_pred, logits_gt) * self.acl_loss_scale
+            metrics["g_loss_acl"] = loss_acl
+            generator_losses.append(metrics["g_loss_acl"])
+
         loss_gen_all = sum(generator_losses)
 
         optim_gen.zero_grad()
@@ -503,6 +607,34 @@ class AudioCodecModel(ModelPT):
             "val_loss_time_domain": loss_time_domain,
             "val_loss_si_sdr": loss_si_sdr,
         }
+        # compute embeddings for speaker consistency loss
+        if self.use_scl_loss:
+            # concate generated and GT waveforms
+            audios_batch = torch.cat((audio.squeeze(1), audio_gen.squeeze(1)), dim=0)
+
+            # get speaker embeddings with grads
+            pred_embs = self.get_speaker_embedding(audios_batch, requires_grad=True)
+
+            # split generated and GT speaker embeddings
+            gt_spk_emb, syn_spk_emb = torch.chunk(pred_embs, 2, dim=0)
+
+            # speaker consistency loss like YourTTS paper
+            loss_scl = -1 * torch.nn.functional.cosine_similarity(gt_spk_emb, syn_spk_emb).mean() * self.scl_loss_scale
+
+            metrics["val_loss_scl"] = loss_scl
+            metrics["val_loss"] += metrics["val_loss_scl"]
+
+        if self.use_asr_consitency_loss:
+            # concate generated and GT waveforms
+            audios_batch = torch.cat((audio.squeeze(1), audio_gen.squeeze(1)), dim=0)
+
+            logits, _ = self.phoneme_asr_model(audios_batch)
+            logits_gt, logits_pred = torch.chunk(logits, 2, dim=0)
+
+            loss_acl = torch.nn.functional.mse_loss(logits_pred, logits_gt) * self.acl_loss_scale
+            metrics["val_loss_acl"] = loss_acl
+            metrics["val_loss"] += metrics["val_loss_acl"]
+
         self.log_dict(metrics, on_epoch=True, sync_dist=True)
 
     def get_dataset(self, cfg):
@@ -590,8 +722,12 @@ class AudioCodecModel(ModelPT):
         sched_config = optim_config.pop("sched", None)
         OmegaConf.set_struct(optim_config, True)
 
+        asr_ph_params = self.phoneme_asr_model.parameters() if self.use_asr_consitency_loss else []
+        se_params = self.speaker_encoder.parameters() if self.use_scl_loss else []
         vq_params = self.vector_quantizer.parameters() if self.vector_quantizer else []
-        gen_params = itertools.chain(self.audio_encoder.parameters(), self.audio_decoder.parameters(), vq_params)
+        gen_params = itertools.chain(
+            self.audio_encoder.parameters(), self.audio_decoder.parameters(), vq_params, asr_ph_params, se_params
+        )
         optim_g = instantiate(optim_config, params=gen_params)
 
         disc_params = self.discriminator.parameters()
diff --git a/nemo/collections/tts/modules/audio_codec_modules.py b/nemo/collections/tts/modules/audio_codec_modules.py
index 057d9f4954..15e264b491 100644
--- a/nemo/collections/tts/modules/audio_codec_modules.py
+++ b/nemo/collections/tts/modules/audio_codec_modules.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import math
+import os
 from abc import ABC, abstractmethod
 from typing import Iterable, List, Optional, Tuple
 
@@ -44,6 +46,13 @@ try:
 except ModuleNotFoundError:
     HAVE_TORCHAUDIO = False
 
+try:
+    import fsspec
+
+    HAVE_FSSPEC = True
+except ModuleNotFoundError:
+    HAVE_FSSPEC = False
+
 
 def get_padding(kernel_size: int, dilation: int = 1) -> int:
     return (kernel_size * dilation - dilation) // 2
@@ -99,21 +108,19 @@ class SLMDiscriminator(NeuralModule):
     ):
         super().__init__()
 
-        if not HAVE_TORCHAUDIO:
-            logging.error('Could not import torchaudio. SLMDiscriminator will not work.')
-
-            raise ModuleNotFoundError(
-                f"torchaudio is not installed but is necessary to instantiate a {self.__class__.__name__}"
-            )
+        if HAVE_TORCHAUDIO:
+            self.resample = torchaudio.transforms.Resample(input_sr, slm_sr)
+        else:
+            self.resample = None
 
         self.slm_model = SSLModel(slm_model_name)
 
         # Freeze slm model
         self.slm_model.freeze()
 
-        self.resample = torchaudio.transforms.Resample(input_sr, slm_sr)
-
-        norm_f = torch.nn.utils.weight_norm if use_spectral_norm == False else torch.nn.utils.spectral_norm
+        norm_f = (
+            torch.nn.utils.parametrizations.weight_norm if use_spectral_norm == False else torch.nn.utils.spectral_norm
+        )
         self.pre = norm_f(nn.Conv1d(slm_hidden * slm_layers, initial_channel, 1, 1, padding=0))
 
         self.convs = nn.ModuleList(
@@ -167,6 +174,318 @@ class SLMDiscriminator(NeuralModule):
         return [y_d_r.unsqueeze(1)], [y_d_g.unsqueeze(1)], [fmap_r], [fmap_g]
 
 
+# Torch version of transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm
+def zero_mean_unit_var_norm(input_values):
+    """
+    Normalized to have zero mean and unit variance
+    """
+    normed_input_values = (input_values - input_values.mean(dim=1).unsqueeze(-1)) / torch.sqrt(
+        input_values.var(dim=1).unsqueeze(-1) + 1e-7
+    )
+    return normed_input_values
+
+
+class PhonemeASR(NeuralModule):
+    def __init__(self, input_sr=22050, model_sr=16000):
+        super().__init__()
+        # self.processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-lv-60-espeak-cv-ft") # processor are not grad friendly
+        self.model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-lv-60-espeak-cv-ft")
+        if HAVE_TORCHAUDIO:
+            self.resample = torchaudio.transforms.Resample(input_sr, model_sr)
+        else:
+            self.resample = None
+
+    def forward(self, audio):
+        audio = self.resample(audio)
+        # input_values = self.processor(audio, return_tensors="pt").input_values
+        input_values = zero_mean_unit_var_norm(audio)
+        logits = self.model(input_values).logits
+        predicted_ids = torch.argmax(logits, dim=-1)
+
+        # transcription = self.processor.batch_decode(predicted_ids)
+        # print("GT Phonemes:", transcription[0])
+        # print("GEN Phonemes:", transcription[len(transcription)//2])
+        return logits, predicted_ids
+
+
+##############
+# Speaker encoder #
+##############
+def load_fsspec(path: str, map_location: str = None, **kwargs):
+    """Like torch.load but can load from other locations (e.g. s3:// , gs://).
+
+    Args:
+        path: Any path or url supported by fsspec.
+        map_location: torch.device or str.
+        cache: If True, cache a remote file locally for subsequent calls. It is cached under `get_user_data_dir()/tts_cache`. Defaults to True.
+        **kwargs: Keyword arguments forwarded to torch.load.
+
+    Returns:
+        Object stored in path.
+    """
+    is_local = os.path.isdir(path) or os.path.isfile(path)
+    if is_local:
+        return torch.load(path, map_location=map_location, **kwargs)
+    else:
+        if HAVE_FSSPEC:
+            with fsspec.open(path, "rb") as f:
+                return torch.load(f, map_location=map_location, **kwargs)
+        else:
+            logging.error('Could not import fsspec. Loading a checkpoint link is not supported!')
+            raise ModuleNotFoundError(f"fsspec is not installed but is necessary to download remote checkpoints !!")
+
+
+class PreEmphasis(NeuralModule):
+    def __init__(self, coefficient=0.97):
+        super().__init__()
+        self.coefficient = coefficient
+        self.register_buffer("filter", torch.FloatTensor([-self.coefficient, 1.0]).unsqueeze(0).unsqueeze(0))
+
+    def forward(self, x):
+        assert len(x.size()) == 2
+
+        x = torch.nn.functional.pad(x.unsqueeze(1), (1, 0), "reflect")
+        return torch.nn.functional.conv1d(x, self.filter).squeeze(1)
+
+
+class SELayer(NeuralModule):
+    def __init__(self, channel, reduction=8):
+        super(SELayer, self).__init__()
+        self.avg_pool = nn.AdaptiveAvgPool2d(1)
+        self.fc = nn.Sequential(
+            nn.Linear(channel, channel // reduction),
+            nn.ReLU(inplace=True),
+            nn.Linear(channel // reduction, channel),
+            nn.Sigmoid(),
+        )
+
+    def forward(self, x):
+        b, c, _, _ = x.size()
+        y = self.avg_pool(x).view(b, c)
+        y = self.fc(y).view(b, c, 1, 1)
+        return x * y
+
+
+class SEBasicBlock(NeuralModule):
+    expansion = 1
+
+    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):
+        super(SEBasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.relu = nn.ReLU(inplace=True)
+        self.se = SELayer(planes, reduction)
+        self.downsample = downsample
+        self.stride = stride
+
+    def forward(self, x):
+        residual = x
+
+        out = self.conv1(x)
+        out = self.relu(out)
+        out = self.bn1(out)
+
+        out = self.conv2(out)
+        out = self.bn2(out)
+        out = self.se(out)
+
+        if self.downsample is not None:
+            residual = self.downsample(x)
+
+        out += residual
+        out = self.relu(out)
+        return out
+
+
+class ResNetSpeakerEncoder(NeuralModule):
+    """Implementation of the model H/ASP without batch normalization in speaker embedding. This model was proposed in: https://arxiv.org/abs/2009.14153
+    Adapted from: https://github.com/clovaai/voxceleb_trainer
+    """
+
+    # pylint: disable=W0102
+    def __init__(
+        self,
+        input_dim=64,
+        proj_dim=512,
+        layers=[3, 4, 6, 3],
+        num_filters=[32, 64, 128, 256],
+        encoder_type="ASP",
+        log_input=True,
+        use_torch_spec=True,
+        audio_config={
+            "fft_size": 512,
+            "win_length": 400,
+            "hop_length": 160,
+            "frame_shift_ms": None,
+            "frame_length_ms": None,
+            "stft_pad_mode": "reflect",
+            "sample_rate": 16000,
+            "resample": False,
+            "preemphasis": 0.97,
+            "ref_level_db": 20,
+            "do_sound_norm": False,
+            "do_trim_silence": False,
+            "trim_db": 60,
+            "power": 1.5,
+            "griffin_lim_iters": 60,
+            "num_mels": 64,
+            "mel_fmin": 0.0,
+            "mel_fmax": 8000.0,
+            "spec_gain": 20,
+            "signal_norm": False,
+            "min_level_db": -100,
+            "symmetric_norm": False,
+            "max_norm": 4.0,
+            "clip_norm": False,
+            "stats_path": None,
+            "do_rms_norm": True,
+            "db_level": -27.0,
+        },
+    ):
+        super(ResNetSpeakerEncoder, self).__init__()
+
+        self.encoder_type = encoder_type
+        self.input_dim = input_dim
+        self.log_input = log_input
+        self.use_torch_spec = use_torch_spec
+        self.audio_config = audio_config
+        self.proj_dim = proj_dim
+
+        self.conv1 = nn.Conv2d(1, num_filters[0], kernel_size=3, stride=1, padding=1)
+        self.relu = nn.ReLU(inplace=True)
+        self.bn1 = nn.BatchNorm2d(num_filters[0])
+
+        self.inplanes = num_filters[0]
+        self.layer1 = self.create_layer(SEBasicBlock, num_filters[0], layers[0])
+        self.layer2 = self.create_layer(SEBasicBlock, num_filters[1], layers[1], stride=(2, 2))
+        self.layer3 = self.create_layer(SEBasicBlock, num_filters[2], layers[2], stride=(2, 2))
+        self.layer4 = self.create_layer(SEBasicBlock, num_filters[3], layers[3], stride=(2, 2))
+
+        self.instancenorm = nn.InstanceNorm1d(input_dim)
+
+        if self.use_torch_spec and HAVE_TORCHAUDIO:
+            self.torch_spec = self.get_torch_mel_spectrogram_class(audio_config)
+        else:
+            self.torch_spec = None
+
+        outmap_size = int(self.input_dim / 8)
+
+        self.attention = nn.Sequential(
+            nn.Conv1d(num_filters[3] * outmap_size, 128, kernel_size=1),
+            nn.ReLU(),
+            nn.BatchNorm1d(128),
+            nn.Conv1d(128, num_filters[3] * outmap_size, kernel_size=1),
+            nn.Softmax(dim=2),
+        )
+
+        if self.encoder_type == "SAP":
+            out_dim = num_filters[3] * outmap_size
+        elif self.encoder_type == "ASP":
+            out_dim = num_filters[3] * outmap_size * 2
+        else:
+            raise ValueError("Undefined encoder")
+
+        self.fc = nn.Linear(out_dim, proj_dim)
+
+        self._init_layers()
+
+    def _init_layers(self):
+        for m in self.modules():
+            if isinstance(m, nn.Conv2d):
+                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
+            elif isinstance(m, nn.BatchNorm2d):
+                nn.init.constant_(m.weight, 1)
+                nn.init.constant_(m.bias, 0)
+
+    def create_layer(self, block, planes, blocks, stride=1):
+        downsample = None
+        if stride != 1 or self.inplanes != planes * block.expansion:
+            downsample = nn.Sequential(
+                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(planes * block.expansion),
+            )
+
+        layers = []
+        layers.append(block(self.inplanes, planes, stride, downsample))
+        self.inplanes = planes * block.expansion
+        for _ in range(1, blocks):
+            layers.append(block(self.inplanes, planes))
+
+        return nn.Sequential(*layers)
+
+    # pylint: disable=R0201
+    def new_parameter(self, *size):
+        out = nn.Parameter(torch.FloatTensor(*size))
+        nn.init.xavier_normal_(out)
+        return out
+
+    def forward(self, x, l2_norm=False):
+        """Forward pass of the model.
+
+        Args:
+            x (Tensor): Raw waveform signal or spectrogram frames. If input is a waveform, `torch_spec` must be `True`
+                to compute the spectrogram on-the-fly.
+            l2_norm (bool): Whether to L2-normalize the outputs.
+
+        Shapes:
+            - x: :math:`(N, 1, T_{in})` or :math:`(N, D_{spec}, T_{in})`
+        """
+        x.squeeze_(1)
+        # if you torch spec compute it otherwise use the mel spec computed by the AP
+        if self.use_torch_spec:
+            x = self.torch_spec(x)
+
+        if self.log_input:
+            x = (x + 1e-6).log()
+        x = self.instancenorm(x).unsqueeze(1)
+
+        x = self.conv1(x)
+        x = self.relu(x)
+        x = self.bn1(x)
+
+        x = self.layer1(x)
+        x = self.layer2(x)
+        x = self.layer3(x)
+        x = self.layer4(x)
+
+        x = x.reshape(x.size()[0], -1, x.size()[-1])
+
+        w = self.attention(x)
+
+        if self.encoder_type == "SAP":
+            x = torch.sum(x * w, dim=2)
+        elif self.encoder_type == "ASP":
+            mu = torch.sum(x * w, dim=2)
+            sg = torch.sqrt((torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-5))
+            x = torch.cat((mu, sg), 1)
+
+        x = x.view(x.size()[0], -1)
+        x = self.fc(x)
+
+        if l2_norm:
+            x = torch.nn.functional.normalize(x, p=2, dim=1)
+        return x
+
+    def get_torch_mel_spectrogram_class(self, audio_config):
+        return torch.nn.Sequential(
+            PreEmphasis(audio_config["preemphasis"]),
+            torchaudio.transforms.MelSpectrogram(
+                sample_rate=audio_config["sample_rate"],
+                n_fft=audio_config["fft_size"],
+                win_length=audio_config["win_length"],
+                hop_length=audio_config["hop_length"],
+                window_fn=torch.hamming_window,
+                n_mels=audio_config["num_mels"],
+            ),
+        )
+
+    def load_checkpoint(self, checkpoint_path: str, strict=True):
+        state = load_fsspec(checkpoint_path, map_location=torch.device("cpu"))
+        self.load_state_dict(state["model"], strict=strict)
+
+
 class CodecActivation(nn.Module):
     """
     Choose between activation based on the input parameter.
@@ -194,6 +513,160 @@ class CodecActivation(nn.Module):
         return self.activation(x)
 
 
+class CausalConvTranspose1dNorm(NeuralModule):
+    """ConvTranspose1d causal padding and normalization."""
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: int,
+        stride: int = 1,
+        groups: int = None,
+        trim_right_ratio: int = 1,
+        bias=True,
+    ):
+        super().__init__()
+
+        self.trim_right_ratio = trim_right_ratio
+
+        # if groups are None, create a group for each out channel as done in Mini Codec
+        groups = out_channels if groups is None else groups
+
+        self.conv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride, groups=groups, bias=bias)
+
+        kernel_size = self.conv.kernel_size[0]
+        stride = self.conv.stride[0]
+        padding_total = kernel_size - stride
+
+        # Trim the padding on the right according to the specified ratio
+        # if trim_right_ratio = 1.0, trim everything from right
+        self.padding_right = math.ceil(padding_total * self.trim_right_ratio)
+        self.padding_left = padding_total - self.padding_right
+
+        # add weight norm
+        self.conv = nn.utils.parametrizations.weight_norm(self.conv)
+
+    def apply_weight_norm(self):
+        weight_norm = nn.utils.parametrizations.weight_norm
+        if hasattr(nn.utils.parametrizations, "weight_norm"):
+            weight_norm = nn.utils.parametrizations.weight_norm
+
+        weight_norm(self.conv)
+
+    def remove_weight_norm(self):
+        nn.utils.remove_weight_norm(self.conv)
+
+    def forward(self, inputs, input_len):
+        hidden_states = self.conv(inputs)
+
+        # unpad
+        end = hidden_states.shape[-1] - self.padding_right
+        hidden_states = hidden_states[..., self.padding_left : end]
+        # mask
+        hidden_states = mask_sequence_tensor(hidden_states, input_len)
+        return hidden_states
+
+
+class CausalConv1dNorm(NeuralModule):
+    """Conv1d with causal padding and normalization."""
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: int,
+        stride: int = 1,
+        dilation: int = 1,
+        groups: int = 1,
+        pad_mode: str = "zeros",
+        extra_pad_mode: str = "constant",
+        bias: bool = True,
+    ):
+        super().__init__()
+        self.extra_pad_mode = extra_pad_mode
+
+        # warn user on unusual setup between dilation and stride
+        if stride > 1 and dilation > 1:
+            print(
+                "CausalConv1dNorm has been initialized with stride > 1 and dilation > 1"
+                f" (kernel_size={kernel_size} stride={stride}, dilation={dilation})."
+            )
+
+        self.conv = nn.Conv1d(
+            in_channels,
+            out_channels,
+            kernel_size,
+            stride,
+            dilation=dilation,
+            groups=groups,
+            bias=bias,
+            padding_mode=pad_mode,
+        )
+
+        kernel_size = self.conv.kernel_size[0]
+        stride = torch.tensor(self.conv.stride[0], dtype=torch.int64)
+        dilation = self.conv.dilation[0]
+
+        # Effective kernel size with dilations.
+        kernel_size = torch.tensor((kernel_size - 1) * dilation + 1, dtype=torch.int64)
+
+        self.register_buffer("stride", stride, persistent=False)
+        self.register_buffer("kernel_size", kernel_size, persistent=False)
+        self.register_buffer("padding_total", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)
+
+        # add weight norm
+        self.conv = nn.utils.parametrizations.weight_norm(self.conv)
+
+    def remove_weight_norm(self):
+        nn.utils.remove_weight_norm(self.conv)
+
+    # Copied from transformers.models.encodec.modeling_encodec.EncodecConv1d._get_extra_padding_for_conv1d
+    def _get_extra_padding_for_conv1d(
+        self,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        """See `pad_for_conv1d`."""
+        length = hidden_states.shape[-1]
+        n_frames = (length - self.kernel_size + self.padding_total) / self.stride + 1
+        n_frames = torch.ceil(n_frames).to(torch.int64) - 1
+        ideal_length = n_frames * self.stride + self.kernel_size - self.padding_total
+
+        return ideal_length - length
+
+    @staticmethod
+    # Copied from transformers.models.encodec.modeling_encodec.EncodecConv1d._pad1d
+    def _pad1d(hidden_states: torch.Tensor, paddings: Tuple[int, int], mode: str = "zero", value: float = 0.0):
+        """Tiny wrapper around torch.nn.functional.pad, just to allow for reflect padding on small input.
+        If this is the case, we insert extra 0 padding to the right before the reflection happens.
+        """
+        length = hidden_states.shape[-1]
+        padding_left, padding_right = paddings
+        if not mode == "reflect":
+            return nn.functional.pad(hidden_states, paddings, mode, value)
+
+        max_pad = max(padding_left, padding_right)
+        extra_pad = 0
+        if length <= max_pad:
+            extra_pad = max_pad - length + 1
+            hidden_states = nn.functional.pad(hidden_states, (0, extra_pad))
+        padded = nn.functional.pad(hidden_states, paddings, mode, value)
+        end = padded.shape[-1] - extra_pad
+        return padded[..., :end]
+
+    def forward(self, inputs, input_len):
+        extra_padding = self._get_extra_padding_for_conv1d(inputs)
+
+        # Left padding for causal
+        hidden_states = self._pad1d(inputs, (self.padding_total, extra_padding), mode=self.extra_pad_mode)
+        hidden_states = self.conv(hidden_states)
+
+        # mask output
+        hidden_states = mask_sequence_tensor(hidden_states, input_len)
+
+        return hidden_states
+
+
 class Conv1dNorm(NeuralModule):
     def __init__(
         self,
@@ -203,6 +676,7 @@ class Conv1dNorm(NeuralModule):
         stride: int = 1,
         dilation: int = 1,
         padding: Optional[int] = None,
+        pad_mode: str = "reflect",
     ):
         super().__init__()
         if not padding:
@@ -214,9 +688,9 @@ class Conv1dNorm(NeuralModule):
             stride=stride,
             padding=padding,
             dilation=dilation,
-            padding_mode="reflect",
+            padding_mode=pad_mode,
         )
-        self.conv = nn.utils.weight_norm(conv)
+        self.conv = nn.utils.parametrizations.weight_norm(conv)
 
     @property
     def input_types(self):
@@ -242,7 +716,7 @@ class Conv1dNorm(NeuralModule):
 
 
 class ConvTranspose1dNorm(NeuralModule):
-    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1):
+    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, groups: int = 1):
         super().__init__()
         padding, output_padding = get_up_sample_padding(kernel_size, stride)
         conv = nn.ConvTranspose1d(
@@ -253,8 +727,9 @@ class ConvTranspose1dNorm(NeuralModule):
             padding=padding,
             output_padding=output_padding,
             padding_mode="zeros",
+            groups=groups,
         )
-        self.conv = nn.utils.weight_norm(conv)
+        self.conv = nn.utils.parametrizations.weight_norm(conv)
 
     @property
     def input_types(self):
@@ -300,7 +775,7 @@ class Conv2dNorm(NeuralModule):
             padding=padding,
             padding_mode="reflect",
         )
-        self.conv = nn.utils.weight_norm(conv)
+        self.conv = nn.utils.parametrizations.weight_norm(conv)
 
     @property
     def input_types(self):
@@ -1030,16 +1505,36 @@ class ResidualBlock(NeuralModule):
         dilation: int = 1,
         dropout_rate: float = 0.0,
         activation: str = "lrelu",
+        is_causal: bool = False,
+        pad_mode: str = "reflect",
     ):
         super(ResidualBlock, self).__init__()
 
         self.input_activation = CodecActivation(activation=activation, channels=channels)
         self.skip_activation = CodecActivation(activation=activation, channels=filters)
         self.dropout = torch.nn.Dropout(dropout_rate)
-        self.input_conv = Conv1dNorm(
-            in_channels=channels, out_channels=filters, kernel_size=kernel_size, dilation=dilation
-        )
-        self.skip_conv = Conv1dNorm(in_channels=filters, out_channels=channels, kernel_size=kernel_size)
+        if not is_causal:
+            self.input_conv = Conv1dNorm(
+                in_channels=channels,
+                out_channels=filters,
+                kernel_size=kernel_size,
+                dilation=dilation,
+                pad_mode=pad_mode,
+            )
+            self.skip_conv = Conv1dNorm(
+                in_channels=filters, out_channels=channels, kernel_size=kernel_size, pad_mode=pad_mode
+            )
+        else:
+            self.input_conv = CausalConv1dNorm(
+                in_channels=channels,
+                out_channels=filters,
+                kernel_size=kernel_size,
+                dilation=dilation,
+                pad_mode=pad_mode,
+            )
+            self.skip_conv = CausalConv1dNorm(
+                in_channels=filters, out_channels=channels, kernel_size=kernel_size, pad_mode=pad_mode
+            )
 
     def remove_weight_norm(self):
         self.input_conv.remove_weight_norm()
@@ -1075,7 +1570,15 @@ class HiFiGANResBlock(NeuralModule):
         activation: Activation for the residual blocks.
     """
 
-    def __init__(self, channels: int, kernel_size: int, dilations: Iterable[int], activation: str):
+    def __init__(
+        self,
+        channels: int,
+        kernel_size: int,
+        dilations: Iterable[int],
+        activation: str,
+        is_causal: bool = False,
+        pad_mode: str = "reflect",
+    ):
         super().__init__()
 
         self.res_blocks = nn.ModuleList(
@@ -1086,6 +1589,8 @@ class HiFiGANResBlock(NeuralModule):
                     kernel_size=kernel_size,
                     dilation=dilation,
                     activation=activation,
+                    is_causal=is_causal,
+                    pad_mode=pad_mode,
                 )
                 for dilation in dilations
             ]
@@ -1127,12 +1632,27 @@ class HiFiGANResLayer(NeuralModule):
 
     """
 
-    def __init__(self, channels: int, kernel_sizes: Iterable[int], dilations: Iterable[int], activation: str):
+    def __init__(
+        self,
+        channels: int,
+        kernel_sizes: Iterable[int],
+        dilations: Iterable[int],
+        activation: str,
+        is_causal: bool = False,
+        pad_mode: str = "reflect",
+    ):
         super().__init__()
 
         self.res_blocks = nn.ModuleList(
             [
-                HiFiGANResBlock(channels=channels, kernel_size=kernel_size, dilations=dilations, activation=activation)
+                HiFiGANResBlock(
+                    channels=channels,
+                    kernel_size=kernel_size,
+                    dilations=dilations,
+                    activation=activation,
+                    is_causal=is_causal,
+                    pad_mode=pad_mode,
+                )
                 for kernel_size in kernel_sizes
             ]
         )
@@ -1159,6 +1679,127 @@ class HiFiGANResLayer(NeuralModule):
         return out
 
 
+class CausalHiFiGANEncoder(NeuralModule):
+    """
+    Causal Audio encoder created by inverting the HiFi-GAN decoder and replacing Conv1D by CausalConv1D.
+
+    Args:
+        encoded_dim: Dimension of encoder output.
+        down_sample_rates: Rate to upsample for each decoder block. The product of the downsample rates will
+            determine the output token rate. For example 2 * 2 * 8 * 8 = 256 samples per token.
+        base_channels: Number of filters in the first convolution. The number of channels will be doubled after each
+            downsample layer.
+        in_kernel_size: Kernel size of the input convolution.
+        out_kernel_size: Kernel size of the output convolution.
+        resblock_kernel_sizes: List of kernel sizes to use in each residual block.
+        resblock_dilation_sizes: List of dilations to use in each residual block.
+        activation: Activation to use in residual and downsample layers, defaults to leaky relu.
+    """
+
+    def __init__(
+        self,
+        encoded_dim: int,
+        down_sample_rates: Iterable[int] = (2, 2, 8, 8),
+        base_channels: int = 32,
+        in_kernel_size: int = 7,
+        out_kernel_size: int = 7,
+        resblock_kernel_sizes: Iterable[int] = (3, 7, 11),
+        resblock_dilation_sizes: Iterable[int] = (1, 3, 5),
+        activation: str = "lrelu",
+        pad_mode: str = "zeros",
+    ):
+        assert in_kernel_size > 0
+        assert out_kernel_size > 0
+
+        super().__init__()
+
+        self.down_sample_rates = down_sample_rates
+        self.pre_conv = CausalConv1dNorm(
+            in_channels=1, out_channels=base_channels, kernel_size=in_kernel_size, pad_mode=pad_mode
+        )
+
+        in_channels = base_channels
+        self.activations = nn.ModuleList([])
+        self.down_sample_conv_layers = nn.ModuleList([])
+        self.res_layers = nn.ModuleList([])
+        for i, down_sample_rate in enumerate(self.down_sample_rates):
+            res_layer = HiFiGANResLayer(
+                channels=in_channels,
+                kernel_sizes=resblock_kernel_sizes,
+                dilations=resblock_dilation_sizes,
+                activation=activation,
+                is_causal=True,
+                pad_mode=pad_mode,
+            )
+            self.res_layers.append(res_layer)
+
+            act = CodecActivation(activation, channels=in_channels)
+            self.activations.append(act)
+
+            out_channels = 2 * in_channels
+            kernel_size = 2 * down_sample_rate
+
+            # padding = get_down_sample_padding(kernel_size=kernel_size, stride=down_sample_rate)
+            down_sample_conv = CausalConv1dNorm(
+                in_channels=in_channels,
+                out_channels=out_channels,
+                kernel_size=kernel_size,
+                stride=down_sample_rate,
+                pad_mode=pad_mode,
+            )
+            in_channels = out_channels
+            self.down_sample_conv_layers.append(down_sample_conv)
+
+        self.post_activation = CodecActivation(activation, channels=in_channels)
+        self.post_conv = CausalConv1dNorm(
+            in_channels=in_channels, out_channels=encoded_dim, kernel_size=out_kernel_size, pad_mode=pad_mode
+        )
+
+    @property
+    def input_types(self):
+        return {
+            "audio": NeuralType(('B', 'T_audio'), AudioSignal()),
+            "audio_len": NeuralType(tuple('B'), LengthsType()),
+        }
+
+    @property
+    def output_types(self):
+        return {
+            "encoded": NeuralType(('B', 'D', 'T_encoded'), EncodedRepresentation()),
+            "encoded_len": NeuralType(tuple('B'), LengthsType()),
+        }
+
+    def remove_weight_norm(self):
+        self.pre_conv.remove_weight_norm()
+        self.post_conv.remove_weight_norm()
+        for res_layer in self.res_layers:
+            res_layer.remove_weight_norm()
+        for down_sample_conv in self.down_sample_conv_layers:
+            down_sample_conv.remove_weight_norm()
+
+    @typecheck()
+    def forward(self, audio, audio_len):
+        encoded_len = audio_len
+        audio = rearrange(audio, "B T -> B 1 T")
+        # [B, C, T_audio]
+        out = self.pre_conv(inputs=audio, input_len=encoded_len)
+        for act, res_layer, down_sample_conv, down_sample_rate in zip(
+            self.activations, self.res_layers, self.down_sample_conv_layers, self.down_sample_rates
+        ):
+            # [B, C, T]
+            out = res_layer(inputs=out, input_len=encoded_len)
+            out = act(out)
+
+            encoded_len = encoded_len // down_sample_rate
+            # [B, 2 * C, T / down_sample_rate]
+            out = down_sample_conv(inputs=out, input_len=encoded_len)
+
+        out = self.post_activation(out)
+        # [B, encoded_dim, T_encoded]
+        encoded = self.post_conv(inputs=out, input_len=encoded_len)
+        return encoded, encoded_len
+
+
 class HiFiGANEncoder(NeuralModule):
     """
     Audio encoder created by inverting the HiFi-GAN decoder.
@@ -1186,6 +1827,7 @@ class HiFiGANEncoder(NeuralModule):
         resblock_kernel_sizes: Iterable[int] = (3, 7, 11),
         resblock_dilation_sizes: Iterable[int] = (1, 3, 5),
         activation: str = "lrelu",
+        pad_mode: str = "reflect",
     ):
         assert in_kernel_size > 0
         assert out_kernel_size > 0
@@ -1193,7 +1835,9 @@ class HiFiGANEncoder(NeuralModule):
         super().__init__()
 
         self.down_sample_rates = down_sample_rates
-        self.pre_conv = Conv1dNorm(in_channels=1, out_channels=base_channels, kernel_size=in_kernel_size)
+        self.pre_conv = Conv1dNorm(
+            in_channels=1, out_channels=base_channels, kernel_size=in_kernel_size, pad_mode=pad_mode
+        )
 
         in_channels = base_channels
         self.activations = nn.ModuleList([])
@@ -1205,6 +1849,7 @@ class HiFiGANEncoder(NeuralModule):
                 kernel_sizes=resblock_kernel_sizes,
                 dilations=resblock_dilation_sizes,
                 activation=activation,
+                pad_mode=pad_mode,
             )
             self.res_layers.append(res_layer)
 
@@ -1221,12 +1866,15 @@ class HiFiGANEncoder(NeuralModule):
                 kernel_size=kernel_size,
                 stride=down_sample_rate,
                 padding=padding,
+                pad_mode=pad_mode,
             )
             in_channels = out_channels
             self.down_sample_conv_layers.append(down_sample_conv)
 
         self.post_activation = CodecActivation(activation, channels=in_channels)
-        self.post_conv = Conv1dNorm(in_channels=in_channels, out_channels=encoded_dim, kernel_size=out_kernel_size)
+        self.post_conv = Conv1dNorm(
+            in_channels=in_channels, out_channels=encoded_dim, kernel_size=out_kernel_size, pad_mode=pad_mode
+        )
 
     @property
     def input_types(self):
@@ -1273,6 +1921,136 @@ class HiFiGANEncoder(NeuralModule):
         return encoded, encoded_len
 
 
+class CausalHiFiGANDecoder(NeuralModule):
+    """
+    Codec decoder using the HiFi-GAN generator architecture with Causal Convolutions.
+
+    Args:
+        input_dim: Input dimension.
+        up_sample_rates: Rate to upsample for each decoder block. The product of the upsample rates should be the same
+            as the overall downsample rate for your encoder. For example, a symmetric encoder/decoder can be created
+            with encoder downsample rates [2, 2, 8, 8] and decoder upsample rates [8, 8, 2, 2].
+        base_channels: Number of filters in the first convolution. The number of channels will be cut in
+            half after each upsample layer.
+        in_kernel_size: Kernel size of the input convolution.
+        out_kernel_size: Kernel size of the output convolution.
+        resblock_kernel_sizes: List of kernel sizes to use in each residual block.
+        resblock_dilation_sizes: List of dilations to use in each residual block.
+        activation: Activation to use in residual and upsample layers, defaults to leaky relu.
+        output_activation: Activation to apply to output. To produce a valid audio signal, it should output values in
+         the range [-1.0, 1.0]. Supports "tanh" and "clamp".
+    """
+
+    def __init__(
+        self,
+        input_dim: int,
+        up_sample_rates: Iterable[int] = (8, 8, 2, 2),
+        base_channels: int = 512,
+        in_kernel_size: int = 7,
+        out_kernel_size: int = 3,
+        resblock_kernel_sizes: Iterable[int] = (3, 7, 11),
+        resblock_dilation_sizes: Iterable[int] = (1, 3, 5),
+        activation: str = "lrelu",
+        output_activation: str = "tanh",
+        pad_mode: str = "zeros",
+        n_groups_equal_to_out_channels: bool = True,
+    ):
+        assert in_kernel_size > 0
+        assert out_kernel_size > 0
+
+        super().__init__()
+
+        self.up_sample_rates = up_sample_rates
+
+        self.pre_conv = CausalConv1dNorm(
+            in_channels=input_dim, out_channels=base_channels, kernel_size=in_kernel_size, pad_mode=pad_mode
+        )
+
+        in_channels = base_channels
+        self.activations = nn.ModuleList([])
+        self.up_sample_conv_layers = nn.ModuleList([])
+        self.res_layers = nn.ModuleList([])
+        for i, up_sample_rate in enumerate(self.up_sample_rates):
+            out_channels = in_channels // 2
+            kernel_size = 2 * up_sample_rate
+
+            act = CodecActivation(activation, channels=in_channels)
+            self.activations.append(act)
+
+            up_sample_conv = CausalConvTranspose1dNorm(
+                in_channels=in_channels,
+                out_channels=out_channels,
+                kernel_size=kernel_size,
+                stride=up_sample_rate,
+                groups=out_channels if n_groups_equal_to_out_channels else 1,
+            )
+            in_channels = out_channels
+            self.up_sample_conv_layers.append(up_sample_conv)
+
+            res_layer = HiFiGANResLayer(
+                channels=in_channels,
+                kernel_sizes=resblock_kernel_sizes,
+                dilations=resblock_dilation_sizes,
+                activation=activation,
+                is_causal=True,
+                pad_mode=pad_mode,
+            )
+            self.res_layers.append(res_layer)
+
+        self.post_activation = CodecActivation(activation, channels=in_channels)
+        self.post_conv = CausalConv1dNorm(
+            in_channels=in_channels, out_channels=1, kernel_size=out_kernel_size, pad_mode=pad_mode
+        )
+        if output_activation == "tanh":
+            self.out_activation = nn.Tanh()
+        elif output_activation == "clamp":
+            self.out_activation = ClampActivation()
+        else:
+            raise ValueError(f"Invalid audio output activation {output_activation}")
+
+    @property
+    def input_types(self):
+        return {
+            "inputs": NeuralType(('B', 'D', 'T_encoded'), VoidType()),
+            "input_len": NeuralType(tuple('B'), LengthsType()),
+        }
+
+    @property
+    def output_types(self):
+        return {
+            "audio": NeuralType(('B', 'T_audio'), AudioSignal()),
+            "audio_len": NeuralType(tuple('B'), LengthsType()),
+        }
+
+    def remove_weight_norm(self):
+        self.pre_conv.remove_weight_norm()
+        for up_sample_conv in self.up_sample_conv_layers:
+            up_sample_conv.remove_weight_norm()
+        for res_layer in self.res_layers:
+            res_layer.remove_weight_norm()
+
+    @typecheck()
+    def forward(self, inputs, input_len):
+        audio_len = input_len
+        # [B, C, T_encoded]
+        out = self.pre_conv(inputs=inputs, input_len=audio_len)
+        for act, res_layer, up_sample_conv, up_sample_rate in zip(
+            self.activations, self.res_layers, self.up_sample_conv_layers, self.up_sample_rates
+        ):
+            audio_len = audio_len * up_sample_rate
+            out = act(out)
+            # [B, C / 2, T * up_sample_rate]
+            out = up_sample_conv(inputs=out, input_len=audio_len)
+            out = res_layer(inputs=out, input_len=audio_len)
+
+        out = self.post_activation(out)
+        # [B, 1, T_audio]
+        out = self.post_conv(inputs=out, input_len=audio_len)
+        audio = self.out_activation(out)
+        audio = rearrange(audio, "B 1 T -> B T")
+        return audio, audio_len
+
+
 class HiFiGANDecoder(NeuralModule):
     """
     Codec decoder using the HiFi-GAN generator architecture.
@@ -1306,6 +2084,8 @@ class HiFiGANDecoder(NeuralModule):
         resblock_dilation_sizes: Iterable[int] = (1, 3, 5),
         activation: str = "lrelu",
         output_activation: str = "tanh",
+        pad_mode: str = "reflect",
+        n_groups_equal_to_out_channels: bool = False,
     ):
         assert in_kernel_size > 0
         assert out_kernel_size > 0
@@ -1313,7 +2093,10 @@ class HiFiGANDecoder(NeuralModule):
         super().__init__()
 
         self.up_sample_rates = up_sample_rates
-        self.pre_conv = Conv1dNorm(in_channels=input_dim, out_channels=base_channels, kernel_size=in_kernel_size)
+
+        self.pre_conv = Conv1dNorm(
+            in_channels=input_dim, out_channels=base_channels, kernel_size=in_kernel_size, pad_mode=pad_mode
+        )
 
         in_channels = base_channels
         self.activations = nn.ModuleList([])
@@ -1327,7 +2110,11 @@ class HiFiGANDecoder(NeuralModule):
             self.activations.append(act)
 
             up_sample_conv = ConvTranspose1dNorm(
-                in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=up_sample_rate
+                in_channels=in_channels,
+                out_channels=out_channels,
+                kernel_size=kernel_size,
+                stride=up_sample_rate,
+                groups=out_channels if n_groups_equal_to_out_channels else 1,
             )
             in_channels = out_channels
             self.up_sample_conv_layers.append(up_sample_conv)
@@ -1337,11 +2124,14 @@ class HiFiGANDecoder(NeuralModule):
                 kernel_sizes=resblock_kernel_sizes,
                 dilations=resblock_dilation_sizes,
                 activation=activation,
+                pad_mode=pad_mode,
             )
             self.res_layers.append(res_layer)
 
         self.post_activation = CodecActivation(activation, channels=in_channels)
-        self.post_conv = Conv1dNorm(in_channels=in_channels, out_channels=1, kernel_size=out_kernel_size)
+        self.post_conv = Conv1dNorm(
+            in_channels=in_channels, out_channels=1, kernel_size=out_kernel_size, pad_mode=pad_mode
+        )
         if output_activation == "tanh":
             self.out_activation = nn.Tanh()
         elif output_activation == "clamp":
@@ -1468,10 +2258,13 @@ class ResNetEncoder(NeuralModule):
         kernel_size: int = 3,
         dropout_rate: float = 0.1,
         activation: str = "lrelu",
+        pad_mode: str = "reflect",
     ):
         super(ResNetEncoder, self).__init__()
 
-        self.pre_conv = Conv1dNorm(in_channels=in_channels, out_channels=hidden_channels, kernel_size=kernel_size)
+        self.pre_conv = Conv1dNorm(
+            in_channels=in_channels, out_channels=hidden_channels, kernel_size=kernel_size, pad_mode=pad_mode
+        )
         self.res_layers = nn.ModuleList(
             [
                 ResidualBlock(
@@ -1480,12 +2273,15 @@ class ResNetEncoder(NeuralModule):
                     kernel_size=kernel_size,
                     dropout_rate=dropout_rate,
                     activation=activation,
+                    pad_mode=pad_mode,
                 )
                 for _ in range(num_layers)
             ]
         )
         self.post_activation = CodecActivation(activation, channels=hidden_channels)
-        self.post_conv = Conv1dNorm(in_channels=hidden_channels, out_channels=out_channels, kernel_size=kernel_size)
+        self.post_conv = Conv1dNorm(
+            in_channels=hidden_channels, out_channels=out_channels, kernel_size=kernel_size, pad_mode=pad_mode
+        )
 
     def remove_weight_norm(self):
         self.pre_conv.remove_weight_norm()
