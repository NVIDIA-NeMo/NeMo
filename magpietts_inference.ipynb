{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ccdc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import magpietts\n",
    "from nemo.collections.tts.data.text_to_speech_dataset import MagpieTTSDataset, DatasetSample\n",
    "from nemo.collections.tts.data.text_to_speech_dataset_lhotse import MagpieTTSLhotseDataset, setup_tokenizers\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5798ac",
   "metadata": {},
   "source": [
    "### Checkpoint Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445f11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checkpoint_file = \"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__LT_768_4_12/checkpoints/magpie-tts--val_loss=10.8018-epoch=87-last.ckpt\"\n",
    "#checkpoint_file = \"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__LT_768_4_12/checkpoints/magpie-tts--val_loss=10.6553-epoch=268-last.ckpt\"\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_2x_MG_768_4_12/checkpoints/magpie-tts--val_loss=10.6772-epoch=231-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_2x_MG_768_4_12/hparams.yaml\"\n",
    "\n",
    "#checkpoint_file = \"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_4x_MG_768_4_12/checkpoints/magpie-tts--val_loss=11.3472-epoch=198-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_4x_MG_768_4_12/hparams.yaml\"\n",
    "# checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_3x_AR_768_4_12/checkpoints/magpie-tts--val_loss=10.7700-epoch=136-last.ckpt\" \n",
    "# hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_3x_AR_768_4_12/hparams.yaml\"\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_3x_MG_768_4_12/checkpoints/magpie-tts--val_loss=11.0669-epoch=200-last.ckpt\"#\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_3x_MG_768_4_12/hparams.yaml\"\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_3x_AR_768_4_12/checkpoints/magpie-tts--val_loss=10.7198-epoch=238-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_3x_AR_768_4_12/hparams.yaml\"\n",
    "\n",
    "#checkpoint_file = \"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_1x_AR_768_4_12/checkpoints/magpie-tts--val_loss=9.8141-epoch=204-last.ckpt\"#\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_1x_AR_768_4_12/hparams.yaml\"\n",
    "\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/lhotse_dcce_lt_AR_768_4_12/checkpoints/MagpieTTS-EN-Lhotse--val_loss=9.5592-step=231018-epoch=32-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/lhotse_dcce_lt_AR_768_4_12/hparams.yaml\"\n",
    "\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/2505_release/grpo_lr2e-7_CerSsim55_ep1.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/2505_release/wandb_hparams.yaml\"\n",
    "\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__21.5_bf16_clip2.5_LR2e-4_CFG_Bugfix/checkpoints/magpie-tts--val_loss=10.8477-epoch=228-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__21.5_bf16_clip2.5_LR2e-4_CFG_Bugfix/hparams.yaml\"\n",
    "\n",
    "# DCCE 1x: shows special token issue\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_1x_MG_768_4_12/checkpoints/magpie-tts--val_loss=10.0863-epoch=198-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_1x_MG_768_4_12/hparams.yaml\"\n",
    "\n",
    "# fp32\n",
    "checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/exps_fixed_causal/dc_21.5_halfcausal_MG_fp32/mapgie-tts-maskgit/0/checkpoints/mapgie-tts-maskgit--val_loss=10.2224-epoch=183-last.ckpt\"\n",
    "hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/exps_fixed_causal/dc_21.5_halfcausal_MG_fp32/mapgie-tts-maskgit/0/hparams.yaml\"\n",
    "\n",
    "# decoder_context, 2x: shows special tokens issue\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__21.5_bf16_clip2.5_LR2e-4_CFG_heads4/checkpoints/magpie-tts--val_loss=10.8582-epoch=151-last.ckpt\" #\"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__21.5_bf16_clip2.5_LR2e-4_CFG_Bugfix/checkpoints/magpie-tts--val_loss=10.8477-epoch=228-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__21.5_bf16_clip2.5_LR2e-4_CFG_heads4/hparams.yaml\" #\"/datap/misc/continuouscheckpoints/maskgit/2x/downsample_2x_MG__21.5_bf16_clip2.5_LR2e-4_CFG_Bugfix/hparams.yaml\"\n",
    "\n",
    "# DCCE 2x downsampled: shows special tokens issue\n",
    "#checkpoint_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_2x_MG_768_4_12/checkpoints/magpie-tts--val_loss=10.6772-epoch=231-last.ckpt\"\n",
    "#hparams_file=\"/datap/misc/continuouscheckpoints/maskgit/2x/dcce_downsample_2x_MG_768_4_12/hparams.yaml\"\n",
    "\n",
    "codecmodel_path=\"/datap/misc/checkpoints/ml-model-INTERSPEECH_2025_abblations_21.5Hz_8_codebooks_2016_codes_enc_non_causal_dec_causal_1.89kbps.nemo\"\n",
    "\n",
    "\n",
    "\n",
    "# Temp out dir for saving audios\n",
    "out_dir = \"/datap/misc/t5tts_inference_notebook_samples\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf2a16",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf66f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hparams_file_from_wandb = False\n",
    "model_cfg = OmegaConf.load(hparams_file)\n",
    "if hparams_file_from_wandb:\n",
    "    model_cfg = model_cfg.cfg.value\n",
    "else:\n",
    "    model_cfg = model_cfg.cfg\n",
    "\n",
    "\n",
    "with open_dict(model_cfg):\n",
    "    model_cfg.codecmodel_path = codecmodel_path\n",
    "    if hasattr(model_cfg, 'text_tokenizer'):\n",
    "        # Backward compatibility for models trained with absolute paths in text_tokenizer\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_dict = \"scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt\"\n",
    "        model_cfg.text_tokenizer.g2p.heteronyms = \"scripts/tts_dataset_files/heteronyms-052722\"\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_probability = 1.0\n",
    "    model_cfg.train_ds = None\n",
    "    model_cfg.validation_ds = None\n",
    "\n",
    "\n",
    "model = magpietts.MagpieTTSModel(cfg=model_cfg)\n",
    "print(\"Loading weights from checkpoint\")\n",
    "ckpt = torch.load(checkpoint_file, weights_only=False)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "print(\"Loaded weights.\")\n",
    "\n",
    "model.use_kv_cache_for_inference = True\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b5711",
   "metadata": {},
   "source": [
    "### Initialize Dataset class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a7271",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_dataset = MagpieTTSDataset(\n",
    "    dataset_meta={},\n",
    "    sample_rate=model.sample_rate,\n",
    "    min_duration=0.5,\n",
    "    max_duration=20,\n",
    "    codec_model_samples_per_frame=model.codec_model_samples_per_frame,\n",
    "    bos_id=model.bos_id,\n",
    "    eos_id=model.eos_id,\n",
    "    context_audio_bos_id=model.context_audio_bos_id,\n",
    "    context_audio_eos_id=model.context_audio_eos_id,\n",
    "    audio_bos_id=model.audio_bos_id,\n",
    "    audio_eos_id=model.audio_eos_id,\n",
    "    num_audio_codebooks=model.num_audio_codebooks,\n",
    "    prior_scaling_factor=None,\n",
    "    load_cached_codes_if_available=True,\n",
    "    dataset_type='test',\n",
    "    tokenizer_config=None,\n",
    "    load_16khz_audio=model.model_type == 'single_encoder_sv_tts',\n",
    "    use_text_conditioning_tokenizer=model.use_text_conditioning_encoder,\n",
    "    pad_context_text_to_max_duration=model.pad_context_text_to_max_duration,\n",
    "    context_duration_min=model.cfg.get('context_duration_min', 5.0),\n",
    "    context_duration_max=model.cfg.get('context_duration_max', 5.0),\n",
    ")\n",
    "test_dataset.text_tokenizer, test_dataset.text_conditioning_tokenizer = setup_tokenizers(\n",
    "           model_cfg.text_tokenizers, \n",
    "           model_cfg.use_text_conditioning_encoder,\n",
    "           mode='test')\n",
    "\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    with sf.SoundFile(file_path) as audio_file:\n",
    "        # Calculate the duration\n",
    "        duration = len(audio_file) / audio_file.samplerate\n",
    "        return duration\n",
    "\n",
    "def create_record(text, context_audio_filepath=None, context_text=None):\n",
    "    dummy_audio_fp = os.path.join(out_dir, \"dummy_audio.wav\")\n",
    "    dummy_audio = sf.write(dummy_audio_fp, np.zeros(22050 * 3), 22050)  # 3 seconds of silence\n",
    "    record = {\n",
    "        'audio_filepath' : dummy_audio_fp,\n",
    "        'duration': 3.0,\n",
    "        'text': text,\n",
    "        'speaker': \"dummy\",\n",
    "    }\n",
    "    if context_text is not None:\n",
    "        assert context_audio_filepath is None\n",
    "        record['context_text'] = context_text\n",
    "    else:\n",
    "        assert context_audio_filepath is not None\n",
    "        record['context_audio_filepath'] = context_audio_filepath\n",
    "        record['context_audio_duration'] = get_audio_duration(context_audio_filepath)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa7a5a",
   "metadata": {},
   "source": [
    "### Set transcript and context pairs to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7374d3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "# Change sample text and prompt audio/text here\n",
    "audio_base_dir = \"/\"\n",
    "test_entries = [\n",
    "    create_record(\n",
    "        text=\"The recollection of a unique event cannot, so Bergson contends, be wholly constituted by habit, and is in fact something radically different from the memory which is habit.\",\n",
    "        #text=\"First you called it T-five-Tee-tee-eS, then Koel-Tee-Tee-es, now Magpie... Make up your minds, folks!\",\n",
    "        #text=\"First you called it T five T T S. Then Koel. Now, Magpie... Make up your mind, folks!\",\n",
    "#         text=\"This is a simple sentence to check my text to speech synthesis model.\",\n",
    "#         text=\" How? \",\n",
    "#         context_text=\"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_WIZWIKI |\",\n",
    "#         Lindy_CMU_FEARFUL\n",
    "#         Lindy_WIZWIKI\n",
    "#         context_audio_filepath=\"/datap/misc/LibriTTSfromNemo/LibriTTS/test-clean/7729/102255/7729_102255_000012_000001.wav\", # Supply either context_audio_filepath or context_text, not both\n",
    "        #context_audio_filepath=\"/datap/misc/Datasets/LibriTTS/test-clean/8230/279154/8230_279154_000004_000009.wav\",\n",
    "        context_audio_filepath=\"/datap/misc/Datasets/LibriTTS/test-clean/7176/88083/7176_88083_000018_000001.wav\",\n",
    "        #context_audio_filepath=\"/home/rfejgin/contexts/boris-snippet-dereverb-denoise-short.wav\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "data_samples = []\n",
    "for entry in test_entries:\n",
    "    dataset_sample = DatasetSample(\n",
    "        dataset_name=\"sample\",\n",
    "        manifest_entry=entry,\n",
    "        audio_dir=audio_base_dir,\n",
    "        feature_dir=audio_base_dir,\n",
    "        text=entry['text'],\n",
    "        speaker=None,\n",
    "        speaker_index=0,\n",
    "        tokenizer_names=[\"english_phoneme\"], # Change this for multilingual: \"english_phoneme\", \"spanish_phoneme\", \"english_chartokenizer\", \"german_chartokenizer\".. \n",
    "    )\n",
    "    data_samples.append(dataset_sample)\n",
    "    \n",
    "test_dataset.data_samples = data_samples\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9866b",
   "metadata": {},
   "source": [
    "### Generate With Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2ef7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "item_idx = 0\n",
    "for bidx, batch in enumerate(test_data_loader):\n",
    "    print(\"Processing batch {} out of {}\".format(bidx, len(test_data_loader)))\n",
    "    model.decoder.reset_cache(use_cache=True)\n",
    "    batch_cuda ={}\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch_cuda[key] = batch[key].cuda()\n",
    "        else:\n",
    "            batch_cuda[key] = batch[key]\n",
    "    import time\n",
    "    st = time.time()\n",
    "    maskgit_n_steps = 8\n",
    "    fixed_schedule_n_unmasked = None\n",
    "    #fixed_schedule_n_unmasked = [0,1,4,8,9,12] #[0, 2, 8, 9, 12]#[0,1,2,8,9,11]#[0,1,4,8,9]#[0,1,2,4,10] #[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] #[0, 1, 2, 5, 8]# [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]# [0, 1, 2, 5, 8]# 4, 8 ]\n",
    "    #fixed_schedule_n_unmasked = #[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]#[0,1,2,8,9]#[0,1,2,3,8,9,10,11] #[0,1,2,4,8,11]#[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    #fixed_schedule_n_unmasked = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31] # [0,1,2,8,9,10,16,17,18,24,25,26]\n",
    "    #fixed_schedule_n_unmasked = [0,2,8,10,16,18,24,26] # [0,2,4,8,10,12,16,18,20,24,26,28]\n",
    "    #fixed_schedule_n_unmasked =  [0,1,8,9]#]#[0,1,16]#][0,1,8,9,16,17,24,25]# [0,1,16]#][0,1,8,9,16,17,24,25]#[0,1,8,16,24]#[0,1,8,9,16,24]#[0,1,8,16]#[0,1,8,9,16,17,24,25]#[0,1,2,8,9,10,16,17,18,24,25,26]# [0,1,2, 8,9,10, 16,17,18]# [0,1,8,9,16,17]\n",
    "    for _ in range(4):\n",
    "        for sampling_type in [\"purity_causal\", \"purity_default\", \"causal\"]:# [\"causal\"]: #[\"default\", \"causal\"]:\n",
    "            for use_local_transformer_for_inference in [True]:#[True]:#[False, True]:\n",
    "                for i, maskgit_noise_scale in enumerate([0]):#,0.1,1]):#[0, 0.01, 0.1, 1]):#[0,1e-3]):#([0.0, 1e-3, 1e-2, 5e-2, 1e-1]):\n",
    "                    if not use_local_transformer_for_inference and i > 0:\n",
    "                        continue\n",
    "                    for apply_prior in [False]:\n",
    "                        predicted_audio, predicted_audio_lens, _, _, rtf_metrics, cross_attn_np, all_heads_attn_np = model.infer_batch(\n",
    "                            batch_cuda, \n",
    "                            max_decoder_steps=430, \n",
    "                            temperature=0.7,#,0.55,#5,#0.7,#0.6, \n",
    "                            topk=80, \n",
    "                            use_cfg=False,#True,\n",
    "                            cfg_scale=2.5,#2.5,#3.0,#2.5,#4.5,#3.5,#5.5,#3.5,#8.0,#3.5,#2.5,#4.5,#2.5,#2.5,\n",
    "                            return_cross_attn_probs=True,\n",
    "                            apply_attention_prior=False,\n",
    "                            compute_all_heads_attn_maps=True,\n",
    "                            use_local_transformer_for_inference=use_local_transformer_for_inference,\n",
    "                            maskgit_n_steps=maskgit_n_steps,\n",
    "                            maskgit_noise_scale=0,#0.1,#maskgit_noise_scale,\n",
    "                            fixed_schedule_n_unmasked=fixed_schedule_n_unmasked,\n",
    "                            dynamic_cfg_scale=False,\n",
    "                            sampling_type=sampling_type\n",
    "                        )\n",
    "                        print(\"generation time\", time.time() - st)\n",
    "                        pprint.pprint(rtf_metrics)\n",
    "                        for idx in range(predicted_audio.size(0)):\n",
    "                            predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "                            predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "                            audio_path = os.path.join(out_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "                            sf.write(audio_path, predicted_audio_np, model.sample_rate)\n",
    "                            print(f\"i={i}\")\n",
    "                            print(test_entries[bidx]['text'])\n",
    "                            print(\"Prior Used? \", apply_prior)\n",
    "                            print(\"use_local_transformer_for_inference? \", use_local_transformer_for_inference)\n",
    "                            print(\"maskgit_n_steps: \", maskgit_n_steps)\n",
    "                            print(\"maskgit_noise_scale: \", maskgit_noise_scale)\n",
    "                            print(\"sampling_type: \", sampling_type)\n",
    "                            print(\"checkpoint: \", checkpoint_file)\n",
    "                            display(Audio(audio_path))\n",
    "                            item_idx += 1\n",
    "\n",
    "                            print(\"------------------------------------\")\n",
    "                            print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684fc14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dab52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
