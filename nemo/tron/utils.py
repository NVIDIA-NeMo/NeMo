import os

import torch.distributed


def get_rank_safe() -> int:
    # In megatron init, args.rank comes from the torchrun env var.
    # Once init has been done, args.rank is updated to value of torch get_rank()
    if torch.distributed.is_initialized():
        return torch.distributed.get_rank()
    else:
        return int(os.getenv("RANK", "0"))


def get_world_size_safe() -> int:
    # In megatron init, args.world_size comes from the torchrun env var.
    # Once init has been done, args.world_size is updated to value of torch get_world_size()
    if torch.distributed.is_initialized():
        return torch.distributed.get_world_size()
    else:
        return int(os.getenv("WORLD_SIZE", "1"))


def get_local_rank_preinit() -> int:
    return int(os.getenv("LOCAL_RANK", "0"))


def print_rank_0(message):
    """If distributed is initialized, print only on rank 0."""
    rank = get_rank_safe()
    if rank == 0:
        print(message, flush=True)
