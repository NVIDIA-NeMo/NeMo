{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a31d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "from nemo.collections.tts.helpers.helpers import plot_spectrogram_to_numpy\n",
    "import IPython.display\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from nemo.collections.tts.models import hifigan, hifigan_ssl, ssl_tts, fastpitch_ssl\n",
    "from nemo.core.neural_types.elements import AudioSignal, MelSpectrogramType\n",
    "import librosa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_model_ckpt_path = \"/home/pneekhara/NeMo2022/SSLCheckPoints/SSLConformer22050_Epoch37.ckpt\"\n",
    "ssl_model = ssl_tts.SSLDisentangler.load_from_checkpoint(ssl_model_ckpt_path, strict=False)\n",
    "ssl_model = ssl_model.cpu()\n",
    "ssl_model.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def85178",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifi_path = \"/home/pneekhara/NeMo2022/HiFiCKPTS/hifigan_libritts/HiFiLibriEpoch334.ckpt\"\n",
    "vocoder = hifigan.HifiGanModel.load_from_checkpoint(hifi_path).cpu()\n",
    "vocoder.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7dae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_featurizer = WaveformFeaturizer(sample_rate=22050, int_values=False, augmentor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fad077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(wav_path, pad_multiple=1024):\n",
    "    wav = wav_featurizer.process(wav_path)\n",
    "    if wav.shape[0]  % pad_multiple != 0:\n",
    "        wav = torch.cat(\n",
    "                [wav, torch.zeros(pad_multiple -wav.shape[0] % pad_multiple, dtype=torch.float)]\n",
    "            )\n",
    "    wav = wav[:-1]\n",
    "    \n",
    "    return wav\n",
    "\n",
    "def load_hifigan_model(ckpt_path):\n",
    "    hifigan_model = hifigan_ssl.HifiGanModel.load_from_checkpoint(ckpt_path)\n",
    "    hifigan_model = hifigan_model.cpu()\n",
    "    hifigan_model.eval()\n",
    "    clear_output()\n",
    "    return hifigan_model\n",
    "\n",
    "def load_fastpitch_model(ckpt_path):\n",
    "    fastpitch_model = fastpitch_ssl.FastPitchModel_SSL.load_from_checkpoint(ckpt_path)\n",
    "    fastpitch_model = fastpitch_model.cpu()\n",
    "    fastpitch_model.eval()\n",
    "    fastpitch_model.non_trainable_models = {'vocoder' : vocoder}\n",
    "    clear_output()\n",
    "    return fastpitch_model\n",
    "    \n",
    "def find_latest_ckpt_path(experiment_dir):\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(experiment_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\"last.ckpt\"):\n",
    "                return os.path.join(root, filename)\n",
    "            \n",
    "def segment_wav(wav, segment_length=44100, hop_size=44100, min_segment_size=22050):\n",
    "    if len(wav) < segment_length:\n",
    "        pad = torch.zeros(segment_length - len(wav))\n",
    "        segment = torch.cat([wav, pad])\n",
    "        return [segment]\n",
    "    else:\n",
    "        si = 0\n",
    "        segments = []\n",
    "        while si < len(wav) - min_segment_size:\n",
    "            segment = wav[si:si+segment_length]\n",
    "#             print(\"Segment\", si)\n",
    "#             IPython.display.display(IPython.display.Audio(segment, rate=22050))\n",
    "            if len(segment) < segment_length:\n",
    "                pad = torch.zeros(segment_length - len(segment))\n",
    "                segment = torch.cat([segment, pad])\n",
    "                \n",
    "            segments.append(segment)\n",
    "            si += hop_size\n",
    "        return segments\n",
    "\n",
    "def get_speaker_stats(ssl_model, wav_featurizer, audio_paths):\n",
    "    all_segments = []\n",
    "    all_wavs = []\n",
    "    for audio_path in audio_paths:\n",
    "        wav = load_wav(audio_path)\n",
    "        segments = segment_wav(wav)\n",
    "        all_segments += segments\n",
    "        all_wavs.append(wav)\n",
    "    \n",
    "    signal_batch = torch.stack(all_segments)\n",
    "    #print(\"signal batch\", signal_batch.shape)\n",
    "    signal_length_batch = torch.stack( [ torch.tensor(signal_batch.shape[1]) for _i in range(len(all_segments)) ] )\n",
    "    #print(\"signal length\", signal_length_batch.shape)\n",
    "    _, speaker_embeddings, _, _, _ = ssl_model.forward_for_export(\n",
    "                    input_signal=signal_batch, input_signal_length=signal_length_batch, normalize_content=True\n",
    "                )\n",
    "    \n",
    "    speaker_embedding = torch.mean(speaker_embeddings, dim=0)\n",
    "    l2_norm = torch.norm(speaker_embedding, p=2)\n",
    "    speaker_embedding = speaker_embedding/l2_norm\n",
    "    non_zero_pc = []\n",
    "    for wav in all_wavs:\n",
    "        pitch_contour = get_pitch_contour(wav)\n",
    "        pitch_contour_nonzero = pitch_contour[pitch_contour != 0]\n",
    "        non_zero_pc.append(pitch_contour_nonzero)\n",
    "    \n",
    "    non_zero_pc = torch.cat(non_zero_pc)\n",
    "    if len(non_zero_pc) > 0:\n",
    "        pitch_mean = non_zero_pc.mean().item()\n",
    "        pitch_std = non_zero_pc.std().item()\n",
    "    else:\n",
    "        print(\"could not find pitch contour\")\n",
    "        pitch_mean = 212.0\n",
    "        pitch_std = 70.0\n",
    "    \n",
    "    return speaker_embedding[None], pitch_mean, pitch_std\n",
    "        \n",
    "        \n",
    "def get_ssl_features_disentsngled(ssl_model, wav_featurizer, audio_path, emb_type=\"embedding_and_probs\", use_unique_tokens=False):\n",
    "    wav = load_wav(audio_path)\n",
    "    audio_signal = wav[None]\n",
    "    audio_signal_length = torch.tensor( [ wav.shape[0] ])\n",
    "    _, speaker_embedding, content_embedding, content_log_probs, encoded_len = ssl_model.forward_for_export(\n",
    "                    input_signal=audio_signal, input_signal_length=audio_signal_length, normalize_content=True\n",
    "                )\n",
    "    \n",
    "    content_embedding = content_embedding[0,:encoded_len[0].item()]\n",
    "    content_log_probs = content_log_probs[:encoded_len[0].item(),0,:]\n",
    "    content_embedding = content_embedding.t()\n",
    "    content_log_probs = content_log_probs.t()\n",
    "    content_probs = torch.exp(content_log_probs)\n",
    "    \n",
    "    if emb_type == \"probs\":\n",
    "        final_content_embedding = content_probs\n",
    "        \n",
    "    elif emb_type == \"embedding\":\n",
    "        final_content_embedding = content_embedding\n",
    "        \n",
    "    elif emb_type == \"log_probs\":\n",
    "        final_content_embedding = content_log_probs\n",
    "        \n",
    "    elif emb_type == \"embedding_and_probs\":\n",
    "        final_content_embedding = torch.cat([content_embedding, content_probs], dim=0)\n",
    "    \n",
    "    duration = torch.ones(final_content_embedding.shape[1]) * 4.0\n",
    "    if use_unique_tokens:\n",
    "        token_predictions = torch.argmax(content_probs, dim=0)\n",
    "        # print(\"token predictions:\", token_predictions)\n",
    "        content_buffer = [final_content_embedding[:, 0]]\n",
    "        unique_content_embeddings = []\n",
    "        unique_tokens = []\n",
    "        durations = []\n",
    "        for _t in range(1, final_content_embedding.shape[1]):\n",
    "            if token_predictions[_t] == token_predictions[_t - 1]:\n",
    "                content_buffer.append(final_content_embedding[:, _t])\n",
    "            else:\n",
    "                durations.append(len(content_buffer) * 4)\n",
    "                unique_content_embeddings.append(torch.mean(torch.stack(content_buffer), dim=0))\n",
    "                content_buffer = [final_content_embedding[:, _t]]\n",
    "                unique_tokens.append(token_predictions[_t].item())\n",
    "\n",
    "        if len(content_buffer) > 0:\n",
    "            durations.append(len(content_buffer) * 4)\n",
    "            unique_content_embeddings.append(torch.mean(torch.stack(content_buffer), dim=0))\n",
    "            unique_tokens.append(token_predictions[_t].item())\n",
    "\n",
    "        unique_content_embedding = torch.stack(unique_content_embeddings)\n",
    "        final_content_embedding = unique_content_embedding.t()\n",
    "        duration = torch.tensor(durations).float()\n",
    "        \n",
    "    return final_content_embedding[None], speaker_embedding, duration[None]\n",
    "\n",
    "def get_pitch_contour(wav, pitch_mean=None, pitch_std=None):\n",
    "    f0, _, _ = librosa.pyin(\n",
    "        wav.numpy(),\n",
    "        fmin=librosa.note_to_hz('C2'),\n",
    "        fmax=librosa.note_to_hz('C7'),\n",
    "        frame_length=1024,\n",
    "        hop_length=256,\n",
    "        sr=22050,\n",
    "        center=True,\n",
    "        fill_na=0.0,\n",
    "    )\n",
    "    pitch_contour = torch.tensor(f0, dtype=torch.float32)\n",
    "    if (pitch_mean is not None) and (pitch_std is not None):\n",
    "        pitch_contour = pitch_contour - pitch_mean\n",
    "        pitch_contour[pitch_contour == -pitch_mean] = 0.0\n",
    "        pitch_contour = pitch_contour / pitch_std\n",
    "        \n",
    "    return pitch_contour\n",
    "    \n",
    "    \n",
    "def vocode_ssl_features_disentangled(hifigan_model, content_embedding, speaker_embedding, pitch_contour=None, compute_pitch=True):\n",
    "    wav_generated = hifigan_model.synthesize_wav(content_embedding, speaker_embedding, pitch_contour, compute_pitch)\n",
    "    return wav_generated\n",
    "\n",
    "def load_speaker_wise_audio_paths(speaker_type=\"seen\"):\n",
    "    if speaker_type == \"seen\":\n",
    "        manifest_path = \"/home/pneekhara/NeMo2022/libri_val_formatted.json\"\n",
    "        speaker_wise_audio_paths = {}\n",
    "        with open(manifest_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                record = json.loads(line)\n",
    "                if record['speaker'] not in speaker_wise_audio_paths:\n",
    "                    speaker_wise_audio_paths[record['speaker']] = []\n",
    "                speaker_wise_audio_paths[record['speaker']].append(record['audio_filepath'])\n",
    "            \n",
    "            filtered_paths = {}\n",
    "            for key in speaker_wise_audio_paths:\n",
    "                if len(speaker_wise_audio_paths[key]) > 1:\n",
    "                    filtered_paths[key] = speaker_wise_audio_paths[key]\n",
    "            return filtered_paths\n",
    "    elif speaker_type == \"vctk\":\n",
    "        manifest_path = \"/home/pneekhara/Datasets/vctk/vctk_test_local.json\"\n",
    "        speaker_wise_audio_paths = {}\n",
    "        with open(manifest_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                record = json.loads(line)\n",
    "                if record['speaker'] not in speaker_wise_audio_paths:\n",
    "                    speaker_wise_audio_paths[record['speaker']] = []\n",
    "                speaker_wise_audio_paths[record['speaker']].append(record['audio_filepath'])\n",
    "            \n",
    "            filtered_paths = {}\n",
    "            for key in speaker_wise_audio_paths:\n",
    "                if len(speaker_wise_audio_paths[key]) > 1:\n",
    "                    filtered_paths[key] = speaker_wise_audio_paths[key][:10]\n",
    "            return filtered_paths\n",
    "    elif speaker_type == \"bengali\":\n",
    "        manifest_path = \"/home/pneekhara/Datasets/BengaliData/bengali_manifest.json\"\n",
    "        speaker_wise_audio_paths = {}\n",
    "        with open(manifest_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                record = json.loads(line)\n",
    "                if record['speaker'] not in speaker_wise_audio_paths:\n",
    "                    speaker_wise_audio_paths[record['speaker']] = []\n",
    "                speaker_wise_audio_paths[record['speaker']].append(record['audio_filepath'])\n",
    "            \n",
    "            filtered_paths = {}\n",
    "            for key in speaker_wise_audio_paths:\n",
    "                if len(speaker_wise_audio_paths[key]) > 1:\n",
    "                    filtered_paths[key] = speaker_wise_audio_paths[key][:10]\n",
    "            return filtered_paths\n",
    "        \n",
    "    else:\n",
    "        manifest_path = \"/home/pneekhara/Datasets/LibriDev/libri_dev_clean_local.json\"\n",
    "        speaker_wise_audio_paths = {}\n",
    "        with open(manifest_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                record = json.loads(line)\n",
    "                if record['speaker'] not in speaker_wise_audio_paths:\n",
    "                    speaker_wise_audio_paths[record['speaker']] = []\n",
    "                speaker_wise_audio_paths[record['speaker']].append(record['audio_filepath'])\n",
    "            \n",
    "            filtered_paths = {}\n",
    "            spk_count = 0\n",
    "            for key in speaker_wise_audio_paths:\n",
    "                if len(speaker_wise_audio_paths[key]) > 1:\n",
    "                    filtered_paths[key] = speaker_wise_audio_paths[key][:10]\n",
    "                    spk_count += 1\n",
    "                    if spk_count >= 10:\n",
    "                        break\n",
    "            return filtered_paths\n",
    "\n",
    "\n",
    "def load_speaker_stats(speaker_wise_paths, speaker_type=\"seen\", recache=False):\n",
    "    pickle_path = \"{}_speaker_stats.pkl\".format(speaker_type)\n",
    "    if os.path.exists(pickle_path) and not recache:\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    speaker_stats = {}\n",
    "    pitch_stats = {}\n",
    "    if speaker_type == \"seen\":\n",
    "        speaker_stats_fp = \"/home/pneekhara/NeMo2022/libri_speaker_stats.json\"\n",
    "        with open(speaker_stats_fp, \"r\") as f:\n",
    "            pitch_stats = json.loads(f.read())\n",
    "        \n",
    "    for speaker in speaker_wise_paths:\n",
    "        print(\"computing stats for {}\".format(speaker))\n",
    "        speaker_embedding, pitch_mean, pitch_std = get_speaker_stats(ssl_model, wav_featurizer, speaker_wise_paths[speaker])\n",
    "        speaker_stats[speaker] = {\n",
    "            'speaker_embedding' : speaker_embedding,\n",
    "            'pitch_mean' : pitch_mean,\n",
    "            'pitch_std' : pitch_std\n",
    "        }\n",
    "        if str(speaker) in pitch_stats:\n",
    "            speaker_stats[speaker][\"pitch_mean\"] = pitch_stats[str(speaker)][\"pitch_mean\"]\n",
    "            speaker_stats[speaker][\"pitch_std\"] = pitch_stats[str(speaker)][\"pitch_std\"]\n",
    "\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(speaker_stats, f)\n",
    "    \n",
    "    return speaker_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_wise_paths_english = load_speaker_wise_audio_paths(\"unseen\")\n",
    "stats_unseen = load_speaker_stats(speaker_wise_paths_english, speaker_type=\"unseen\", recache=False)\n",
    "\n",
    "speaker_wise_paths = load_speaker_wise_audio_paths(\"bengali\")\n",
    "stats = load_speaker_stats(speaker_wise_paths, speaker_type=\"bengali\", recache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_wise_paths = {**speaker_wise_paths, **speaker_wise_paths_english}\n",
    "stats = {**stats, **stats_unseen}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5caffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_audio(fastpitch_model, speaker_stats, speaker_wise_paths, pitch_conditioning=True, compute_pitch=False, compute_duration=False, use_unique_tokens=False, n_audio=1, n_speakers=10):\n",
    "    spk_count = 0\n",
    "    for speaker in speaker_wise_paths:\n",
    "        spk_count+=1\n",
    "        if spk_count > n_speakers:\n",
    "            break\n",
    "        speaker_stat = speaker_stats[speaker]\n",
    "        for wav_path in speaker_wise_paths[speaker][:n_audio]:\n",
    "            content_embedding, _, duration = get_ssl_features_disentsngled(ssl_model, wav_featurizer, wav_path, emb_type=\"embedding_and_probs\", use_unique_tokens=use_unique_tokens)\n",
    "            pitch_contour = get_pitch_contour( load_wav(wav_path), pitch_mean=speaker_stat[\"pitch_mean\"], pitch_std=speaker_stat[\"pitch_std\"] )[None]\n",
    "            \n",
    "            print(\"Original Audio Speaker {}\".format(speaker))\n",
    "            IPython.display.display(IPython.display.Audio(load_wav(wav_path), rate=22050))\n",
    "            with torch.no_grad():\n",
    "                print(\"Reconstructed Audio Speaker {}\".format(speaker))\n",
    "                wav_generated = fastpitch_model.synthesize_wav(content_embedding, speaker_stat['speaker_embedding'], pitch_contour=pitch_contour, compute_pitch=compute_pitch,compute_duration=compute_duration, durs_gt=duration)\n",
    "                IPython.display.display(IPython.display.Audio(wav_generated[0], rate=22050))\n",
    "            print(\"**************************\")\n",
    "            \n",
    "def swap_speakers(fastpitch_model, speaker_stats, speaker_wise_paths, spk1, spk2, pitch_conditioning=True, compute_pitch=False, compute_duration=False, use_unique_tokens=False, n_audio=1, n_speakers=10):\n",
    "    wav_path1 = speaker_wise_paths[spk1][0]\n",
    "    wav_path2 = speaker_wise_paths[spk2][0]\n",
    "    \n",
    "    speaker_embedding1 =speaker_stats[spk1][\"speaker_embedding\"]\n",
    "    speaker_embedding2 =speaker_stats[spk2][\"speaker_embedding\"]\n",
    "    \n",
    "    content_embedding1, _, duration1 = get_ssl_features_disentsngled(ssl_model, wav_featurizer, wav_path1, emb_type=\"embedding_and_probs\", use_unique_tokens=use_unique_tokens)\n",
    "    content_embedding2, _, duration2 = get_ssl_features_disentsngled(ssl_model, wav_featurizer, wav_path2, emb_type=\"embedding_and_probs\", use_unique_tokens=use_unique_tokens)\n",
    "    \n",
    "    pitch_contour1 = get_pitch_contour( load_wav(wav_path1), pitch_mean=speaker_stats[spk1][\"pitch_mean\"], pitch_std=speaker_stats[spk1][\"pitch_std\"] )[None]\n",
    "    pitch_contour2 = get_pitch_contour( load_wav(wav_path2), pitch_mean=speaker_stats[spk2][\"pitch_mean\"], pitch_std=speaker_stats[spk2][\"pitch_std\"] )[None]\n",
    "    \n",
    "    print(\"Real Audio Speaker {}\".format(spk1))\n",
    "    IPython.display.display(IPython.display.Audio(load_wav(wav_path1), rate=22050))\n",
    "    \n",
    "    print(\"Real Audio Speaker {}\".format(spk2))\n",
    "    IPython.display.display(IPython.display.Audio(load_wav(wav_path2), rate=22050))\n",
    "    \n",
    "    print(\"Content of {}, Voice of {}\".format(spk1, spk2))\n",
    "    wav_generated = fastpitch_model.synthesize_wav(content_embedding1, speaker_embedding2, pitch_contour=pitch_contour1, compute_pitch=compute_pitch,compute_duration=compute_duration, durs_gt=duration1)\n",
    "    IPython.display.display(IPython.display.Audio(wav_generated[0], rate=22050))\n",
    "    \n",
    "    print(\"Content of {}, Voice of {}\".format(spk2, spk1))\n",
    "    wav_generated = fastpitch_model.synthesize_wav(content_embedding2, speaker_embedding1, pitch_contour=pitch_contour2, compute_pitch=compute_pitch,compute_duration=compute_duration, durs_gt=duration2)\n",
    "    IPython.display.display(IPython.display.Audio(wav_generated[0], rate=22050))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = \"/home/pneekhara/NeMo2022/tensorboards/FastPitch/ConstLR_PitchConditioningWithEncEpoch37/Epoch319.ckpt\"\n",
    "ckpt_path = \"/home/pneekhara/NeMo2022/tensorboards/FastPitch/DurationPredictor/SegDurPerSampleEpoch604.ckpt\"\n",
    "fasptich_model = load_fastpitch_model(ckpt_path)\n",
    "# reconstruct_audio(fasptich_model, stats, speaker_wise_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efaa1d",
   "metadata": {},
   "source": [
    "## Reconstructed using Predicted Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_audio(fasptich_model, stats, speaker_wise_paths, compute_pitch=False, compute_duration=False, use_unique_tokens=False, n_audio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e9001",
   "metadata": {},
   "source": [
    "## Swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_wise_paths.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_speakers(fasptich_model, stats, speaker_wise_paths, '00737', '1673', compute_pitch=True, compute_duration=True, use_unique_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_speakers(fasptich_model, stats, speaker_wise_paths, 283, 616, compute_pitch=True, compute_duration=False, use_unique_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef958670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo2022",
   "language": "python",
   "name": "nemo2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
