{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ccdc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import MagpieTTSModel\n",
    "from nemo.collections.tts.data.text_to_speech_dataset import MagpieTTSDataset, DatasetSample\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5798ac",
   "metadata": {},
   "source": [
    "### Checkpoint Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = \"/datap/misc/experiment_checkpoints/JuneRelease/cer_ssim_priornull_hparams.yaml\"\n",
    "checkpoint_file = \"/datap/misc/experiment_checkpoints/JuneRelease/cer_ssim_priornull_epoch1.ckpt\"\n",
    "# codecmodel_path = \"/datap/misc/checkpoints/AudioCodec_21Hz_no_eliz.nemo\"\n",
    "codecmodel_path = \"/datap/misc/checkpoints/21fps_causal_codecmodel.nemo\"\n",
    "\n",
    "\n",
    "# Temp out dir for saving audios\n",
    "out_dir = \"/datap/misc/t5tts_inference_notebook_samples\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf2a16",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf66f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def update_config(model_cfg, codecmodel_path, legacy_codebooks=False):\n",
    "    ''' helper function to rename older yamls from t5 to magpie '''\n",
    "    model_cfg.codecmodel_path = codecmodel_path\n",
    "    if hasattr(model_cfg, 'text_tokenizer'):\n",
    "        # Backward compatibility for models trained with absolute paths in text_tokenizer\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_dict = \"scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt\"\n",
    "        model_cfg.text_tokenizer.g2p.heteronyms = \"scripts/tts_dataset_files/heteronyms-052722\"\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_probability = 1.0\n",
    "    model_cfg.train_ds = None\n",
    "    model_cfg.validation_ds = None\n",
    "    if \"t5_encoder\" in model_cfg:\n",
    "        model_cfg.encoder = model_cfg.t5_encoder\n",
    "        del model_cfg.t5_encoder\n",
    "    if \"t5_decoder\" in model_cfg:\n",
    "        model_cfg.decoder = model_cfg.t5_decoder\n",
    "        del model_cfg.t5_decoder\n",
    "    if hasattr(model_cfg, 'decoder') and hasattr(model_cfg.decoder, 'prior_eps'):\n",
    "        # Added to prevent crash after removing arg from transformer_2501.py in https://github.com/blisc/NeMo/pull/56\n",
    "        del model_cfg.decoder.prior_eps\n",
    "    if hasattr(model_cfg, 'use_local_transformer') and model_cfg.use_local_transformer:\n",
    "        # For older checkpoints trained with a different parameter name\n",
    "        model_cfg.local_transformer_type = \"autoregressive\"\n",
    "        del model_cfg.use_local_transformer\n",
    "\n",
    "    if legacy_codebooks:\n",
    "        # Added to address backward compatibility arising from\n",
    "        #  https://github.com/blisc/NeMo/pull/64\n",
    "        print(\"WARNING: Using legacy codebook indices for backward compatibility. Should only be used with old checkpoints.\")\n",
    "        num_audio_tokens_per_codebook = model_cfg.num_audio_tokens_per_codebook\n",
    "        model_cfg.forced_num_all_tokens_per_codebook = num_audio_tokens_per_codebook\n",
    "        model_cfg.forced_audio_eos_id = num_audio_tokens_per_codebook - 1\n",
    "        model_cfg.forced_audio_bos_id = num_audio_tokens_per_codebook - 2\n",
    "        if model_cfg.model_type == 'decoder_context_tts':\n",
    "            model_cfg.forced_context_audio_eos_id = num_audio_tokens_per_codebook - 3\n",
    "            model_cfg.forced_context_audio_bos_id = num_audio_tokens_per_codebook - 4\n",
    "            model_cfg.forced_mask_token_id = num_audio_tokens_per_codebook - 5\n",
    "        else:\n",
    "            model_cfg.forced_context_audio_eos_id = num_audio_tokens_per_codebook - 1\n",
    "            model_cfg.forced_context_audio_bos_id = num_audio_tokens_per_codebook - 2\n",
    "    if hasattr(model_cfg, 'sample_rate'):\n",
    "        # This was removed from the config and is now in the model class\n",
    "        sample_rate = model_cfg.sample_rate\n",
    "        del model_cfg.sample_rate\n",
    "    else:\n",
    "        sample_rate = None\n",
    "    return model_cfg, sample_rate\n",
    "    \n",
    "\n",
    "model_cfg = OmegaConf.load(hparams_file).cfg\n",
    "\n",
    "with open_dict(model_cfg):\n",
    "    model_cfg, cfg_sample_rate = update_config(model_cfg, codecmodel_path, False)\n",
    "\n",
    "\n",
    "model = MagpieTTSModel(cfg=model_cfg)\n",
    "print(\"Loading weights from checkpoint\")\n",
    "ckpt = torch.load(checkpoint_file, weights_only=False)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "print(\"Loaded weights.\")\n",
    "\n",
    "model.use_kv_cache_for_inference = True\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b5711",
   "metadata": {},
   "source": [
    "### Initialize Dataset class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_duration_min = model.cfg.get('context_duration_min', 5.0)\n",
    "context_duration_max = model.cfg.get('context_duration_max', 5.0)\n",
    "test_dataset = MagpieTTSDataset(\n",
    "    dataset_meta={},\n",
    "    sample_rate=model.sample_rate,\n",
    "    min_duration=0.5,\n",
    "    max_duration=20,\n",
    "    codec_model_samples_per_frame=model.codec_model_samples_per_frame,\n",
    "    bos_id=model.bos_id,\n",
    "    eos_id=model.eos_id,\n",
    "    context_audio_bos_id=model.context_audio_bos_id,\n",
    "    context_audio_eos_id=model.context_audio_eos_id,\n",
    "    audio_bos_id=model.audio_bos_id,\n",
    "    audio_eos_id=model.audio_eos_id,\n",
    "    num_audio_codebooks=model.num_audio_codebooks,\n",
    "    prior_scaling_factor=None,\n",
    "    load_cached_codes_if_available=False,\n",
    "    dataset_type='test',\n",
    "    tokenizer_config=None,\n",
    "    load_16khz_audio=model.model_type == 'single_encoder_sv_tts',\n",
    "    use_text_conditioning_tokenizer=model.use_text_conditioning_encoder,\n",
    "    pad_context_text_to_max_duration=model.pad_context_text_to_max_duration,\n",
    "    context_duration_min=context_duration_min,\n",
    "    context_duration_max=context_duration_max,\n",
    ")\n",
    "test_dataset.text_tokenizer = model.tokenizer\n",
    "test_dataset.text_conditioning_tokenizer = model.text_conditioning_tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    with sf.SoundFile(file_path) as audio_file:\n",
    "        # Calculate the duration\n",
    "        duration = len(audio_file) / audio_file.samplerate\n",
    "        return duration\n",
    "\n",
    "def create_record(text, context_audio_filepath=None, context_text=None):\n",
    "    dummy_audio_fp = os.path.join(out_dir, \"dummy_audio.wav\")\n",
    "    dummy_audio = sf.write(dummy_audio_fp, np.zeros(22050 * 3), 22050)  # 3 seconds of silence\n",
    "    record = {\n",
    "        'audio_filepath' : dummy_audio_fp,\n",
    "        'duration': 3.0,\n",
    "        'text': text,\n",
    "        'speaker': \"dummy\",\n",
    "    }\n",
    "    if context_text is not None:\n",
    "        assert context_audio_filepath is None\n",
    "        record['context_text'] = context_text\n",
    "    else:\n",
    "        assert context_audio_filepath is not None\n",
    "        record['context_audio_filepath'] = context_audio_filepath\n",
    "        record['context_audio_duration'] = get_audio_duration(context_audio_filepath)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa7a5a",
   "metadata": {},
   "source": [
    "### Set transcript and context pairs to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7374d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# Change sample text and prompt audio/text here\n",
    "\n",
    " \n",
    " # Let me confirm one, one, one, one, one, x, two, two two, two two, two, one, two, four, four, h t t p, four, five, six, seven, eight. Is that correct?\n",
    " # Let me confirm S D S D two, two, two, two, one, two, four, four, h t t p, four, five, six, seven, eight. Is that correct?\n",
    " # His phone number is one one one one, nine nine nine nine nine, nine nine, nine, right\n",
    "\n",
    "audio_base_dir = \"/\"\n",
    "test_entries = [\n",
    "    create_record(\n",
    "        text=\" one, two, two, three, three, three, four, four, four, four, five, five, five, five, five, six, six, six, six, six, six.\",\n",
    "        context_audio_filepath=\"/datap/misc/LibriTTSfromNemo/LibriTTS/test-clean/8230/279154/8230_279154_000004_000009.wav\",\n",
    "    ),\n",
    "    create_record(\n",
    "        text=\" Let me confirm that number: two, one, two, four, four, four, five, six, seven, eight. Is that correct?\",\n",
    "        context_audio_filepath=\"/datap/misc/LibriTTSfromNemo/LibriTTS/test-clean/8230/279154/8230_279154_000004_000009.wav\",\n",
    "    ),\n",
    "    create_record(\n",
    "        text=\" Let me confirm S D S D two, two, two, two, one, two, four, four, h t t p, four, five, six, seven, eight. Is that correct?\",\n",
    "        context_audio_filepath=\"/datap/misc/LibriTTSfromNemo/LibriTTS/test-clean/8230/279154/8230_279154_000004_000009.wav\",\n",
    "    ),\n",
    "    create_record(\n",
    "        text=\" His phone number is one one one one, nine nine nine nine nine, nine nine, nine, right\",\n",
    "        context_audio_filepath=\"/datap/misc/LibriTTSfromNemo/LibriTTS/test-clean/8230/279154/8230_279154_000004_000009.wav\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "data_samples = []\n",
    "for entry in test_entries:\n",
    "    dataset_sample = DatasetSample(\n",
    "        dataset_name=\"sample\",\n",
    "        manifest_entry=entry,\n",
    "        audio_dir=audio_base_dir,\n",
    "        feature_dir=audio_base_dir,\n",
    "        text=entry['text'],\n",
    "        speaker=None,\n",
    "        speaker_index=0,\n",
    "        tokenizer_names=[\"english_phoneme\"], # Change this for multilingual: \"english_phoneme\", \"spanish_phoneme\", \"english_chartokenizer\", \"german_chartokenizer\".. \n",
    "    )\n",
    "    data_samples.append(dataset_sample)\n",
    "    \n",
    "test_dataset.data_samples = data_samples\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9866b",
   "metadata": {},
   "source": [
    "### Generate With Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "item_idx = 0\n",
    "for bidx, batch in enumerate(test_data_loader):\n",
    "    print(\"Processing batch {} out of {}\".format(bidx, len(test_data_loader)))\n",
    "    model.decoder.reset_cache(use_cache=True)\n",
    "    batch_cuda ={}\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch_cuda[key] = batch[key].cuda()\n",
    "        else:\n",
    "            batch_cuda[key] = batch[key]\n",
    "    import time\n",
    "    st = time.time()\n",
    "    \n",
    "    for _ in range(1):\n",
    "        for use_local_transformer_for_inference in [False]:\n",
    "            for apply_prior in [True]:\n",
    "                predicted_audio, predicted_audio_lens, _, _, rtf_metrics, cross_attn_np, all_heads_attn_np = model.infer_batch(\n",
    "                    batch_cuda, \n",
    "                    max_decoder_steps=430, \n",
    "                    temperature=0.6,\n",
    "                    topk=80,\n",
    "                    use_cfg=True,\n",
    "                    cfg_scale=2.5,\n",
    "                    prior_epsilon=0.0,\n",
    "                    lookahead_window_size=5,\n",
    "                    return_cross_attn_probs=True,\n",
    "                    estimate_alignment_from_layers=[4,6,10],\n",
    "                    apply_attention_prior=apply_prior,\n",
    "                    apply_prior_to_layers=[4,6,10],\n",
    "                    compute_all_heads_attn_maps=True,\n",
    "                    start_prior_after_n_audio_steps=0,\n",
    "                    use_local_transformer_for_inference=use_local_transformer_for_inference\n",
    "                )\n",
    "                print(\"generation time\", time.time() - st)\n",
    "                pprint.pprint(rtf_metrics)\n",
    "                for idx in range(predicted_audio.size(0)):\n",
    "                    predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "                    predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "                    audio_path = os.path.join(out_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "                    sf.write(audio_path, predicted_audio_np, model.sample_rate)\n",
    "                    print(test_entries[bidx]['text'])\n",
    "                    print(\"Prior Used?\", apply_prior)\n",
    "                    print(\"use_local_transformer\", use_local_transformer_for_inference)\n",
    "                    display(Audio(audio_path))\n",
    "                    item_idx += 1\n",
    "                    plt.imshow(cross_attn_np[idx])\n",
    "                    plt.show()\n",
    "                    # for hidx, head_cross_attn in enumerate(all_heads_attn_np[idx]):\n",
    "                    #     layer_num = hidx // model.cfg.decoder.xa_n_heads\n",
    "                    #     head_num = hidx % model.cfg.decoder.xa_n_heads\n",
    "                    #     print(\"item, layer, head\", idx, layer_num, head_num)\n",
    "                    #     plt.imshow(all_heads_attn_np[idx][hidx])\n",
    "                    #     plt.show()\n",
    "\n",
    "                print(\"------------------------------------\")\n",
    "                print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb9800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dab52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
