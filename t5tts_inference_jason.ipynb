{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import scripts.magpietts.evalset_config as evalset_config\n",
    "import scripts.magpietts.evaluate_generated_audio as evaluate_generated_audio\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "from PIL import Image\n",
    "import IPython.display as ipd\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest\n",
    "from nemo.collections.tts.data.text_to_speech_dataset import MagpieTTSDataset\n",
    "from nemo.collections.tts.models import MagpieTTSModel\n",
    "from nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers import AggregatedTTSTokenizer, IPATokenizer\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5798ac",
   "metadata": {},
   "source": [
    "### Checkpoint Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "codecmodel_path = \"/home/jasoli/models/NanoCodec_21FPS.nemo\"\n",
    "# nemo_file = \"/home/jasoli/experiments/nemo_experiments/magpieTTS_en_release_2505/MP_CE_dc_NC21FPS_8cb_lhotseDatasets2505_lr2e-4_tbd700_vbd300_precisionbf16-mixed_0/magpieTTS/checkpoints/magpieTTS.nemo\"\n",
    "nemo_file = \"/home/jasoli/experiments/nemo_experiments/magpieTTS_en_release_2505/eos_CausalTextEncoderDecoderAudioContext_audioCodec21fpsCausalDecoder_32_headDim128_lhotseDatasets2505_tbd500_vbd300_tnw6_vnw2_preComputeBinsTemp0.8/magpieTTS.nemo\"\n",
    "\n",
    "def update_config(model_cfg, codecmodel_path, legacy_codebooks=False):\n",
    "    ''' helper function to rename older yamls from t5 to magpie '''\n",
    "    model_cfg.codecmodel_path = codecmodel_path\n",
    "    if hasattr(model_cfg, 'text_tokenizer'):\n",
    "        # Backward compatibility for models trained with absolute paths in text_tokenizer\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_dict = \"scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt\"\n",
    "        model_cfg.text_tokenizer.g2p.heteronyms = \"scripts/tts_dataset_files/heteronyms-052722\"\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_probability = 1.0\n",
    "    model_cfg.train_ds = None\n",
    "    model_cfg.validation_ds = None\n",
    "    if \"t5_encoder\" in model_cfg:\n",
    "        model_cfg.encoder = model_cfg.t5_encoder\n",
    "        del model_cfg.t5_encoder\n",
    "    if \"t5_decoder\" in model_cfg:\n",
    "        model_cfg.decoder = model_cfg.t5_decoder\n",
    "        del model_cfg.t5_decoder\n",
    "    if hasattr(model_cfg, 'decoder') and hasattr(model_cfg.decoder, 'prior_eps'):\n",
    "        # Added to prevent crash after removing arg from transformer_2501.py in https://github.com/blisc/NeMo/pull/56\n",
    "        del model_cfg.decoder.prior_eps\n",
    "    if legacy_codebooks:\n",
    "        # Added to address backward compatibility arising from\n",
    "        #  https://github.com/blisc/NeMo/pull/64\n",
    "        print(\"WARNING: Using legacy codebook indices for backward compatibility. Should only be used with old checkpoints.\")\n",
    "        num_audio_tokens_per_codebook = model_cfg.num_audio_tokens_per_codebook\n",
    "        model_cfg.forced_num_all_tokens_per_codebook = num_audio_tokens_per_codebook\n",
    "        model_cfg.forced_audio_eos_id = num_audio_tokens_per_codebook - 1\n",
    "        model_cfg.forced_audio_bos_id = num_audio_tokens_per_codebook - 2\n",
    "        if model_cfg.model_type == 'decoder_context_tts':\n",
    "            model_cfg.forced_context_audio_eos_id = num_audio_tokens_per_codebook - 3\n",
    "            model_cfg.forced_context_audio_bos_id = num_audio_tokens_per_codebook - 4\n",
    "            model_cfg.forced_mask_token_id = num_audio_tokens_per_codebook - 5\n",
    "        else:\n",
    "            model_cfg.forced_context_audio_eos_id = num_audio_tokens_per_codebook - 1\n",
    "            model_cfg.forced_context_audio_bos_id = num_audio_tokens_per_codebook - 2\n",
    "\n",
    "    return model_cfg\n",
    "\n",
    "model_cfg = MagpieTTSModel.restore_from(nemo_file, return_config=True)\n",
    "with open_dict(model_cfg):\n",
    "    model_cfg = update_config(model_cfg, codecmodel_path, False)\n",
    "model = MagpieTTSModel.restore_from(nemo_file, override_config_path=model_cfg)\n",
    "model.use_kv_cache_for_inference = True\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b5711",
   "metadata": {},
   "source": [
    "### Initialize Dataset class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta_info = {\n",
    "    'manifest_path' : '/home/jasoli/data_prime/manifests/test_clean_jr.json',\n",
    "    'audio_dir' : '/mnt/drive1/data/LibriTTS/',\n",
    "    'feature_dir' : None,\n",
    "}\n",
    "dataset_meta = {'test': dataset_meta_info}\n",
    "context_durration_min = model.cfg.get('context_duration_min', 5.0)\n",
    "context_durration_max = model.cfg.get('context_duration_max', 5.0)\n",
    "if context_durration_min < 5.0 and context_durration_max > 5.0:\n",
    "    context_durration_min = 5.0\n",
    "    context_durration_max = 5.0 # @pneekhara - For multiencoder models, I want fixed size contexts for fair eval. Not too important though.\n",
    "test_dataset = MagpieTTSDataset(\n",
    "    dataset_meta=dataset_meta,\n",
    "    sample_rate=model.sample_rate,\n",
    "    min_duration=0.5,\n",
    "    max_duration=20,\n",
    "    codec_model_samples_per_frame=model.codec_model_samples_per_frame,\n",
    "    bos_id=model.bos_id,\n",
    "    eos_id=model.eos_id,\n",
    "    context_audio_bos_id=model.context_audio_bos_id,\n",
    "    context_audio_eos_id=model.context_audio_eos_id,\n",
    "    audio_bos_id=model.audio_bos_id,\n",
    "    audio_eos_id=model.audio_eos_id,\n",
    "    num_audio_codebooks=model.num_audio_codebooks,\n",
    "    prior_scaling_factor=None,\n",
    "    load_cached_codes_if_available=False,\n",
    "    dataset_type='test',\n",
    "    tokenizer_config=None,\n",
    "    load_16khz_audio=model.model_type == 'single_encoder_sv_tts',\n",
    "    use_text_conditioning_tokenizer=model.use_text_conditioning_encoder,\n",
    "    pad_context_text_to_max_duration=model.pad_context_text_to_max_duration,\n",
    "    context_duration_min=context_durration_min,\n",
    "    context_duration_max=context_durration_max,\n",
    ")\n",
    "test_dataset.text_tokenizer = model.tokenizer\n",
    "# Set phoneme prob = 1 for g2p\n",
    "g2p = None\n",
    "if isinstance(model.tokenizer, AggregatedTTSTokenizer):\n",
    "    g2p = model.tokenizer.tokenizers[\"english_phoneme\"].g2p\n",
    "elif isinstance(model.tokenizer, IPATokenizer):\n",
    "    g2p = model.tokenizer.g2p\n",
    "if g2p is not None:\n",
    "    g2p.phoneme_probability = 1.0\n",
    "test_dataset.text_conditioning_tokenizer = model.text_conditioning_tokenizer\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=2,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for bidx, batch in enumerate(test_data_loader):\n",
    "    batch_cuda ={}\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch_cuda[key] = batch[key].cuda()\n",
    "        else:\n",
    "            batch_cuda[key] = batch[key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa7a5a",
   "metadata": {},
   "source": [
    "### Set transcript and context pairs to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7374d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_attention_prior = True\n",
    "def construct_inference_prior(self, prior_epsilon, cross_attention_scores,\n",
    "                              text_lens, text_time_step_attended, attended_timestep_counter,\n",
    "                              unfinished_texts, finished_texts_counter, end_indices, batch_size):\n",
    "    # Attn prior for the next timestep\n",
    "    _attn_prior = torch.zeros(cross_attention_scores.shape[0], 1, cross_attention_scores.shape[1]) + prior_epsilon\n",
    "    _attn_prior = _attn_prior.to(cross_attention_scores.device)\n",
    "    for bidx in range(cross_attention_scores.shape[0]):\n",
    "        if bidx < batch_size:\n",
    "            _text_len = text_lens[bidx]\n",
    "            _attn_prior[bidx, 0, :] = 1.0\n",
    "            # if text_lens[bidx] <= 5:\n",
    "            #     # Very short sentences, No Prior\n",
    "            #     _attn_prior[bidx, 0, :] = 1.0\n",
    "            # else:\n",
    "            #     # _attn_prior[bidx, 0, max(1, text_time_step_attended[bidx]-2)] = 0.1 # Slight exposure to history for better pronounciation. Not very important.\n",
    "            #     _attn_prior[bidx, 0, max(1, text_time_step_attended[bidx]-2)] = 1.\n",
    "            #     _attn_prior[bidx, 0, max(1, text_time_step_attended[bidx]-1)] = 1.\n",
    "            #     _attn_prior[bidx, 0, text_time_step_attended[bidx]] = 1.\n",
    "            #     _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+1, _text_len - 1) ] = 1.0\n",
    "            #     _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+2, _text_len - 1) ] = 1.\n",
    "            #     _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+3, _text_len - 1) ] = 1.\n",
    "\n",
    "            # Penalize timesteps that have been attended to more than 10 times\n",
    "            for _timestep in attended_timestep_counter[bidx]:\n",
    "                if attended_timestep_counter[bidx][_timestep] >= 10:\n",
    "                    # This means the timestep has been attended to more than 10 times (To avoid getting stuck)\n",
    "                    _attn_prior[bidx, 0, _timestep] = prior_epsilon\n",
    "\n",
    "            unfinished_texts[bidx] = False\n",
    "            if text_time_step_attended[bidx] < text_lens[bidx] - 3:\n",
    "                # This means the sentence has not ended\n",
    "                if bidx not in end_indices:\n",
    "                    unfinished_texts[bidx] = True\n",
    "\n",
    "            if text_time_step_attended[bidx] >= text_lens[bidx] - 5 or bidx in end_indices:\n",
    "                if bidx not in finished_texts_counter:\n",
    "                    finished_texts_counter[bidx] = 0\n",
    "\n",
    "    for bidx in finished_texts_counter:\n",
    "        finished_texts_counter[bidx] += 1\n",
    "        if finished_texts_counter[bidx] > 10:\n",
    "            # This means we have been within the text EOS window for atleast 10 timesteps\n",
    "            # We should allow EOS to be predicted now.\n",
    "            unfinished_texts[bidx] = False\n",
    "\n",
    "    return _attn_prior, unfinished_texts, finished_texts_counter\n",
    "\n",
    "model.construct_inference_prior = construct_inference_prior.__get__(model, MagpieTTSModel)  # Replacing the method for instance1\n",
    "model.get_most_attended_text_timestep = get_most_attended_text_timestep.__get__(model, MagpieTTSModel)  # Replacing the method for instance1\n",
    "\n",
    "predicted_audio, predicted_audio_lens, predicted_codes, predicted_codes_lens, rtf_metrics, cross_attention_maps, headwise_cross_attention_maps  = model.infer_batch(\n",
    "    batch_cuda,\n",
    "    max_decoder_steps=440,\n",
    "    temperature=0.6,\n",
    "    topk=80,\n",
    "    use_cfg=True,\n",
    "    cfg_scale=2.5,\n",
    "    return_cross_attn_probs=True,\n",
    "    apply_attention_prior=apply_attention_prior,\n",
    "    prior_epsilon=0.,\n",
    "    lookahead_window_size=10,\n",
    "    estimate_alignment_from_layers=None,\n",
    "    apply_prior_to_layers=None,\n",
    "    start_prior_after_n_audio_steps=10,\n",
    "    use_local_transformer_for_inference=False,\n",
    "    compute_all_heads_attn_maps=True\n",
    ")\n",
    "for idx in range(predicted_audio.size(0)):\n",
    "    cross_attn_map_image = Image.fromarray(cross_attention_maps[idx])\n",
    "    # cross_attn_map_image.save(os.path.join(audio_dir, f\"cross_attn_map_{item_idx}.png\"))\n",
    "\n",
    "    predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "    predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "    # audio_path = os.path.join(pred_audio_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "    # sf.write(audio_path, predicted_audio_np, model.sample_rate)\n",
    "    # codes_path = os.path.join(pred_audio_dir, f\"predicted_codes_{item_idx}.pt\")\n",
    "    # torch.save(predicted_codes[idx][:predicted_codes_lens[idx]], codes_path)\n",
    "    # codec_file_paths.append(codes_path)\n",
    "    # context_audio_path = manifest_records[item_idx].get('context_audio_filepath', None)\n",
    "    # target_audio_path = manifest_records[item_idx].get('audio_filepath', None)\n",
    "    # if context_audio_path is not None:\n",
    "    #     context_audio_path = os.path.join(dataset_meta_info[dataset]['audio_dir'], context_audio_path)\n",
    "    # if target_audio_path is not None:\n",
    "    #     target_audio_path = os.path.join(dataset_meta_info[dataset]['audio_dir'], target_audio_path)\n",
    "    # if os.path.exists(context_audio_path):\n",
    "    #     shutil.copy(context_audio_path, os.path.join(audio_dir, f\"context_audio_{item_idx}.wav\"))\n",
    "    # if os.path.exists(target_audio_path):\n",
    "    #     shutil.copy(target_audio_path, os.path.join(audio_dir, f\"target_audio_{item_idx}.wav\"))\n",
    "    # item_idx += 1\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 60))\n",
    "    ax.imshow(cross_attn_map_image)\n",
    "    ipd.display(ipd.Audio(predicted_audio_np, rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9866b",
   "metadata": {},
   "source": [
    "### Generate With Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_attention_prior = True\n",
    "def get_most_attended_text_timestep(self, alignment_attention_scores, last_attended_timesteps,\n",
    "                               text_lens, lookahead_window_size, attended_timestep_counter, batch_size):\n",
    "    \"\"\"\n",
    "    Returns the most attended timestep for each batch item\n",
    "    \"\"\"\n",
    "    text_time_step_attended = []\n",
    "    for bidx in range(batch_size):\n",
    "        last_attended_timestep = last_attended_timesteps[-1][bidx]\n",
    "        if attended_timestep_counter[bidx].get(last_attended_timestep, 0) >= 8:\n",
    "            # This is probably an attention sink! Move to the next timestep\n",
    "            last_attended_timestep += 1\n",
    "        window_size = lookahead_window_size\n",
    "        window_end = min(last_attended_timestep + window_size, text_lens[bidx] - 3) # Ignore the last 3 timesteps\n",
    "        item_attention_scores = alignment_attention_scores[bidx,last_attended_timestep:window_end]\n",
    "        if item_attention_scores.size(0) == 0:\n",
    "            # This means the sentence has ended\n",
    "            attended_timestep = text_lens[bidx] - 1\n",
    "        else:\n",
    "            attended_timestep = item_attention_scores.argmax().item() + last_attended_timestep\n",
    "        text_time_step_attended.append(attended_timestep)\n",
    "        attended_timestep_counter[bidx][attended_timestep] = attended_timestep_counter[bidx].get(attended_timestep, 0) + 1\n",
    "    return text_time_step_attended, attended_timestep_counter\n",
    "\n",
    "def construct_inference_prior(self, prior_epsilon, cross_attention_scores,\n",
    "                              text_lens, text_time_step_attended, attended_timestep_counter,\n",
    "                              unfinished_texts, finished_texts_counter, end_indices, batch_size):\n",
    "    # Attn prior for the next timestep\n",
    "    _attn_prior = torch.zeros(cross_attention_scores.shape[0], 1, cross_attention_scores.shape[1]) + prior_epsilon\n",
    "    _attn_prior = _attn_prior.to(cross_attention_scores.device)\n",
    "    for bidx in range(cross_attention_scores.shape[0]):\n",
    "        if bidx < batch_size:\n",
    "            _text_len = text_lens[bidx]\n",
    "            # _attn_prior[bidx, 0, :] = 1.0\n",
    "            # if text_lens[bidx] <= 5:\n",
    "            #     # Very short sentences, No Prior\n",
    "            #     _attn_prior[bidx, 0, :] = 1.0\n",
    "            # else:\n",
    "            #     # _attn_prior[bidx, 0, max(1, text_time_step_attended[bidx]-2)] = 0.1 # Slight exposure to history for better pronounciation. Not very important.\n",
    "            # _attn_prior[bidx, 0, max(1, text_time_step_attended[bidx]-2)] = 1.\n",
    "            _attn_prior[bidx, 0, max(1, text_time_step_attended[bidx]-1)] = 1.\n",
    "            _attn_prior[bidx, 0, text_time_step_attended[bidx]] = 1.\n",
    "            _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+1, _text_len - 1) ] = 1.0\n",
    "            _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+2, _text_len - 1) ] = 1.\n",
    "            _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+3, _text_len - 1) ] = 1.\n",
    "            _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+4, _text_len - 1) ] = 1.\n",
    "            _attn_prior[bidx, 0, min(text_time_step_attended[bidx]+5, _text_len - 1) ] = 1.\n",
    "\n",
    "            # Penalize timesteps that have been attended to more than 10 times\n",
    "            for _timestep in attended_timestep_counter[bidx]:\n",
    "                if attended_timestep_counter[bidx][_timestep] >= 10:\n",
    "                    # This means the timestep has been attended to more than 10 times (To avoid getting stuck)\n",
    "                    _attn_prior[bidx, 0, _timestep] = prior_epsilon\n",
    "\n",
    "            unfinished_texts[bidx] = False\n",
    "            if text_time_step_attended[bidx] < text_lens[bidx] - 3:\n",
    "                # This means the sentence has not ended\n",
    "                if bidx not in end_indices:\n",
    "                    unfinished_texts[bidx] = True\n",
    "\n",
    "            if text_time_step_attended[bidx] >= text_lens[bidx] - 5 or bidx in end_indices:\n",
    "                if bidx not in finished_texts_counter:\n",
    "                    finished_texts_counter[bidx] = 0\n",
    "\n",
    "    for bidx in finished_texts_counter:\n",
    "        finished_texts_counter[bidx] += 1\n",
    "        if finished_texts_counter[bidx] > 10:\n",
    "            # This means we have been within the text EOS window for atleast 10 timesteps\n",
    "            # We should allow EOS to be predicted now.\n",
    "            unfinished_texts[bidx] = False\n",
    "\n",
    "    return _attn_prior, unfinished_texts, finished_texts_counter\n",
    "\n",
    "model.construct_inference_prior = construct_inference_prior.__get__(model, MagpieTTSModel)  # Replacing the method for instance1\n",
    "model.get_most_attended_text_timestep = get_most_attended_text_timestep.__get__(model, MagpieTTSModel)  # Replacing the method for instance1\n",
    "\n",
    "predicted_audio, predicted_audio_lens, predicted_codes, predicted_codes_lens, rtf_metrics, cross_attention_maps, headwise_cross_attention_maps  = model.infer_batch(\n",
    "    batch_cuda,\n",
    "    max_decoder_steps=440,\n",
    "    temperature=0.6,\n",
    "    topk=80,\n",
    "    use_cfg=True,\n",
    "    cfg_scale=2.5,\n",
    "    return_cross_attn_probs=True,\n",
    "    apply_attention_prior=apply_attention_prior,\n",
    "    prior_epsilon=0.,\n",
    "    lookahead_window_size=10,\n",
    "    apply_prior_to_layers=[4,6,10],\n",
    "    estimate_alignment_from_layers=[4,6,10],\n",
    "    start_prior_after_n_audio_steps=0,\n",
    "    use_local_transformer_for_inference=False,\n",
    "    compute_all_heads_attn_maps=True\n",
    ")\n",
    "for idx in range(predicted_audio.size(0)):\n",
    "    cross_attn_map_image = Image.fromarray(cross_attention_maps[idx])\n",
    "    # cross_attn_map_image.save(os.path.join(audio_dir, f\"cross_attn_map_{item_idx}.png\"))\n",
    "\n",
    "    predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "    predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "    # audio_path = os.path.join(pred_audio_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "    # sf.write(audio_path, predicted_audio_np, model.sample_rate)\n",
    "    # codes_path = os.path.join(pred_audio_dir, f\"predicted_codes_{item_idx}.pt\")\n",
    "    # torch.save(predicted_codes[idx][:predicted_codes_lens[idx]], codes_path)\n",
    "    # codec_file_paths.append(codes_path)\n",
    "    # context_audio_path = manifest_records[item_idx].get('context_audio_filepath', None)\n",
    "    # target_audio_path = manifest_records[item_idx].get('audio_filepath', None)\n",
    "    # if context_audio_path is not None:\n",
    "    #     context_audio_path = os.path.join(dataset_meta_info[dataset]['audio_dir'], context_audio_path)\n",
    "    # if target_audio_path is not None:\n",
    "    #     target_audio_path = os.path.join(dataset_meta_info[dataset]['audio_dir'], target_audio_path)\n",
    "    # if os.path.exists(context_audio_path):\n",
    "    #     shutil.copy(context_audio_path, os.path.join(audio_dir, f\"context_audio_{item_idx}.wav\"))\n",
    "    # if os.path.exists(target_audio_path):\n",
    "    #     shutil.copy(target_audio_path, os.path.join(audio_dir, f\"target_audio_{item_idx}.wav\"))\n",
    "    # item_idx += 1\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 60))\n",
    "    ax.imshow(cross_attn_map_image)\n",
    "    ipd.display(ipd.Audio(predicted_audio_np, rate=model.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headwise_cross_attention_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(headwise_cross_attention_maps[0])):\n",
    "    cross_attn_map_image = Image.fromarray(headwise_cross_attention_maps[1][idx])\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 60))\n",
    "    print(idx)\n",
    "    ax.imshow(cross_attn_map_image)\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab96bfc-24ff-4554-a67d-6cd2b25c779e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
