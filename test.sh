#!/bin/bash

python examples/nlp/language_modeling/megatron_gpt_pretraining.py \
        trainer.devices=1 \
        trainer.accelerator=gpu \
        trainer.log_every_n_steps=1 \
        trainer.val_check_interval=2 \
        trainer.limit_val_batches=2 \
        trainer.accumulate_grad_batches=1 \
        trainer.max_steps=3 \
        trainer.precision=16 \
        trainer.gradient_clip_val=1.0 \
        exp_manager.exp_dir=examples/nlp/language_modeling/gpt_pretrain_results \
        ++model.name=megatron_gpt_full_te_layer_autocast \
        model.mcore_gpt=True \
        model.tensor_model_parallel_size=1 \
        model.optim.name=fused_adam \
        model.optim.lr=2e-4 \
        model.optim.sched.warmup_steps=1 \
        model.optim.sched.constant_steps=1 \
        model.optim.sched.min_lr=8e-5 \
        model.max_position_embeddings=128 \
        model.encoder_seq_length=128 \
        model.data.seq_length=128 \
        model.normalization=layernorm1p \
        model.bias_activation_fusion=True \
        model.bias_dropout_add_fusion=True \
        model.num_layers=8 \
        model.hidden_size=256 \
        model.num_attention_heads=8 \
        model.activations_checkpoint_method=null \
        model.activations_checkpoint_granularity=null \
        model.activations_checkpoint_num_layers=null \
        model.data.data_prefix='[1.0, /home/data/test_text_document]' \
        model.data.index_mapping_dir=null \
        ++model.data.add_fim=True \
        ++model.data.fim.extra_tokens.prefix='fim_prefix' \
        ++model.data.fim.extra_tokens.middle='fim_middle' \
        ++model.data.fim.extra_tokens.suffix='fim_suffix' \
        ++model.data.fim.extra_tokens.pad='fim_pad' \
        ++model.data.fim.extra_tokens.eod='endoftext'
