t": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning with Megatron for Languages Other than English.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_0K1lsW1dj9"
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell\n",
        "\n",
        "# install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pC0slAc0h9zN"
      },
      "source": [
        "# If you're not using Colab, you might need to upgrade jupyter notebook to avoid the following error:\n",
        "# 'ImportError: IProgress not found. Please update jupyter and ipywidgets.'\n",
        "\n",
        "! pip install ipywidgets\n",
        "! jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "# Please restart the kernel after running this cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzqD2WDFOIN-"
      },
      "source": [
        "from nemo.collections import nlp as nemo_nlp\n",
        "from nemo.utils.exp_manager import exp_manager\n",
        "\n",
        "import os\n",
        "import wget \n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import zipfile\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daYw_Xll2ZR9"
      },
      "source": [
        "# Task Description\n",
        "**Transfer Learning** is a machine learning technique to use an already trained model on a task to be reused on a second task as a starting point. In this tutorial, we will show how NVIDIA's Megatron and Google's BERT language models that are trained on English language can be used for other languages, e.g., Persian language. The downstream task that we will be using for this purpose will be Named entity recognition (NER). NER is the task of detecting and classifying key information (entities) in text.\n",
        "For example, in a sentence:  `Mary lives in Santa Clara and works at NVIDIA`, we should detect that `Mary` is a person, `Santa Clara` is a location and `NVIDIA` is a company.\n",
        "\n",
        "For more information on transfer learning, please see the following:\n",
        "https://ruder.io/transfer-learning/\n",
        "https://ieeexplore.ieee.org/abstract/document/5288526\n",
        "\n",
        "In this tutorial we will be using Megatron and BERT language models. You can have access to papers of these models using the following: https://arxiv.org/abs/1909.08053, https://arxiv.org/abs/1810.04805\n",
        "\n",
        "To read more about other topics and downstream task that can be done in NeMo, you can see the NeMo's tutorial page here: https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/v1.0.2/starthere/tutorials.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnuziSwJ1yEB"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "In this tutorial we are going to use Persian Peyma dataset for our NER task. It can be downloaded from here: http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/\n",
        "\n",
        "Peyma is a hand annotated Persian corpus for NER task with 709 documents which include 302530 tokens. Using IOB encoding, 41148 tokens are labeld with one of the following name entities and others are labeled with O.   \n",
        "\n",
        "* DAT = Date\n",
        "* LOC = Location\n",
        "* MON = Money\n",
        "* ORG = Organization\n",
        "* PCT = Percentage\n",
        "* PER = Person\n",
        "* TIM = Time\n",
        "\n",
        "Each of these has a label staring with **B** that indicates it is the fist token of the name entity and with **I** for others. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcZ3nb_-SVT"
      },
      "source": [
        "# NeMo Token Classification Data Format\n",
        "\n",
        "[TokenClassification Model](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/token_classification/token_classification_model.py) in NeMo supports NER and other token level classification tasks, as long as the data follows the format specified below. \n",
        "\n",
        "Token Classification Model requires the data to be split into 2 files: \n",
        "* text.txt  \n",
        "* labels.txt. \n",
        "\n",
        "Each line of the **text.txt** file contains text sequences, where words are separated with spaces, i.e.: \n",
        "[WORD] [SPACE] [WORD] [SPACE] [WORD].\n",
        "\n",
        "The **labels.txt** file contains corresponding labels for each word in text.txt, the labels are separated with spaces, i.e.:\n",
        "[LABEL] [SPACE] [LABEL] [SPACE] [LABEL].\n",
        "\n",
        "Example of a text.txt file:\n",
        "```\n",
        "دبیر شورای عالی انقلاب فرهنگی از گنجانده شدن 5 زبان خارجی جدید در برنامه درسی مدارس خبر داد.\n",
        "```\n",
        "Corresponding labels.txt file:\n",
        "```\n",
        "O B_ORG I_ORG I_ORG I_ORG O O O O O O O O O O O O O O \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL58EWkd2ZVb"
      },
      "source": [
        "## Download and preprocess the data¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8HZrDmr12_-"
      },
      "source": [
        "DATA_DIR = \"DATA_DIR\"\n",
        "WORK_DIR = \"WORK_DIR\"\n",
        "MODEL_CONFIG = \"token_classification_config.yaml\"\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z2tCEIXZa90"
      },
      "source": [
        "You can download the Peyma dataset from the following url: http://en.itrc.ac.ir/sites/default/files/pictures/NER.rar\n",
        "\n",
        "After downloading the data, you will see NER.rar. Upload it here; for this purpose, you may use **files** from Google colab:\n",
        "\n",
        "*from google.colab import files*\n",
        "\n",
        "*uploaded = files.upload()*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhUzIeF0Yg0l"
      },
      "source": [
        "Let's extract files from the rar file. It will generate two folders: 300K and 600K. We will only use the first one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y01BdjPRW-7B"
      },
      "source": [
        "!pip install patool\n",
        "import patoolib\n",
        "patoolib.extract_archive(\"NER.rar\", outdir=DATA_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaDgL-sQaX2e"
      },
      "source": [
        "Putting data from all 709 files into a single one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0T4CzJvbBJ4"
      },
      "source": [
        "file_all = DATA_DIR + \"/all_data.txt\"\n",
        "with open(file_all, \"w\") as f1:\n",
        "  for filename in os.listdir(DATA_DIR + \"/NER/300K\"):\n",
        "    #print(filename)\n",
        "    with open(DATA_DIR + \"/NER/300K/\" + filename, \"r\") as f2:\n",
        "      for line in f2:\n",
        "        if (not line.startswith(\"پیام\")):\n",
        "          f1.write(line)\n",
        "        else:\n",
        "          # to scape from extra \\n at the end of each file \n",
        "          f1.write(line + \"\\n\")\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzVuET8HESFB"
      },
      "source": [
        "Now, you need to convert this data into NeMo compatible format before starting the training process. For this purpose, you can run [examples/nlp/token_classification/data/import_from_iob_format.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/token_classification/data/import_from_iob_format.py) on your train and dev files, as follows:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "python examples/nlp/token_classification/data/import_from_iob_format.py --data_file PATH_TO_IOB_FORMAT_DATAFILE, e.g., \"DATA_DIR/all_data.txt\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj0rXbYXbivW"
      },
      "source": [
        "Now we process the data to remove potentially any repeated sentences and then split them into train and dev sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgvnTlqzbq5-"
      },
      "source": [
        "sent_dict = dict()\n",
        "line_removed = dict()\n",
        "line_counter = 0\n",
        "with open(DATA_DIR + \"/text_all_not_repeated.txt\", \"w\") as f1:\n",
        "  with open(DATA_DIR + \"/text_all_data.txt\", \"r\") as f2:\n",
        "    for line in f2:\n",
        "      line_counter += 1\n",
        "      if (not line in sent_dict):\n",
        "        sent_dict[line] = 1\n",
        "        f1.write(line)\n",
        "      else:\n",
        "        line_removed[line_counter] = 1\n",
        "#labels:\n",
        "line_counter = 0\n",
        "with open(DATA_DIR + \"/labels_all_not_repeated.txt\", \"w\") as f1:\n",
        "  with open(DATA_DIR + \"/labels_all_data.txt\", \"r\") as f2:\n",
        "    for line in f2:\n",
        "      line_counter += 1\n",
        "      if(not line_counter in line_removed):\n",
        "        f1.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oHQYsMMbugP"
      },
      "source": [
        "total_data = 8984\n",
        "train_share = 0.85\n",
        "used_lines_train = dict()\n",
        "flag = 1\n",
        "count = 0\n",
        "while flag:\n",
        "  idx = random.randint(1, total_data)\n",
        "  if (not idx in used_lines_train):\n",
        "    used_lines_train[idx] = 1\n",
        "    #print(idx)\n",
        "    count += 1\n",
        "  if (count/total_data > train_share):\n",
        "    flag = 0\n",
        "print(count)\n",
        "print(count/total_data)\n",
        "line_counter = 0\n",
        "with open(DATA_DIR+ \"/text_train.txt\", \"w\") as f1:\n",
        "  with open(DATA_DIR + \"/text_dev.txt\", \"w\") as f2:\n",
        "    with open(DATA_DIR + \"/text_all_not_repeated.txt\", \"r\") as f3:\n",
        "      for line in f3:\n",
        "        line_counter += 1\n",
        "        if (line_counter in used_lines_train):\n",
        "          f1.write(line)\n",
        "        else:\n",
        "          f2.write(line)\n",
        "\n",
        "line_counter = 0\n",
        "with open(DATA_DIR + \"/labels_train.txt\", \"w\") as f1:\n",
        "  with open(DATA_DIR + \"/labels_dev.txt\", \"w\") as f2:\n",
        "    with open(DATA_DIR + \"/labels_all_not_repeated.txt\", \"r\") as f3:\n",
        "      for line in f3:\n",
        "        line_counter += 1\n",
        "        if (line_counter in used_lines_train):\n",
        "          f1.write(line)\n",
        "        else:\n",
        "          f2.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q-GWNwDbzKl"
      },
      "source": [
        "Finally, we remove files that are not needed anymore and also rename the files into those that are assumed by NeMo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II20ustub5BF"
      },
      "source": [
        "os.system(\"rm \" + DATA_DIR + \"/all_data.txt\")\n",
        "os.system(\"rm \" + DATA_DIR + \"/text_all_data.txt\")\n",
        "os.system(\"rm \" + DATA_DIR + \"/labels_all_data.txt\")\n",
        "os.system(\"rm \" + DATA_DIR + \"/text_all_not_repeated.txt\")\n",
        "os.system(\"rm \" + DATA_DIR + \"/labels_all_not_repeated.txt\")\n",
        "os.system(\"rm -r -f \" + DATA_DIR + \"/NER\")\n",
        "os.system(\"rm NER.rar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Ty5_S7Ye8h"
      },
      "source": [
        "Now, the data folder should contain these 4 files:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8vsyh3JZH26"
      },
      "source": [
        "\n",
        "\n",
        "* labels_dev.txt\n",
        "* labels_train.txt\n",
        "* text_dev.txt\n",
        "* text_train.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB0oLE4R9EhJ"
      },
      "source": [
        "! ls -l {DATA_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UDPgadLN6SG"
      },
      "source": [
        "# let's take a look at the data \n",
        "print('Text:')\n",
        "! head -n 5 {DATA_DIR}/text_train.txt\n",
        "\n",
        "print('\\nLabels:')\n",
        "! head -n 5 {DATA_DIR}/labels_train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_whKCxfTMo6Y"
      },
      "source": [
        "# Model configuration\n",
        "\n",
        "Our Named Entity Recognition model is comprised of the pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a Token Classification layer.\n",
        "\n",
        "The model is defined in a config file which declares multiple important sections. They are:\n",
        "- **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
        "\n",
        "- **trainer**: Any argument to be passed to PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1gA8PsJ13MJ"
      },
      "source": [
        "# download the model's configuration file \n",
        "config_dir = WORK_DIR + '/configs/'\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
        "    print('Downloading config file...')\n",
        "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
        "else:\n",
        "    print ('config file is already exists')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3KmWMvSUQw"
      },
      "source": [
        "# this line will print the entire config of the model\n",
        "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
        "print(config_path)\n",
        "config = OmegaConf.load(config_path)\n",
        "print(OmegaConf.to_yaml(config))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCgWzNBkaQLZ"
      },
      "source": [
        "# Fine-tuing the model using Peyma dataset\n",
        "\n",
        "We select one of BERT or Megatron models as starting point and use the above processed Peyma dataset to fine-tune them for the Persian data. Since neither of BERT nor Megatron are trained on Persian data, this will be a good example of transfer learning.\n",
        "\n",
        "## Setting up Data within the config\n",
        "\n",
        "Among other things, the config file contains dictionaries called dataset, train_ds and validation_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
        "\n",
        "We assume that both training and evaluation files are in the same directory and use the default names mentioned during the data download step. \n",
        "So, to start model training, we simply need to specify `model.dataset.data_dir`, like we are going to do below.\n",
        "\n",
        "Also notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user.\n",
        "\n",
        "Let us now add the data directory path to the config.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQHCJN-ZaoLp"
      },
      "source": [
        "# in this tutorial train and dev datasets are located in the same folder, so it is enought to add the path of the data directory to the config\n",
        "config.model.dataset.data_dir = DATA_DIR\n",
        "\n",
        "# if you want to use the full dataset, set NUM_SAMPLES to -1\n",
        "NUM_SAMPLES = 1000\n",
        "config.model.train_ds.num_samples = NUM_SAMPLES\n",
        "config.model.validation_ds.num_samples = NUM_SAMPLES\n",
        "\n",
        "config.trainer.max_epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB96-3sTc3yk"
      },
      "source": [
        "## Building the PyTorch Lightning Trainer\n",
        "\n",
        "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
        "\n",
        "Let's first instantiate a Trainer object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tG4FzZ4Ui60"
      },
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knF6QeQQdMrH"
      },
      "source": [
        "# lets modify some trainer configs\n",
        "# checks if we have GPU available and uses it\n",
        "cuda = 1 if torch.cuda.is_available() else 0\n",
        "config.trainer.gpus = cuda\n",
        "\n",
        "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
        "\n",
        "# for mixed precision training, uncomment the line below (precision should be set to 16 and amp_level to O1):\n",
        "# config.trainer.amp_level = O1\n",
        "\n",
        "# remove distributed training flags\n",
        "config.trainer.accelerator = None\n",
        "\n",
        "# setup max number of steps to reduce training time for demonstration purposes of this tutorial\n",
        "config.trainer.max_steps = 32\n",
        "\n",
        "trainer = pl.Trainer(**config.trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IlEMdVxdr6p"
      },
      "source": [
        "## Setting up a NeMo Experiment¶\n",
        "\n",
        "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uztqGAmdrYt"
      },
      "source": [
        "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
        "\n",
        "# the exp_dir provides a path to the current experiment for easy access\n",
        "exp_dir = str(exp_dir)\n",
        "exp_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tjLhUvL_o7_"
      },
      "source": [
        "Before initializing the model, we might want to modify some of the model configs. For example, we might want to modify the pretrained BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xeuc2i7Y_nP5"
      },
      "source": [
        "# get the list of supported BERT-like models, for the complete list of HugginFace models, see https://huggingface.co/models\n",
        "print(nemo_nlp.modules.get_pretrained_lm_models_list(include_external=True))\n",
        "\n",
        "# specify BERT-like model, you want to use\n",
        "#PRETRAINED_BERT_MODEL = \"megatron-bert-uncased\"\n",
        "PRETRAINED_BERT_MODEL = \"bert-base-multilingual-uncased\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK2xglXyAUOO"
      },
      "source": [
        "# add the specified above model parameters to the config\n",
        "config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzNZNAVRjDD-"
      },
      "source": [
        "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders we'll be prepared for training and evaluation.\n",
        "Also, the pretrained BERT model will be downloaded, note it can take up to a few minutes depending on the size of the chosen BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgsGLydWo-6-"
      },
      "source": [
        "model_from_scratch = nemo_nlp.models.TokenClassificationModel(cfg=config.model, trainer=trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ592Tx4pzyB"
      },
      "source": [
        "## Monitoring training progress\n",
        "Optionally, you can create a Tensorboard visualization to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTJr16_pp0aS"
      },
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  COLAB_ENV = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "  COLAB_ENV = False\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if COLAB_ENV:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir {exp_dir}\n",
        "else:\n",
        "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqLZHfWX2ymY"
      },
      "source": [
        "In case of using Megatron LM (megatron-bert-uncased and megatron-bert-cased), you will need to provide checkpoint, config file and vocab file. For getting checkpoints and config files, see: https://github.com/NVIDIA/Megatron-LM, https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m/files\n",
        "\n",
        "Megatron LM by default uses BERT's vocab_file which is missing a Persian letter and all Persian numbers. To get better results, it is better to remove 10 characters (the size of the vocab_file has to be fixed) from this vocab_file and add the following:\n",
        "\n",
        "ژ\n",
        "\n",
        "١\n",
        "\n",
        "۲\n",
        "\n",
        "۳\n",
        "\n",
        "۴\n",
        "\n",
        "۵\n",
        "\n",
        "۶\n",
        "\n",
        "۷\n",
        "\n",
        "۸\n",
        "\n",
        "۹\n",
        "\n",
        "After having these checkpoint, cnfig and vocab files, you can give them to the model as is shown in the next code block. If you do not want to change any of these, you can skip the below code block and go to the next one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njToe80Y20fB"
      },
      "source": [
        "config.model.language_model.lm_checkpoint=<PATH TO CHECKPOINT>\n",
        "config.model.language_model.config_file=<PATH TO CONFIG FILE>\n",
        "config.model.tokenizer.vocab_file = <PATH TO VOCAB FILE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj1pdEdD0Vm3"
      },
      "source": [
        "See how it performs before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo1oVGIT0aBZ"
      },
      "source": [
        "# define the list of queries for inference\n",
        "queries = [\n",
        "    'حمید طاهایی افزود : برای اجرای این طرحها 0 میلیارد و 0 میلیون ریال اعتبار هزینه شده است . ',\n",
        "    'دکتر اصغری دبیر چهارمین همایش انجمن زمین‌شناسی ایران در این زمینه گفت : از مجموع چهار صد مقاله رسیده به دبیرخانه همایش ، يك صد و هشتاد مقاله ظرف مدت دو روز در هشت سالن همایش برگزار شد . '\n",
        "]\n",
        "results = model_from_scratch.add_predictions(queries)\n",
        "\n",
        "for query, result in zip(queries, results):\n",
        "    print()\n",
        "    print(f'Query : {query}')\n",
        "    print(f'Result: {result.strip()}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyElt0Es-aSk"
      },
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUvnSpyjp0Dh"
      },
      "source": [
        "# start model training\n",
        "trainer.fit(model_from_scratch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lFo27PJ0o3W"
      },
      "source": [
        "See how it gets better after:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fNcBnz80rLO"
      },
      "source": [
        "results = model_from_scratch.add_predictions(queries)\n",
        "\n",
        "for query, result in zip(queries, results):\n",
        "    print()\n",
        "    print(f'Query : {query}')\n",
        "    print(f'Result: {result.strip()}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxBiIKMlH8yv"
      },
      "source": [
        "After training for 100 epochs, with the default config and NUM_SAMPLES = -1 (i.e.all data is used), your model performance should look similar to this: \n",
        "```\n",
        "    label                                                precision    recall       f1           support\n",
        "    O (label_id: 0)                                         99.31      99.18      99.24      35765\n",
        "    B_DAT (label_id: 1)                                     83.87      87.84      85.81        296\n",
        "    B_LOC (label_id: 2)                                     92.56      92.89      92.72        830\n",
        "    B_MON (label_id: 3)                                     96.00      97.30      96.64         74\n",
        "    B_ORG (label_id: 4)                                     92.20      92.85      92.52        853\n",
        "    B_PCT (label_id: 5)                                    100.00      98.11      99.05         53\n",
        "    B_PER (label_id: 6)                                     95.41      94.74      95.07        570\n",
        "    B_TIM (label_id: 7)                                     81.25      86.67      83.87         30\n",
        "    I_DAT (label_id: 8)                                     88.92      87.56      88.24        394\n",
        "    I_LOC (label_id: 9)                                     84.73      89.12      86.87        386\n",
        "    I_MON (label_id: 10)                                    96.85      99.08      97.95        217\n",
        "    I_ORG (label_id: 11)                                    92.81      92.81      92.81       1433\n",
        "    I_PCT (label_id: 12)                                   100.00     100.00     100.00         59\n",
        "    I_PER (label_id: 13)                                    96.60      95.90      96.25        415\n",
        "    I_TIM (label_id: 14)                                    73.68     100.00      84.85         28\n",
        "    -------------------\n",
        "    micro avg                                               98.31      98.31      98.31      41403\n",
        "    macro avg                                               91.61      94.27      92.79      41403\n",
        "    weighted avg                                            98.33      98.31      98.32      41403\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IyoAMIWIrx6"
      },
      "source": [
        "If Megatron LM is used, the performance will look like this:\n",
        "```\n",
        "    label                                                precision    recall       f1           support\n",
        "    O (label_id: 0)                                         98.08      98.92      98.50      29511\n",
        "    B_DAT (label_id: 1)                                     78.63      80.91      79.75        241\n",
        "    B_LOC (label_id: 2)                                     87.52      84.96      86.22        685\n",
        "    B_MON (label_id: 3)                                     93.06      98.53      95.71         68\n",
        "    B_ORG (label_id: 4)                                     86.42      83.22      84.79        566\n",
        "    B_PCT (label_id: 5)                                     97.62      95.35      96.47         43\n",
        "    B_PER (label_id: 6)                                     85.85      71.39      77.96        374\n",
        "    B_TIM (label_id: 7)                                     62.50      75.00      68.18         20\n",
        "    I_DAT (label_id: 8)                                     84.18      79.64      81.85        334\n",
        "    I_LOC (label_id: 9)                                     75.67      69.00      72.18        329\n",
        "    I_MON (label_id: 10)                                    96.23     100.00      98.08        204\n",
        "    I_ORG (label_id: 11)                                    87.56      78.83      82.96        973\n",
        "    I_PCT (label_id: 12)                                    97.87      97.87      97.87         47\n",
        "    I_PER (label_id: 13)                                    88.75      76.62      82.24        278\n",
        "    I_TIM (label_id: 14)                                    56.25      90.00      69.23         20\n",
        "    -------------------\n",
        "    micro avg                                               96.67      96.67      96.67      33693\n",
        "    macro avg                                               85.08      85.35      84.80      33693\n",
        "    weighted avg                                            96.59      96.67      96.60      33693\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdmOP2Pzpqa4"
      },
      "source": [
        "# Labeling your own data\n",
        "\n",
        "If you have raw data, NeMo recommends using the Datasaur labeling platform to apply labels to data. Datasaur was designed specifically for labeling text data and supports basic NLP labeling tasks such as Named Entity Recognition and text classification through advanced NLP tasks such as dependency parsing and coreference resolution. You can sign up for Datasaur for free at https://datasaur.ai/sign-up/. Once you upload a file, you can choose from multiple NLP project types and use the Datasaur interface to label the data. After labeling, you can export the labeled data using the conll_2003 format, which integrates directly with NeMo. A video walkthrough can be found here: https://www.youtube.com/watch?v=I9WVmnnSciE.\n"
      ]
    }
  ]
}
