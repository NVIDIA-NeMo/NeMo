{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech_Commands.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R12Yn6W1dt9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install unidecode\n",
        "\n",
        "# ## Install NeMo\n",
        "!python -m pip install --upgrade git+https://github.com/NVIDIA/NeMo.git@candidate#egg=nemo_toolkit[all]\n",
        "\n",
        "## Install TorchAudio\n",
        "!pip install torchaudio>=0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ycGIaZfSLE",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This Speech Command recognition tutorial is based on the MatchboxNet model from the paper [\"MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition\"](https://arxiv.org/abs/2004.08531). MatchboxNet is a modified form of the QuartzNet architecture from the paper \"[QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions](https://arxiv.org/pdf/1910.10261.pdf)\" with a modified decoder head to suit classification tasks.\n",
        "\n",
        "The notebook will follow the steps below:\n",
        "\n",
        " - Dataset preparation: Preparing Google Speech Commands dataset\n",
        "\n",
        " - Audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)\n",
        "\n",
        " - Data augmentation using SpecAugment \"[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)\" to increase number of data samples.\n",
        " \n",
        " - Develop a small Neural classification model which can be trained efficiently.\n",
        " \n",
        " - Model training on the Google Speech Commands dataset in NeMo.\n",
        " \n",
        " - Evaluation of error cases of the model by audibly hearing the samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I62_LJzc-p2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some utility imports\n",
        "import os\n",
        "from omegaconf import OmegaConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_M8wpkwd7d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is where the Google Speech Commands directory will be placed.\n",
        "# Change this if you don't want the data to be extracted in the current directory.\n",
        "# Select the version of the dataset required as well (can be 1 or 2)\n",
        "DATASET_VER = 1\n",
        "data_dir = './google_dataset_v{0}/'.format(DATASET_VER)\n",
        "\n",
        "if DATASET_VER == 1:\n",
        "  MODEL_CONFIG = \"matchboxnet_3x1x64_v1.yaml\"\n",
        "else:\n",
        "  MODEL_CONFIG = \"matchboxnet_3x1x64_v2.yaml\"\n",
        "\n",
        "if not os.path.exists(f\"configs/{MODEL_CONFIG}\"):\n",
        "  !wget -P configs/ \"https://raw.githubusercontent.com/NVIDIA/NeMo/candidate/examples/asr/conf/{MODEL_CONFIG}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvfwv9Hjf1Uv",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "We will be using the open source Google Speech Commands Dataset (we will use V1 of the dataset for the tutorial, but require very minor changes to support V2 dataset). These scripts below will download the dataset and convert it to a format suitable for use with NeMo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VL10OXTf8ts",
        "colab_type": "text"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "The dataset must be prepared using the scripts provided under the `{NeMo root directory}/scripts` sub-directory. \n",
        "\n",
        "Run the following command below to download the data preparation script and execute it.\n",
        "\n",
        "**NOTE**: You should have at least 4GB of disk space available if youâ€™ve used --data_version=1; and at least 6GB if you used --data_version=2. Also, it will take some time to download and process, so go grab a coffee.\n",
        "\n",
        "**NOTE**: You may additionally pass a `--rebalance` flag at the end of the `process_speech_commands_data.py` script to rebalance the class samples in the manifest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqKe6_uLfzKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"process_speech_commands_data.py\"):\n",
        "  !wget https://raw.githubusercontent.com/NVIDIA/NeMo/candidate/scripts/process_speech_commands_data.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTsxp0nZ1zqo",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the manifest file\n",
        "\n",
        "The manifest file is a simple file which has the full path to the audio file, the duration of the audio file and the label that is assigned to that audio file. \n",
        "\n",
        "This notebook is only a demonstration and therefore we will use the `--skip_duration` flag to speed up construction of the manifest file.\n",
        "\n",
        "**NOTE: When replicating the results of the paper, do not use this flag and prepare the manifest file with correct durations.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWUtDpzKgop9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir {data_dir}\n",
        "!python process_speech_commands_data.py --data_root={data_dir} --data_version={DATASET_VER} --skip_duration --log\n",
        "print(\"Dataset ready !\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVsPFxJtg30p",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the path to manifest files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytTFGVe0g9wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = 'google_speech_recognition_v{0}'.format(DATASET_VER)\n",
        "dataset_basedir = os.path.join(data_dir, dataset_path)\n",
        "\n",
        "train_dataset = os.path.join(dataset_basedir, 'train_manifest.json')\n",
        "val_dataset = os.path.join(dataset_basedir, 'validation_manifest.json')\n",
        "test_dataset = os.path.join(dataset_basedir, 'validation_manifest.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0SZy9SEhOBf",
        "colab_type": "text"
      },
      "source": [
        "## Read a few rows of the manifest file \n",
        "\n",
        "Manifest files are the data structure used by NeMo to declare a few important details about the data :\n",
        "\n",
        "1) `audio_filepath`: Refers to the path to the raw audio file <br>\n",
        "2) `command`: The class label (or speech command) of this sample <br>\n",
        "3) `duration`: The length of the audio file, in seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYBidCMIhKQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 5 {train_dataset}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-pyUBedh8f4",
        "colab_type": "text"
      },
      "source": [
        "# Training - Preparation\n",
        "\n",
        "We will be training a MatchboxNet model from the paper [\"MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition\"](https://arxiv.org/abs/2004.08531). The benefit of MatchboxNet over JASPER models is that they use 1D Time-Channel Separable Convolutions, which greatly reduce the number of parameters required to obtain good model accuracy.\n",
        "\n",
        "MatchboxNet models generally follow the model definition pattern QuartzNet-[BxRXC], where B is the number of blocks, R is the number of convolutional sub-blocks and C is the number of channels in these blocks. Each sub-block contains a 1-D masked convolution, batch normalization, ReLU, and dropout.\n",
        "\n",
        "An image of QuartzNet, the base configuration of MatchboxNet models, is provided below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sV4riijHJF",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://developer.nvidia.com/blog/wp-content/uploads/2020/05/quartznet-model-architecture-1-625x742.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieAPOM9thTN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NeMo's \"core\" package\n",
        "import nemo\n",
        "# NeMo's ASR collection - this collections contains complete ASR models and\n",
        "# building blocks (modules) for ASR\n",
        "import nemo.collections.asr as nemo_asr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss9gLcDv30jI",
        "colab_type": "text"
      },
      "source": [
        "## Model Configuration\n",
        "The MatchboxNet model is defined in a config file which declares multiple important sections\n",
        "\n",
        "They are:\n",
        "\n",
        "1) `model`: All arguments that will relate to the Model - preprocessors, encoder, decoder, optimizer and schedulers, datasets and any other related information\n",
        "\n",
        "2) `trainer`: Any argument to be passed to PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoVAs9h1lfci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This line will print the entire config of the MatchboxNet model\n",
        "config_path = f\"configs/{MODEL_CONFIG}\"\n",
        "config = OmegaConf.load(config_path)\n",
        "print(config.pretty())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2lJPR0a3qww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preserve some useful parameters\n",
        "labels = config.model.labels\n",
        "sample_rate = config.sample_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_pmjeed78rJ",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the datasets within the config\n",
        "\n",
        "If you'll notice, there are a few config dictionaries called `train_ds`, `validation_ds` and `test_ds`. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIe6Qfs18MiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(config.model.train_ds.pretty())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb01hl868Uc3",
        "colab_type": "text"
      },
      "source": [
        "### `???` inside configs\n",
        "\n",
        "You will often notice that some configs have `???` in place of paths. This is used as a placeholder so that the user can change the value at a later time.\n",
        "\n",
        "Lets add the paths to the manifests to the config above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m181HXev8T97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.model.train_ds.manifest_filepath = train_dataset\n",
        "config.model.validation_ds.manifest_filepath = val_dataset\n",
        "config.model.test_ds.manifest_filepath = test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbXngoCM5IRG",
        "colab_type": "text"
      },
      "source": [
        "## Building the PyTorch Lightning Trainer\n",
        "\n",
        "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem !\n",
        "\n",
        "Lets first instantiate a Trainer object!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYtvdBlG5afU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRN18CdH51nN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(config.trainer.pretty())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHf6cHvm6H9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets modify some trainer configs for this demo\n",
        "# Checks if we have GPU available and uses it\n",
        "cuda = 1 if torch.cuda.is_available() else 0\n",
        "config.trainer.gpus = cuda\n",
        "\n",
        "# Reduces maximum number of epochs to 5 for quick demonstration\n",
        "config.trainer.max_epochs = 5\n",
        "\n",
        "# Remove distributed training flags\n",
        "config.trainer.distributed_backend = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB9nr7G56G3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = pl.Trainer(**config.trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wt603Vq6sqX",
        "colab_type": "text"
      },
      "source": [
        "## Setting up a NeMo Experiment\n",
        "\n",
        "NeMo has an experiment manager that handles logging and checkpointing for us, so lets use it ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfWJFg7p6Ezf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nemo.utils.exp_manager import exp_manager"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC-QPoW44-p2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqi6rkNR7Dph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The exp_dir provides a path to the current experiment for easy access\n",
        "exp_dir = str(exp_dir)\n",
        "exp_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0zz-vHH7Uuh",
        "colab_type": "text"
      },
      "source": [
        "## Building the MatchboxNet Model\n",
        "\n",
        "MatchboxNet is an ASR model with a classification task - it generates one label for the entire provided audio stream. Therefore we encapsulate it inside the `EncDecClassificationModel` as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRMrKhyf5vhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "asr_model = nemo_asr.models.EncDecClassificationModel(cfg=config.model, trainer=trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA9UND-Q_oyw",
        "colab_type": "text"
      },
      "source": [
        "# Training a MatchboxNet Model\n",
        "\n",
        "As MatchboxNet is inherently a PyTorch Lightning Model, it can easily be trained in a single line - `trainer.fit(model)` !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ngKcRFqBfIF",
        "colab_type": "text"
      },
      "source": [
        "### Monitoring training progress\n",
        "\n",
        "Before we begin training, lets first create a Tensorboard visualization to monitor progress\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyfec0PDBsXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L5ymu-QBxmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir {exp_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZApuELDIKQgC",
        "colab_type": "text"
      },
      "source": [
        "### Training for 5 epochs\n",
        "We see below that the model begins to get modest scores on the validation set after just 5 epochs of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xiUUJlH5KdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.fit(asr_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkds1jSvKgSc",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation on the Test set\n",
        "\n",
        "Lets compute the final score on the test set via `trainer.test(model)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mULTrhEJ_6wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.test(asr_model, ckpt_path=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQntce8cLiUC",
        "colab_type": "text"
      },
      "source": [
        "# Fast Training\n",
        "\n",
        "We can dramatically improve the time taken to train this model by using Multi GPU training along with Mixed Precision.\n",
        "\n",
        "For multi-gpu training, take a look at [the PyTorch Lightning Multi-GPU training section](https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html)\n",
        "\n",
        "For mixed-precision training, take a look at [the PyTorch Lightning Mixed-Precision training section](https://pytorch-lightning.readthedocs.io/en/latest/apex.html)\n",
        "\n",
        "```python\n",
        "# Mixed precision:\n",
        "trainer = Trainer(amp_level='O1', precision=16)\n",
        "\n",
        "# Trainer with a distributed backend:\n",
        "trainer = Trainer(gpus=2, num_nodes=2, distributed_backend='ddp')\n",
        "\n",
        "# Of course, you can combine these flags as well.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDHkunjM8y6",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation of incorrectly predicted samples\n",
        "\n",
        "Given that we have a trained model, which performs reasonably well, lets try to listen to the samples where the model is least confident in its predictions.\n",
        "\n",
        "For this, we need support of the librosa library.\n",
        "\n",
        "**NOTE**: The following code depends on librosa. To install it, run the following code block first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3w3LhHcKuD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcJrZ72sNCkM",
        "colab_type": "text"
      },
      "source": [
        "## Extract the predictions from the model\n",
        "\n",
        "We want to possess the actual logits of the model instead of just the final evaluation score, so we can define a function to perform the forward step for us without computing the final loss. Instead, we extract the logits per batch of samples provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxdviYtOFjK",
        "colab_type": "text"
      },
      "source": [
        "## Accessing the data loaders\n",
        "\n",
        "We can utilize the `setup_test_data` method in order to instantiate a dataloader for the dataset want to analyse.\n",
        "\n",
        "For convenience, we can access these instantiated data loaders using the following accessors - `asr_model._train_dl`, `asr_model._validation_dl` and `asr_model._test_dl`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB0QZCAmM656",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "asr_model.setup_test_data(config.model.test_ds)\n",
        "test_dl = asr_model._test_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA7gXawcPoip",
        "colab_type": "text"
      },
      "source": [
        "## Partial Test Step\n",
        "\n",
        "Below we define a utility function to perform most of the test step. For reference, the test step is defined as follows:\n",
        "\n",
        "```python\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        audio_signal, audio_signal_len, labels, labels_len = batch\n",
        "        logits = self.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
        "        loss_value = self.loss(logits=logits, labels=labels)\n",
        "        correct_counts, total_counts = self._accuracy(logits=logits, labels=labels)\n",
        "        return {'test_loss': loss_value, 'test_correct_counts': correct_counts, 'test_total_counts': total_counts}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBsDOm5ROpQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def extract_logits(model, dataloader):\n",
        "  logits_buffer = []\n",
        "  label_buffer = []\n",
        "\n",
        "  # Follow the above definition of the test_step\n",
        "  for batch in dataloader:\n",
        "    audio_signal, audio_signal_len, labels, labels_len = batch\n",
        "    logits = model(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
        "\n",
        "    logits_buffer.append(logits)\n",
        "    label_buffer.append(labels)\n",
        "    print(\".\", end='')\n",
        "  print()\n",
        "  \n",
        "  print(\"Finished extracting logits !\")\n",
        "  logits = torch.cat(logits_buffer, 0)\n",
        "  labels = torch.cat(label_buffer, 0)\n",
        "  return logits, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZSdprUlOuoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cpu_model = asr_model.cpu()\n",
        "cpu_model.eval()\n",
        "logits, labels = extract_logits(cpu_model, test_dl)\n",
        "print(\"Logits:\", logits.shape, \"Labels :\", labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wd0ukgNXRBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute accuracy - `_accuracy` is a PyTorch Lightning Metric !\n",
        "correct_count, total_count = cpu_model._accuracy(logits=logits, labels=labels)\n",
        "print(\"Accuracy : \", float(correct_count * 100.) / float(total_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwN9OSqCauSH",
        "colab_type": "text"
      },
      "source": [
        "## Filtering out incorrect samples\n",
        "Let us now filter out the incorrectly labeled samples from the total set of samples in the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1YJvsmcZ0uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import json\n",
        "import IPython.display as ipd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZAT9yGAayvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First lets create a utility class to remap the integer class labels to actual string label\n",
        "class ReverseMapLabel:\n",
        "    def __init__(self, data_loader):\n",
        "        self.label2id = dict(data_loader.dataset.label2id)\n",
        "        self.id2label = dict(data_loader.dataset.id2label)\n",
        "\n",
        "    def __call__(self, pred_idx, label_idx):\n",
        "        return self.id2label[pred_idx], self.id2label[label_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3GSXvYHa4KJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, lets get the indices of all the incorrectly labeled samples\n",
        "sample_idx = 0\n",
        "incorrect_preds = []\n",
        "rev_map = ReverseMapLabel(test_dl)\n",
        "\n",
        "# Remember, evaluated_tensor = (loss, logits, labels)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "probas, preds = torch.max(probs, dim=-1)\n",
        "\n",
        "incorrect_ids = (preds != labels).nonzero()\n",
        "for idx in incorrect_ids:\n",
        "    proba = float(probas[idx][0])\n",
        "    pred = int(preds[idx][0])\n",
        "    label = int(labels[idx][0])\n",
        "    idx = int(idx[0]) + sample_idx\n",
        "\n",
        "    incorrect_preds.append((idx, *rev_map(pred, label), proba))\n",
        "\n",
        "print(f\"Num test samples : {total_count.item()}\")\n",
        "print(f\"Num errors : {len(incorrect_preds)}\")\n",
        "\n",
        "# First lets sort by confidence of prediction\n",
        "incorrect_preds = sorted(incorrect_preds, key=lambda x: x[-1], reverse=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JgGo71gcDtD",
        "colab_type": "text"
      },
      "source": [
        "## Examine a subset of incorrect samples\n",
        "Lets print out the (test id, predicted label, ground truth label, confidence) tuple of first 20 incorrectly labeled samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37wNJsNbcw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for incorrect_sample in incorrect_preds[:20]:\n",
        "    print(str(incorrect_sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDnwYsDKcLv9",
        "colab_type": "text"
      },
      "source": [
        "##  Define a threshold below which we designate a model's prediction as \"low confidence\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpvzeh4PcGJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter out how many such samples exist\n",
        "low_confidence_threshold = 0.25\n",
        "count_low_confidence = len(list(filter(lambda x: x[-1] <= low_confidence_threshold, incorrect_preds)))\n",
        "print(f\"Number of low confidence predictions : {count_low_confidence}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERXyXvCAcSKR",
        "colab_type": "text"
      },
      "source": [
        "## Lets hear the samples which the model has least confidence in !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxjNVjX8cPNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First lets create a helper function to parse the manifest files\n",
        "def parse_manifest(manifest):\n",
        "    data = []\n",
        "    for line in manifest:\n",
        "        line = json.loads(line)\n",
        "        data.append(line)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWxqw5k-cUVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, lets create a helper function to actually listen to certain samples\n",
        "def listen_to_file(sample_id, pred=None, label=None, proba=None):\n",
        "    # Load the audio waveform using librosa\n",
        "    filepath = test_samples[sample_id]['audio_filepath']\n",
        "    audio, sample_rate = librosa.load(filepath)\n",
        "\n",
        "    if pred is not None and label is not None and proba is not None:\n",
        "        print(f\"Sample : {sample_id} Prediction : {pred} Label : {label} Confidence = {proba: 0.4f}\")\n",
        "    else:\n",
        "        print(f\"Sample : {sample_id}\")\n",
        "\n",
        "    return ipd.Audio(audio, rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPj1tFNIcXaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets load the test manifest into memory\n",
        "test_samples = []\n",
        "with open(test_dataset, 'r') as test_f:\n",
        "    test_samples = test_f.readlines()\n",
        "\n",
        "test_samples = parse_manifest(test_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt7b_uiScZcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally, lets listen to all the audio samples where the model made a mistake\n",
        "# Note: This list of incorrect samples may be quite large, so you may choose to subsample `incorrect_preds`\n",
        "count = min(count_low_confidence, 20)  # replace this line with just `count_low_confidence` to listen to all samples with low confidence\n",
        "\n",
        "for sample_id, pred, label, proba in incorrect_preds[:count]:\n",
        "    ipd.display(listen_to_file(sample_id, pred=pred, label=label, proba=proba))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3cFvN5vcbjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}