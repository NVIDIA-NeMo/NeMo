{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# NeMo AutoModel LoRA Fine-tuning — End-to-End, Config‑Driven Template\n",
        "\n",
        "This notebook is a practical, reproducible playbook for fine-tuning any Hugging Face causal LLM with NeMo 2.0 using a single, config‑driven workflow. It uses the modular training code under this folder (`config.py`, `data_modules.py`, `recipe_factory.py`, `executors.py`, `train.py`) to:\n",
        "- Build a complete TrainingConfig in Python or from YAML/JSON\n",
        "- Validate the setup with a dry‑run before launching\n",
        "- Run locally or on SLURM (same config) via NeMo‑Run executors\n",
        "- Fine‑tune with LoRA by default, or switch to full fine‑tuning with a single flag\n",
        "- Work with HF datasets or local JSON/JSONL conversation files, including tool‑use metadata\n",
        "\n",
        "## What you will do here\n",
        "- Configure model/data/optimizer/trainer/compute with `TrainingConfig`\n",
        "- Inspect and adapt conversation formatting (incl. tool use) in `data_modules.py`\n",
        "- Create LoRA or full‑FT recipes with `recipe_factory.py`\n",
        "- Choose local or SLURM execution with `executors.py`\n",
        "- Dry‑run to validate, then launch training and store checkpoints\n",
        "- Optionally save/load configs to YAML/JSON for reproducibility.\n",
        "\n",
        "## Requirements\n",
        "- NVIDIA NeMo container (e.g., `nvcr.io/nvidia/nemo:25.07`)\n",
        "- GPU access and (optionally) SLURM credentials\n",
        "- Hugging Face token if your model is gated\n",
        "\n",
        "> Tip: Run cells top‑to‑bottom. Cells with a `DO_*` flag are safe toggles to enable/disable heavier actions like training or SLURM runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project folder: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/NeMo/tutorials/llm/automodel/train\n",
            "nemo_run and nemo imported OK\n",
            "Local modules imported OK\n"
          ]
        }
      ],
      "source": [
        "# Environment checks and imports\n",
        "DO_IMPORTS = True  # set False to skip if running in a restricted env\n",
        "\n",
        "if DO_IMPORTS:\n",
        "    import os, sys\n",
        "    from pathlib import Path\n",
        "\n",
        "    try:\n",
        "        PROJECT_ROOT = Path(__file__).resolve().parent\n",
        "    except NameError:\n",
        "        PROJECT_ROOT = Path.cwd()\n",
        "    print(\"Project folder:\", PROJECT_ROOT)\n",
        "\n",
        "    # Ensure local imports work\n",
        "    if str(PROJECT_ROOT) not in sys.path:\n",
        "        sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "    # Quick dependency check\n",
        "    try:\n",
        "        import nemo_run as run  # Provided by the NeMo run utilities\n",
        "        from nemo.collections import llm\n",
        "        print(\"nemo_run and nemo imported OK\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: nemo_run/nemo import issue:\", e)\n",
        "\n",
        "    # Local modules\n",
        "    try:\n",
        "        import config as cfg\n",
        "        import executors\n",
        "        import recipe_factory\n",
        "        import data_modules\n",
        "        import train as train_script\n",
        "        print(\"Local modules imported OK\")\n",
        "    except Exception as e:\n",
        "        print(\"Local import issue:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration: Production-ready, flexible by design\n",
        "\n",
        "Use `config.py` as the single source of truth for training. You can:\n",
        "- Load from YAML/JSON files, or construct in Python.\n",
        "- Override any field via CLI flags or environment variables.\n",
        "- Keep secrets (e.g., `HF_TOKEN`) only in env vars.\n",
        "\n",
        "Key config blocks:\n",
        "- `ModelConfig`: `name` (HF repo or local path), `cache_dir`, `token`\n",
        "- `DataConfig`: HF dataset name or `[train.jsonl, val.jsonl]`, `seq_length`, `micro_batch_size`, `split`, `tokenizer_name`\n",
        "- `LoRAConfig`: `target_modules`, `dim`, `dropout`, init methods\n",
        "- `OptimizerConfig`: LR, warmup, weight decay, scheduler knobs\n",
        "- `TrainerConfig`: steps, validation/checkpoint/logging cadence\n",
        "- `ComputeConfig`: local/SLURM, nodes, gpus, time, tunnels, container\n",
        "- `PathConfig`: checkpoints/data roots\n",
        "- `EnvironmentConfig`: NCCL/NVTE/TRANSFORMERS_OFFLINE, etc.\n",
        "\n",
        "Repro tips:\n",
        "- Pin `container_image` and save the resolved YAML/JSON next to checkpoints.\n",
        "- Set `experiment_name`/version to track runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config built. Key fields:\n",
            "Model: ModelConfig(name='mistralai/Mistral-7B-Instruct-v0.3', cache_dir='/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/NeMo/tutorials/llm/automodel/train/models/hf_cache', token='hf_...')\n",
            "Data: DataConfig(dataset_name=['/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl', '/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl'], seq_length=1024, micro_batch_size=2, split='train[:200]', tokenizer_name=['/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl', '/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl'])\n",
            "LoRA: LoRAConfig(target_modules=['o_proj'], dim=8, dropout=0.1, lora_A_init_method='xavier', lora_B_init_method='zero')\n",
            "Trainer: TrainerConfig(max_steps=5, num_sanity_val_steps=0, val_check_interval=1, log_every_n_steps=1, checkpoint_filename='LoRA_Finetune', version=1)\n",
            "Compute: ComputeConfig(nodes=1, gpus_per_node=1, time='00:30:00', use_slurm=False, user=None, host=None, remote_job_dir=None, account=None, partition=None, container_image='nvcr.io/nvidia/nemo:25.07', custom_mounts=None, retries=0, tunnel_type='ssh')\n"
          ]
        }
      ],
      "source": [
        "# Build a TrainingConfig in Python\n",
        "DO_BUILD_CONFIG = True\n",
        "\n",
        "if DO_BUILD_CONFIG:\n",
        "    # Minimal quick-test config; adjust for your use case\n",
        "    base_config = cfg.TrainingConfig(\n",
        "        model=cfg.ModelConfig(\n",
        "            name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "            cache_dir=str((Path.cwd() / \"models\" / \"hf_cache\").resolve()),\n",
        "            token=os.environ.get(\"HF_TOKEN\"),\n",
        "        ),\n",
        "        data=cfg.DataConfig(\n",
        "            # Can be a HF dataset name, or a list of [train.jsonl/json, val.jsonl/json]\n",
        "            dataset_name=\"rajpurkar/squad\",\n",
        "            seq_length=1024,\n",
        "            micro_batch_size=2,\n",
        "            split=\"train[:200]\",  # or ['train[:200]','validation[:50]'] for train/val\n",
        "            tokenizer_name=None,   # defaults to model name\n",
        "        ),\n",
        "        lora=cfg.LoRAConfig(\n",
        "            target_modules=[\"o_proj\"],\n",
        "            dim=8,\n",
        "            dropout=0.1,\n",
        "        ),\n",
        "        optimizer=cfg.OptimizerConfig(\n",
        "            lr=2e-4,\n",
        "            weight_decay=0.01,\n",
        "            warmup_steps=10,\n",
        "        ),\n",
        "        trainer=cfg.TrainerConfig(\n",
        "            max_steps=5,\n",
        "            log_every_n_steps=1,\n",
        "            val_check_interval=1,\n",
        "            checkpoint_filename=\"LoRA_Finetune\",\n",
        "            version=1,\n",
        "        ),\n",
        "        compute=cfg.ComputeConfig(\n",
        "            nodes=1,\n",
        "            gpus_per_node=1,\n",
        "            time=\"00:30:00\",\n",
        "            use_slurm=False,\n",
        "            tunnel_type=\"ssh\",\n",
        "        ),\n",
        "        paths=cfg.PathConfig(\n",
        "            project_root=str(Path.cwd()),\n",
        "            checkpoint_dir=None,  # will default to project_root/models/checkpoints\n",
        "            data_dir=None,        # will default to project_root/data\n",
        "        ),\n",
        "        environment=cfg.EnvironmentConfig(\n",
        "            transformers_offline=\"0\",\n",
        "            torch_nccl_avoid_record_streams=\"1\",\n",
        "        ),\n",
        "        experiment_name=\"Notebook_LoRA_Quickstart\",\n",
        "    )\n",
        "\n",
        "    print(\"Config built. Key fields:\")\n",
        "    print(\"Model:\", base_config.model)\n",
        "    print(\"Data:\", base_config.data)\n",
        "    print(\"LoRA:\", base_config.lora)\n",
        "    print(\"Trainer:\", base_config.trainer)\n",
        "    print(\"Compute:\", base_config.compute)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data modules: flexible chat + tool-use support\n",
        "\n",
        "`data_modules.py` provides `CustomHFDataModule`:\n",
        "- Accepts a HF dataset name or local `[train.jsonl, val.jsonl]`.\n",
        "- Applies a chat template compatible with HF tokenizers.\n",
        "- Supports tool calls/results serialization inside messages.\n",
        "- Emits `input_ids`, `labels`, and `loss_mask`.\n",
        "\n",
        "Your data\n",
        "- Point `data.dataset_name` to HF datasets or your JSON/JSONL files.\n",
        "- Customize `formatting_prompts_func_with_chat_template` to match your schema.\n",
        "- Adjust `seq_length`, `micro_batch_size`, `split`, `tokenizer_name`.\n",
        "\n",
        "Minimal JSONL example:\n",
        "```json\n",
        "{\"messages\":[{\"role\":\"system\",\"content\":\"You are a helpful virtual assistant.\"},{\"role\":\"user\",\"content\":\"Hi\"}, {\"role\":\"assistant\",\"content\":\"Hello!\"}]}\n",
        "```\n",
        "\n",
        "Tool-use example (assistant emits tool calls, tool returns results):\n",
        "```json\n",
        "{\"messages\":[\n",
        "  {\"role\":\"user\",\"content\":\"Weather in SF?\"},\n",
        "  {\"role\":\"assistant\",\"tool_calls\":[{\"id\":\"c1\",\"type\":\"function\",\"function\":{\"name\":\"get_weather\",\"arguments\":{\"city\":\"San Francisco\"}}}]},\n",
        "  {\"role\":\"tool\",\"tool_call_id\":\"c1\",\"content\":\"{ \\\"temp\\\": 20, \\\"unit\\\": \\\"C\\\" }\"},\n",
        "  {\"role\":\"assistant\",\"content\":\"It is 20°C in SF.\"}\n",
        "]}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LmsysHFDataModule:\n",
            "    def formatting_prompts_func_with_chat_template(self, example: Dict[str, Any], start_of_turn_token: Optional[str] = None) -> Dict[str, List[int]]:\n",
            "        \"\"\"\n",
            "        Format any conversation example using Mistral chat template.\n",
            "        \n",
            "        Args:\n",
            "            example: Dataset example, preferably with a 'messages' list and optional 'tools'.\n",
            "            start_of_turn_token: Token marking start of assistant response\n",
            "            \n",
            "        Returns:\n",
            "            Dictionary with input_ids, labels, and loss_mask\n",
            "        \"\"\"\n",
            "        tools = example.get('tools', [])\n",
            "\n",
            "        formatted_text: List[Dict[str, str]] = []\n",
            "        raw_messages = example.get('messages')\n",
            "\n",
            "        # Build system prompt that includes the available tools\n",
            "        tools_block = '[AVAILABLE_TOOLS]' + json.dumps(tools, separators=(',', ':')) + '[/AVAILABLE_TOOLS]'\n",
            "        if raw_messages[0].get('role') == 'system':\n",
            "            system_content = str(raw_messages[0].get('content', ''))\n",
            "            if '[AVAILABLE_TOOLS]' not in system_content:\n",
            "                system_content = (system_content + '\\n' + tools_block).strip()\n",
            "            formatted_text.append({'role': 'system', 'content': system_content})\n",
            "            start_index = 1\n",
            "        else:\n",
            "            system_content = 'You are a helpful assistant.'\n",
            "            formatted_text.append({'role': 'system', 'content': (system_content + '\\n' + tools_block)})\n",
            "            start_index = 0\n",
            "\n",
            "        # Process the rest of the turns\n",
            "        for msg in raw_messages[start_index:]:\n",
            "            role = msg.get('role')\n",
            "            if role in ('user', 'assistant'):\n",
            "                tool_calls = msg.get('tool_calls')\n",
            "                if role == 'assistant' and isinstance(tool_calls, list) and len(tool_calls) > 0:\n",
            "                    # Normalize tool calls -> keep id, type, function {name, arguments}\n",
            "                    normalized_calls: List[Dict[str, Any]] = []\n",
            "                    for call in tool_calls:\n",
            "                        call_id = call.get('id')\n",
            "                        call_type = call.get('type', 'function')\n",
            "                        fn = call.get('function', {}) or {}\n",
            "                        fn_name = fn.get('name')\n",
            "                        fn_args = fn.get('arguments')\n",
            "                        if isinstance(fn_args, str):\n",
            "                            try:\n",
            "                                fn_args = json.loads(fn_args)\n",
            "                            except Exception:\n",
            "                                pass\n",
            "                        normalized_calls.append({\n",
            "                            'id': call_id,\n",
            "                            'type': call_type,\n",
            "                            'function': {\n",
            "                                'name': fn_name,\n",
            "                                'arguments': fn_args\n",
            "                            }\n",
            "                        })\n",
            "                    formatted_text.append({\n",
            "                        'role': 'assistant',\n",
            "                        'content': '[TOOL_CALLS]' + json.dumps(normalized_calls, separators=(',', ':'))\n",
            "                    })\n",
            "                else:\n",
            "                    content = msg.get('content', '')\n",
            "                    formatted_text.append({'role': role, 'content': str(content)})\n",
            "            elif role == 'tool':\n",
            "                tool_call_id = msg.get('tool_call_id')\n",
            "                tool_content = msg.get('content', '')\n",
            "                result_obj = {'tool_call_id': tool_call_id, 'content': str(tool_content)}\n",
            "                formatted_text.append({\n",
            "                    'role': 'assistant',\n",
            "                    'content': '[TOOL_RESULTS]' + json.dumps(result_obj, separators=(',', ':')) + '[/TOOL_RESULTS]'\n",
            "                })\n",
            "            else:\n",
            "                continue\n",
            "\n",
            "        input_ids = self.get_chat_template(self.tokenizer)[0].apply_chat_template(formatted_text, tools=tools, add_generation_prompt=True)\n",
            "        \n",
            "        if isinstance(start_of_turn_token, str):\n",
            "            start_of_turn_token_id = self.tokenizer(start_of_turn_token, add_special_tokens=False)['input_ids'][0]\n",
            "            first_start_of_turn_token_id = input_ids.index(start_of_turn_token_id)\n",
            "            response_start = input_ids.index(start_of_turn_token_id, first_start_of_turn_token_id + 1) + 1\n",
            "        else:\n",
            "            response_start = 0\n",
            "            \n",
            "        loss_mask = [0] * response_start + [1] * (len(input_ids) - response_start)\n",
            "        \n",
            "        return dict(\n",
            "            input_ids=input_ids,\n",
            "            labels=input_ids[1:] + [getattr(self.tokenizer, 'eos_token_id', None) or input_ids[-1]],\n",
            "            loss_mask=loss_mask,\n",
            "        )\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Inspect data module functions\n",
        "import inspect\n",
        "\n",
        "print(\"CustomHFDataModule:\")\n",
        "print(inspect.getsource(data_modules.CustomHFDataModule.formatting_prompts_func_with_chat_template))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recipes: LoRA and full fine-tuning, switchable\n",
        "\n",
        "`recipe_factory.py` exposes:\n",
        "- `create_lora_recipe(config)`: PEFT with `llm.peft.LoRA`\n",
        "- `create_full_finetune_recipe(config)`: full model FT\n",
        "- `create_recipe(config, recipe_type)`: one-line switch\n",
        "\n",
        "Production knobs:\n",
        "- LoRA: `config.lora.*` (rank, dropout, targets)\n",
        "- Optimizer/scheduler: `config.optimizer.*`\n",
        "- Trainer: `config.trainer.*` (precision, grad clip, logging)\n",
        "- Resumption/checkpointing via `config.trainer` and `paths`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recipe created. Trainer max_steps: 5\n",
            "LoRA dim: 8\n",
            "Data micro_batch_size: 2\n"
          ]
        }
      ],
      "source": [
        "# Create and inspect a LoRA recipe (no training yet)\n",
        "DO_CREATE_RECIPE = True\n",
        "\n",
        "if DO_CREATE_RECIPE:\n",
        "    recipe = recipe_factory.create_recipe(base_config, recipe_type=\"lora\")\n",
        "    # Show a few key fields for verification\n",
        "    print(\"Recipe created. Trainer max_steps:\", recipe.trainer.max_steps)\n",
        "    print(\"LoRA dim:\", getattr(getattr(recipe, 'peft', None), 'dim', None))\n",
        "    print(\"Data micro_batch_size:\", recipe.data.micro_batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executors: same config locally and on SLURM\n",
        "\n",
        "`executors.py` picks an executor from `config.compute.use_slurm`:\n",
        "- Local: `run.LocalExecutor` with `torchrun`\n",
        "- SLURM: `run.SlurmExecutor` (SSH or local tunnel)\n",
        "\n",
        "Production guidance:\n",
        "- Pin `container_image`, set `custom_mounts` for datasets and checkpoints.\n",
        "- Configure `account`, `partition`, `remote_job_dir`, `nodes`, `gpus_per_node`, `time`, `retries`.\n",
        "- Use `dry_run=True` to preflight mounts, tokens, and dataset access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executor type: LocalExecutor\n",
            "ntasks_per_node: 1\n"
          ]
        }
      ],
      "source": [
        "# Create the appropriate executor to validate settings (no run yet)\n",
        "DO_CREATE_EXECUTOR = True  # set True to test executor construction\n",
        "\n",
        "if DO_CREATE_EXECUTOR:\n",
        "    exe = executors.create_executor(base_config)\n",
        "    print(\"Executor type:\", type(exe).__name__)\n",
        "    # Print a few key attributes if available\n",
        "    if hasattr(exe, 'ntasks_per_node'):\n",
        "        print(\"ntasks_per_node:\", getattr(exe, 'ntasks_per_node'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dry-run: fast preflight for production\n",
        "\n",
        "Use `train.run_training(..., dry_run=True)` to validate without starting training.\n",
        "- Verifies config coherence (paths, dataset, tokens, LoRA/FT settings)\n",
        "- Builds the recipe and executor\n",
        "- Checks environment variables and mounts\n",
        "\n",
        "Run this before every change to catch issues early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dry-run validation passed.\n"
          ]
        }
      ],
      "source": [
        "# Validate only\n",
        "DO_DRY_RUN = True\n",
        "\n",
        "if DO_DRY_RUN:\n",
        "    try:\n",
        "        train_script.run_training(base_config, recipe_type=\"lora\", dry_run=True)\n",
        "        print(\"Dry-run validation passed.\")\n",
        "    except Exception as e:\n",
        "        print(\"Dry-run validation failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local training: small, fast, reproducible\n",
        "\n",
        "Use this to sanity‑check your full pipeline before scaling out.\n",
        "- Start with tiny splits and low `max_steps`.\n",
        "- Ensure `HF_TOKEN` is set and container is pinned.\n",
        "- Logs and checkpoints go under `paths.checkpoint_dir`.\n",
        "\n",
        "Tip: Keep a “smoke test” config checked into version control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting lora training with experiment: Notebook_LoRA_Quickstart\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.3\n",
            "Dataset: ['/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl', '/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl']\n",
            "Compute: 1 nodes, 1 GPUs/node\n",
            "Configuration validation passed\n",
            "Environment configured with 9 variables\n",
            "Creating lora training recipe\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment Notebook_LoRA_Quickstart with id: Notebook_LoRA_Quickstart_1755510148</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[92m──────────── \u001b[0m\u001b[1;35mEntering Experiment Notebook_LoRA_Quickstart with id: Notebook_LoRA_Quickstart_1755510148\u001b[0m\u001b[92m ────────────\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Log directory is: /root/.nemo_run/experiments/Notebook_LoRA_Quickstart/Notebook_LoRA_Quickstart_1755510148/lora_training\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:42:31] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job lora_training for experiment Notebook_LoRA_Quickstart</span>                    <a href=\"file:///opt/Run/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/Run/nemo_run/run/experiment.py#771\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771</span></a>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[02:42:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job lora_training for experiment Notebook_LoRA_Quickstart\u001b[0m                    \u001b]8;id=260609;file:///opt/Run/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=563149;file:///opt/Run/nemo_run/run/experiment.py#771\u001b\\\u001b[2m771\u001b[0m\u001b]8;;\u001b\\\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Log directory is: /root/.nemo_run/experiments/Notebook_LoRA_Quickstart/Notebook_LoRA_Quickstart_1755510148/lora_training\n",
            "Launched app: local_persistent://nemo_run/lora_training-hx1wtvqnqmk61c\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment Notebook_LoRA_Quickstart_1755510148 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[92m────────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment Notebook_LoRA_Quickstart_1755510148 to finish\u001b[0m\u001b[92m ───────────────────────\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">Notebook_LoRA_Quickstart_1755510148</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mNotebook_LoRA_Quickstart_1755510148\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">lora_training</span>\n",
              "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
              "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
              "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: lora_training-hx1wtvqnqmk61c\n",
              "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/Notebook_LoRA_Quickstart/Notebook_LoRA_Quickstart_1755510148/lora_training\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mlora_training\u001b[0m\n",
              "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
              "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
              "- \u001b[1;32mJob id\u001b[0m: lora_training-hx1wtvqnqmk61c\n",
              "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/Notebook_LoRA_Quickstart/Notebook_LoRA_Quickstart_1755510148/lora_training\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Waiting for job lora_training-hx1wtvqnqmk61c to finish [log=True]...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a_training/0 I0818 02:42:32.767000 185042 torch/distributed/run.py:649] Using nproc_per_node=1.\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195] Starting elastic_operator with launch configs:\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   min_nodes        : 1\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   max_nodes        : 1\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   nproc_per_node   : 1\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   run_id           : 8284\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   rdzv_backend     : c10d\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   rdzv_endpoint    : localhost:0\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   rdzv_configs     : {'timeout': 900}\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   max_restarts     : 0\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   monitor_interval : 0.1\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   log_dir          : /root/.nemo_run/experiments/Notebook_LoRA_Quickstart/Notebook_LoRA_Quickstart_1755510148/lora_training/nemo_run/lora_training-hx1wtvqnqmk61c/torchelastic/lora_training\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195]   metrics_cfg      : {}\n",
            "a_training/0 I0818 02:42:32.768000 185042 torch/distributed/launcher/api.py:195] \n",
            "a_training/0 I0818 02:42:32.772000 185042 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python\n",
            "a_training/0 I0818 02:42:32.772000 185042 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   master_addr=cw-dfw-h100-002-204-003.cm.cluster\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   master_port=11811\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0]\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0]\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0]\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[1]\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[1]\n",
            "a_training/0 I0818 02:42:32.856000 185042 torch/distributed/elastic/agent/server/api.py:525] \n",
            "a_training/0 I0818 02:42:32.857000 185042 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group\n",
            "a_training/0 I0818 02:42:32.857000 185042 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True\n",
            "a_training/0 I0818 02:42:32.858000 185042 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
            "a_training/0 I0818 02:42:32.858000 185042 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
            "a_training/0 [default0]:[NeMo W 2025-08-18 02:42:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
            "a_training/0 [default0]:      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
            "a_training/0 [default0]:    \n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:42:42 nemo_logging:393] use_linear_ce_loss: True\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Generating train split: 0 examples [00:00, ? examples/s]\n",
            "a_training/0 [default0]:Generating train split: 5 examples [00:00, 870.22 examples/s]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Generating validation split: 0 examples [00:00, ? examples/s]a_training/0 [default0]:[NeMo I 2025-08-18 02:42:44 nemo_logging:393] Using passed HF dataset DatasetDict({\n",
            "a_training/0 [default0]:        train: Dataset({\n",
            "a_training/0 [default0]:            features: ['conversation_id', 'tools', 'messages'],\n",
            "a_training/0 [default0]:            num_rows: 5\n",
            "a_training/0 [default0]:        })\n",
            "a_training/0 [default0]:        validation: Dataset({\n",
            "a_training/0 [default0]:            features: ['conversation_id', 'tools', 'messages'],\n",
            "a_training/0 [default0]:            num_rows: 5\n",
            "a_training/0 [default0]:        })\n",
            "a_training/0 [default0]:    })\n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:42:44 nemo_logging:393] HF dataset has the following splits: dict_keys(['train', 'validation'])\n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:42:44 nemo_logging:393] Disabling try_restore_best_ckpt restoration for adapters\n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:42:44 nemo_logging:393] Experiments will be logged at /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/NeMo/tutorials/llm/automodel/train/models/checkpoints/LoRA_Finetune_recipe\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Generating validation split: 5 examples [00:00, 1730.61 examples/s]\n",
            "a_training/0 [default0]:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "a_training/0 [default0]:GPU available: True (cuda), used: True\n",
            "a_training/0 [default0]:TPU available: False, using: 0 TPU cores\n",
            "a_training/0 [default0]:HPU available: False, using: 0 HPUs\n",
            "a_training/0 [default0]:`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
            "a_training/0 [default0]:`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
            "a_training/0 [default0]:[NeMo W 2025-08-18 02:42:44 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
            "a_training/0 [default0]:[NeMo W 2025-08-18 02:42:44 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/NeMo/tutorials/llm/automodel/train/models/checkpoints/tb_logs\n",
            "a_training/0 [default0]:[NeMo W 2025-08-18 02:42:44 nemo_logging:405] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
            "a_training/0 [default0]:[NeMo W 2025-08-18 02:42:44 nemo_logging:405] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 5. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
            "a_training/0 [default0]:[NeMo W 2025-08-18 02:42:44 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/NeMo/tutorials/llm/automodel/train/models/checkpoints/LoRA_Finetune_recipe/checkpoints. Training from scratch.\n",
            "a_training/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "a_training/0 [default0]:[W818 02:42:45.786122191 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "a_training/0 [default0]:----------------------------------------------------------------------------------------------------\n",
            "a_training/0 [default0]:distributed_backend=nccl\n",
            "a_training/0 [default0]:All distributed processes registered. Starting with 1 processes\n",
            "a_training/0 [default0]:----------------------------------------------------------------------------------------------------\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:NCCL version 2.27.3+cuda12.9\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Map:   0%|          | 0/5 [00:00<?, ? examples/s]\n",
            "a_training/0 [default0]:Map: 100%|██████████| 5/5 [00:00<00:00, 181.83 examples/s]\n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:42:48 nemo_logging:393] Setting up ModelTransform for stage: TrainerFn.FITTING\n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:42:48 nemo_logging:393] Found model_transform attribute on pl_module\n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:42:48 nemo_logging:393] Set model_transform to: <function _call_counter.<locals>.wrapper at 0x154dc57402c0>\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Map:   0%|          | 0/5 [00:00<?, ? examples/s]\n",
            "a_training/0 [default0]:Map: 100%|██████████| 5/5 [00:00<00:00, 360.79 examples/s]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Fetching 3 files:  33%|███▎      | 1/3 [00:19<00:39, 19.72s/it]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Fetching 3 files: 100%|██████████| 3/3 [00:19<00:00,  6.60s/it]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.00s/it]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.04s/it]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.94s/it]\n",
            "a_training/0 [default0]:Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n",
            "a_training/0 [default0]:[NeMo I 2025-08-18 02:43:20 nemo_logging:393] Configuring model with attn_implementation: sdpa\n",
            "a_training/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:  | Name  | Type               | Params | Mode  | FLOPs\n",
            "a_training/0 [default0]:-------------------------------------------------------------\n",
            "a_training/0 [default0]:0 | model | MistralForCausalLM | 7.3 B  | train | 0    \n",
            "a_training/0 [default0]:-------------------------------------------------------------\n",
            "a_training/0 [default0]:2.1 M     Trainable params\n",
            "a_training/0 [default0]:7.2 B     Non-trainable params\n",
            "a_training/0 [default0]:7.3 B     Total params\n",
            "a_training/0 [default0]:29,000.483Total estimated model params size (MB)\n",
            "a_training/0 [default0]:519       Modules in train mode\n",
            "a_training/0 [default0]:0         Modules in eval mode\n",
            "a_training/0 [default0]:0         Total Flops\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Training: |          | 0/? [00:00<?, ?it/s]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Training:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "a_training/0 [default0]:Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] [NeMo I 2025-08-18 02:43:27 nemo_logging:393] Setting up optimizers\n",
            "a_training/0 [default0]:Current pytorch-triton version: 3.3.0+git96316ce52.nvinternal, Required triton version: 3.2.0\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 0:  33%|███▎      | 1/3 [00:02<00:05,  0.37it/s]\n",
            "a_training/0 [default0]:Epoch 0:  33%|███▎      | 1/3 [00:02<00:05,  0.37it/s, train_step_timing in s=2.710]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[Aa_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\u001b[Aa_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 0:  33%|███▎      | 1/3 [00:02<00:05,  0.35it/s, train_step_timing in s=2.710]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 0:  67%|██████▋   | 2/3 [00:02<00:01,  0.67it/s, train_step_timing in s=2.710]\n",
            "a_training/0 [default0]:Epoch 0:  67%|██████▋   | 2/3 [00:02<00:01,  0.67it/s, train_step_timing in s=0.0972]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 35.70it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 0:  67%|██████▋   | 2/3 [00:03<00:01,  0.66it/s, train_step_timing in s=0.0972]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 0: 100%|██████████| 3/3 [00:04<00:00,  0.64it/s, train_step_timing in s=0.0972]\n",
            "a_training/0 [default0]:Epoch 0: 100%|██████████| 3/3 [00:04<00:00,  0.64it/s, train_step_timing in s=1.650, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 56.08it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 0: 100%|██████████| 3/3 [00:04<00:00,  0.63it/s, train_step_timing in s=1.650, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 0: 100%|██████████| 3/3 [00:04<00:00,  0.62it/s, train_step_timing in s=1.650, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]\n",
            "a_training/0 [default0]:Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s, train_step_timing in s=1.650, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]        \n",
            "a_training/0 [default0]:Epoch 1:   0%|          | 0/3 [00:00<?, ?it/s, train_step_timing in s=1.650, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 1:  33%|███▎      | 1/3 [00:00<00:00,  8.21it/s, train_step_timing in s=1.650, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]\n",
            "a_training/0 [default0]:Epoch 1:  33%|███▎      | 1/3 [00:00<00:00,  8.20it/s, train_step_timing in s=0.116, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 35.56it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 1:  33%|███▎      | 1/3 [00:00<00:00,  6.25it/s, train_step_timing in s=0.116, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 1:  67%|██████▋   | 2/3 [00:00<00:00,  6.43it/s, train_step_timing in s=0.116, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]\n",
            "a_training/0 [default0]:Epoch 1:  67%|██████▋   | 2/3 [00:00<00:00,  6.43it/s, train_step_timing in s=0.149, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 33.02it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 1:  67%|██████▋   | 2/3 [00:00<00:00,  5.72it/s, train_step_timing in s=0.149, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  6.63it/s, train_step_timing in s=0.149, reduced_train_loss=1.220, tps=707.0, lr=1.82e-5]\n",
            "a_training/0 [default0]:Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  6.62it/s, train_step_timing in s=0.101, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 57.38it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  6.12it/s, train_step_timing in s=0.101, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 1: 100%|██████████| 3/3 [00:00<00:00,  5.46it/s, train_step_timing in s=0.101, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]\n",
            "a_training/0 [default0]:Epoch 1:   0%|          | 0/3 [00:00<?, ?it/s, train_step_timing in s=0.101, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]        \n",
            "a_training/0 [default0]:Epoch 2:   0%|          | 0/3 [00:00<?, ?it/s, train_step_timing in s=0.101, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 2:  33%|███▎      | 1/3 [00:01<00:02,  0.86it/s, train_step_timing in s=0.101, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]\n",
            "a_training/0 [default0]:Epoch 2:  33%|███▎      | 1/3 [00:01<00:02,  0.86it/s, train_step_timing in s=1.150, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 38.35it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 2:  33%|███▎      | 1/3 [00:01<00:02,  0.84it/s, train_step_timing in s=1.150, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 2:  67%|██████▋   | 2/3 [00:01<00:00,  1.52it/s, train_step_timing in s=1.150, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]\n",
            "a_training/0 [default0]:Epoch 2:  67%|██████▋   | 2/3 [00:01<00:00,  1.52it/s, train_step_timing in s=0.116, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[Aa_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 34.86it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 2:  67%|██████▋   | 2/3 [00:01<00:00,  1.48it/s, train_step_timing in s=0.116, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s, train_step_timing in s=0.116, reduced_train_loss=0.767, tps=5391.0, lr=3.64e-5]\n",
            "a_training/0 [default0]:Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s, train_step_timing in s=0.0987, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 57.67it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s, train_step_timing in s=0.0987, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s, train_step_timing in s=0.0987, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]\n",
            "a_training/0 [default0]:Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s, train_step_timing in s=0.0987, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]        \n",
            "a_training/0 [default0]:Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, train_step_timing in s=0.0987, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]\n",
            "a_training/0 [default0]:Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 10.54it/s, train_step_timing in s=0.0987, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]\n",
            "a_training/0 [default0]:Epoch 3:  50%|█████     | 1/2 [00:00<00:00, 10.53it/s, train_step_timing in s=0.0895, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[Aa_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 35.00it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 3:  50%|█████     | 1/2 [00:00<00:00,  7.58it/s, train_step_timing in s=0.0895, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  7.96it/s, train_step_timing in s=0.0895, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]\n",
            "a_training/0 [default0]:Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  7.96it/s, train_step_timing in s=0.118, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5] \n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 32.91it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  6.90it/s, train_step_timing in s=0.118, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 3: |          | 3/? [00:00<00:00,  7.71it/s, train_step_timing in s=0.118, reduced_train_loss=2.660, tps=2112.0, lr=5.45e-5]    \n",
            "a_training/0 [default0]:Epoch 3: |          | 3/? [00:00<00:00,  7.71it/s, train_step_timing in s=0.0972, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 57.99it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 3: |          | 3/? [00:00<00:00,  7.02it/s, train_step_timing in s=0.0972, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]\n",
            "a_training/0 [default0]:Epoch 3: |          | 3/? [00:00<00:00,  6.19it/s, train_step_timing in s=0.0972, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]\n",
            "a_training/0 [default0]:Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, train_step_timing in s=0.0972, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]    \n",
            "a_training/0 [default0]:Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, train_step_timing in s=0.0972, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s, train_step_timing in s=0.0972, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]\n",
            "a_training/0 [default0]:Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s, train_step_timing in s=0.0904, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[Aa_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 35.36it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  7.56it/s, train_step_timing in s=0.0904, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 4: |          | 2/? [00:00<00:00,  7.95it/s, train_step_timing in s=0.0904, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]    \n",
            "a_training/0 [default0]:Epoch 4: |          | 2/? [00:00<00:00,  7.94it/s, train_step_timing in s=0.118, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5] \n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 34.18it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 4: |          | 2/? [00:00<00:00,  6.89it/s, train_step_timing in s=0.118, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]a_training/0 [default0]:\n",
            "a_training/0 [default0]:Epoch 4: |          | 3/? [00:00<00:00,  7.69it/s, train_step_timing in s=0.118, reduced_train_loss=11.50, tps=6728.0, lr=7.27e-5]\n",
            "a_training/0 [default0]:Epoch 4: |          | 3/? [00:00<00:00,  7.69it/s, train_step_timing in s=0.098, reduced_train_loss=17.50, tps=6735.0, lr=9.09e-5]\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[Aa_training/0 [default0]:\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 57.59it/s]\u001b[A\n",
            "a_training/0 [default0]:\n",
            "a_training/0 [default0]:                                                                      \u001b[A\n",
            "a_training/0 [default0]:Epoch 4: |          | 3/? [00:00<00:00,  7.02it/s, train_step_timing in s=0.098, reduced_train_loss=17.50, tps=6735.0, lr=9.09e-5]\n",
            "a_training/0 [default0]:Epoch 4: |          | 3/? [00:00<00:00,  6.19it/s, train_step_timing in s=0.098, reduced_train_loss=17.50, tps=6735.0, lr=9.09e-5]\n",
            "a_training/0 [default0]:Epoch 4: |          | 3/? [00:00<00:00,  6.19it/s, train_step_timing in s=0.098, reduced_train_loss=17.50, tps=6735.0, lr=9.09e-5]\n",
            "a_training/0 [default0]:`Trainer.fit` stopped: `max_steps=5` reached.\n",
            "a_training/0 I0818 02:43:42.923000 185042 torch/distributed/elastic/agent/server/api.py:879] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
            "a_training/0 I0818 02:43:42.923000 185042 torch/distributed/elastic/agent/server/api.py:932] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
            "a_training/0 I0818 02:43:42.924000 185042 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.00019359588623046875 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Job lora_training-hx1wtvqnqmk61c finished: SUCCEEDED\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
              "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['lora_training']</span><span style=\"background-color: #272822\">                                               </span>\n",
              "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
              "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Notebook_LoRA_Quickstart_1755510148\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                         </span>\n",
              "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
              "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"lora_training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                                              </span>\n",
              "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"lora_training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                                    </span>\n",
              "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['lora_training']\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m\n",
              "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
              "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mNotebook_LoRA_Quickstart_1755510148\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m\n",
              "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
              "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mlora_training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
              "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mlora_training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                                    \u001b[0m\n",
              "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
              "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
              "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status Notebook_LoRA_Quickstart_1755510148</span><span style=\"background-color: #272822\">                                                         </span>\n",
              "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs Notebook_LoRA_Quickstart_1755510148 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                         </span>\n",
              "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel Notebook_LoRA_Quickstart_1755510148 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                       </span>\n",
              "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
              "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
              "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNotebook_LoRA_Quickstart_1755510148\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m\n",
              "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNotebook_LoRA_Quickstart_1755510148\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m\n",
              "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNotebook_LoRA_Quickstart_1755510148\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m\n",
              "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training completed successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started/completed.\n"
          ]
        }
      ],
      "source": [
        "# Local run (small)\n",
        "DO_LOCAL_TRAIN = True  # set True to run a quick local training\n",
        "\n",
        "if DO_LOCAL_TRAIN:\n",
        "    try:\n",
        "        train_script.run_training(base_config, recipe_type=\"lora\", dry_run=False)\n",
        "        print(\"Training started/completed.\")\n",
        "    except Exception as e:\n",
        "        print(\"Training failed to start:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SLURM: scale out without code changes\n",
        "\n",
        "Set `compute.use_slurm=True`, then populate:\n",
        "- `account`, `partition`, `remote_job_dir`, `nodes`, `gpus_per_node`\n",
        "- Tunnels (`user`, `host`) as needed\n",
        "- `container_image`, `custom_mounts`, `time`, `retries`\n",
        "\n",
        "Cluster‑specific knobs vary (e.g., `gres`, `gpus_per_node`). Start from `slurm_config.yaml`, then use `dry_run=True` to validate before submitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting lora training with experiment: Notebook_LoRA_Quickstart\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.3\n",
            "Dataset: ['/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl', '/lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_mlops/soverign_ai/data/conversations.jsonl']\n",
            "Compute: 1 nodes, 8 GPUs/node\n",
            "Configuration validation passed\n",
            "Environment configured with 9 variables\n",
            "Dry run completed successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SLURM dry-run validation passed.\n"
          ]
        }
      ],
      "source": [
        "# Example: Switch to SLURM (do not run by default)\n",
        "DO_SLURM_EXAMPLE = True\n",
        "\n",
        "if DO_SLURM_EXAMPLE:\n",
        "    slurm_cfg = base_config\n",
        "    slurm_cfg.compute.use_slurm = True\n",
        "    slurm_cfg.compute.account = \"your_account\"\n",
        "    slurm_cfg.compute.partition = \"your_partition\"\n",
        "    slurm_cfg.compute.remote_job_dir = \"/path/to/remote/jobdir\"\n",
        "    slurm_cfg.compute.nodes = 1\n",
        "    slurm_cfg.compute.gpus_per_node = 8\n",
        "    slurm_cfg.compute.tunnel_type = \"ssh\"  # or \"local\"\n",
        "    slurm_cfg.compute.user = \"your_user\"\n",
        "    slurm_cfg.compute.host = \"cluster.hostname\"\n",
        "    slurm_cfg.compute.container_image = \"nvcr.io/nvidia/nemo:25.07\"\n",
        "    slurm_cfg.compute.custom_mounts = [\"/home:/home\"] # Add any other custom mounts here\n",
        "\n",
        "    try:\n",
        "        # Validate only\n",
        "        train_script.run_training(slurm_cfg, recipe_type=\"lora\", dry_run=True)\n",
        "        print(\"SLURM dry-run validation passed.\")\n",
        "    except Exception as e:\n",
        "        print(\"SLURM dry-run validation failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and reuse configurations\n",
        "\n",
        "Promote notebooks to scripts/CLI with saved configs.\n",
        "- Use `TrainingConfig.to_yaml()` / `to_json()` to persist resolved configs\n",
        "- Commit templates; track exact run configs with checkpoints\n",
        "- Load configs in CI or non‑interactive jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save config\n",
        "DO_SAVE_CONFIG = False\n",
        "\n",
        "if DO_SAVE_CONFIG:\n",
        "    out_yaml = Path.cwd() / \"my_config.yaml\"\n",
        "    out_json = Path.cwd() / \"my_config.json\"\n",
        "    base_config.to_yaml(str(out_yaml))\n",
        "    base_config.to_json(str(out_json))\n",
        "    print(\"Saved:\", out_yaml)\n",
        "    print(\"Saved:\", out_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips for production and flexibility\n",
        "\n",
        "- Models: update `model.name` and `tokenizer_name`; pin container and HF revision.\n",
        "- Data: point to HF datasets or local JSON/JSONL; customize the chat formatter for your schema/tooling.\n",
        "- LoRA vs Full FT: start with LoRA; switch to full when you need capacity and have budget.\n",
        "- Context/batching: balance `seq_length` + batch size with GPU RAM; scale using gradient accumulation.\n",
        "- Optimizer/scheduler: use warmup; start higher LR for adapters, lower for full FT.\n",
        "- Validation: keep small slices for quick signals; monitor TB logs.\n",
        "- SLURM: preflight with `dry_run=True`; verify mounts, account/partition, and gres.\n",
        "- Reproducibility: save resolved configs, seeds, and frequent checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load configs from provided templates\n",
        "\n",
        "You can start from the example configs like `basic_lora.yaml` and modify in-place.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: load YAML config template\n",
        "DO_LOAD_TEMPLATE = False\n",
        "\n",
        "if DO_LOAD_TEMPLATE:\n",
        "    template_path = PROJECT_ROOT / \"basic_lora.yaml\"\n",
        "    try:\n",
        "        loaded_cfg = cfg.TrainingConfig.from_yaml(str(template_path))\n",
        "        print(\"Loaded template. Model:\", loaded_cfg.model.name)\n",
        "        print(\"Data:\", loaded_cfg.data)\n",
        "        print(\"Trainer max_steps:\", loaded_cfg.trainer.max_steps)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load template:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use local JSON/JSONL files\n",
        "\n",
        "You can point `data.dataset_name` to a list of two paths: `[train_file, val_file]`. For quick tests, you can reuse the same file for both.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Switch config to JSON files (example)\n",
        "DO_USE_LOCAL_JSON = False\n",
        "\n",
        "if DO_USE_LOCAL_JSON:\n",
        "    train_path = PROJECT_ROOT / \"data/conversations_train.jsonl\"\n",
        "    val_path = PROJECT_ROOT / \"data/conversations_val.jsonl\"\n",
        "    base_config.data.dataset_name = [str(train_path), str(val_path)]\n",
        "    base_config.data.split = [\"train[:2]\", \"validation[:1]\"]  # optional when using local json\n",
        "    print(\"Using JSON files:\", base_config.data.dataset_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Typical Errors and Resolutions\n",
        "\n",
        "- Missing HF Token: Set `HF_TOKEN` in your environment; avoid hard‑coding in configs.\n",
        "- Dataset path issues: If using local JSON/JSONL, make sure both train and val files exist and are readable by the container; mount paths via `custom_mounts`.\n",
        "- Tokenizer mismatch: If your tokenizer differs from the model, set `data.tokenizer_name` explicitly.\n",
        "- SLURM GPU config: Clusters vary; confirm `gres`/`gpus_per_node` with admins. Use `dry_run=True` to validate before submit.\n",
        "- Container parity: Pin `compute.container_image` and ensure CUDA/driver compatibility with your cluster.\n",
        "- Checkpoint directory permissions: Ensure `paths.checkpoint_dir` is writable in both local and remote contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Production checklist\n",
        "\n",
        "- Define and save a pinned config (YAML/JSON) per run; include container image tag.\n",
        "- Run `dry_run=True` on local and SLURM executors after any config change.\n",
        "- Keep a smoke‑test config (tiny split, few steps) to validate the pipeline quickly.\n",
        "- Version control data schemas and chat formatting logic in `data_modules.py`.\n",
        "- Monitor logs/metrics and checkpoint frequently; enable resume behavior in trainer.\n",
        "- Validate HF access and mount paths in the exact runtime (container/cluster) you will use.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
