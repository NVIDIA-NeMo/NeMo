{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9d27250020aba6",
   "metadata": {},
   "source": [
    "# Finetuning Llama 3.2 Model into Embedding Model\n",
    "\n",
    "## Goal\n",
    "\n",
    "While LLaMA 3.2 is a powerful large language model (LLM) pre-trained on diverse datasets, its application to specific downstream tasks—such as semantic search, document retrieval, or natural language understanding—requires adapting the model to effectively generate dense vector representations (embeddings). In this tutorial, we will demonstrate how to finetune this model and convert it into a state-of-the-art embedding model for retrieval-augmented generation (RAG) tasks.\n",
    "\n",
    "The key architectural change involves modifying the LLaMA model to optimize its performance in generating embeddings by replacing causal attention with bidirectional attention. This change enables the decoder-only model to create embeddings that are contextually relevant, semantically rich, and capable of improving the efficiency and accuracy of tasks like information retrieval, clustering, and text classification.\n",
    "\n",
    "Our primary goals for this tutorial are as follows:\n",
    "\n",
    " * Demonstrate the ease of automatically converting the model with essential architectural changes for embedding model training\n",
    " * Improve the model's performance and accuracy in generating dense vector representations (embeddings)\n",
    " * Provide guidelines for finetuning embedding models, including hyperparameter choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0f1b96249be78",
   "metadata": {},
   "source": [
    "# NeMo Tools and Resources\n",
    "\n",
    "* [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)\n",
    "\n",
    "# Software Requirements\n",
    "\n",
    "* Access to latest NeMo Framework NGC Containers\n",
    "\n",
    "\n",
    "# Hardware Requirements\n",
    "\n",
    "* This playbook has been tested on the following hardware: Single A6000, Single H100, 2xA6000, 8xH100. It can be scaled to multiple GPUs as well as multiple nodes by modifying the appropriate parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87846682e01e1a50",
   "metadata": {},
   "source": [
    "#### Launch the NeMo Framework container as follows: \n",
    "\n",
    "Depending on the number of gpus, `--gpus` might need to adjust accordingly:\n",
    "```\n",
    "docker run -it -p 8080:8080 -p 8088:8088 --rm --gpus '\"device=0,1\"' --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:25.02\n",
    "```\n",
    "\n",
    "#### Launch Jupyter Notebook as follows: \n",
    "```\n",
    "jupyter notebook --allow-root --ip 0.0.0.0 --port 8088 --no-browser --NotebookApp.token=''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f0670-319d-4983-b4cc-4e8bd379b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import Literal, Optional, Union\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be39e17-4ed6-4456-ab77-616d3155a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbeddingModelAdapter(torch.nn.Module):\n",
    "    \"\"\"Wraps a Text embedding model with pooling and normalization.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        normalize: bool,\n",
    "        pooling_module: torch.nn.Module,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.normalize = normalize\n",
    "        self.pooling_module = pooling_module\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.model.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        dimensions: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        if token_type_ids is not None:\n",
    "            inputs[\"token_type_ids\"] = token_type_ids\n",
    "        outputs = self.model(**inputs)\n",
    "        hidden_states = outputs[\"last_hidden_state\"].to(torch.float32)\n",
    "        embeddings = self.pooling_module(hidden_states, inputs[\"attention_mask\"])\n",
    "\n",
    "        if dimensions is not None:\n",
    "            if not torch.all(dimensions > 0):\n",
    "                raise ValueError(\"Dimensions must be positive\")\n",
    "\n",
    "            fill_value = torch.tensor(\n",
    "                float(\"-inf\"), dtype=embeddings.dtype, device=embeddings.device\n",
    "            )\n",
    "\n",
    "            clipped_dimensions = torch.where(\n",
    "                dimensions < embeddings.shape[1],\n",
    "                dimensions,\n",
    "                torch.tensor(embeddings.shape[1], device=embeddings.device),\n",
    "            )\n",
    "\n",
    "            embeddings = embeddings.masked_fill(\n",
    "                torch.arange(embeddings.shape[1], device=embeddings.device)\n",
    "                >= clipped_dimensions.unsqueeze(-1),\n",
    "                fill_value,\n",
    "            )[:, : dimensions.max()]\n",
    "\n",
    "        if self.normalize:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc05430-d756-4b05-9010-43868f50bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(torch.nn.Module):\n",
    "    def __init__(self, pooling_mode: str):\n",
    "        super().__init__()\n",
    "        self.pooling_mode = pooling_mode\n",
    "\n",
    "    def forward(\n",
    "        self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "\n",
    "        pool_type = self.pooling_mode\n",
    "        if pool_type == \"avg\":\n",
    "            epsilon = 1e-9  # A small value to avoid division by zero\n",
    "            emb = last_hidden.sum(dim=1) / (attention_mask.sum(dim=1)[..., None] + epsilon)\n",
    "        elif pool_type == \"cls\":  # tokenizer padding right\n",
    "            emb = last_hidden[:, 0]\n",
    "        elif pool_type == \"cls__left\":  # tokenizer padding left\n",
    "            seq_idxs = (1 - attention_mask).sum(dim=1)\n",
    "            batch_size = last_hidden.shape[0]\n",
    "            batch_idxs = torch.arange(batch_size, device=last_hidden.device)\n",
    "            emb = last_hidden[batch_idxs, seq_idxs]\n",
    "        elif pool_type == \"last\":  # tokenizer padding left\n",
    "            emb = last_hidden[:, -1]\n",
    "        elif pool_type == \"last__right\":  # tokenizer padding right\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden.shape[0]\n",
    "            emb = last_hidden[torch.arange(batch_size, device=last_hidden.device), sequence_lengths]\n",
    "        else:\n",
    "            raise ValueError(f\"pool_type {pool_type} not supported\")\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115dac32-8274-48f0-98cd-91e5ca444fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformers_model(\n",
    "    model_name_or_path: Union[str, os.PathLike[str]],\n",
    "    normalize: bool,\n",
    "    pooling_mode: Optional[Literal[\"avg\", \"cls\", \"last\"]] = None,\n",
    "    torch_dtype: Optional[Union[torch.dtype, str]] = None,\n",
    "    trust_remote_code: bool = False,\n",
    "):\n",
    "    # check that the tokenizer matches the requirements of the pooling mode\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path, trust_remote_code=trust_remote_code\n",
    "    )\n",
    "    pooling_mode = pooling_mode or \"avg\"\n",
    "    if pooling_mode == \"last\" and tokenizer.padding_side == \"right\":\n",
    "        pooling_mode = \"last__right\"  # type: ignore\n",
    "    if pooling_mode == \"cls\" and tokenizer.padding_side == \"left\":\n",
    "        pooling_mode = \"cls__left\"  # type: ignore\n",
    "\n",
    "    # load the model\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name_or_path, torch_dtype=torch_dtype, trust_remote_code=trust_remote_code\n",
    "    ).eval()\n",
    "\n",
    "    # configure pooling\n",
    "    pooling_module = Pooling(pooling_mode=pooling_mode)\n",
    "\n",
    "    # NV-Embed-v1 model has seperate embedding model and a built-in pooling module\n",
    "    if (\n",
    "        model.__class__.__name__ == \"NVEmbedModel\"\n",
    "        and hasattr(model, \"latent_attention_model\")\n",
    "        and hasattr(model, \"embedding_model\")\n",
    "    ):\n",
    "        pooling_module = model.latent_attention_model\n",
    "        model = model.embedding_model\n",
    "\n",
    "    adapted_model = TextEmbeddingModelAdapter(\n",
    "        model=model, normalize=normalize, pooling_module=pooling_module\n",
    "    )\n",
    "    return adapted_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5c0ac-94e2-4af4-95bb-05bf3cf854b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbeddingModelAdapter(\n",
    "    model_path: str | os.PathLike[str],\n",
    "    normalize: bool,\n",
    "    *args,\n",
    "    pooling_mode: Optional[Literal[\"avg\", \"cls\", \"last\"]] = None,\n",
    "    trust_remote_code: bool = False,\n",
    "    **kwargs,\n",
    ") -> TextEmbeddingModelAdapter:\n",
    "    \"\"\"Returns a callable that returns a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the model.\n",
    "        normalize: Whether or not to normalize embeddings.\n",
    "        pooling_mode: Pooling to apply.\n",
    "        trust_remote_code: Whether or not to run custom code.\n",
    "\n",
    "    Returns:\n",
    "        TextEmbeddingModelAdapter.\n",
    "    \"\"\"\n",
    "    return get_transformers_model(\n",
    "        model_path,\n",
    "        normalize=normalize,\n",
    "        pooling_mode=pooling_mode,\n",
    "        trust_remote_code=trust_remote_code,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cfd71-225b-4874-9fa9-c45a6d6dc99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "hf_model_path = \"/opt/checkpoints/llama_embedding_converted_hf\"\n",
    "quantization_calibration_data = \"/opt/checkpoints/question_doc_pairs_500.json\"\n",
    "\n",
    "# HF model parameters\n",
    "pooling_mode = \"last\"\n",
    "normalize = False\n",
    "\n",
    "# ONNX params\n",
    "opset = 17\n",
    "onnx_export_path = \"/opt/checkpoints/llama_embedding_onnx/\"\n",
    "export_dtype = \"fp32\"\n",
    "use_dimension_arg = True\n",
    "\n",
    "# TRT params\n",
    "trt_model_path = Path(\"/opt/checkpoints/llama_embedding_trt/model.plan\")\n",
    "override_layers_to_fp32 = [\"/model/norm/\", \"/pooling_module\", \"/ReduceL2\", \"/Div\", ]\n",
    "override_layernorm_precision_to_fp32 = True\n",
    "profiling_verbosity = \"layer_names_only\"\n",
    "\n",
    "# Other params\n",
    "quantize_model = False\n",
    "quantization_type = \"fp8\"\n",
    "export_to_trt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539a33a-fea9-4168-a179-c277120767fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the model first\n",
    "model, tokenizer = EmbeddingModelAdapter(\n",
    "    model_path=hf_model_path,\n",
    "    normalize=normalize,\n",
    "    pooling_mode=pooling_mode,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd98f4-1cd4-4c0b-8b92-7bb79991de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.export.onnx_llm_exporter import OnnxLLMExporter\n",
    "\n",
    "if use_dimension_arg:\n",
    "    input_names = [\"input_ids\", \"attention_mask\", \"dimensions\"]\n",
    "    dynamic_axes_input = {\"input_ids\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "                            \"attention_mask\": {0: \"batch_size\", 1: \"seq_length\"}, \"dimensions\": {0: \"batch_size\"}}\n",
    "else:\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    dynamic_axes_input = {\"input_ids\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "                            \"attention_mask\": {0: \"batch_size\", 1: \"seq_length\"}}\n",
    "\n",
    "output_names = [\"embeddings\"]\n",
    "dynamic_axes_output = {\"embeddings\": {0: \"batch_size\", 1: \"embedding_dim\"}}\n",
    "\n",
    "onnx_exporter = OnnxLLMExporter(\n",
    "    onnx_model_dir=onnx_export_path, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "if quantize_model:\n",
    "    onnx_exporter.ptq(\n",
    "        calibration_data=quantization_calibration_data,\n",
    "        quantization_type=quantization_type,\n",
    "    )\n",
    "\n",
    "onnx_exporter.export(    \n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset=opset,\n",
    "    dynamic_axes_input=dynamic_axes_input,\n",
    "    dynamic_axes_output=dynamic_axes_output,\n",
    "    export_dtype=\"fp32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aab9b9-97d0-485c-8d86-dbd21b9a6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_to_trt:\n",
    "    if use_dimension_arg:\n",
    "        input_profiles = [{\"input_ids\": [[1, 3], [16, 128], [64, 256]], \"attention_mask\": [[1, 3], [16, 128], [64, 256]],\n",
    "                            \"dimensions\": [[1], [16], [64]]}]\n",
    "    else:\n",
    "        input_profiles = [{\"input_ids\": [[1, 3], [16, 128], [64, 256]], \"attention_mask\": [[1, 3], [16, 128], [64, 256]]}]\n",
    "\n",
    "    onnx_exporter.export_onnx_to_trt(\n",
    "        trt_model_path=Path(trt_model_path),\n",
    "        profiles=input_profiles,\n",
    "        override_layernorm_precision_to_fp32=override_layernorm_precision_to_fp32,\n",
    "        override_layers_to_fp32=override_layers_to_fp32,\n",
    "        profiling_verbosity=profiling_verbosity,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051200b7-6eba-44db-b223-059f1dfb60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"hello\", \"world\"]\n",
    "\n",
    "if use_dimension_arg:\n",
    "    prompt = onnx_exporter.get_tokenizer(prompt)\n",
    "    prompt[\"dimensions\"] = [[2]]\n",
    "\n",
    "print(onnx_exporter.forward(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3a912-efee-4aad-8cdb-4189a999035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nemo.export.tensorrt_lazy_compiler import TRTEngine\n",
    "#engine = TRTEngine(plan_path=\"/opt/checkpoints/llama_embedding_trt/model.plan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
