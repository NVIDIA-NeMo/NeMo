{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41624854-08ba-4dc0-89d9-b86d75885b66",
   "metadata": {},
   "source": [
    "## Optimizing Supervised Fine-Tuning (SFT) with FP8 Precision\n",
    "This tutorial focuses on integrating FP8 Training to enhance the efficiency of Supervised Fine-Tuning (SFT) for Large Language Models (LLMs). \n",
    "\n",
    "FP8 is a lower-precision numerical format that offers significant advantages over traditional mixed-precision training (BF16 and FP32), including faster computation and reduced memory consumption without a notable loss in model accuracy.\n",
    "\n",
    "The core of FP8 training lies in managing the wide dynamic range of values present in transformer architectures. To achieve this, specialized scaling strategies are employed, such as per-tensor and per-block scaling. Per-tensor scaling applies a unique scaling factor to each tensor, while per-block scaling, a more granular method, further optimizes accuracy on newer hardware like NVIDIA Blackwell GPUs. These strategies are crucial for maintaining numerical stability and ensuring the reliability of the training process. The NVIDIA NeMo Framework simplifies this by providing high-level configurations for these FP8 recipes, making it easier to integrate them into your SFT workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e948cb-c2b3-47d7-99d0-37916d810123",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6951697-91f8-422f-82fd-740e68f8c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                                                                                                                                                                        \n",
    "import torch                                                                                                                                                                                                     \n",
    "import fiddle as fdl\n",
    "from typing import List, Optional\n",
    "\n",
    "from nemo import lightning as nl                                                                                                                                                                                 \n",
    "from nemo.collections import llm       \n",
    "from nemo.collections.llm import import_ckpt\n",
    "                                                                                                                                                                                                                                                                                                                                                                  \n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from lightning.pytorch.loggers import TensorBoardLogger,WandbLogger     \n",
    "from nemo.lightning.pytorch.callbacks import ModelCheckpoint                                                                                                                                                     \n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule    \n",
    "from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer                                                                                                                                \n",
    "from nemo.collections.llm.gpt.model.llama import Llama31Config8B, LlamaModel\n",
    "from nemo.collections.llm.recipes.optim.adam import distributed_fused_adam_with_cosine_annealing                                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225ca31-825e-4a34-b0be-443fee945b20",
   "metadata": {},
   "source": [
    "### Converting Hugging Face Checkpoint to NeMo Format\n",
    "Before we can perform Supervised Fine-Tuning with FP8, we need to convert our desired model from a Hugging Face checkpoint into the NeMo format. \n",
    "\n",
    "This conversion is a crucial first step that allows the NeMo framework to manage the model's architecture and weights, making it compatible with NeMo's advanced training features, including FP8 quantization and scaling recipes.\n",
    "\n",
    "To perform this conversion, you will use a Python script, for example, named `01_convert_to_nemo.py`. You will need to specify the path to your Hugging Face model (`hf_model_path`) and the desired output path for the NeMo model (`nemo_model_path`) within the script. The script should then be executed from your terminal.\n",
    "\n",
    "Below is an example of such a script for converting the `Llama-3.1-8B-Instruct model`:\n",
    "\n",
    "```\n",
    "import sys\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm import import_ckpt\n",
    "\n",
    "nemo_model_path  = \"/workspace/nemo/models/Llama-3.1-8B-Instruct-Nemo\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hf_model_path = \"/workspace/nemo/models/Llama-3.1-8B-Instruct/\"\n",
    "    import_ckpt(model=llm.LlamaModel(llm.Llama31Config8B()), source=f\"hf://{hf_model_path}\", output_path=nemo_model_path) \n",
    "```\n",
    "\n",
    "The output directory will look like below, confirming the successful conversion:\n",
    "\n",
    "```\n",
    "Converted Llama model to Nemo, model saved to /workspace/nemo/models/Llama-3.1-8B-Instruct-Nemo in torch.bfloat16.\n",
    "✓ Checkpoint imported to /workspace/nemo/models/Llama-3.1-8B-Instruct-Nemo\n",
    "Imported Checkpoint\n",
    "├── context/\n",
    "│   ├── artifacts/\n",
    "│   │   └── generation_config.json\n",
    "│   ├── nemo_tokenizer/\n",
    "│   │   ├── chat_template.jinja\n",
    "│   │   ├── special_tokens_map.json\n",
    "│   │   ├── tokenizer.json\n",
    "│   │   └── tokenizer_config.json\n",
    "│   ├── io.json\n",
    "│   └── model.yaml\n",
    "└── weights/\n",
    "    ├── .metadata\n",
    "    ├── __0_0.distcp\n",
    "    ├── __0_1.distcp\n",
    "    └── common.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dae91-63c7-46c9-af03-c043c2d4de9c",
   "metadata": {},
   "source": [
    "### Configuring the Training Parameters\n",
    "\n",
    "Now that the model has been converted to the `nemo` format, we can proceed to configure the training parameters for our Supervised Fine-Tuning (SFT) job. \n",
    "\n",
    "This involves setting key values related to model parallelism, sequence length, and other performance-related options.\n",
    "\n",
    "Configuration Parameters:\n",
    "- `sequence_length=8192` : Sets the maximum length of the input sequence that the model can handle during training. A larger value allows the model to process more context, which is beneficial for tasks that require a deep understanding of long documents or conversations.\n",
    "- `tensor_parallel_size=1` : Controls how the model's layers are sharded across GPUs. A value of 2, for instance, would shard the layers across two GPUs. A value of 1 indicates that tensor parallelism is not enabled, and all layers of the model are kept on a single GPU.\n",
    "- `pipeline_parallel_size=1` : Controls the distribution of the model's layers across multiple GPUs in a pipeline fashion. A value of 2 would split the model into two stages, each running on a separate GPU. A value of 1 means pipeline parallelism is not enabled, and the entire model, from input to output, will be processed on a single GPU.\n",
    "- `context_parallel_size=1` : Controls the sharding of input sequences across GPUs. A value of 2 would split the input context into two parts and process them on different GPUs. A value of 1 indicates that context parallelism is not used, which is consistent with a single-GPU setup.\n",
    "- `sequence_parallel=False` : Determines whether to enable sequence parallelism. Setting it to False means sequence parallelism is not used.\n",
    "- `hf_tokenizer_path='/workspace/nemo/models/Llama-3.1-8B-Instruct/'` : Specifies the file path to the tokenizer associated with the original Hugging Face model. The tokenizer is essential for converting text data into numerical tokens that the model can understand.\n",
    "- `micro_batch_size=8` : Defines the batch size for a single GPU. It represents the number of samples processed by a single device before gradients are accumulated.\n",
    "- `global_batch_size=256` : Total effective batch size across all GPUs. It's typically a multiple of the micro_batch_size and the number of GPUs. A larger global_batch_size often allows for a more stable training process and is achieved by accumulating gradients from multiple micro_batch_size steps.\n",
    "\n",
    "This combination of parameters is the standard configuration for running the SFT tutorial on a single GPU, ensuring that the entire model fits on the device and is processed without parallelization overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e388ba6-90b8-4fa8-8e4f-bc9f5d0d21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=8192                                                                                                                                                                                             \n",
    "tensor_parallel_size=1                                                                                                                                                                                           \n",
    "pipeline_parallel_size=1                                                                                                                                                                                         \n",
    "virtual_pipeline_parallel_size=0                                                                                                                                                                             \n",
    "context_parallel_size=1                                                                                                                                                                                          \n",
    "sequence_parallel=False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b08de-9e24-4938-8b74-b72f0d744ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer_path='/workspace/nemo/models/Llama-3.1-8B-Instruct/'                                                                                                                                                      \n",
    "                                                                                                                                                                                                                 \n",
    "micro_batch_size=8                                                                                                                                                                                              \n",
    "global_batch_size=256\n",
    "load_optimizer=False     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d4f92-a1a0-46bd-8922-55f463fddecb",
   "metadata": {},
   "source": [
    "### Setting up the Data Module\n",
    "Next, we will configure the data loader for our training job. The `llm.SquadDataModule` class is a data module provided by NeMo that is designed to handle the SQuAD dataset format. It automatically tokenizes the data and prepares it for training, adhering to the configurations we have already set.\n",
    "\n",
    "We will instantiate this class, passing in our previously defined parameters:\n",
    "\n",
    "- `seq_length` : The maximum sequence length for the model.\n",
    "- `tokenizer` : The path to the Hugging Face tokenizer.\n",
    "- `micro_batch_size` : The batch size per GPU.\n",
    "- `global_batch_size` : The total effective batch size.\n",
    "\n",
    "This ensures that our data preparation is consistent with the model's architecture and training settings.\n",
    "```\n",
    "train_dl = llm.SquadDataModule(\n",
    "    seq_length=sequence_length,\n",
    "    tokenizer=hf_tokenizer_path,\n",
    "    micro_batch_size=micro_batch_size,\n",
    "    global_batch_size=global_batch_size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f85bc4-36a2-4635-aaa9-3438c28c50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = llm.SquadDataModule(seq_length=sequence_length, tokenizer=hf_tokenizer_path, micro_batch_size=micro_batch_size, global_batch_size=global_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640d787-d8a7-49bb-9a54-d1968a836e09",
   "metadata": {},
   "source": [
    "### Weights & Biases (WandB) Logging\n",
    "To track our training progress, we will integrate Weights & Biases (WandB) logging. By using the `WandbLogger`, we can monitor key metrics like loss, learning rate, and other relevant information throughout the training process.\n",
    "\n",
    "We will define an `experiment_name` and `wandb_project_name` to organize our runs within the WandB dashboard. Then, we will instantiate the `WandbLogger` with these names.\n",
    "\n",
    "```\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "experiment_name=\"sft-llama-3.1-nemo2-mcore-fp8\"\n",
    "wandb_project_name='nemo2-sft-tutorial'\n",
    "\n",
    "wandb = WandbLogger(\n",
    "    project=wandb_project_name,\n",
    "    name=experiment_name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc661210-4796-4ac9-b4b3-be8a6fbbf104",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"sft-llama-3.1-nemo2-mcore-fp8\"\n",
    "wandb_project_name='nemo2-sft-tutorial'\n",
    "\n",
    "wandb = WandbLogger(\n",
    "    project=wandb_project_name,\n",
    "    name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a876f-d126-408b-87c1-d62606b107c6",
   "metadata": {},
   "source": [
    "### Optimizer Configuration\n",
    "In this step, we will configure the optimizer and learning rate scheduler for our SFT training. \n",
    "\n",
    "We will use the `distributed_fused_adam_with_cosine_annealing` function, which provides a high-performance Adam-based optimizer and a cosine annealing schedule. \n",
    "\n",
    "Configuration Parameters:\n",
    "\n",
    "- `learning_rate=5e-6` : This is the maximum learning rate (`max_lr`) that the scheduler will use. A smaller learning rate is often used for fine-tuning to prevent the model from deviating too far from its pre-trained state.\n",
    "- `warmup_steps=50` : This defines the number of steps during which the learning rate will gradually increase from a small value to the `max_lr`. Warm-up helps to stabilize training at the beginning.\n",
    "- `min_lr=5e-7` : This is the minimum learning rate that the scheduler will anneal down to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156973fc-e00f-4313-a715-e1a95790348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate= 5e-6                                                                                                                                                                                              \n",
    "warmup_steps=50                                                                                                                                                                                                  \n",
    "min_lr=5e-7\n",
    "optim_config = distributed_fused_adam_with_cosine_annealing(\n",
    "        max_lr=learning_rate,\n",
    "        min_lr=min_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        adam_beta2=0.98\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d7014-67f7-4af4-ac63-7e96a70dbf0d",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Initialization\n",
    "Now that we have all the configurations in place, we can instantiate the tokenizer and the Llama model itself. We will use the `get_nmt_tokenizer` function to load the tokenizer from our specified path. Then, we will create the model instance using the LlamaModel class, passing in the model's configuration and the tokenizer we just created. This prepares the model for the training process.\n",
    "```\n",
    "tokenizer = get_nmt_tokenizer(library='huggingface', model_name=hf_tokenizer_path)\n",
    "config = Llama31Config8B()\n",
    "model = LlamaModel(config=config, tokenizer=tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ade04-7b6d-4254-b60b-8acc9f9705f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_nmt_tokenizer(library='huggingface', model_name=hf_tokenizer_path)\n",
    "config = Llama31Config8B()\n",
    "model = LlamaModel(config=config, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfcce0-1c1a-4b0d-9b43-8b944798e74c",
   "metadata": {},
   "source": [
    "### FP8 Recipes: Scaling Strategies for Mixed Precision Training\n",
    "This section introduces the different FP8 recipes, which are crucial for managing numerical stability and performance during mixed-precision training. These recipes are configured using the `MegatronMixedPrecision` plugin, which enables the use of FP8 for accelerated training on compatible hardware.\n",
    "\n",
    "The choice of recipe depends on the desired balance between performance and training stability.\n",
    "\n",
    "`bf16` : This recipe represents the baseline. It performs training using `bfloat16` mixed precision without any FP8 integration. This serves as a control for comparing the performance and accuracy of the FP8 recipes.\n",
    "\n",
    "```\n",
    "if recipe == \"bf16\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "    )\n",
    "```\n",
    "\n",
    "`delayed` : This is a FP8 recipe that uses a history of maximum absolute values (amax) to determine the scaling factors. This approach is decently stable and is often a good starting point for FP8 training.\n",
    "\n",
    "- `fp8_recipe=\"delayed\"` : Specifies the use of the delayed scaling strategy.\n",
    "- `fp8_amax_history_len=1024` : Sets the length of the amax history used to compute the scaling factor.\n",
    "- `fp8_amax_compute_algo=\"max\"` : Determines the algorithm for computing the amax value from the history.\n",
    "\n",
    "```\n",
    "if recipe == \"delayed\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        fp8=\"hybrid\",\n",
    "        fp8_recipe=\"delayed\",\n",
    "        fp8_margin=0,\n",
    "        fp8_amax_history_len=1024,\n",
    "        fp8_amax_compute_algo=\"max\",\n",
    "        fp8_param_gather=True,\n",
    "    )\n",
    "```\n",
    "\n",
    "`tensorwise_fp8` : This recipe applies per-tensor scaling but offers additional control over which layers remain in bfloat16 precision for improved stability.\n",
    "\n",
    "- `first_last_layers_bf16=True` : Keeps the first and last layers of the model in bf16 precision.\n",
    "- `num_layers_at_start_in_bf16=1` : Specifies the number of layers at the beginning to keep in bf16.\n",
    "- `num_layers_at_end_in_bf16=1:` Specifies the number of layers at the end to keep in bf16.\n",
    "\n",
    "```\n",
    "if recipe == \"tensorwise_fp8\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        fp8=\"hybrid\",\n",
    "        fp8_recipe=\"tensorwise\",\n",
    "        first_last_layers_bf16=True,\n",
    "        num_layers_at_start_in_bf16=1,\n",
    "        num_layers_at_end_in_bf16=1,\n",
    "        fp8_param_gather=True,\n",
    "    )\n",
    "```\n",
    "\n",
    "`mxfp8` : This recipe, a block scaling strategy, is designed for NVIDIA's Blackwell GPUs. This is supported on newer hardware like the NVIDIA Blackwell architecture and can provide better accuracy and stability by accommodating local variations in magnitude within a single tensor.\n",
    "\n",
    "\n",
    "```\n",
    "if recipe == \"mxfp8\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        fp8=\"hybrid\",\n",
    "        fp8_recipe=\"mxfp8\",\n",
    "        fp8_param_gather=True,\n",
    "    )\n",
    "```\n",
    "\n",
    "`blockwise_fp8` : This is an advanced per-block scaling strategy, offering a more granular approach to quantization by assigning a dedicated scaling factor to small, contiguous blocks within a tensor. \n",
    "\n",
    "```\n",
    "if recipe == \"blockwise_fp8\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        fp8=\"hybrid\",\n",
    "        fp8_recipe=\"blockwise\",\n",
    "        fp8_param_gather=True,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769d545-4617-47c9-bda3-fbb2002d68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = \"bf16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b07d50-183e-4695-a9b8-59559028e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recipe == \"bf16\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "    precision=\"bf16-mixed\",)\n",
    "\n",
    "if recipe == \"delayed\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "    precision=\"bf16-mixed\",\n",
    "    fp8=\"hybrid\",\n",
    "    fp8_recipe=\"delayed\",\n",
    "    fp8_margin=0,\n",
    "    fp8_amax_history_len=1024,\n",
    "    fp8_amax_compute_algo=\"max\",\n",
    "    fp8_param_gather=True,)\n",
    "    \n",
    "if recipe == \"mxfp8\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "    precision=\"bf16-mixed\",\n",
    "    fp8=\"hybrid\",\n",
    "    fp8_recipe=\"mxfp8\",\n",
    "    fp8_param_gather=True,)\n",
    "\n",
    "if recipe == \"tensorwise_fp8\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "    precision=\"bf16-mixed\",\n",
    "    fp8=\"hybrid\",\n",
    "    fp8_recipe=\"tensorwise\",\n",
    "    first_last_layers_bf16=True,\n",
    "    num_layers_at_start_in_bf16=1,\n",
    "    num_layers_at_end_in_bf16=1,\n",
    "    fp8_param_gather=True,)\n",
    "\n",
    "if recipe == \"blockwise_fp8\":\n",
    "    plugins = nl.MegatronMixedPrecision(\n",
    "    precision=\"bf16-mixed\",\n",
    "    fp8=\"hybrid\",\n",
    "    fp8_recipe=\"blockwise\",\n",
    "    fp8_param_gather=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934beac-0fa1-45e6-b51e-ba9cd5d0287b",
   "metadata": {},
   "source": [
    "### Setting up the Trainer and Strategy\n",
    "\n",
    "Finally, we will configure the `Trainer` and the `MegatronStrategy`. The Trainer handles the core training loop, while the `MegatronStrategy` is a specialized plugin for distributed training of large models using the Megatron-LM framework.\n",
    "\n",
    "Trainer Parameters:\n",
    "- `num_nodes=1` : The number of compute nodes to use.\n",
    "- `devices=8` : The number of GPUs to use per node. In this example, we're using 8 GPUs on a single node.\n",
    "- `max_steps=10000` : The total number of steps to run.\n",
    "- `log_every_n_steps=10` : The frequency (in steps) to log training metrics.\n",
    "- `val_check_interval=200` : The frequency (in steps) to run a validation epoch.\n",
    "- `accelerator=\"gpu\"` : Specifies that training should run on GPUs.\n",
    "- `strategy=strategy` : The distributed training strategy to use.\n",
    "- `plugins=plugins` : The mixed precision plugin, which includes our FP8 recipe.\n",
    "- `logger=wandb` : The logger to use for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f58eb4-85e0-438a-9c84-0056f8b4d8ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes=1                                                                                                                                                                                                          \n",
    "gpu_devices=8\n",
    "max_steps=10000     \n",
    "log_every_n_steps=10\n",
    "val_check_interval=200                                                                                                                                                                                           \n",
    "limit_val_batches=8  \n",
    "\n",
    "\n",
    "strategy = nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=tensor_parallel_size,\n",
    "        pipeline_model_parallel_size=pipeline_parallel_size,\n",
    "        pipeline_dtype=torch.bfloat16,\n",
    "        virtual_pipeline_parallel_size=virtual_pipeline_parallel_size,\n",
    "        sequence_parallel=sequence_parallel,\n",
    "        context_parallel_size=context_parallel_size,\n",
    "        ckpt_load_optimizer=load_optimizer,\n",
    "        ckpt_load_strictness=\"log_all\")\n",
    "\n",
    "trainer = nl.Trainer(\n",
    "        num_nodes=nodes,\n",
    "        devices=gpu_devices,\n",
    "        max_steps=max_steps,\n",
    "        log_every_n_steps=log_every_n_steps,\n",
    "        val_check_interval=val_check_interval,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=plugins,\n",
    "        logger=wandb\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85138afa-7f24-408b-898c-2d9f46bc9ee0",
   "metadata": {},
   "source": [
    "### Running the Fine-Tuning Job\n",
    "\n",
    "With all the components configured, the final step is to start the fine-tuning process by calling the `llm.finetune` function. This function ties together the model, data, trainer, optimizer, and logging configurations to launch the training job.\n",
    "\n",
    "Note: Since this tutorial is configured for multi-GPU training (`devices=8`), it cannot be run directly in a Jupyter Notebook environment. Jupyter Notebooks have limitations with multi-process execution, which is required for distributed training. \n",
    "\n",
    "```\n",
    "llm.finetune(\n",
    "    model=model,\n",
    "    data=train_dl,\n",
    "    trainer=trainer,\n",
    "    optim=optim,\n",
    "    log=logger,\n",
    "    resume=resume\n",
    ")\n",
    "```\n",
    "\n",
    "Instead, you should save all of the code in a single Python file, for example, `02_main.py`, and execute it from your terminal using \n",
    "\n",
    "`torchrun --nnodes 1 --nproc-per-node 8 02_main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e81b8f-e815-48f6-952b-bfee7e6ae840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
