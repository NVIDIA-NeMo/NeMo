{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) LLMs for Function Calling\n",
    "\n",
    "In the application of LLMs, agents represent an exciting field. They are intelligent systems capable of simulating human-like intelligent behavior to perform specific tasks or services. LLM-based agents can leverage the powerful comprehension and generation capabilities of LLMs while also incorporating the planning and function-calling abilities to accomplish many complex tasks.\n",
    "\n",
    "In this tutorial, we will demonstrate how to perform SFT to learn function-calling (tool learning) using NeMo 2.0. NeMo 2.0 introduces Python-based configurations, PyTorch Lightning’s modular abstractions, and NeMo-Run for scaling experiments across multiple GPUs. In this notebook, we will use NeMo-Run to streamline the configuration and execution of our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo Tools and Resources\n",
    "\n",
    "* [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)\n",
    "\n",
    "# Software Requirements\n",
    "\n",
    "* Access to latest NeMo Framework NGC Containers\n",
    "\n",
    "\n",
    "# Hardware Requirements\n",
    "\n",
    "* This playbook has been tested on the following hardware: Single A6000, Single H100, 2xA6000, 8xH100. It can be scaled to multiple GPUs as well as multiple nodes by modifying the appropriate parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch the NeMo Framework container as follows: \n",
    "\n",
    "Depending on the number of gpus, `--gpus` might need to adjust accordingly:\n",
    "```\n",
    "docker run -it -p 8080:8080 -p 8088:8088 --rm --gpus '\"device=0,1\"' --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:25.02\n",
    "```\n",
    "\n",
    "#### Launch Jupyter Notebook as follows: \n",
    "```\n",
    "jupyter notebook --allow-root --ip 0.0.0.0 --port 8088 --no-browser --NotebookApp.token=''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Construct the Dataset\n",
    "\n",
    "An LLM agent is a system that leverages a LLM as its core engine, capable of executing specific tasks by invoking functionalities known as tools. These tools can be APIs, databases, calculators, etc., allowing the agent to obtain necessary information or perform operations while completing tasks. As shown below, LLM understands the usage of each tool via three text fields: `name`, `description`, `parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:13:43.342769Z",
     "start_time": "2025-02-12T15:13:43.338024Z"
    }
   },
   "outputs": [],
   "source": [
    "tool1 = {\n",
    "    'name': 'strategy_query', \n",
    "    'description': 'Check the initial quotes for financial products.', \n",
    "    'parameters': {'product': {'type': 'string', 'description': 'Product type.'},\n",
    "                   'term': {'type': 'string', 'description': 'Term.'}}\n",
    "}\n",
    "tool2 = {}\n",
    "tools = [tool1, tool2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform fine-tuning in NeMo 2.0, we should first transform the training dataset into a predefined format in NeMo 2.0. According to the different training strategies you use, there are two types of function-calling dataset formats:\n",
    "\n",
    "* The first one is a single-turn function-calling dataset. For each piece of data, the assistant only calls the function once, and the conversation does not record the function-calling execution result. We focus on training the LLM for correct function selection and correct function parameters generation.\n",
    "\n",
    "* The second one is a multi-turn function-calling dataset. For each piece of data, the assistant calls the function once or more than once, and the conversation records the function-calling execution result. The assistant will make the next generation based on the function's return. It looks closer to a real-life scenario.\n",
    "\n",
    "Both types of datasets work for function-calling SFT. You can choose one of them or a combination of them, as long as it's suitable for your target.\n",
    "\n",
    "#### Option 1: Use the Single-Turn Function-Calling Dataset\n",
    "\n",
    "Let's take a data from [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) as an example. The original data format is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'system': 'SYSTEM: You are a helpful assistant with access to the following functions. Use them if required -\\n{\\n    \"name\": \"generate_password\",\\n    \"description\": \"Generate a random password\",\\n    \"parameters\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"length\": {\\n                \"type\": \"integer\",\\n                \"description\": \"The length of the password\"\\n            },\\n            \"include_symbols\": {\\n                \"type\": \"boolean\",\\n                \"description\": \"Whether to include symbols in the password\"\\n            }\\n        },\\n        \"required\": [\\n            \"length\"\\n        ]\\n    }\\n}\\n',\n",
       "  'chat': 'USER: Hi, I need a new password for my account. Can you generate one for me?\\n\\n\\nASSISTANT: Of course, I can help with that. How long would you like your password to be? And do you want it to include symbols? <|endoftext|>\\n\\n\\nUSER: I would like it to be 12 characters long and yes, please include symbols.\\n\\n\\nASSISTANT: <functioncall> {\"name\": \"generate_password\", \"arguments\": \\'{\"length\": 12, \"include_symbols\": true}\\'} <|endoftext|>\\n\\n\\nFUNCTION RESPONSE: {\"password\": \"4&7jB#9L!2Z%\"}\\n\\n\\nASSISTANT: Here is your new password: 4&7jB#9L!2Z%. Please make sure to save it in a secure place. <|endoftext|>\\n\\n\\nUSER: Great, thanks! Can you also help me book a flight to New York?\\n\\n\\nASSISTANT: I\\'m sorry, but as an AI, I don\\'t have the capability to book flights or perform other external tasks. My primary function is to assist with tasks related to the functions provided to me. In this case, I can help generate passwords. For booking a flight, I recommend using a trusted travel website or contacting your travel agent. <|endoftext|>\\n\\n\\n'},)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"system\": \"SYSTEM: You are a helpful assistant with access to the following functions. Use them if required -\\n{\\n    \\\"name\\\": \\\"generate_password\\\",\\n    \\\"description\\\": \\\"Generate a random password\\\",\\n    \\\"parameters\\\": {\\n        \\\"type\\\": \\\"object\\\",\\n        \\\"properties\\\": {\\n            \\\"length\\\": {\\n                \\\"type\\\": \\\"integer\\\",\\n                \\\"description\\\": \\\"The length of the password\\\"\\n            },\\n            \\\"include_symbols\\\": {\\n                \\\"type\\\": \\\"boolean\\\",\\n                \\\"description\\\": \\\"Whether to include symbols in the password\\\"\\n            }\\n        },\\n        \\\"required\\\": [\\n            \\\"length\\\"\\n        ]\\n    }\\n}\\n\",\n",
    "    \"chat\": \"USER: Hi, I need a new password for my account. Can you generate one for me?\\n\\n\\nASSISTANT: Of course, I can help with that. How long would you like your password to be? And do you want it to include symbols? <|endoftext|>\\n\\n\\nUSER: I would like it to be 12 characters long and yes, please include symbols.\\n\\n\\nASSISTANT: <functioncall> {\\\"name\\\": \\\"generate_password\\\", \\\"arguments\\\": '{\\\"length\\\": 12, \\\"include_symbols\\\": true}'} <|endoftext|>\\n\\n\\nFUNCTION RESPONSE: {\\\"password\\\": \\\"4&7jB#9L!2Z%\\\"}\\n\\n\\nASSISTANT: Here is your new password: 4&7jB#9L!2Z%. Please make sure to save it in a secure place. <|endoftext|>\\n\\n\\nUSER: Great, thanks! Can you also help me book a flight to New York?\\n\\n\\nASSISTANT: I'm sorry, but as an AI, I don't have the capability to book flights or perform other external tasks. My primary function is to assist with tasks related to the functions provided to me. In this case, I can help generate passwords. For booking a flight, I recommend using a trusted travel website or contacting your travel agent. <|endoftext|>\\n\\n\\n\"\n",
    "},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should transform it into the NeMo chat dataset format, which consists of three fields: `mask`, `system` and `conversations`. \n",
    "\n",
    "* `mask`: The role that needs to be masked out to prevent the role from participating in loss calculation.\n",
    "\n",
    "* `system`: System prompt.\n",
    "\n",
    "* `conversations`: For each role, the conversation consists of two fields `from` and `value`.\n",
    "\n",
    "We can transform the original data into the format shown below. Since we're constructing a single-turn function-calling dataset, you can end the conversation at the point of the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ \n",
    "    \"mask\": \"User\", \n",
    "    \"system\": \"\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"User\", \n",
    "            \"value\": \"You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections. Here is a list of functions in JSON format that you can invoke.\\n<AVAILABLE_TOOLS>\\n{\\n    \\\"name\\\": \\\"generate_password\\\",\\n    \\\"description\\\": \\\"Generate a random password\\\",\\n    \\\"parameters\\\": {\\n        \\\"type\\\": \\\"object\\\",\\n        \\\"properties\\\": {\\n            \\\"length\\\": {\\n                \\\"type\\\": \\\"integer\\\",\\n                \\\"description\\\": \\\"The length of the password\\\"\\n            },\\n            \\\"include_symbols\\\": {\\n                \\\"type\\\": \\\"boolean\\\",\\n                \\\"description\\\": \\\"Whether to include symbols in the password\\\"\\n            }\\n        },\\n        \\\"required\\\": [\\n            \\\"length\\\"\\n        ]\\n    }\\n}\\n\\n{\\n    \\\"name\\\": \\\"create_task\\\",\\n    \\\"description\\\": \\\"Create a new task in a task management system\\\",\\n    \\\"parameters\\\": {\\n        \\\"type\\\": \\\"object\\\",\\n        \\\"properties\\\": {\\n            \\\"title\\\": {\\n                \\\"type\\\": \\\"string\\\",\\n                \\\"description\\\": \\\"The title of the task\\\"\\n            },\\n            \\\"due_date\\\": {\\n                \\\"type\\\": \\\"string\\\",\\n                \\\"format\\\": \\\"date\\\",\\n                \\\"description\\\": \\\"The due date of the task\\\"\\n            },\\n            \\\"priority\\\": {\\n                \\\"type\\\": \\\"string\\\",\\n                \\\"enum\\\": [\\n                    \\\"low\\\",\\n                    \\\"medium\\\",\\n                    \\\"high\\\"\\n                ],\\n                \\\"description\\\": \\\"The priority of the task\\\"\\n            }\\n        },\\n        \\\"required\\\": [\\n            \\\"title\\\",\\n            \\\"due_date\\\",\\n            \\\"priority\\\"\\n        ]\\n    }\\n}\\n\\n</AVAILABLE_TOOLS>\\nIf you decide to invoke any of the function(s), put it in the format of <TOOLCALL>[func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]</TOOLCALL>\\nYou SHOULD NOT include any other information in the response.\\n\\nI need a new password. Can you generate one for me?\"\n",
    "        }, \n",
    "        {\n",
    "            \"from\": \"Assistant\", \n",
    "            \"value\": \"Of course. How long would you like your password to be? And would you like it to include symbols?\"\n",
    "        }, \n",
    "        {\n",
    "            \"from\": \"User\", \n",
    "            \"value\": \"I would like it to be 12 characters long and yes, please include symbols.\"\n",
    "        }, \n",
    "        {\n",
    "            \"from\": \"Assistant\", \n",
    "            \"value\": \"<TOOLCALL>[generate_password(length=12, include_symbols=True)]</TOOLCALL>\"\n",
    "        }\n",
    "    ]\n",
    "},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the single-turn function-calling datasets described above, we've successfully fine-tuned [nvidia/Mistral-NeMo-Minitron-8B-Instruct](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct) based on [nvidia/Mistral-NeMo-Minitron-8B-Base](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base), which has a general function-calling ability. If you're interested, you can quickly experience its capabilities [NIM online](https://build.nvidia.com/nvidia/mistral-nemo-minitron-8b-8k-instruct). If you want to reproduce a model like [nvidia/Mistral-NeMo-Minitron-8B-Instruct](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct) using NeMo, you can refer to the three open-source datasets we used. \n",
    "*Note that we also used some internal datasets that are not open-sourced.*\n",
    "\n",
    "* [nvidia/Daring-Anteater](https://huggingface.co/datasets/nvidia/Daring-Anteater)\n",
    "\n",
    "* [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n",
    "\n",
    "* [Salesforce/xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k)\n",
    "\n",
    "* lr=1e-6 and 3 epoches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1 Download the HuggingFace Dataset\n",
    "As the purpose of this tutorial, we are going to download [Salesforce/xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k).\n",
    "First let's download the datasets from Hugging Face. You need to have valid HuggingFace token in order to access this gated repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `write` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `write`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <HF_TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|████████████████████████████████████████████████| 60/60 [00:00<00:00, 114.39ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94371637"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "xlam_ds = load_dataset('Salesforce/xlam-function-calling-60k', split='train')\n",
    "xlam_ds.to_json('xlam.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have `Salesforce/xlam-function-calling-60k` raw dataset file downloaded as `xlam.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_sft_function_calling_demo\tnemo2-chat-sft-function-calling.ipynb\n",
      "datasets\t\t\tnemo_experiments\n",
      "dialog.first30k.augment1.jsonl\tnemo_inference.py\n",
      "dialog_first30k_augment1.jsonl\txlam.jsonl\n",
      "import_baichuan2_7b.py\t\txlam_dataset\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2 Convert the Dataset to NeMo Chat SFT Dataset\n",
    "We then convert the raw dataset to NeMo style using the following data transformation script.\n",
    "NeMo's `ChatDataModule` requires `data_root` contains one `training.jsonl` and `validation.jsonl` for training and validation sets.\n",
    "\n",
    "Let's first define some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import ast, copy\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "possible_headers = [\n",
    "    \"You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections. Here is a list of functions in JSON format that you can invoke.\\n\",\n",
    "    \"You are a function-calling assistant. Your task is to identify and execute the appropriate functions from a given list based on the user's question. If no suitable function is available, specify this. If required parameters are missing, indicate this as well. Return only the function call in the specified format. Here is the list of available functions in JSON format\",\n",
    "    \"Imagine you are an AI designed to call functions. Given a question and a set of functions, your role is to make the necessary function calls. If a function cannot be used, state this. If parameters are missing, mention it. Here are the available functions\\n\",\n",
    "    \"You are an AI agent specialized in executing function calls. Your mission is to interpret questions and determine the correct functions to execute from a provided list. If no function applies, or if parameters are missing, you must indicate this. Below are the functions you can call\\n\",\n",
    "    \"You are an intelligent agent capable of invoking functions based on user queries. Given a question and a list of functions, your task is to identify and execute the appropriate functions. If no function is suitable, specify this. If required parameters are missing, indicate this as well. Return only the function call in the specified format. Here is the list of available functions in JSON format\\n\",\n",
    "    \"As an AI assistant, you are tasked with determining the appropriate function calls based on a question and a list of available functions. If no function can be used, or if parameters are missing, indicate this. Return only the function calls in the specified format. Functions are detailed in JSON format.\"\n",
    "]\n",
    "rejection_prompts = [\n",
    "    \"I'm sorry, but after reviewing the available tools, I couldn't find a function that suits your request. Please provide more information or specify a different function. If you need assistance with anything else, feel free to ask.\",\n",
    "    \"<TOOLCALL>[]</TOOLCALL>\"\n",
    "]\n",
    "def process_system_turn(j):\n",
    "    if random.choice([0,1]) == 0:\n",
    "        j[\"tools\"] = json.loads(j[\"tools\"])\n",
    "    header = random.choice(possible_headers)\n",
    "    tools = json.dumps(j[\"tools\"], indent=4) if isinstance(j[\"tools\"], dict) else j[\"tools\"]\n",
    "    if isinstance(tools, list):\n",
    "        tools = str(tools)\n",
    "    if random.choice([0,1]) == 0:\n",
    "        system = header + \"<AVAILABLE_TOOLS>\\n\" + tools + \"</AVAILABLE_TOOLS>\" + '\\n' + \"\"\"If you decide to invoke any of the function(s), put it in the format of <TOOLCALL>[func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]</TOOLCALL>\\nYou SHOULD NOT include any other information in the response.\"\"\"\n",
    "    else:\n",
    "        system = header +  \"\"\"If you decide to invoke any of the function(s), put it in the format of <TOOLCALL>[func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]</TOOLCALL>\\nYou SHOULD NOT include any other information in the response.\\n\"\"\" + \"<AVAILABLE_TOOLS>\\n\" + json.dumps(j[\"tools\"], indent=4) + \"</AVAILABLE_TOOLS>\" \n",
    "\n",
    "\n",
    "    return system\n",
    "\n",
    "def put_system_ito_user(j):\n",
    "    \n",
    "    system = j[\"system\"]\n",
    "    j[\"system\"] = \"\"\n",
    "    j[\"conversations\"][0][\"value\"] = system + '\\n\\n' + j[\"conversations\"][0][\"value\"]\n",
    "\n",
    "    return j\n",
    "\n",
    "def get_all_functions(jlines, arg=\"tools\"):\n",
    "    functions = []\n",
    "    for j in jlines:\n",
    "        f = get_functions(j, arg)\n",
    "        functions += f\n",
    "    return functions\n",
    "\n",
    "def get_functions(j, arg=\"tools\"):\n",
    "    try:\n",
    "        f = ast.literal_eval(j[arg])\n",
    "    except:\n",
    "        f = json.loads(j[arg])\n",
    "    if not isinstance(f, list):\n",
    "        f = [f]\n",
    "    \n",
    "    return f\n",
    "\n",
    "def process_function(functions):\n",
    "    try:\n",
    "        functions = ast.literal_eval(functions)\n",
    "    except:\n",
    "        try:\n",
    "            functions = json.loads(functions)\n",
    "        except:\n",
    "            print(functions)\n",
    "            return None\n",
    "\n",
    "    outputs=[]\n",
    "    for function in functions:\n",
    "        out = \"\"\n",
    "        name = function[\"name\"]\n",
    "        out += name + \"(\"\n",
    "        try:\n",
    "            arguments = json.loads(function[\"arguments\"]) if isinstance(function[\"arguments\"], str) else function[\"arguments\"]\n",
    "            if len(arguments) == 0:\n",
    "                return out + \")\"\n",
    "\n",
    "            for arg, v in arguments.items():\n",
    "                if isinstance(v, str):\n",
    "                    out += arg + \"=\" + '\"' + str(v) + '\", '\n",
    "                else:\n",
    "                    out += arg + \"=\" + str(v) + ', '\n",
    "            out = out[:-2] + \")\"\n",
    "            outputs.append(out)\n",
    "        except:\n",
    "            print(\"sec error\", function)\n",
    "\n",
    "    return \"<TOOLCALL>[\" + \", \".join(outputs) + \"]</TOOLCALL>\"\n",
    "\n",
    "def write_nemo_datasetfile(json_objects, output_folder, rejection_rate=0.3, train_ratio=0.95):\n",
    "    all_function = get_all_functions(json_objects, \"tools\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    #augmentation: add more functions to increase difficulty\n",
    "    for j in json_objects:\n",
    "        tools = json.loads(j[\"tools\"])\n",
    "        n = random.choice([0]*10 + [i for i in range(10)])\n",
    "        aug_f = random.sample(all_function, n)\n",
    "        diff_f = [f for f in aug_f if f not in tools]\n",
    "        tools += diff_f\n",
    "        random.shuffle(tools)\n",
    "        j[\"tools\"] = json.dumps(tools)\n",
    "\n",
    "    # augmentation: add rejection\n",
    "    rejs = []\n",
    "    for j in random.sample(json_objects, int(rejection_rate * len(json_objects))):\n",
    "        tools = json.loads(j[\"tools\"])\n",
    "        n = len(tools)\n",
    "        aug_f = random.sample(all_function, n)\n",
    "        diff_f = [f for f in aug_f if f not in tools]\n",
    "        tools = diff_f\n",
    "        if len(tools) == 0:\n",
    "            continue\n",
    "        new_j = copy.deepcopy(j)\n",
    "        new_j[\"tools\"] = json.dumps(tools)\n",
    "        new_j[\"rejection\"] = True\n",
    "        rejs.append(new_j)\n",
    "\n",
    "    # Adding the rejections to the list\n",
    "    json_objects += rejs\n",
    "    output = []\n",
    "    for j in json_objects:\n",
    "        d = {}\n",
    "        d[\"system\"] = process_system_turn(j)\n",
    "        d[\"mask\"] = \"User\"\n",
    "        if j.get(\"rejection\", False):\n",
    "            answer = random.choice(rejection_prompts)\n",
    "        else:\n",
    "            answer = process_function(j[\"answers\"])\n",
    "    \n",
    "        if answer == None:\n",
    "            continue\n",
    "        q = j[\"query\"]\n",
    "        d[\"conversations\"] = [{\"from\":\"User\", \"value\": q}, {\"from\":\"Assistant\", \"value\": answer}]\n",
    "    \n",
    "        output.append(d)\n",
    "        d = put_system_ito_user(d)\n",
    "        output.append(d)\n",
    "\n",
    "    # Split into train/val set\n",
    "    train_fout = open(f'{output_folder}/training.jsonl', 'w')\n",
    "    validation_fout = open(f'{output_folder}/validation.jsonl', 'w')\n",
    "    split_index = int(len(output) * train_ratio)\n",
    "    random.shuffle(output)\n",
    "    train_objects = output[:split_index]\n",
    "    val_objects = output[split_index:]\n",
    "\n",
    "    with open(f'{output_folder}/training.jsonl', 'w') as f:\n",
    "        for obj in train_objects:\n",
    "            f.write(json.dumps(obj) + '\\n')\n",
    "    with open(f'{output_folder}/validation.jsonl', 'w') as f:\n",
    "        for obj in val_objects:\n",
    "            f.write(json.dumps(obj) + '\\n')\n",
    "    print(f'Saved training.jsonl and validation.jsonl to {output_folder}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the raw dataset into training set and validation set using a fraction of 95%/5%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training.jsonl and validation.jsonl to xlam_dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "f_input = open(\"xlam.jsonl\")\n",
    "train_ratio = 0.90\n",
    "all_objects = [json.loads(l) for l in f_input.readlines()][:10000]\n",
    "\n",
    "write_nemo_datasetfile(all_objects, 'xlam_dataset', train_ratio=0.95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.jsonl\tvalidation.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls xlam_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use the Multi-Turns Function-Calling Dataset\n",
    "\n",
    "For multi-turn function-calling dataset construction, the process is similar to constructing a single-turn function-calling dataset. The only difference is that we need to add one more role, 'Function,' to represent the function-calling return. Let's take the data below as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat': [{'role': 'user', 'content': 'Is there overnight lending available?'},\n",
       "  {'role': 'assistant',\n",
       "   'func_call': {'function': 'strategy_query',\n",
       "    'params': {'term': 'overnight'}}},\n",
       "  {'role': 'assistant',\n",
       "   'func_return': {'strategy_query': [{'product': 'lending',\n",
       "      'term': 'overnight',\n",
       "      'amount': '1 billion',\n",
       "      'interest_rate': '2.0%'}]}},\n",
       "  {'role': 'assistant', 'content': 'Yes，1 billion，2.0%. Are you interested?'},\n",
       "  {'role': 'user', 'content': '2.0% is too high. I have to think about it.'},\n",
       "  {'role': 'assistant',\n",
       "   'func_call': {'function': 'transaction_cancel', 'params': {}}},\n",
       "  {'role': 'assistant',\n",
       "   'func_return': {'response': 'The transaction has been cancelled.'}}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"chat\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Is there overnight lending available?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"func_call\": {\n",
    "                    \"function\": \"strategy_query\",\n",
    "                    \"params\": {\n",
    "                        \"term\": \"overnight\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"func_return\": {\n",
    "                    \"strategy_query\": [\n",
    "                        {\n",
    "                            \"product\": \"lending\",\n",
    "                            \"term\": \"overnight\",\n",
    "                            \"amount\": \"1 billion\",\n",
    "                            \"interest_rate\": \"2.0%\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Yes，1 billion，2.0%. Are you interested?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"2.0% is too high. I have to think about it.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"func_call\": {\n",
    "                    \"function\": \"transaction_cancel\",\n",
    "                    \"params\": {}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"func_return\": {\n",
    "                    \"response\": \"The transaction has been cancelled.\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the original data into the NeMo chat format shown below. Note that, compared to the single-turn function-calling dataset, we add the 'Function' role in conversations to record the function-calling execution result. We should also mask out the user and function roles to prevent them from participating in loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mask': 'User,Function',\n",
       " 'system': \"Answer the following questions as best you can. You have access to the following tools:\\n\\ninquiry: Call this tool to interact with the inquiry API. What is the inquiry API useful for? 查询金融产品的价格。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\\n                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\\n                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\\nstrategy_query: Call this tool to interact with the strategy_query API. What is the strategy_query API useful for? 查询金融产品的交易策略。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True}]\\ntransaction_confirm: Call this tool to interact with the transaction_confirm API. What is the transaction_confirm API useful for? 确认交易。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\\n                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\\n                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\\ntransaction_cancel: Call this tool to interact with the transaction_cancel API. What is the transaction_cancel API useful for? 取消交易。 Parameters: []\\n\\nOutput the following format:\\n\\nAction: the action to take, should be one of [inquiry, strategy_query, transaction_confirm, transaction_cancel]\\nAction Input: the input to the action\",\n",
       " 'conversations': [{'from': 'User',\n",
       "   'value': 'Is there overnight lending available?'},\n",
       "  {'from': 'Assistant',\n",
       "   'value': 'Action: strategy_query\\\\nAction Input: {\\\\n \"term\": \"overnight\"\\\\n}'},\n",
       "  {'from': 'Function',\n",
       "   'value': '{\"error\": \"\", \"response\": {\\\\n \"product\": \"lending\",\\\\n \"term\": \"overnight\",\\\\n \"amount\": \"1 billion\",\\\\n \"interest_rate\": \"2.0%\"\\\\n}}'},\n",
       "  {'from': 'Assistant', 'value': 'Yes, 1 billion, 2.0%. Are you interested?'},\n",
       "  {'from': 'User', 'value': '2.0% is too high. I have to think about it.'},\n",
       "  {'from': 'Assistant',\n",
       "   'value': 'Action: transaction_cancel\\\\nAction Input: {}'},\n",
       "  {'from': 'Function',\n",
       "   'value': '{\"error\": \"\", \"response\": \"The transaction has been cancelled.\"}'}]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"mask\": \"User,Function\",\n",
    "    \"system\": \"Answer the following questions as best you can. You have access to the following tools:\\n\\ninquiry: Call this tool to interact with the inquiry API. What is the inquiry API useful for? 查询金融产品的价格。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\\n                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\\n                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\\nstrategy_query: Call this tool to interact with the strategy_query API. What is the strategy_query API useful for? 查询金融产品的交易策略。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True}]\\ntransaction_confirm: Call this tool to interact with the transaction_confirm API. What is the transaction_confirm API useful for? 确认交易。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\\n                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\\n                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\\ntransaction_cancel: Call this tool to interact with the transaction_cancel API. What is the transaction_cancel API useful for? 取消交易。 Parameters: []\\n\\nOutput the following format:\\n\\nAction: the action to take, should be one of [inquiry, strategy_query, transaction_confirm, transaction_cancel]\\nAction Input: the input to the action\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"value\": \"Is there overnight lending available?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Assistant\",\n",
    "            \"value\": \"Action: strategy_query\\\\nAction Input: {\\\\n \\\"term\\\": \\\"overnight\\\"\\\\n}\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Function\",\n",
    "            \"value\": \"{\\\"error\\\": \\\"\\\", \\\"response\\\": {\\\\n \\\"product\\\": \\\"lending\\\",\\\\n \\\"term\\\": \\\"overnight\\\",\\\\n \\\"amount\\\": \\\"1 billion\\\",\\\\n \\\"interest_rate\\\": \\\"2.0%\\\"\\\\n}}\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Assistant\",\n",
    "            \"value\": \"Yes, 1 billion, 2.0%. Are you interested?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"value\": \"2.0% is too high. I have to think about it.\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Assistant\",\n",
    "            \"value\": \"Action: transaction_cancel\\\\nAction Input: {}\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"Function\",\n",
    "            \"value\": \"{\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"The transaction has been cancelled.\\\"}\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Use NeMo-Run with NeMo2 Recipe\n",
    "\n",
    "After transforming the datasets, we should split and save the datasets into `training.jsonl`, `validation.jsonl` and `test.jsonl` under a folder. We can now start SFT using NeMo-Run and assign the datasets directory path to `dataset_root` in `nemo_run.Config`. NeMo-Run will automatically tokenize the datasets and save the binary under the same data folder. Despite the different dataset formats, whether it is a single-turn function-calling dataset or a multi-turn function-calling dataset, the training script using NeMo-Run remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.45.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.6.0a0+ecf3bae40a.nv25.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Baichuan dependency\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Auto Download and Convert the Baichuan2 7B model to NeMo2\n",
    "Baichuan2 7B model can be automatically downloaded and converted th NeMo2 format with the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting import_baichuan2_7b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile import_baichuan2_7b.py\n",
    "from nemo.collections import llm\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llm.import_ckpt(\n",
    "        model=llm.Baichuan2Model(config=llm.Baichuan2Config7B()),\n",
    "        source=\"hf://baichuan-inc/Baichuan2-7B-Base\",\n",
    "        overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-13 07:19:00 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "config.json: 100%|█████████████████████████████| 738/738 [00:00<00:00, 9.30MB/s]\n",
      "configuration_baichuan.py: 100%|███████████| 2.38k/2.38k [00:00<00:00, 44.6MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:\n",
      "- configuration_baichuan.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "modeling_baichuan.py: 100%|█████████████████| 33.1k/33.1k [00:00<00:00, 173MB/s]\n",
      "quantizer.py: 100%|█████████████████████████| 9.07k/9.07k [00:00<00:00, 138MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:\n",
      "- quantizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "generation_utils.py: 100%|█████████████████| 2.97k/2.97k [00:00<00:00, 61.0MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:\n",
      "- generation_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:\n",
      "- modeling_baichuan.py\n",
      "- quantizer.py\n",
      "- generation_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "[WARNING  | transformers_modules.baichuan-inc.Baichuan2-7B-Base.f9d4d8dd2f7a3dbede3bda3b0cf0224e9272bbe5.modeling_baichuan]: Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "pytorch_model.bin.index.json: 100%|█████████| 18.7k/18.7k [00:00<00:00, 213MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "pytorch_model-00001-of-00002.bin:   0%|             | 0.00/9.93G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   0%|    | 10.5M/9.93G [00:00<03:30, 47.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   0%|    | 21.0M/9.93G [00:00<02:44, 60.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   0%|    | 31.5M/9.93G [00:00<02:12, 74.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|    | 52.4M/9.93G [00:00<01:38, 99.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|     | 73.4M/9.93G [00:00<01:22, 120MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|      | 105M/9.93G [00:00<01:03, 155MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|      | 126M/9.93G [00:01<01:00, 161MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|      | 157M/9.93G [00:01<00:52, 185MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|      | 178M/9.93G [00:01<00:52, 187MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|▏     | 210M/9.93G [00:01<00:48, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|▏     | 241M/9.93G [00:01<00:45, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏     | 273M/9.93G [00:01<00:42, 227MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏     | 304M/9.93G [00:01<00:40, 240MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏     | 336M/9.93G [00:01<00:39, 241MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏     | 367M/9.93G [00:02<00:38, 249MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏     | 398M/9.93G [00:02<00:38, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   4%|▎     | 430M/9.93G [00:02<00:38, 247MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎     | 461M/9.93G [00:02<00:39, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎     | 493M/9.93G [00:02<00:39, 239MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎     | 524M/9.93G [00:02<00:38, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎     | 556M/9.93G [00:02<00:37, 249MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎     | 587M/9.93G [00:02<00:36, 255MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎     | 619M/9.93G [00:03<00:36, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▍     | 650M/9.93G [00:03<00:38, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▍     | 682M/9.93G [00:03<00:37, 247MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▍     | 713M/9.93G [00:03<00:37, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▍     | 744M/9.93G [00:03<00:37, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍     | 776M/9.93G [00:03<00:36, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍     | 807M/9.93G [00:03<00:38, 239MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   8%|▌     | 839M/9.93G [00:03<00:37, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▌     | 870M/9.93G [00:04<00:37, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▌     | 902M/9.93G [00:04<00:37, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▌     | 933M/9.93G [00:04<00:36, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|▌     | 965M/9.93G [00:04<00:36, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|▌     | 996M/9.93G [00:04<00:37, 241MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|▌    | 1.03G/9.93G [00:04<00:37, 240MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|▌    | 1.06G/9.93G [00:04<00:36, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|▌    | 1.09G/9.93G [00:04<00:35, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|▌    | 1.12G/9.93G [00:05<00:34, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  12%|▌    | 1.15G/9.93G [00:05<00:35, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  12%|▌    | 1.18G/9.93G [00:05<00:35, 247MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  12%|▌    | 1.22G/9.93G [00:05<00:35, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  13%|▋    | 1.25G/9.93G [00:05<00:34, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  13%|▋    | 1.28G/9.93G [00:05<00:34, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  13%|▋    | 1.31G/9.93G [00:05<00:34, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  14%|▋    | 1.34G/9.93G [00:05<00:34, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  14%|▋    | 1.37G/9.93G [00:06<00:34, 251MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  14%|▋    | 1.41G/9.93G [00:06<00:36, 231MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  14%|▋    | 1.44G/9.93G [00:06<00:35, 239MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  15%|▋    | 1.47G/9.93G [00:06<00:45, 187MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  15%|▋    | 1.49G/9.93G [00:06<00:45, 187MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  15%|▊    | 1.51G/9.93G [00:06<00:46, 182MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  16%|▊    | 1.54G/9.93G [00:06<00:42, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  16%|▊    | 1.57G/9.93G [00:07<00:38, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  16%|▊    | 1.60G/9.93G [00:07<00:37, 224MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  16%|▊    | 1.64G/9.93G [00:07<00:35, 234MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  17%|▊    | 1.67G/9.93G [00:07<00:36, 229MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  17%|▊    | 1.70G/9.93G [00:07<00:43, 191MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  17%|▊    | 1.72G/9.93G [00:07<00:43, 191MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  18%|▉    | 1.75G/9.93G [00:07<00:40, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  18%|▉    | 1.78G/9.93G [00:08<00:37, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  18%|▉    | 1.81G/9.93G [00:08<00:38, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  19%|▉    | 1.85G/9.93G [00:08<00:36, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  19%|▉    | 1.88G/9.93G [00:08<00:35, 226MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  19%|▉    | 1.91G/9.93G [00:08<00:39, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|▉    | 1.94G/9.93G [00:08<00:37, 211MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|▉    | 1.97G/9.93G [00:08<00:35, 227MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|█    | 2.00G/9.93G [00:09<00:33, 237MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|█    | 2.03G/9.93G [00:09<00:31, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  21%|█    | 2.07G/9.93G [00:09<00:31, 251MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  21%|█    | 2.10G/9.93G [00:09<00:45, 174MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  21%|█    | 2.13G/9.93G [00:09<00:40, 192MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  22%|█    | 2.16G/9.93G [00:09<00:38, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  22%|█    | 2.19G/9.93G [00:10<00:35, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  22%|█    | 2.22G/9.93G [00:10<00:42, 180MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|█▏   | 2.24G/9.93G [00:10<01:02, 123MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|█▏   | 2.26G/9.93G [00:10<01:13, 105MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|█▏   | 2.29G/9.93G [00:11<01:05, 117MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.31G/9.93G [00:11<01:16, 99.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|█▏   | 2.33G/9.93G [00:11<01:11, 106MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|█▏   | 2.35G/9.93G [00:11<01:14, 102MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|█▏   | 2.37G/9.93G [00:11<01:07, 112MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.39G/9.93G [00:12<01:16, 98.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|█▏   | 2.41G/9.93G [00:12<01:11, 105MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|█▏   | 2.43G/9.93G [00:12<01:13, 102MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  25%|▉   | 2.45G/9.93G [00:12<01:17, 96.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  25%|█▏   | 2.47G/9.93G [00:12<01:09, 107MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  25%|█▎   | 2.50G/9.93G [00:13<01:05, 114MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  25%|█▎   | 2.52G/9.93G [00:13<01:04, 115MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.54G/9.93G [00:13<01:18, 94.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.55G/9.93G [00:13<01:25, 86.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.57G/9.93G [00:13<01:21, 90.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.58G/9.93G [00:14<01:26, 85.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.59G/9.93G [00:14<01:34, 78.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.61G/9.93G [00:14<01:26, 84.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.63G/9.93G [00:14<01:28, 82.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.64G/9.93G [00:14<01:27, 82.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.65G/9.93G [00:15<01:40, 72.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.67G/9.93G [00:15<01:17, 94.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|█▎   | 2.71G/9.93G [00:15<00:55, 130MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  28%|█▍   | 2.74G/9.93G [00:15<00:45, 157MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  28%|█▍   | 2.77G/9.93G [00:15<00:39, 180MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  28%|█▍   | 2.80G/9.93G [00:15<00:36, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  28%|█▍   | 2.83G/9.93G [00:15<00:33, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▍   | 2.86G/9.93G [00:15<00:31, 225MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▍   | 2.89G/9.93G [00:16<00:30, 231MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▍   | 2.93G/9.93G [00:16<00:29, 235MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▍   | 2.96G/9.93G [00:16<00:28, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▌   | 2.99G/9.93G [00:16<00:28, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▌   | 3.02G/9.93G [00:16<00:28, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▌   | 3.05G/9.93G [00:16<00:28, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▌   | 3.08G/9.93G [00:16<00:27, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▌   | 3.11G/9.93G [00:16<00:27, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▌   | 3.15G/9.93G [00:17<00:28, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▌   | 3.18G/9.93G [00:17<00:27, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▌   | 3.21G/9.93G [00:17<00:27, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▋   | 3.24G/9.93G [00:17<00:27, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▋   | 3.27G/9.93G [00:17<00:27, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▋   | 3.30G/9.93G [00:17<00:27, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▋   | 3.33G/9.93G [00:17<00:26, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▋   | 3.37G/9.93G [00:18<00:25, 255MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▋   | 3.40G/9.93G [00:18<00:25, 254MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▋   | 3.43G/9.93G [00:18<00:28, 229MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▋   | 3.46G/9.93G [00:18<00:27, 232MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▊   | 3.49G/9.93G [00:18<00:29, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▊   | 3.52G/9.93G [00:18<00:28, 227MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▊   | 3.55G/9.93G [00:18<00:28, 226MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▊   | 3.59G/9.93G [00:18<00:28, 226MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▊   | 3.62G/9.93G [00:19<00:26, 234MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▊   | 3.65G/9.93G [00:19<00:26, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▊   | 3.68G/9.93G [00:19<00:25, 249MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▊   | 3.71G/9.93G [00:19<00:24, 255MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▉   | 3.74G/9.93G [00:19<00:24, 258MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▉   | 3.77G/9.93G [00:19<00:23, 258MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▉   | 3.81G/9.93G [00:19<00:24, 253MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▉   | 3.84G/9.93G [00:19<00:24, 251MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▉   | 3.87G/9.93G [00:20<00:24, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▉   | 3.90G/9.93G [00:20<00:23, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▉   | 3.93G/9.93G [00:20<00:23, 253MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▉   | 3.96G/9.93G [00:20<00:23, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  40%|██   | 4.00G/9.93G [00:20<00:23, 253MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  41%|██   | 4.03G/9.93G [00:20<00:23, 249MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  41%|██   | 4.06G/9.93G [00:20<00:23, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  41%|██   | 4.09G/9.93G [00:20<00:23, 251MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  41%|██   | 4.12G/9.93G [00:21<00:23, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  42%|██   | 4.15G/9.93G [00:21<00:22, 258MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  42%|██   | 4.18G/9.93G [00:21<00:21, 262MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  42%|██   | 4.22G/9.93G [00:21<00:21, 265MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  43%|██▏  | 4.25G/9.93G [00:21<00:22, 258MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  43%|██▏  | 4.28G/9.93G [00:21<00:22, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  43%|██▏  | 4.31G/9.93G [00:21<00:22, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  44%|██▏  | 4.34G/9.93G [00:21<00:21, 254MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  44%|██▏  | 4.37G/9.93G [00:22<00:21, 257MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  44%|██▏  | 4.40G/9.93G [00:22<00:21, 253MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  45%|██▏  | 4.44G/9.93G [00:22<00:21, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  45%|██▏  | 4.47G/9.93G [00:22<00:21, 249MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  45%|██▎  | 4.50G/9.93G [00:22<00:21, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  46%|██▎  | 4.53G/9.93G [00:22<00:22, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  46%|██▎  | 4.56G/9.93G [00:22<00:21, 247MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  46%|██▎  | 4.59G/9.93G [00:22<00:21, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|██▎  | 4.62G/9.93G [00:23<00:21, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|██▎  | 4.66G/9.93G [00:23<00:21, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|██▎  | 4.69G/9.93G [00:23<00:21, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|██▎  | 4.72G/9.93G [00:23<00:21, 241MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|██▍  | 4.75G/9.93G [00:23<00:21, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|██▍  | 4.78G/9.93G [00:23<00:20, 247MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|██▍  | 4.81G/9.93G [00:23<00:20, 254MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|██▍  | 4.84G/9.93G [00:23<00:20, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|██▍  | 4.88G/9.93G [00:24<00:20, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|██▍  | 4.91G/9.93G [00:24<00:20, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  50%|██▍  | 4.94G/9.93G [00:24<00:20, 240MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  50%|██▌  | 4.97G/9.93G [00:24<00:20, 241MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  50%|██▌  | 5.00G/9.93G [00:24<00:20, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  51%|██▌  | 5.03G/9.93G [00:24<00:19, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  51%|██▌  | 5.06G/9.93G [00:24<00:19, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  51%|██▌  | 5.10G/9.93G [00:25<00:19, 248MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|██▌  | 5.13G/9.93G [00:25<00:20, 236MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|██▌  | 5.16G/9.93G [00:25<00:34, 139MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|██▌  | 5.18G/9.93G [00:25<00:44, 108MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|██▌  | 5.20G/9.93G [00:26<00:43, 110MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.22G/9.93G [00:26<00:47, 98.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.24G/9.93G [00:26<00:52, 90.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.26G/9.93G [00:26<00:50, 92.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.27G/9.93G [00:27<00:51, 91.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|██▏ | 5.28G/9.93G [00:27<00:51, 90.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|██▋  | 5.31G/9.93G [00:27<00:43, 107MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.33G/9.93G [00:27<00:50, 92.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.34G/9.93G [00:27<00:52, 87.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.35G/9.93G [00:27<01:01, 74.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.37G/9.93G [00:28<00:54, 84.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.38G/9.93G [00:28<01:00, 74.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.39G/9.93G [00:28<01:06, 68.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.40G/9.93G [00:28<01:01, 73.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.42G/9.93G [00:28<00:45, 98.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.44G/9.93G [00:29<01:01, 73.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.45G/9.93G [00:29<01:03, 70.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.46G/9.93G [00:29<01:04, 69.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.47G/9.93G [00:29<01:05, 68.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.51G/9.93G [00:29<00:48, 90.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▊  | 5.53G/9.93G [00:30<00:42, 105MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.55G/9.93G [00:30<00:51, 84.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▊  | 5.57G/9.93G [00:30<00:42, 103MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▊  | 5.60G/9.93G [00:30<00:32, 133MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▊  | 5.63G/9.93G [00:30<00:35, 121MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▊  | 5.66G/9.93G [00:31<00:29, 147MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▊  | 5.69G/9.93G [00:31<00:24, 170MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▉  | 5.73G/9.93G [00:31<00:23, 180MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▉  | 5.76G/9.93G [00:31<00:21, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▉  | 5.79G/9.93G [00:31<00:19, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▉  | 5.82G/9.93G [00:31<00:18, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▉  | 5.85G/9.93G [00:31<00:18, 226MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▉  | 5.88G/9.93G [00:31<00:17, 232MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▉  | 5.91G/9.93G [00:32<00:18, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▉  | 5.95G/9.93G [00:32<00:18, 221MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  60%|███  | 5.98G/9.93G [00:32<00:17, 226MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  60%|███  | 6.01G/9.93G [00:32<00:17, 229MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  61%|███  | 6.04G/9.93G [00:32<00:16, 234MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  61%|███  | 6.07G/9.93G [00:32<00:16, 239MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  61%|███  | 6.10G/9.93G [00:32<00:16, 226MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  62%|███  | 6.13G/9.93G [00:33<00:16, 234MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  62%|███  | 6.17G/9.93G [00:33<00:15, 238MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  62%|███  | 6.20G/9.93G [00:33<00:15, 240MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  63%|███▏ | 6.23G/9.93G [00:33<00:15, 241MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  63%|███▏ | 6.26G/9.93G [00:33<00:15, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  63%|███▏ | 6.29G/9.93G [00:33<00:14, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  64%|███▏ | 6.32G/9.93G [00:33<00:15, 231MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  64%|███▏ | 6.35G/9.93G [00:33<00:15, 234MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  64%|███▏ | 6.39G/9.93G [00:34<00:15, 232MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  65%|███▏ | 6.42G/9.93G [00:34<00:15, 225MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  65%|███▏ | 6.45G/9.93G [00:34<00:17, 194MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  65%|███▎ | 6.47G/9.93G [00:34<00:18, 189MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  65%|███▎ | 6.50G/9.93G [00:34<00:16, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  66%|███▎ | 6.53G/9.93G [00:34<00:15, 221MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  66%|███▎ | 6.56G/9.93G [00:34<00:14, 235MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  66%|███▎ | 6.60G/9.93G [00:35<00:14, 237MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  67%|███▎ | 6.63G/9.93G [00:35<00:13, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  67%|███▎ | 6.66G/9.93G [00:35<00:13, 251MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  67%|███▎ | 6.69G/9.93G [00:35<00:12, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  68%|███▍ | 6.72G/9.93G [00:35<00:12, 257MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  68%|███▍ | 6.75G/9.93G [00:35<00:12, 253MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  68%|███▍ | 6.78G/9.93G [00:35<00:12, 257MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  69%|███▍ | 6.82G/9.93G [00:35<00:11, 262MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  69%|███▍ | 6.85G/9.93G [00:36<00:11, 261MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  69%|███▍ | 6.88G/9.93G [00:36<00:11, 261MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  70%|███▍ | 6.91G/9.93G [00:36<00:11, 263MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  70%|███▍ | 6.94G/9.93G [00:36<00:11, 254MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  70%|███▌ | 6.97G/9.93G [00:36<00:12, 247MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  71%|███▌ | 7.00G/9.93G [00:36<00:11, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  71%|███▌ | 7.04G/9.93G [00:36<00:11, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  71%|███▌ | 7.07G/9.93G [00:36<00:11, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  71%|███▌ | 7.10G/9.93G [00:37<00:11, 247MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  72%|███▌ | 7.13G/9.93G [00:37<00:12, 232MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  72%|███▌ | 7.16G/9.93G [00:37<00:11, 234MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  72%|███▌ | 7.19G/9.93G [00:37<00:11, 240MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███▋ | 7.22G/9.93G [00:37<00:11, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███▋ | 7.26G/9.93G [00:37<00:11, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███▋ | 7.29G/9.93G [00:37<00:10, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  74%|███▋ | 7.32G/9.93G [00:38<00:10, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  74%|███▋ | 7.35G/9.93G [00:38<00:10, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  74%|███▋ | 7.38G/9.93G [00:38<00:10, 233MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███▋ | 7.41G/9.93G [00:38<00:10, 233MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███▋ | 7.44G/9.93G [00:38<00:10, 227MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███▊ | 7.48G/9.93G [00:38<00:10, 236MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  76%|███▊ | 7.51G/9.93G [00:38<00:10, 237MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  76%|███▊ | 7.54G/9.93G [00:38<00:09, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  76%|███▊ | 7.57G/9.93G [00:39<00:09, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███▊ | 7.60G/9.93G [00:39<00:09, 240MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███▊ | 7.63G/9.93G [00:39<00:09, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███▊ | 7.67G/9.93G [00:39<00:09, 230MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███▊ | 7.70G/9.93G [00:39<00:09, 237MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  78%|███▉ | 7.73G/9.93G [00:39<00:09, 231MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  78%|███▉ | 7.76G/9.93G [00:39<00:09, 227MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  78%|███▉ | 7.79G/9.93G [00:40<00:09, 227MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▉ | 7.82G/9.93G [00:40<00:08, 237MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▉ | 7.85G/9.93G [00:40<00:08, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▉ | 7.89G/9.93G [00:40<00:09, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▉ | 7.92G/9.93G [00:40<00:15, 130MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▉ | 7.94G/9.93G [00:41<00:17, 112MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|████ | 7.96G/9.93G [00:41<00:18, 105MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|████ | 7.98G/9.93G [00:41<00:19, 100MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████ | 8.00G/9.93G [00:41<00:18, 104MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.02G/9.93G [00:42<00:21, 87.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.04G/9.93G [00:42<00:19, 98.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.06G/9.93G [00:42<00:20, 92.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████ | 8.10G/9.93G [00:42<00:15, 116MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████ | 8.12G/9.93G [00:42<00:17, 107MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████ | 8.15G/9.93G [00:43<00:13, 133MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████ | 8.18G/9.93G [00:43<00:10, 161MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████▏| 8.21G/9.93G [00:43<00:09, 186MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████▏| 8.24G/9.93G [00:43<00:08, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████▏| 8.27G/9.93G [00:43<00:10, 156MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████▏| 8.29G/9.93G [00:44<00:14, 112MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████▏| 8.32G/9.93G [00:44<00:14, 108MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████▏| 8.34G/9.93G [00:44<00:15, 104MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████▏| 8.36G/9.93G [00:44<00:14, 106MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████▏| 8.38G/9.93G [00:44<00:14, 106MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  85%|████▏| 8.41G/9.93G [00:45<00:11, 134MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  85%|████▏| 8.44G/9.93G [00:45<00:09, 161MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  85%|████▎| 8.47G/9.93G [00:45<00:07, 183MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  86%|████▎| 8.50G/9.93G [00:45<00:07, 194MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  86%|████▎| 8.54G/9.93G [00:45<00:06, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  86%|████▎| 8.57G/9.93G [00:45<00:06, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  87%|████▎| 8.60G/9.93G [00:45<00:05, 228MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  87%|████▎| 8.63G/9.93G [00:46<00:05, 229MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  87%|████▎| 8.66G/9.93G [00:46<00:05, 235MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  87%|████▎| 8.69G/9.93G [00:46<00:05, 223MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  88%|████▍| 8.72G/9.93G [00:46<00:05, 221MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  88%|████▍| 8.76G/9.93G [00:46<00:05, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  88%|████▍| 8.79G/9.93G [00:46<00:05, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  89%|████▍| 8.82G/9.93G [00:46<00:05, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  89%|████▍| 8.85G/9.93G [00:47<00:05, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  89%|████▍| 8.88G/9.93G [00:47<00:04, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|████▍| 8.91G/9.93G [00:47<00:04, 206MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|████▍| 8.93G/9.93G [00:47<00:04, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|████▌| 8.97G/9.93G [00:47<00:04, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|████▌| 8.99G/9.93G [00:47<00:04, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  91%|████▌| 9.01G/9.93G [00:47<00:04, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  91%|████▌| 9.03G/9.93G [00:47<00:04, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  91%|████▌| 9.05G/9.93G [00:48<00:04, 186MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  91%|████▌| 9.08G/9.93G [00:48<00:04, 183MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  92%|████▌| 9.10G/9.93G [00:48<00:04, 172MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  92%|████▌| 9.13G/9.93G [00:48<00:04, 190MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  92%|████▌| 9.16G/9.93G [00:48<00:03, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  93%|████▋| 9.20G/9.93G [00:48<00:03, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  93%|████▋| 9.23G/9.93G [00:48<00:03, 223MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  93%|████▋| 9.26G/9.93G [00:49<00:02, 228MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|████▋| 9.29G/9.93G [00:49<00:02, 234MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|████▋| 9.32G/9.93G [00:49<00:02, 240MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|████▋| 9.35G/9.93G [00:49<00:02, 246MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|████▋| 9.38G/9.93G [00:49<00:02, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  95%|████▋| 9.42G/9.93G [00:49<00:02, 242MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  95%|████▊| 9.45G/9.93G [00:49<00:02, 241MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  95%|████▊| 9.48G/9.93G [00:49<00:01, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  96%|████▊| 9.51G/9.93G [00:50<00:01, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  96%|████▊| 9.54G/9.93G [00:50<00:01, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  96%|████▊| 9.57G/9.93G [00:50<00:01, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  97%|████▊| 9.60G/9.93G [00:50<00:01, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  97%|████▊| 9.64G/9.93G [00:50<00:01, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  97%|████▊| 9.67G/9.93G [00:50<00:01, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  98%|████▉| 9.70G/9.93G [00:50<00:00, 244MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  98%|████▉| 9.73G/9.93G [00:50<00:00, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  98%|████▉| 9.76G/9.93G [00:51<00:00, 245MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  99%|████▉| 9.79G/9.93G [00:51<00:00, 243MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  99%|████▉| 9.83G/9.93G [00:51<00:00, 252MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  99%|████▉| 9.86G/9.93G [00:51<00:00, 255MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin: 100%|████▉| 9.89G/9.93G [00:51<00:00, 250MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin: 100%|█████| 9.93G/9.93G [00:51<00:00, 192MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 1/2 [00:51<00:51, 51.97s/it]\n",
      "pytorch_model-00002-of-00002.bin:   0%|             | 0.00/5.08G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   0%|    | 10.5M/5.08G [00:00<02:25, 34.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   0%|    | 21.0M/5.08G [00:00<01:47, 47.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|    | 41.9M/5.08G [00:00<01:08, 73.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|    | 62.9M/5.08G [00:00<00:51, 98.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   2%|     | 94.4M/5.08G [00:00<00:36, 138MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   2%|▏     | 126M/5.08G [00:01<00:28, 174MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   3%|▏     | 157M/5.08G [00:01<00:24, 198MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   4%|▏     | 189M/5.08G [00:01<00:22, 220MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   4%|▎     | 220M/5.08G [00:01<00:20, 236MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   5%|▎     | 252M/5.08G [00:01<00:19, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   6%|▎     | 283M/5.08G [00:01<00:18, 254MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   6%|▎     | 315M/5.08G [00:01<00:19, 249MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   7%|▍     | 346M/5.08G [00:01<00:18, 254MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   7%|▍     | 377M/5.08G [00:02<00:18, 253MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   8%|▍     | 409M/5.08G [00:02<00:18, 253MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   9%|▌     | 440M/5.08G [00:02<00:18, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   9%|▌     | 472M/5.08G [00:02<00:18, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  10%|▌     | 503M/5.08G [00:02<00:18, 249MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  11%|▋     | 535M/5.08G [00:02<00:18, 249MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  11%|▋     | 566M/5.08G [00:02<00:18, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  12%|▋     | 598M/5.08G [00:02<00:18, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  12%|▋     | 629M/5.08G [00:03<00:18, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  13%|▊     | 661M/5.08G [00:03<00:17, 249MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  14%|▊     | 692M/5.08G [00:03<00:17, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  14%|▊     | 724M/5.08G [00:03<00:17, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  15%|▉     | 755M/5.08G [00:03<00:17, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  15%|▉     | 786M/5.08G [00:03<00:17, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  16%|▉     | 818M/5.08G [00:03<00:17, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  17%|█     | 849M/5.08G [00:03<00:17, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  17%|█     | 881M/5.08G [00:04<00:17, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  18%|█     | 912M/5.08G [00:04<00:17, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|█     | 944M/5.08G [00:04<00:16, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|█▏    | 975M/5.08G [00:04<00:16, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|▉    | 1.01G/5.08G [00:04<00:16, 247MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|█    | 1.04G/5.08G [00:04<00:16, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  21%|█    | 1.07G/5.08G [00:04<00:16, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|█    | 1.10G/5.08G [00:04<00:16, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|█    | 1.13G/5.08G [00:05<00:16, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  23%|█▏   | 1.16G/5.08G [00:05<00:16, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  24%|█▏   | 1.20G/5.08G [00:05<00:16, 241MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  24%|█▏   | 1.23G/5.08G [00:05<00:15, 241MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  25%|█▏   | 1.26G/5.08G [00:05<00:15, 241MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  25%|█▎   | 1.29G/5.08G [00:05<00:15, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  26%|█▎   | 1.32G/5.08G [00:05<00:15, 239MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  27%|█▎   | 1.35G/5.08G [00:05<00:15, 239MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  27%|█▎   | 1.38G/5.08G [00:06<00:15, 237MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  28%|█▍   | 1.42G/5.08G [00:06<00:15, 239MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  28%|█▍   | 1.45G/5.08G [00:06<00:15, 240MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  29%|█▍   | 1.48G/5.08G [00:06<00:19, 188MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  30%|█▍   | 1.51G/5.08G [00:06<00:17, 204MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  30%|█▌   | 1.54G/5.08G [00:06<00:16, 213MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  31%|█▌   | 1.57G/5.08G [00:07<00:15, 220MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  32%|█▌   | 1.60G/5.08G [00:07<00:15, 225MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  32%|█▌   | 1.64G/5.08G [00:07<00:15, 229MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  33%|█▋   | 1.67G/5.08G [00:07<00:14, 233MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  33%|█▋   | 1.70G/5.08G [00:07<00:18, 184MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  34%|█▋   | 1.73G/5.08G [00:07<00:16, 198MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  35%|█▋   | 1.76G/5.08G [00:07<00:15, 215MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  35%|█▊   | 1.79G/5.08G [00:08<00:14, 226MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  36%|█▊   | 1.82G/5.08G [00:08<00:14, 230MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  37%|█▊   | 1.86G/5.08G [00:08<00:13, 232MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  37%|█▊   | 1.89G/5.08G [00:08<00:18, 174MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  38%|█▉   | 1.92G/5.08G [00:08<00:16, 190MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  38%|█▉   | 1.95G/5.08G [00:08<00:15, 204MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  39%|█▉   | 1.98G/5.08G [00:08<00:14, 218MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  40%|█▉   | 2.01G/5.08G [00:09<00:13, 232MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  40%|██   | 2.04G/5.08G [00:09<00:12, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  41%|██   | 2.08G/5.08G [00:09<00:12, 250MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  42%|██   | 2.11G/5.08G [00:09<00:17, 172MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  42%|██   | 2.14G/5.08G [00:09<00:15, 191MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  43%|██▏  | 2.17G/5.08G [00:09<00:14, 205MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  43%|██▏  | 2.20G/5.08G [00:10<00:13, 216MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  44%|██▏  | 2.23G/5.08G [00:10<00:19, 146MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  44%|██▏  | 2.25G/5.08G [00:10<00:21, 128MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  45%|██▏  | 2.28G/5.08G [00:10<00:21, 133MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  45%|██▎  | 2.30G/5.08G [00:11<00:26, 103MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  46%|██▎  | 2.32G/5.08G [00:11<00:24, 113MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  46%|█▊  | 2.34G/5.08G [00:11<00:29, 92.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  46%|█▊  | 2.36G/5.08G [00:11<00:27, 98.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  47%|██▎  | 2.38G/5.08G [00:11<00:26, 104MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  47%|██▎  | 2.40G/5.08G [00:12<00:25, 103MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|█▉  | 2.42G/5.08G [00:12<00:28, 91.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|█▉  | 2.43G/5.08G [00:12<00:30, 86.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|█▉  | 2.45G/5.08G [00:12<00:27, 96.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|█▉  | 2.46G/5.08G [00:12<00:28, 91.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|█▉  | 2.49G/5.08G [00:13<00:27, 95.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|█▉  | 2.50G/5.08G [00:13<00:29, 88.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|█▉  | 2.51G/5.08G [00:13<00:29, 88.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|█▉  | 2.52G/5.08G [00:13<00:31, 80.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|█▉  | 2.54G/5.08G [00:13<00:29, 85.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|██  | 2.55G/5.08G [00:13<00:31, 80.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|██  | 2.56G/5.08G [00:14<00:34, 73.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 2.57G/5.08G [00:14<00:33, 75.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 2.58G/5.08G [00:14<00:34, 72.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 2.59G/5.08G [00:14<00:35, 70.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 2.60G/5.08G [00:14<00:32, 76.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  52%|██▌  | 2.63G/5.08G [00:14<00:19, 126MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  52%|██▌  | 2.66G/5.08G [00:14<00:15, 159MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  53%|██▋  | 2.69G/5.08G [00:15<00:13, 183MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  54%|██▋  | 2.73G/5.08G [00:15<00:11, 201MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  54%|██▋  | 2.76G/5.08G [00:15<00:10, 213MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  55%|██▋  | 2.79G/5.08G [00:15<00:10, 222MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  56%|██▊  | 2.82G/5.08G [00:15<00:09, 229MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  56%|██▊  | 2.85G/5.08G [00:15<00:09, 235MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  57%|██▊  | 2.88G/5.08G [00:15<00:09, 237MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  57%|██▊  | 2.92G/5.08G [00:15<00:09, 240MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  58%|██▉  | 2.95G/5.08G [00:16<00:08, 241MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  59%|██▉  | 2.98G/5.08G [00:16<00:08, 240MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  59%|██▉  | 3.01G/5.08G [00:16<00:08, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  60%|██▉  | 3.04G/5.08G [00:16<00:08, 238MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  61%|███  | 3.07G/5.08G [00:16<00:08, 241MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  61%|███  | 3.10G/5.08G [00:16<00:08, 240MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  62%|███  | 3.14G/5.08G [00:16<00:08, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  62%|███  | 3.17G/5.08G [00:16<00:07, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  63%|███▏ | 3.20G/5.08G [00:17<00:07, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  64%|███▏ | 3.23G/5.08G [00:17<00:07, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  64%|███▏ | 3.26G/5.08G [00:17<00:07, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|███▏ | 3.29G/5.08G [00:17<00:07, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|███▎ | 3.32G/5.08G [00:17<00:07, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  66%|███▎ | 3.36G/5.08G [00:17<00:07, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|███▎ | 3.39G/5.08G [00:17<00:06, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|███▎ | 3.42G/5.08G [00:17<00:06, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  68%|███▍ | 3.45G/5.08G [00:18<00:06, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|███▍ | 3.48G/5.08G [00:18<00:06, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|███▍ | 3.51G/5.08G [00:18<00:06, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|███▍ | 3.54G/5.08G [00:18<00:06, 247MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|███▌ | 3.58G/5.08G [00:18<00:06, 249MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  71%|███▌ | 3.61G/5.08G [00:18<00:05, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|███▌ | 3.64G/5.08G [00:18<00:05, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|███▌ | 3.67G/5.08G [00:19<00:05, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  73%|███▋ | 3.70G/5.08G [00:19<00:05, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  74%|███▋ | 3.73G/5.08G [00:19<00:05, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  74%|███▋ | 3.76G/5.08G [00:19<00:05, 240MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|███▋ | 3.80G/5.08G [00:19<00:05, 232MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|███▊ | 3.83G/5.08G [00:19<00:05, 236MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  76%|███▊ | 3.86G/5.08G [00:19<00:05, 239MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  77%|███▊ | 3.89G/5.08G [00:19<00:04, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  77%|███▊ | 3.92G/5.08G [00:20<00:04, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███▉ | 3.95G/5.08G [00:20<00:04, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███▉ | 3.98G/5.08G [00:20<00:04, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  79%|███▉ | 4.02G/5.08G [00:20<00:04, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  80%|███▉ | 4.05G/5.08G [00:20<00:04, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  80%|████ | 4.08G/5.08G [00:20<00:04, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  81%|████ | 4.11G/5.08G [00:20<00:03, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  82%|████ | 4.14G/5.08G [00:20<00:03, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  82%|████ | 4.17G/5.08G [00:21<00:03, 250MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  83%|████▏| 4.20G/5.08G [00:21<00:03, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  83%|████▏| 4.24G/5.08G [00:21<00:03, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  84%|████▏| 4.27G/5.08G [00:21<00:03, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  85%|████▏| 4.30G/5.08G [00:21<00:03, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  85%|████▎| 4.33G/5.08G [00:21<00:03, 247MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  86%|████▎| 4.36G/5.08G [00:21<00:02, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  87%|████▎| 4.39G/5.08G [00:21<00:02, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  87%|████▎| 4.42G/5.08G [00:22<00:02, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  88%|████▍| 4.46G/5.08G [00:22<00:02, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  88%|████▍| 4.49G/5.08G [00:22<00:02, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  89%|████▍| 4.52G/5.08G [00:22<00:02, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  90%|████▍| 4.55G/5.08G [00:22<00:02, 242MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  90%|████▌| 4.58G/5.08G [00:22<00:02, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  91%|████▌| 4.61G/5.08G [00:22<00:01, 247MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  91%|████▌| 4.65G/5.08G [00:23<00:01, 247MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  92%|████▌| 4.68G/5.08G [00:23<00:01, 240MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|████▋| 4.71G/5.08G [00:23<00:01, 244MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|████▋| 4.74G/5.08G [00:23<00:01, 243MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  94%|████▋| 4.77G/5.08G [00:23<00:01, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  95%|████▋| 4.80G/5.08G [00:23<00:01, 247MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  95%|████▊| 4.83G/5.08G [00:23<00:00, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  96%|████▊| 4.87G/5.08G [00:23<00:00, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  96%|████▊| 4.90G/5.08G [00:24<00:00, 248MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  97%|████▊| 4.93G/5.08G [00:24<00:00, 249MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  98%|████▉| 4.96G/5.08G [00:24<00:00, 249MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  98%|████▉| 4.99G/5.08G [00:24<00:00, 245MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  99%|████▉| 5.02G/5.08G [00:24<00:00, 246MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin: 100%|████▉| 5.05G/5.08G [00:24<00:00, 228MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin: 100%|█████| 5.08G/5.08G [00:25<00:00, 203MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [01:17<00:00, 38.64s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.73it/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 795/795 [00:00<00:00, 13.7MB/s]\n",
      "tokenization_baichuan.py: 100%|█████████████| 9.63k/9.63k [00:00<00:00, 133MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:\n",
      "- tokenization_baichuan.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "tokenizer.model: 100%|█████████████████████| 2.00M/2.00M [00:00<00:00, 21.0MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 548/548 [00:00<00:00, 11.6MB/s]\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: False\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-02-13 07:20:24 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "    \n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2025-02-13 07:20:24 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n",
      "    \n",
      "[NeMo I 2025-02-13 07:20:24 nemo_logging:393] Padded vocab_size: 125696, original vocab_size: 125696, dummy tokens: 0.\n",
      "[NeMo W 2025-02-13 07:20:44 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-13 07:20:52 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "Converted Baichuan model to Nemo, model saved to /root/.cache/nemo/models/baichuan-inc/Baichuan2-7B-Base\n",
      "\u001b[32m $\u001b[0m\u001b[32mNEMO_MODELS_CACHE\u001b[0m\u001b[32m=\u001b[0m\u001b[32m/root/.cache/nemo/\u001b[0m\u001b[32mmodels\u001b[0m\u001b[32m \u001b[0m\n",
      "\u001b[32m✓ Checkpoint imported to \u001b[0m\u001b[32m/root/.cache/nemo/models/baichuan-inc/\u001b[0m\u001b[32mBaichuan2-7B-Base\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!torchrun import_baichuan2_7b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script \n",
    "- Downloads the Baichuan2 7B model from Hugging Face (if not already downloaded).\n",
    "- Automatically converts it into the NeMo format.\n",
    "\n",
    "Note:\n",
    "- The script can only run in a Python environment, not in a Jupyter notebook.\n",
    "- You need to have access to ```baichuan-inc/Baichuan2-7B-Base``` [repo on Hugging Face](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base).\n",
    "\n",
    "The conversion will create a ```baichuan-inc/Baichuan2-7B-Base``` folder in the default ```$NEMO_HOME/models``` directory. \n",
    "```$NEMO_HOME``` centralizes and stores all models and datasets used for NeMo training. By default `$NEMO_HOME stores to ```/root/.cache/nemo```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Finetuning Baichuan2 7B using Function-Calling Dataset\n",
    "\n",
    "For this step we use the NeMo2 predefined recipe. \n",
    "\n",
    "First we define the recipe and executor for using NeMo2 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-13 07:21:38 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "from nemo.collections import llm\n",
    "\n",
    "def configure_recipe(nodes: int = 1, gpus_per_node: int = 1):\n",
    "    recipe = llm.recipes.baichuan2_7b.finetune_recipe(\n",
    "        num_nodes=nodes,\n",
    "        num_gpus_per_node=gpus_per_node,\n",
    "    )\n",
    "    return recipe\n",
    "\n",
    "def local_executor_torchrun(devices: int = 1) -> run.LocalExecutor:\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\")\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about NeMo Executor [here](https://github.com/NVIDIA/NeMo-Run/blob/main/docs/source/guides/execution.md).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the recipe\n",
    "# Make sure you set the gpus_per_node as expected\n",
    "recipe = configure_recipe(gpus_per_node=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1739462160</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1739462160\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1739462160/nemo.collections.llm.api.finetune\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07:56:00] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.finetune for experiment </span>                        <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#724\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.finetune</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07:56:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.finetune for experiment \u001b[0m                        \u001b]8;id=659235;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=754008;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#724\u001b\\\u001b[2m724\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.finetune\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1739462160/nemo.collections.llm.api.finetune\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.finetune-mnjn2bsc2qf4cd\n",
      "AppStatus:\n",
      "    State: RUNNING\n",
      "    Num Restarts: 0\n",
      "    Roles: \n",
      "    Msg: <NONE>\n",
      "    Structured Error Msg: <NONE>\n",
      "    UI URL: file:///root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1739462160/nemo.collections.llm.api.finetune/nemo_run/nemo.collections.llm.api.finetune-mnjn2bsc2qf4cd\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.finetune_1739462160 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.finetune_1739462160 to finish\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune_1739462160</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.finetune_1739462160\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.finetune-mnjn2bsc2qf4cd\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1739462160/nemo.collections.llm.api.finetune\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.finetune\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.finetune-mnjn2bsc2qf4cd\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1739462160/nemo.collections.llm.api.finetune\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.finetune-mnjn2bsc2qf4cd to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.finetune/0 I0213 07:56:01.581000 2223442 torch/distributed/run.py:675] Using nproc_per_node=8.\n",
      "i.finetune/0 W0213 07:56:01.582000 2223442 torch/distributed/run.py:792] \n",
      "i.finetune/0 W0213 07:56:01.582000 2223442 torch/distributed/run.py:792] *****************************************\n",
      "i.finetune/0 W0213 07:56:01.582000 2223442 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "i.finetune/0 W0213 07:56:01.582000 2223442 torch/distributed/run.py:792] *****************************************\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194] Starting elastic_operator with launch configs:\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   min_nodes        : 1\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   max_nodes        : 1\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   nproc_per_node   : 8\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   run_id           : 8791\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   rdzv_backend     : c10d\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   rdzv_endpoint    : localhost:0\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   rdzv_configs     : {'timeout': 900}\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   max_restarts     : 0\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   monitor_interval : 0.1\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1739462160/nemo.collections.llm.api.finetune/nemo_run/nemo.collections.llm.api.finetune-mnjn2bsc2qf4cd/torchelastic/nemo.collections.llm.api.finetune\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194]   metrics_cfg      : {}\n",
      "i.finetune/0 I0213 07:56:01.582000 2223442 torch/distributed/launcher/api.py:194] \n",
      "i.finetune/0 I0213 07:56:01.585000 2223442 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python\n",
      "i.finetune/0 I0213 07:56:01.586000 2223442 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   master_addr=eos0531.eos.clusters.nvidia.com\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   master_port=38239\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
      "i.finetune/0 I0213 07:56:01.873000 2223442 torch/distributed/elastic/agent/server/api.py:525] \n",
      "i.finetune/0 I0213 07:56:01.874000 2223442 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group\n",
      "i.finetune/0 I0213 07:56:01.874000 2223442 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True\n",
      "i.finetune/0 I0213 07:56:01.875000 2223442 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.finetune/0 I0213 07:56:01.875000 2223442 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.finetune/0 [default5]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default5]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default5]:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "i.finetune/0 [default7]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default7]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default6]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default6]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default1]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default1]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default0]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default0]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default4]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default4]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default2]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default2]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default3]:[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "i.finetune/0 [default3]:[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "i.finetune/0 [default0]:[NeMo W 2025-02-13 07:56:17 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "i.finetune/0 [default0]:      cm = get_cmap(\"Set1\")\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:18 nemo_logging:393] Disabling try_restore_best_ckpt restoration for adapters\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:18 nemo_logging:393] Experiments will be logged at chat_sft_function_calling_demo\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "i.finetune/0 [default0]:[NeMo E 2025-02-13 07:56:18 nemo_logging:417] nemo logger received explicit_log_dir: chat_sft_function_calling_demo and at least one of dir: chat_sft_function_calling_demo, or version: None. Please note that dir, name, and version will be ignored.\n",
      "i.finetune/0 [default0]:[NeMo W 2025-02-13 07:56:18 nemo_logging:405] NeMoLogger is logging to chat_sft_function_calling_demo, but it already exists.\n",
      "i.finetune/0 [default0]:[NeMo W 2025-02-13 07:56:18 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to chat_sft_function_calling_demo/tb_logs\n",
      "i.finetune/0 [default0]:[NeMo W 2025-02-13 07:56:18 nemo_logging:405] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "i.finetune/0 [default0]:[NeMo W 2025-02-13 07:56:18 nemo_logging:405] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 40. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:19 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "i.finetune/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:distributed_backend=nccl\n",
      "i.finetune/0 [default0]:All distributed processes registered. Starting with 8 processes\n",
      "i.finetune/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default1]:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "i.finetune/0 [default6]:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "i.finetune/0 [default7]:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "i.finetune/0 [default4]:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "i.finetune/0 [default2]:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "i.finetune/0 [default3]:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:23 nemo_logging:393] Setting up ModelTransform for stage: TrainerFn.FITTING\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:23 nemo_logging:393] Found model_transform attribute on pl_module\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:23 nemo_logging:393] Set model_transform to: <function _call_counter.<locals>.wrapper at 0x7ffecc505300>\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:23 nemo_logging:393] Padded vocab_size: 125696, original vocab_size: 125696, dummy tokens: 0.\n",
      "i.finetune/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default6]:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default5]:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default7]:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:23 nemo_logging:393] Copying Trainer's 'max_steps' (40) to LR scheduler's 'max_steps'.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:23 num_microbatches_calculator:228] setting number of microbatches to constant 4\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:23 nemo_logging:393] Doing selective restore from RestoreConfig(path='/root/.cache/nemo/models/baichuan-inc/Baichuan2-7B-Base', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default4]:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Restoring model weights from RestoreConfig(path='/root/.cache/nemo/models/baichuan-inc/Baichuan2-7B-Base', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Finished restoring from RestoreConfig(path='/root/.cache/nemo/models/baichuan-inc/Baichuan2-7B-Base', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Building data files\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:  | Name   | Type     | Params | Mode \n",
      "i.finetune/0 [default0]:--------------------------------------------\n",
      "i.finetune/0 [default0]:0 | module | GPTModel | 7.5 B  | train\n",
      "i.finetune/0 [default0]:--------------------------------------------\n",
      "i.finetune/0 [default0]:7.5 B     Trainable params\n",
      "i.finetune/0 [default0]:0         Non-trainable params\n",
      "i.finetune/0 [default0]:7.5 B     Total params\n",
      "i.finetune/0 [default0]:30,023.893Total estimated model params size (MB)\n",
      "i.finetune/0 [default0]:649       Modules in train mode\n",
      "i.finetune/0 [default0]:0         Modules in eval mode\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Time building 0 / 1 mem-mapped files: 0:00:00.059388\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Time building 0 / 1 mem-mapped files: 0:00:00.049194\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Loading data files\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Loading xlam_dataset/training.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Time loading 1 mem-mapped files: 0:00:00.001992\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:30 nemo_logging:393] Computing global indices\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.0.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.0.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.1.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.1.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.2.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.2.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.3.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.3.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.4.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.4.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.5.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.5.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.6.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.6.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.7.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.7.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.8.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.8.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.9.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.9.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.10.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.10.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.11.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.11.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.12.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.12.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.13.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.13.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.14.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.14.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.15.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.15.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.16.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.16.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.17.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.17.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.18.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.18.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.19.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.19.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.20.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.20.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.21.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.21.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.22.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.22.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.23.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.23.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.24.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.24.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.25.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.25.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.26.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.26.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.27.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.27.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.28.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.28.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.29.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.29.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.30.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.30.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_proj\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_qkv\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.31.mlp.linear_fc1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Adding lora to: module.decoder.layers.31.mlp.linear_fc2\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] After applying model_transform:\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:  | Name   | Type     | Params | Mode \n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:--------------------------------------------\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:0 | module | GPTModel | 7.6 B  | train\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:--------------------------------------------\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:67.4 M    Trainable params\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:7.5 B     Non-trainable params\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:7.6 B     Total params\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:30,293.377Total estimated model params size (MB)\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:1289      Modules in train mode\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:0         Modules in eval mode\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Initializing model parallel\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 7573344256\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393]  > number of trainable parameters: 67371008 (0.89% of total)\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "i.finetune/0 [default0]:    Params for bucket 1 (67371008 elements):\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_proj.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc2.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_proj.adapter.linear_in.weight\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 nemo_logging:393] Setting up optimizers\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:56:32 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.98, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "i.finetune/0 [default0]:[NeMo W 2025-02-13 07:56:39 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!\n",
      "i.finetune/0 [default0]:[NeMo W 2025-02-13 07:56:39 rerun_state_machine:211] RerunStateMachine initialized in mode RerunMode.DISABLED\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 0/39 | lr: 1.961e-06 | global_batch_size: 32 | global_step: 0 | reduced_train_loss: 0.6076\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 1/39 | lr: 3.922e-06 | global_batch_size: 32 | global_step: 1 | reduced_train_loss: 0.7243 | consumed_samples: 64\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 2/39 | lr: 5.882e-06 | global_batch_size: 32 | global_step: 2 | reduced_train_loss: 0.7578 | consumed_samples: 96\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 3/39 | lr: 7.843e-06 | global_batch_size: 32 | global_step: 3 | reduced_train_loss: 0.7651 | consumed_samples: 128\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 4/39 | lr: 9.804e-06 | global_batch_size: 32 | global_step: 4 | reduced_train_loss: 0.5629 | consumed_samples: 160\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 5/39 | lr: 1.176e-05 | global_batch_size: 32 | global_step: 5 | reduced_train_loss: 0.5186 | consumed_samples: 192\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 6/39 | lr: 1.373e-05 | global_batch_size: 32 | global_step: 6 | reduced_train_loss: 0.5877 | consumed_samples: 224\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 7/39 | lr: 1.569e-05 | global_batch_size: 32 | global_step: 7 | reduced_train_loss: 0.5162 | consumed_samples: 256\n",
      "i.finetune/0 [default1]:[rank1]:W0213 07:56:54.281000 2223454 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default1]:[rank1]:W0213 07:56:54.281000 2223454 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default1]:[rank1]:W0213 07:56:54.281000 2223454 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 1632, actual 784\n",
      "i.finetune/0 [default1]:[rank1]:W0213 07:56:54.281000 2223454 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default1]:[rank1]:W0213 07:56:54.281000 2223454 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default5]:[rank5]:W0213 07:56:54.273000 2223458 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default5]:[rank5]:W0213 07:56:54.273000 2223458 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default5]:[rank5]:W0213 07:56:54.273000 2223458 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 1648, actual 2176\n",
      "i.finetune/0 [default5]:[rank5]:W0213 07:56:54.273000 2223458 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default5]:[rank5]:W0213 07:56:54.273000 2223458 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default3]:[rank3]:W0213 07:56:54.284000 2223456 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default3]:[rank3]:W0213 07:56:54.284000 2223456 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default3]:[rank3]:W0213 07:56:54.284000 2223456 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 1024, actual 640\n",
      "i.finetune/0 [default3]:[rank3]:W0213 07:56:54.284000 2223456 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default3]:[rank3]:W0213 07:56:54.284000 2223456 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default4]:[rank4]:W0213 07:56:54.278000 2223457 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default4]:[rank4]:W0213 07:56:54.278000 2223457 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default4]:[rank4]:W0213 07:56:54.278000 2223457 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 1776, actual 960\n",
      "i.finetune/0 [default4]:[rank4]:W0213 07:56:54.278000 2223457 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default4]:[rank4]:W0213 07:56:54.278000 2223457 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default6]:[rank6]:W0213 07:56:54.286000 2223459 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default6]:[rank6]:W0213 07:56:54.286000 2223459 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default6]:[rank6]:W0213 07:56:54.286000 2223459 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 1280, actual 896\n",
      "i.finetune/0 [default6]:[rank6]:W0213 07:56:54.286000 2223459 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default6]:[rank6]:W0213 07:56:54.286000 2223459 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default7]:[rank7]:W0213 07:56:54.270000 2223460 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default7]:[rank7]:W0213 07:56:54.270000 2223460 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default7]:[rank7]:W0213 07:56:54.270000 2223460 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 2048, actual 1280\n",
      "i.finetune/0 [default7]:[rank7]:W0213 07:56:54.270000 2223460 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default7]:[rank7]:W0213 07:56:54.270000 2223460 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 8/39 | lr: 1.765e-05 | global_batch_size: 32 | global_step: 8 | reduced_train_loss: 0.6177 | consumed_samples: 288\n",
      "i.finetune/0 [default2]:[rank2]:W0213 07:56:55.951000 2223455 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default2]:[rank2]:W0213 07:56:55.951000 2223455 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default2]:[rank2]:W0213 07:56:55.951000 2223455 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 1872, actual 1008\n",
      "i.finetune/0 [default2]:[rank2]:W0213 07:56:55.951000 2223455 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default2]:[rank2]:W0213 07:56:55.951000 2223455 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 9/39 | lr: 1.961e-05 | global_batch_size: 32 | global_step: 9 | reduced_train_loss: 0.5292 | consumed_samples: 320\n",
      "i.finetune/0 [default0]:[rank0]:W0213 07:56:57.683000 2223453 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default0]:[rank0]:W0213 07:56:57.683000 2223453 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default0]:[rank0]:W0213 07:56:57.683000 2223453 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/1: tensor 'L['exp_logits']' size mismatch at index 0. expected 2416, actual 784\n",
      "i.finetune/0 [default0]:[rank0]:W0213 07:56:57.683000 2223453 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default0]:[rank0]:W0213 07:56:57.683000 2223453 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 10/39 | lr: 2.157e-05 | global_batch_size: 32 | global_step: 10 | reduced_train_loss: 0.507 | consumed_samples: 352\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 11/39 | lr: 2.353e-05 | global_batch_size: 32 | global_step: 11 | reduced_train_loss: 0.617 | consumed_samples: 384\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 12/39 | lr: 2.549e-05 | global_batch_size: 32 | global_step: 12 | reduced_train_loss: 0.4336 | consumed_samples: 416\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 13/39 | lr: 2.745e-05 | global_batch_size: 32 | global_step: 13 | reduced_train_loss: 0.3228 | consumed_samples: 448\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 14/39 | lr: 2.941e-05 | global_batch_size: 32 | global_step: 14 | reduced_train_loss: 0.2974 | consumed_samples: 480\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 15/39 | lr: 3.137e-05 | global_batch_size: 32 | global_step: 15 | reduced_train_loss: 0.2696 | consumed_samples: 512\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 16/39 | lr: 3.333e-05 | global_batch_size: 32 | global_step: 16 | reduced_train_loss: 0.371 | consumed_samples: 544\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 17/39 | lr: 3.529e-05 | global_batch_size: 32 | global_step: 17 | reduced_train_loss: 0.2654 | consumed_samples: 576\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 18/39 | lr: 3.725e-05 | global_batch_size: 32 | global_step: 18 | reduced_train_loss: 0.1576 | consumed_samples: 608\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 19/39 | lr: 3.922e-05 | global_batch_size: 32 | global_step: 19 | reduced_train_loss: 0.2565 | consumed_samples: 640\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 20/39 | lr: 4.118e-05 | global_batch_size: 32 | global_step: 20 | reduced_train_loss: 0.166 | consumed_samples: 672\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 21/39 | lr: 4.314e-05 | global_batch_size: 32 | global_step: 21 | reduced_train_loss: 0.2562 | consumed_samples: 704\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 22/39 | lr: 4.51e-05 | global_batch_size: 32 | global_step: 22 | reduced_train_loss: 0.2775 | consumed_samples: 736\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 23/39 | lr: 4.706e-05 | global_batch_size: 32 | global_step: 23 | reduced_train_loss: 0.2772 | consumed_samples: 768\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 24/39 | lr: 4.902e-05 | global_batch_size: 32 | global_step: 24 | reduced_train_loss: 0.2381 | consumed_samples: 800\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 25/39 | lr: 5.098e-05 | global_batch_size: 32 | global_step: 25 | reduced_train_loss: 0.1314 | consumed_samples: 832\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 26/39 | lr: 5.294e-05 | global_batch_size: 32 | global_step: 26 | reduced_train_loss: 0.1224 | consumed_samples: 864\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 27/39 | lr: 5.49e-05 | global_batch_size: 32 | global_step: 27 | reduced_train_loss: 0.1227 | consumed_samples: 896\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 28/39 | lr: 5.686e-05 | global_batch_size: 32 | global_step: 28 | reduced_train_loss: 0.2096 | consumed_samples: 928\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 29/39 | lr: 5.882e-05 | global_batch_size: 32 | global_step: 29 | reduced_train_loss: 0.1033 | consumed_samples: 960\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 30/39 | lr: 6.078e-05 | global_batch_size: 32 | global_step: 30 | reduced_train_loss: 0.09491 | consumed_samples: 992\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 31/39 | lr: 6.275e-05 | global_batch_size: 32 | global_step: 31 | reduced_train_loss: 0.1452 | consumed_samples: 1024\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 32/39 | lr: 6.471e-05 | global_batch_size: 32 | global_step: 32 | reduced_train_loss: 0.0851 | consumed_samples: 1056\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 33/39 | lr: 6.667e-05 | global_batch_size: 32 | global_step: 33 | reduced_train_loss: 0.07826 | consumed_samples: 1088\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 34/39 | lr: 6.863e-05 | global_batch_size: 32 | global_step: 34 | reduced_train_loss: 0.07483 | consumed_samples: 1120\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 35/39 | lr: 7.059e-05 | global_batch_size: 32 | global_step: 35 | reduced_train_loss: 0.1123 | consumed_samples: 1152\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 36/39 | lr: 7.255e-05 | global_batch_size: 32 | global_step: 36 | reduced_train_loss: 0.1109 | consumed_samples: 1184\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 37/39 | lr: 7.451e-05 | global_batch_size: 32 | global_step: 37 | reduced_train_loss: 0.07391 | consumed_samples: 1216\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 38/39 | lr: 7.647e-05 | global_batch_size: 32 | global_step: 38 | reduced_train_loss: 0.06249 | consumed_samples: 1248\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 39/39 | lr: 7.843e-05 | global_batch_size: 32 | global_step: 39 | reduced_train_loss: 0.06387 | consumed_samples: 1280\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: `Trainer.fit` stopped: `max_steps=40` reached.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:57:50 nemo_logging:393] Scheduled async checkpoint save for chat_sft_function_calling_demo/checkpoints/model_name=0--val_loss=0.00-step=39-consumed_samples=1280.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:57:50 nemo_logging:393] Pending async checkpoint saves. Finalizing them synchronously now\n",
      "i.finetune/0 [default0]:[NeMo I 2025-02-13 07:57:51 nemo_logging:393] Async checkpoint save for step 40 (chat_sft_function_calling_demo/checkpoints/model_name=0--val_loss=0.00-step=39-consumed_samples=1280.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 I0213 07:58:07.499000 2223442 torch/distributed/elastic/agent/server/api.py:879] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "i.finetune/0 I0213 07:58:07.500000 2223442 torch/distributed/elastic/agent/server/api.py:932] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "i.finetune/0 I0213 07:58:07.501000 2223442 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.0002608299255371094 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.finetune-mnjn2bsc2qf4cd finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']</span><span style=\"background-color: #272822\">                           </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune_1739462160\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune_1739462160\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.finetune_1739462160</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.finetune_1739462160 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.finetune_1739462160 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1739462160\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1739462160\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1739462160\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recipe.resume.restore_config.path = \"nemo://baichuan-inc/Baichuan2-7B-Base\"\n",
    "recipe.data = run.Config(\n",
    "    llm.ChatDataModule,\n",
    "    dataset_root=\"xlam_dataset\",\n",
    "    seq_length=4096,\n",
    "    micro_batch_size=1,\n",
    "    global_batch_size=32,\n",
    ")\n",
    "recipe.trainer.limit_val_batches = 0\n",
    "recipe.trainer.max_steps = 40\n",
    "recipe.log.use_datetime_version = False\n",
    "# adjust other hyperparameters as needed\n",
    "# for example:\n",
    "# recipe.optim.config.lr = 1e-6\n",
    "# recipe.trainer.strategy.tensor_model_parallel_size = 2\n",
    "# recipe.log.ckpt.save_top_k = 3\n",
    "\n",
    "executor = local_executor_torchrun(devices=recipe.trainer.devices)\n",
    "run.run(recipe, executor=executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the SFT fininshes, you should see the logs and find the final checkpoint location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'model_name=0--val_loss=0.00-step=39-consumed_samples=1280.0-last'\n"
     ]
    }
   ],
   "source": [
    "!ls chat_sft_function_calling_demo/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Evaluate the Trained Model\n",
    "\n",
    "After successfully training a checkpoint, we should evaluate the effectiveness of the trained model. We can first convert the checkpoint back to a Hugging Face checkpoint, deploy inference, perform benchmarking, and verify the downstream tasks.\n",
    "\n",
    "### Run NeMo Framework Inference\n",
    "\n",
    "First, we can quickly check the trained model performance via NeMo inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nemo_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nemo_inference.py\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "import nemo.lightning as nl\n",
    "\n",
    "strategy = nl.MegatronStrategy(\n",
    "    tensor_model_parallel_size=1,\n",
    "    pipeline_model_parallel_size=1,\n",
    "    context_parallel_size=1,\n",
    "    sequence_parallel=False,\n",
    "    setup_optimizers=False,\n",
    "    store_optimizer_states=False,\n",
    ")\n",
    "\n",
    "trainer = nl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    num_nodes=1,\n",
    "    strategy=strategy,\n",
    "    plugins=nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        params_dtype=torch.bfloat16,\n",
    "        pipeline_dtype=torch.bfloat16,\n",
    "        autocast_enabled=False,\n",
    "        grad_reduce_in_fp32=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "source = {\n",
    "    \"mask\": \"User,Function\",\n",
    "    \"system\": \"Answer the following questions as best you can. You have access to the following tools:\\n\\ninquiry: Call this tool to interact with the inquiry API. What is the inquiry API useful for? 查询金融产品的价格。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\\n                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\\n                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\\nstrategy_query: Call this tool to interact with the strategy_query API. What is the strategy_query API useful for? 查询金融产品的交易策略。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True}]\\ntransaction_confirm: Call this tool to interact with the transaction_confirm API. What is the transaction_confirm API useful for? 确认交易。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\\n                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\\n                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\\n                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\\ntransaction_cancel: Call this tool to interact with the transaction_cancel API. What is the transaction_cancel API useful for? 取消交易。 Parameters: []\\n\\nOutput the following format:\\n\\nAction: the action to take, should be one of [inquiry, strategy_query, transaction_confirm, transaction_cancel]\\nAction Input: the input to the action\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"User\",\n",
    "            \"value\": \"Is there overnight lending available?\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "special_tokens = {\n",
    "                \"system_turn_start\": \"<extra_id_0>\",\n",
    "                \"turn_start\": \"<extra_id_1>\",\n",
    "                \"label_start\": \"<extra_id_2>\",\n",
    "                \"end_of_turn\": \"\\n\",\n",
    "                \"end_of_name\": \"\\n\",\n",
    "            }\n",
    "from nemo.collections.nlp.data.language_modeling.megatron.gpt_sft_chat_dataset import _get_header_conversation_type_mask_role\n",
    "# Apply prompt template to be the same format as training\n",
    "header, conversation, data_type, mask_role = _get_header_conversation_type_mask_role(source, special_tokens)\n",
    "prompts = [conversation]\n",
    "\n",
    "from nemo.collections.llm import api\n",
    "results = api.generate(\n",
    "    path=\"chat_sft_function_calling_demo/checkpoints/model_name=0--val_loss=0.00-step=39-consumed_samples=1280.0-last\",\n",
    "    prompts=prompts,\n",
    "    trainer=trainer,\n",
    "    inference_params=CommonInferenceParams(\n",
    "        temperature=1.0,\n",
    "        top_p=0,  # greedy decoding\n",
    "        top_k=1,  # greedy decoding\n",
    "        num_tokens_to_generate=50,\n",
    "    ),\n",
    "    text_only=True,\n",
    ")\n",
    "if torch.distributed.get_rank() == 0:\n",
    "    for i, r in enumerate(results):\n",
    "        print(prompts[i])\n",
    "        print(\"*\" * 50)\n",
    "        print(r)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: True\n",
      "[INFO     | lightning.pytorch.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | lightning.pytorch.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "[INFO     | lightning.pytorch.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-13 08:02:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "INFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[INFO     | lightning.fabric.utilities.distributed]: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Padded vocab_size: 125696, original vocab_size: 125696, dummy tokens: 0.\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 7505973248\n",
      "[NeMo I 2025-02-13 08:02:21 nemo_logging:393] Doing selective restore from RestoreConfig(path='/root/.cache/nemo/models/baichuan-inc/Baichuan2-7B-Base', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Restoring model weights from RestoreConfig(path='/root/.cache/nemo/models/baichuan-inc/Baichuan2-7B-Base', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Finished restoring from RestoreConfig(path='/root/.cache/nemo/models/baichuan-inc/Baichuan2-7B-Base', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.0.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.0.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.0.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.0.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.1.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.1.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.1.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.1.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.2.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.2.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.2.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.2.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.3.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.3.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.3.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.3.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.4.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.4.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.4.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.4.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.5.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.5.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.5.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.5.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.6.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.6.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.6.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.6.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.7.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.7.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.7.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.7.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.8.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.8.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.8.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.8.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.9.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.9.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.9.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.9.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.10.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.10.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.10.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.10.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.11.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.11.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.11.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.11.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.12.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.12.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.12.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.12.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.13.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.13.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.13.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.13.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.14.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.14.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.14.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.14.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.15.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.15.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.15.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.15.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.16.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.16.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.16.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.16.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.17.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.17.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.17.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.17.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.18.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.18.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.18.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.18.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.19.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.19.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.19.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.19.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.20.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.20.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.20.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.20.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.21.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.21.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.21.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.21.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.22.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.22.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.22.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.22.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.23.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.23.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.23.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.23.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.24.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.24.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.24.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.24.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.25.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.25.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.25.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.25.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.26.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.26.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.26.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.26.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.27.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.27.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.27.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.27.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.28.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.28.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.28.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.28.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.29.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.29.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.29.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.29.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.30.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.30.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.30.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.30.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.31.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.31.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.31.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:02:34 nemo_logging:393] Adding lora to: module.module.module.decoder.layers.31.mlp.linear_fc2\n",
      "<extra_id_0>System\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "inquiry: Call this tool to interact with the inquiry API. What is the inquiry API useful for? 查询金融产品的价格。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\n",
      "                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\n",
      "                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\n",
      "                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\n",
      "strategy_query: Call this tool to interact with the strategy_query API. What is the strategy_query API useful for? 查询金融产品的交易策略。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\n",
      "                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True}]\n",
      "transaction_confirm: Call this tool to interact with the transaction_confirm API. What is the transaction_confirm API useful for? 确认交易。 Parameters: [{'name': 'product', 'type': 'string', 'description': '产品类型', 'required': True},\n",
      "                  {'name': 'term', 'type': 'string', 'description': '期限', 'required': True},\n",
      "                  {'name': 'amount', 'type': 'string', 'description': '交易额度', 'required': True},\n",
      "                  {'name': 'interest_rate', 'type': 'string', 'description': '利率', 'required': True}]\n",
      "transaction_cancel: Call this tool to interact with the transaction_cancel API. What is the transaction_cancel API useful for? 取消交易。 Parameters: []\n",
      "\n",
      "Output the following format:\n",
      "\n",
      "Action: the action to take, should be one of [inquiry, strategy_query, transaction_confirm, transaction_cancel]\n",
      "Action Input: the input to the action\n",
      "<extra_id_1>User\n",
      "Is there overnight lending available?\n",
      "\n",
      "**************************************************\n",
      "<extra_id_1>inquiry\n",
      "<extra_id_1>inquiry\n",
      "<extra_id_1>inquiry\n",
      "<extra_id_1>inquiry\n",
      "<extra_id_1>inquiry\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun nemo_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert NeMo2 Model to HuggingFace Format\n",
    "\n",
    "If you're satisfied with the trained model's performance, we can continue. For the benchmark and downstream task assessment in the next two steps, the applications we will use only accept OpenAI API format inference requests. Therefore, we should first convert the saved checkpoint to a Hugging Face checkpoint for further deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing convert_to_hf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile convert_to_hf.py\n",
    "from pathlib import Path\n",
    "from nemo.collections.llm import export_ckpt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_ckpt(\n",
    "        path=Path(\"chat_sft_function_calling_demo/checkpoints/model_name=0--val_loss=0.00-step=39-consumed_samples=1280.0-last\"),\n",
    "        target=\"hf\",\n",
    "        output_path=Path(\"chat_sft_function_calling_demo/sft_hf\"),\n",
    "        overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-13 08:06:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: False\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-02-13 08:06:58 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "    \n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2025-02-13 08:06:58 nemo_logging:393] Padded vocab_size: 125696, original vocab_size: 125696, dummy tokens: 0.\n",
      "[NeMo I 2025-02-13 08:07:37 nemo_logging:393] Copying Trainer's 'max_steps' (-1) to LR scheduler's 'max_steps'.\n",
      "[NeMo I 2025-02-13 08:07:37 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 7505973248\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.0.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.0.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.1.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.1.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.2.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.2.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.3.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.3.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.4.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.4.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.5.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.5.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.6.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.6.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.7.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.7.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.8.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.8.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.9.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.9.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.10.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.10.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.11.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.11.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.12.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.12.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.13.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.13.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.14.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.14.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.15.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.15.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.16.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.16.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.17.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.17.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.18.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.18.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.19.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.19.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.20.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.20.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.21.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.21.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.22.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.22.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.23.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.23.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.24.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.24.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.25.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.25.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.26.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.26.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.27.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.27.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.28.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.28.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.29.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.29.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.30.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.30.mlp.linear_fc2\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_proj\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_qkv\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.31.mlp.linear_fc1\n",
      "[NeMo I 2025-02-13 08:07:38 nemo_logging:393] Adding lora to: module.decoder.layers.31.mlp.linear_fc2\n",
      "[WARNING  | transformers_modules.baichuan-inc.Baichuan2-7B-Base.f9d4d8dd2f7a3dbede3bda3b0cf0224e9272bbe5.modeling_baichuan]: Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.35it/s]\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "\u001b[32m✓ Checkpoint exported to chat_sft_function_calling_demo/sft_hf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!torchrun convert_to_hf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can follow the steps in [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/apps) to establish an OpenAI API, allowing us to deploy the model and handle inference requests efficiently.\n",
    "\n",
    "### Benchmark the Fine-Tuned Model\n",
    "\n",
    "To benchmark the function-calling ability of the fine-tuned LLM, you can refer to the [berkeley-function-call-leaderboard](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard). You will get benchmark results similar to those shown below.\n",
    "\n",
    "...\n",
    "\n",
    "🔍 Running test: multiple\n",
    "\n",
    "✅ Test completed: multiple. 🎯 Accuracy: 0.89\n",
    "\n",
    "🔍 Running test: parallel\n",
    "\n",
    "✅ Test completed: parallel. 🎯 Accuracy: 0.87\n",
    "\n",
    "🔍 Running test: parallel_multiple\n",
    "\n",
    "✅ Test completed: parallel_multiple. 🎯 Accuracy: 0.835\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Downstream Task Assessment\n",
    "\n",
    "If you are not ready with your own agent, below we provide an agent demo for your quick assessment. Since the returns of the function calls are hard-coded in this demo, we recommend you use the conversations below and input them in order:\n",
    "\n",
    "> Do you have overnight call loan?\n",
    "\n",
    "> If the interest rate can drop to 1.5%, I will proceed.\n",
    "\n",
    "> Okay, confirm the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"qwen-agent[gui,rag,code_interpreter,python_executor]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you should modify the LLM service configuration according to your model name and server address, such as `http://10.123.123.123:8000/v1`. If you want to try LLMs running on [NIM online](https://build.nvidia.com/explore/discover), you need to apply for a free API key and use the server address `https://integrate.api.nvidia.com/v1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing start_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile start_app.py\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "from typing import Optional\n",
    "from qwen_agent.agents import ReActChat\n",
    "from qwen_agent.gui import WebUI\n",
    "\n",
    "from qwen_agent.tools.base import BaseTool, register_tool\n",
    "import json\n",
    "\n",
    "def init_agent_service():\n",
    "    llm_cfg = {\n",
    "        'model': 'nvidia/mistral-nemo-minitron-8b-instruct',\n",
    "        'model_server': 'https://integrate.api.nvidia.com/v1', # http://10.137.164.245:8000/v1\n",
    "        'api_key': \"nvapi-YOUR-API-KEY\",\n",
    "    }\n",
    "    tools = ['inquiry', 'strategy_query', 'transaction_confirm', 'transaction_cancel']\n",
    "    bot = ReActChat(llm=llm_cfg,\n",
    "                    name='match transaction agent',\n",
    "                    description='This agent can help to match transaction.',\n",
    "                    function_list=tools)\n",
    "    return bot\n",
    "\n",
    "@register_tool('inquiry')\n",
    "class Inquiry(BaseTool): \n",
    "    description = 'After the initial quote, if the customer negotiates, use this tool to check the prices available for financial products.'\n",
    "    parameters = [{'name': 'product', 'type': 'string', 'description': 'Product type.', 'required': True},\n",
    "                  {'name': 'term', 'type': 'string', 'description': 'Term.', 'required': True},\n",
    "                  {'name': 'amount', 'type': 'string', 'description': 'Transaction amount.', 'required': True},\n",
    "                  {'name': 'interest_rate', 'type': 'string', 'description': 'Interest rate.', 'required': True},]\n",
    "    \n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        return json.dumps({'term': 'overnight', 'amount': '1 billion', 'interest_rate': '1.5%'},\n",
    "                ensure_ascii=False)\n",
    "\n",
    "@register_tool('strategy_query')\n",
    "class StrategyQuery(BaseTool): \n",
    "    description = 'Check the initial quotes for financial products.'\n",
    "    parameters = [{'name': 'product', 'type': 'string', 'description': 'Product type.', 'required': True},\n",
    "                  {'name': 'term', 'type': 'string', 'description': 'Term.', 'required': True},]\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        return json.dumps({'product': 'call loan', 'term': 'overnight', 'amount': '1 billion', 'interest_rate': '1.6%'},\n",
    "                ensure_ascii=False)\n",
    "\n",
    "@register_tool('transaction_confirm')\n",
    "class TransactionConfirm(BaseTool):\n",
    "    description = 'Confirm the transaction.'\n",
    "    parameters = [{'name': 'product', 'type': 'string', 'description': 'Product type.', 'required': True},\n",
    "                  {'name': 'term', 'type': 'string', 'description': 'Term.', 'required': True},\n",
    "                  {'name': 'amount', 'type': 'string', 'description': 'Transaction amount.', 'required': True},\n",
    "                  {'name': 'interest_rate', 'type': 'string', 'description': 'Interest rate.', 'required': True},]\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        return json.dumps({'response': 'success'},\n",
    "                ensure_ascii=False)\n",
    "\n",
    "@register_tool('transaction_cancel')\n",
    "class TransactionCancel(BaseTool): \n",
    "    description = 'Cancel the transaction.'\n",
    "    parameters = []\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        return json.dumps({'response': 'success'},\n",
    "                ensure_ascii=False)\n",
    "\n",
    "\n",
    "def app_gui():\n",
    "    bot = init_agent_service()\n",
    "    chatbot_config = {\n",
    "        'prompt.suggestions': ['Do you have overnight call loan?', 'If the interest rate can drop to 1.5%, I will proceed.', 'Okay, confirm the transaction.']\n",
    "    }\n",
    "    WebUI(bot, chatbot_config=chatbot_config).run(share=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app_gui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please Run When You're Done!\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
