{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45b25c3-08b2-4a7e-b0cd-67293f15c307",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "## Optimizing Hugging Face Models with Supervised Fine-Tuning (SFT) in NeMo 2.0\n",
    "\n",
    "NeMo 2.0 now allows users to perform SFT and PEFT using Hugging Face (HF) LLMs. This notebook demonstrates how to perform SFT with Hugging Face LLMs to make the models more performant on a specific task with. NeMo 2.0 utilizes HF's auto classes to download and load HF's transformer models, and wraps these models to turn them into lightning modules in order to perform tasks such as SFT and PEFT with NeMo 2.0.\n",
    "\n",
    "[AutoModel](https://huggingface.co/docs/transformers/en/model_doc/auto) is the generic model class that will be instantiated as one of the model classes of the library when created with the from_pretrained() class method. There are many AutoModel classes in HF and each of them covers a specific group of transformer model architectures. AutoModel class loads mainly the base transformer model that converts embeddings to hidden states where a specific AutoModel class such as AutoModelForCausalLM has a causal language modeling head on top of the base model.\n",
    "\n",
    "In this notebook, we will focus on the models that can be loaded using the HF's `AutoModelForCausalLM` class.\n",
    "\n",
    "## Data\n",
    "We will use [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset which is a reading comprehension dataset, consisting of questions and answers pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a50bad-f356-4076-8c5c-66b4481029dc",
   "metadata": {},
   "source": [
    "## Step 1. Import Modules and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e16913-6a08-4ad8-835e-311fbb5af01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import fiddle as fdl\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from nemo.lightning.pytorch.callbacks import JitConfig, JitTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe3c7d-9d36-47d2-9107-361025d175a0",
   "metadata": {},
   "source": [
    "We will be using SquadDataModule that NeMo 2.0. provides. This data module extends the `FineTuningDataModule`, so that it has access to existing data handling logic including the packed sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6a132-688e-4ad3-94ae-557e57ab77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataModuleWithPthDataloader(llm.SquadDataModule):\n",
    "    \"\"\"Creates a squad dataset with a PT dataloader\"\"\"\n",
    "\n",
    "    def _create_dataloader(self, dataset, mode, **kwargs) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            batch_size=self.micro_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def squad(tokenizer, mbs=1, gbs=2) -> pl.LightningDataModule:\n",
    "    \"\"\"Instantiates a SquadDataModuleWithPthDataloader and return it\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): the tokenizer to use\n",
    "\n",
    "    Returns:\n",
    "        pl.LightningDataModule: the dataset to train with.\n",
    "    \"\"\"\n",
    "    return SquadDataModuleWithPthDataloader(\n",
    "        tokenizer=tokenizer,\n",
    "        seq_length=512,\n",
    "        micro_batch_size=mbs,\n",
    "        global_batch_size=gbs,\n",
    "        num_workers=0,\n",
    "        dataset_kwargs={\n",
    "            \"sanity_check_dist_workers\": False,\n",
    "            \"get_attention_mask_from_fusion\": True,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23943ee-ffa1-497d-a395-3e4767271341",
   "metadata": {},
   "source": [
    "Now, we will set some variables including the HF model name, maximum steps, number of GPUs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780a047-febb-4d97-a59a-99d8ee036332",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\" # HF model name. This can be the path of the downloaded model as well.\n",
    "strategy = \"auto\" # Distributed training strategy such as DDP, FSDP2, etc.\n",
    "devices = 1 # Number of GPUs.\n",
    "max_steps = 100 # Number of steps in the training loop.\n",
    "accelerator = \"gpu\"\n",
    "wandb_project = None\n",
    "use_torch_jit = False # torch jit can be enabled.\n",
    "ckpt_folder=\"/opt/checkpoints/automodel_experiments/\" # Path for saving the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3578630-05b7-4a8c-8b5d-a7d9e847f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb = None\n",
    "if wandb_project is not None:\n",
    "    model = '_'.join(args.model.split('/')[-2:])\n",
    "    wandb = WandbLogger(\n",
    "        project=args.wandb_project,\n",
    "        name=f'{model}_dev{args.devices}_strat_{args.strategy}',\n",
    "    )\n",
    "\n",
    "callbacks = []\n",
    "if use_torch_jit:\n",
    "    jit_config = JitConfig(use_torch=True, torch_kwargs={'dynamic': False}, use_thunder=False)\n",
    "    callbacks = [JitTransform(jit_config)]\n",
    "\n",
    "callbacks.append(\n",
    "    nl.ModelCheckpoint(\n",
    "        every_n_train_steps=max_steps // 2,\n",
    "        dirpath=ckpt_folder,\n",
    "    )\n",
    ")\n",
    "\n",
    "if strategy == 'fsdp2':\n",
    "    astrategy = nl.FSDP2Strategy(data_parallel_size=devices, tensor_parallel_size=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llm.api.finetune(\n",
    "        model=llm.HFAutoModelForCausalLM(model_name=model_name),\n",
    "        data=squad(llm.HFAutoModelForCausalLM.configure_tokenizer(model_name), gbs=devices),\n",
    "        trainer=nl.Trainer(\n",
    "            devices=devices,\n",
    "            max_steps=max_steps,\n",
    "            accelerator=\"gpu\",\n",
    "            strategy=strategy,\n",
    "            log_every_n_steps=1,\n",
    "            limit_val_batches=0.0,\n",
    "            num_sanity_val_steps=0,\n",
    "            accumulate_grad_batches=1,\n",
    "            gradient_clip_val=1.0,\n",
    "            use_distributed_sampler=False,\n",
    "            logger=wandb,\n",
    "            callbacks=callbacks,\n",
    "            precision=\"bf16\",\n",
    "        ),\n",
    "        optim=fdl.build(llm.adam.pytorch_adam_with_flat_lr(lr=1e-5)),\n",
    "        log=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bdec21-b33d-44af-b494-7023bfa26db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
