{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dummy Data for Run DAPT Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Used Code\n",
    "\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_ROOT_DIR = \"/work/Data\" # Change Path to your directory\n",
    "\n",
    "\n",
    "def generate_random_text(length=100):\n",
    "    # Increase the frequency of alphabets compared to special characters\n",
    "    characters = string.ascii_letters * 5 + string.digits * 3 + string.punctuation + \" \\n\" + \" \"*5\n",
    "    return ''.join(random.choices(characters, k=length))\n",
    "\n",
    "def calculate_line_count_and_size(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    line_count = len(lines)\n",
    "    size_in_bytes = len(text.encode('utf-8'))\n",
    "    return line_count, size_in_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1. Create Domain Adapted Pretrining Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dapt_dummy_data(num_records, output_file):\n",
    "    categories = [\"text\"]\n",
    "    file_extensions = [\".txt\", \".pdf\", \".md\"]\n",
    "    file_types = [\"text\"]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(num_records):\n",
    "        random_text = generate_random_text(random.randint(50, 200))\n",
    "        line_count, size_in_bytes = calculate_line_count_and_size(random_text)\n",
    "\n",
    "        record = {\n",
    "            \"text\": random_text,\n",
    "            \"id\": f\"id_{i+1}\",\n",
    "            \"file_extension\": random.choice(file_extensions),\n",
    "            \"file_type\": random.choice(file_types),\n",
    "            \"category\": random.choice(categories),\n",
    "            \"line_count\": line_count,\n",
    "            \"size_in_bytes\": size_in_bytes,\n",
    "            \"path\": f\"/path/to/file_{i+1}.{random.choice(file_extensions)}\"\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Generated {num_records} records and saved to {output_file}\")\n",
    "\n",
    "num_records = 100\n",
    "num_files = 4\n",
    "output_folder = os.path.join(DATA_ROOT_DIR, \"dapt\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "for i in range(num_files):\n",
    "    output_file = os.path.join(output_folder, f\"dapt_data{i}.jsonl\")\n",
    "    \n",
    "    generate_dapt_dummy_data(num_records, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. Create Alignment Dummy Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (1) Supervise finetuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_text_length = 50\n",
    "max_text_length = 200\n",
    "\n",
    "def generate_sft_dummy_data(num_records, output_file):\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        random_text = generate_random_text(random.randint(min_text_length, max_text_length))\n",
    "\n",
    "        record = {\n",
    "            \"input\": generate_random_text(random.randint(min_text_length, max_text_length)),\n",
    "            \"output\": generate_random_text(random.randint(min_text_length, max_text_length))\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Generated {num_records} records and saved to {output_file}\")\n",
    "\n",
    "output_folder = os.path.join(DATA_ROOT_DIR, \"sft\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "output_file = os.path.join(output_folder, f\"sft_train_data.jsonl\")\n",
    "num_records = 1000\n",
    "generate_sft_dummy_data(num_records, output_file)\n",
    "output_file = os.path.join(output_folder, f\"sft_val_data.jsonl\")\n",
    "num_records = 50\n",
    "generate_sft_dummy_data(num_records, output_file)\n",
    "output_file = os.path.join(output_folder, f\"sft_test_data.jsonl\")\n",
    "num_records = 50\n",
    "generate_sft_dummy_data(num_records, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Attribute Model Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attribute_dummy_data(num_records, output_file):\n",
    "    ALL_ATTRIBUTES = [\"quality\", \"toxicity\", \"humor\", \"creativity\", \"helpfulness\", \"correctness\", \"coherence\", \"complexity\", \"verbosity\"]\n",
    "    SYSTEM_PROMPT = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "\n",
    "    LABEL_PREFIX = \"<extra_id_2>\"\n",
    "    \n",
    "    SCORE_MAX = 4\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        user_text = generate_random_text(random.randint(50, 200))\n",
    "        assistant_text = generate_random_text(random.randint(50, 200))\n",
    "        \n",
    "        random_label = []\n",
    "\n",
    "        for _ in ALL_ATTRIBUTES:\n",
    "            if random.random() > 0.6:\n",
    "                random_label.append(-100)\n",
    "            else:\n",
    "                random_label.append(round(np.random.uniform(0.5, SCORE_MAX+0.5, size=1)[0]))\n",
    "        \n",
    "        SYSTEM_PROMPT_TEMPLATE = \"<extra_id_0>System\\n{value}\\n\".format(value=SYSTEM_PROMPT)\n",
    "        USER_TURN_TEMPLATE = \"<extra_id_1>User\\n{value}\\n\".format(value=user_text)\n",
    "        ASSISTANT_TURN_TEMPLATE = \"<extra_id_1>Assistant\\n{value}\\n\".format(value=assistant_text)\n",
    "                \n",
    "        text = f\"{SYSTEM_PROMPT_TEMPLATE}{USER_TURN_TEMPLATE}{ASSISTANT_TURN_TEMPLATE}{LABEL_PREFIX}\"\n",
    "\n",
    "        record = {\n",
    "            \"text\": text,\n",
    "            \"label\": random_label\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Generated {num_records} records and saved to {output_file}\") \n",
    "    \n",
    "    \n",
    "output_folder = os.path.join(DATA_ROOT_DIR, \"reg\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "output_file = os.path.join(output_folder, f\"reg_train_data.jsonl\")\n",
    "num_records = 1000\n",
    "generate_attribute_dummy_data(num_records, output_file)\n",
    "output_file = os.path.join(output_folder, f\"reg_val_data.jsonl\")\n",
    "num_records = 50\n",
    "generate_attribute_dummy_data(num_records, output_file)\n",
    "output_file = os.path.join(output_folder, f\"reg_test_data.jsonl\")\n",
    "num_records = 50\n",
    "generate_attribute_dummy_data(num_records, output_file)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Chat data for making pseudo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_dummy_data(num_records, output_file, add_label=False):\n",
    "    SYSTEM_PROMPT = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        user_text = generate_random_text(random.randint(50, 200))\n",
    "        assistant_text = generate_random_text(random.randint(50, 200))\n",
    "\n",
    "        ALL_ATTRIBUTES = [\"quality\", \"toxicity\", \"humor\", \"creativity\", \"helpfulness\", \"correctness\", \"coherence\", \"complexity\", \"verbosity\"]\n",
    "\n",
    "        if add_label:\n",
    "            random_label = \"\"\n",
    "            SCORE_MAX = 4\n",
    "            for att in ALL_ATTRIBUTES:\n",
    "                if random.random() > 0.6:\n",
    "                    continue\n",
    "                else:\n",
    "                    random_label+=f\"{att}:{round(np.random.uniform(0.5, SCORE_MAX+0.5, size=1)[0])}\"\n",
    "        else:\n",
    "            random_label = \"no_label\"\n",
    "            \n",
    "        record = {\n",
    "            \"system\": SYSTEM_PROMPT,\n",
    "            \"mask\": \"User\",\n",
    "            \"conversations\": [\n",
    "                    {\n",
    "                    \"from\": \"User\",\n",
    "                    \"value\": user_text\n",
    "                    },\n",
    "                    {\n",
    "                    \"from\": \"Assistant\",\n",
    "                    \"value\": assistant_text,\n",
    "                    \"label\": random_label\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        data.append(record)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Generated {num_records} records and saved to {output_file}\") \n",
    "    \n",
    "output_folder = os.path.join(DATA_ROOT_DIR, \"chat\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "output_file = os.path.join(output_folder, f\"chat_train_data.jsonl\")\n",
    "num_records = 1000\n",
    "generate_chat_dummy_data(num_records, output_file)\n",
    "output_file = os.path.join(output_folder, f\"chat_val_data.jsonl\")\n",
    "num_records = 50\n",
    "generate_chat_dummy_data(num_records, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Create Domain Adapted Retrieval Dummy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dummy Text Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_dummy_data(num_records, output_file):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        record = generate_random_text(random.randint(50, 200))\n",
    "        data.append(record)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Generated {num_records} records and saved to {output_file}\")    \n",
    "\n",
    "num_files = 4\n",
    "num_records = 1000\n",
    "output_folder = os.path.join(DATA_ROOT_DIR, \"docs\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "for i in range(num_files):\n",
    "    output_file = os.path.join(output_folder, f\"train_data{i}.txt\")\n",
    "    generate_text_dummy_data(num_records, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
