{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. DAPT (Domain Adaptive Pre-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, you need to create [step 1 dummy data](./Step0_Dummy_Data.ipynb) or prepare real data ([see here](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/dapt-curation)) for the actual DAPT.\n",
    "If you plan to train the real model, make sure to prepare not only the domain-specific data but also general-purpose data to be used in the continued pretraining.\n",
    "\n",
    "We use huggingface \"meta-llama/Llama-3.1-8B\" model for practice.\n",
    "\n",
    "In this step, you will perform domain-adaptive tokenization and domain-adaptive continued pretraining (DAPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Domain-adaptive tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save all dapt text data to /work/Data/all_dapt_text.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "MODEL_ROOT_DIR = \"/work/Models\" # change to your path\n",
    "DATA_ROOT_DIR = \"/work/Data\"\n",
    "\n",
    "all_files = glob.glob(f\"{DATA_ROOT_DIR}/dapt/*.jsonl\") # DAPT Data Path \n",
    "\n",
    "all_texts = \"\"\n",
    "for data_file in all_files:\n",
    "    with jsonlines.open(data_file) as reader:\n",
    "        for obj in reader:\n",
    "            all_texts+=obj[\"text\"]+\"\\n\"\n",
    "                \n",
    "# Write the text data into a file\n",
    "all_text_file = f\"{DATA_ROOT_DIR}/all_dapt_text.txt\"\n",
    "with open(all_text_file, 'w') as data_fp:\n",
    "  data_fp.write(all_texts)\n",
    "  \n",
    "print(f\"Save all dapt text data to {all_text_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Model file already exists, overriding old model file !\n",
      "[NeMo I 2024-12-19 05:10:07 sentencepiece_tokenizer:378] Processing /work/Data/all_dapt_text.txt and store at /work/Data/tokenizer_spe_bpe_v100\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/work/Data/all_dapt_text.txt --model_prefix=/work/Data/tokenizer_spe_bpe_v100/tokenizer --vocab_size=100 --shuffle_input_sentence=true --hard_vocab_limit=false --model_type=bpe --character_coverage=1.0 --bos_id=-1 --eos_id=-1 --normalization_rule_name=nmt_nfkc_cf --remove_extra_whitespaces=false\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /work/Data/all_dapt_text.txt\n",
      "  input_format: \n",
      "  model_prefix: /work/Data/tokenizer_spe_bpe_v100/tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc_cf\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: /work/Data/all_dapt_text.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 5519 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=505014\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 5519 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 5519\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 13783\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=543 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=490 size=20 all=3699 active=2130 piece=em\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: /work/Data/tokenizer_spe_bpe_v100/tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: /work/Data/tokenizer_spe_bpe_v100/tokenizer.vocab\n",
      "Serialized tokenizer at location : /work/Data/tokenizer_spe_bpe_v100\n"
     ]
    }
   ],
   "source": [
    "tokenizer_spe_type = \"bpe\"\n",
    "vocab_size = 100 # target vocab size for domain specific data\n",
    "\n",
    "!python /opt/NeMo/scripts/tokenizers/process_asr_text_tokenizer.py --data_file $all_text_file --data_root=$DATA_ROOT_DIR --vocab_size=$vocab_size --tokenizer=spe --spe_type=$tokenizer_spe_type  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model  tokenizer.vocab  vocab.txt\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer_dir = DATA_ROOT_DIR + f\"/tokenizer_spe_{tokenizer_spe_type}_v{vocab_size}\"\n",
    "\n",
    "! ls $custom_tokenizer_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Add domain specific token to original tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2024-12-19 05:10:16 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-19 05:10:17 tokenizer_utils:106] tokenizer_model: /work/Data/tokenizer_spe_bpe_v100/tokenizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.collections import common as nemo_common\n",
    "from omegaconf import OmegaConf\n",
    "import huggingface_hub as hf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "HF_LLM_MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "domain_tokenizer = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"sentencepiece\", tokenizer_model=custom_tokenizer_dir+\"/tokenizer.model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_LLM_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Only Vocab:  ['jv', '<unk>', 'nq', 'zr', 'zg', '▁', 'uq']\n"
     ]
    }
   ],
   "source": [
    "# Filtering Domain-Only Token\n",
    "\n",
    "general_vocab = set(tokenizer.vocab.keys())\n",
    "domain_vocab = set(domain_tokenizer.vocab)\n",
    "domain_only_vocab = domain_vocab - general_vocab\n",
    "domain_only_vocab = list(domain_only_vocab)\n",
    "print(\"Domain Only Vocab: \", domain_only_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ori Vocab:  128256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Vocab:  128263\n"
     ]
    }
   ],
   "source": [
    "print(\"Ori Vocab: \", len(tokenizer))\n",
    "tokenizer.add_tokens(domain_only_vocab)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"New Vocab: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Reinitialize embedding matrix on LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_mean(tokens, tokenizer):\n",
    "    # get embedding initialize values\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    embedding_values = []\n",
    "    with torch.no_grad():\n",
    "        for token in tokens:\n",
    "            split_token = tokenizer.tokenize(token, add_special_tokens=False)\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(split_token)\n",
    "            embeddings = embedding_layer.weight[token_ids]\n",
    "            avg_embedding = embeddings.mean(dim=0)\n",
    "            embedding_values.append(avg_embedding)\n",
    "            \n",
    "    return embedding_values\n",
    "\n",
    "embedding_values = get_embedding_mean(domain_only_vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_embedding_value(tokens, new_tokenizer, mean_emb_values):\n",
    "    new_embedding_layer = model.get_input_embeddings()\n",
    "    output_embedding_layers = model.get_output_embeddings()\n",
    "    with torch.no_grad():\n",
    "        for i, token in enumerate(tokens):\n",
    "            token_id = new_tokenizer.convert_tokens_to_ids(token)\n",
    "            new_embedding_layer.weight[token_id] = mean_emb_values[i]\n",
    "            output_embedding_layers.weight[token_id] = torch.zeros_like(mean_emb_values[i])\n",
    "            \n",
    "\n",
    "set_embedding_value(domain_only_vocab, tokenizer, embedding_values)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for jv: [-0.00240288 -0.00399607  0.00210693 ...  0.00032894 -0.00118068\n",
      "  0.0008361 ] Is Same:  [ True  True  True ...  True  True  True]\n",
      "Output Embedding for jv: [0. 0. 0. ... 0. 0. 0.]\n",
      "Embedding for <unk>: [-0.00240288 -0.00399607  0.00210693 ...  0.00032894 -0.00118068\n",
      "  0.0008361 ] Is Same:  [ True  True  True ...  True  True  True]\n",
      "Output Embedding for <unk>: [0. 0. 0. ... 0. 0. 0.]\n",
      "Embedding for nq: [-0.00240288 -0.00399607  0.00210693 ...  0.00032894 -0.00118068\n",
      "  0.0008361 ] Is Same:  [ True  True  True ...  True  True  True]\n",
      "Output Embedding for nq: [0. 0. 0. ... 0. 0. 0.]\n",
      "Embedding for zr: [-0.00240288 -0.00399607  0.00210693 ...  0.00032894 -0.00118068\n",
      "  0.0008361 ] Is Same:  [ True  True  True ...  True  True  True]\n",
      "Output Embedding for zr: [0. 0. 0. ... 0. 0. 0.]\n",
      "Embedding for zg: [-0.00240288 -0.00399607  0.00210693 ...  0.00032894 -0.00118068\n",
      "  0.0008361 ] Is Same:  [ True  True  True ...  True  True  True]\n",
      "Output Embedding for zg: [0. 0. 0. ... 0. 0. 0.]\n",
      "Embedding for ▁: [-0.00240288 -0.00399607  0.00210693 ...  0.00032894 -0.00118068\n",
      "  0.0008361 ] Is Same:  [ True  True  True ...  True  True  True]\n",
      "Output Embedding for ▁: [0. 0. 0. ... 0. 0. 0.]\n",
      "Embedding for uq: [-0.00240288 -0.00399607  0.00210693 ...  0.00032894 -0.00118068\n",
      "  0.0008361 ] Is Same:  [ True  True  True ...  True  True  True]\n",
      "Output Embedding for uq: [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check Init is Okay\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "output_embedding_layer = model.get_output_embeddings()\n",
    "\n",
    "for i, token in enumerate(domain_only_vocab):\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    ori_value = embedding_values[i].data.numpy()\n",
    "    init_value = embedding_layer.weight[token_id].data.numpy()\n",
    "    out_value = output_embedding_layer.weight[token_id].data.numpy()\n",
    "    print(f\"Embedding for {token}: {init_value}\", \"Is Same: \", ori_value==init_value)\n",
    "    print(f\"Output Embedding for {token}: {out_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Converted Model\n",
    "new_hf_model_path = f\"{MODEL_ROOT_DIR}/llama3-new-token\"\n",
    "\n",
    "tokenizer.save_pretrained(new_hf_model_path)\n",
    "model.save_pretrained(new_hf_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Convert HF model to .nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-19 05:12:38 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo I 2024-12-19 05:12:39 convert_llama_hf_to_nemo:128] loading checkpoint /work/Models/llama3-new-token\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:01<00:00,  4.64it/s]\n",
      "hf_config: {'vocab_size': 128263, 'max_position_embeddings': 131072, 'hidden_size': 4096, 'intermediate_size': 14336, 'num_hidden_layers': 32, 'num_attention_heads': 32, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'pretraining_tp': 1, 'use_cache': True, 'rope_theta': 500000.0, 'rope_scaling': {'factor': 8.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}, 'attention_bias': False, 'attention_dropout': 0.0, 'mlp_bias': False, 'head_dim': 128, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': torch.float32, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['LlamaForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 128000, 'pad_token_id': None, 'eos_token_id': 128001, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '/work/Models/llama3-new-token', '_commit_hash': None, '_attn_implementation_internal': 'sdpa', '_attn_implementation_autoset': True, 'transformers_version': '4.46.0', 'model_type': 'llama'}\n",
      "named parameters:\n",
      "- model.embed_tokens.weight\n",
      "- model.layers.0.self_attn.q_proj.weight\n",
      "- model.layers.0.self_attn.k_proj.weight\n",
      "- model.layers.0.self_attn.v_proj.weight\n",
      "- model.layers.0.self_attn.o_proj.weight\n",
      "- model.layers.0.mlp.gate_proj.weight\n",
      "- model.layers.0.mlp.up_proj.weight\n",
      "- model.layers.0.mlp.down_proj.weight\n",
      "- model.layers.0.input_layernorm.weight\n",
      "- model.layers.0.post_attention_layernorm.weight\n",
      "- model.layers.1.self_attn.q_proj.weight\n",
      "- model.layers.1.self_attn.k_proj.weight\n",
      "- model.layers.1.self_attn.v_proj.weight\n",
      "- model.layers.1.self_attn.o_proj.weight\n",
      "- model.layers.1.mlp.gate_proj.weight\n",
      "- model.layers.1.mlp.up_proj.weight\n",
      "- model.layers.1.mlp.down_proj.weight\n",
      "- model.layers.1.input_layernorm.weight\n",
      "- model.layers.1.post_attention_layernorm.weight\n",
      "- model.layers.2.self_attn.q_proj.weight\n",
      "- model.layers.2.self_attn.k_proj.weight\n",
      "- model.layers.2.self_attn.v_proj.weight\n",
      "- model.layers.2.self_attn.o_proj.weight\n",
      "- model.layers.2.mlp.gate_proj.weight\n",
      "- model.layers.2.mlp.up_proj.weight\n",
      "- model.layers.2.mlp.down_proj.weight\n",
      "- model.layers.2.input_layernorm.weight\n",
      "- model.layers.2.post_attention_layernorm.weight\n",
      "- model.layers.3.self_attn.q_proj.weight\n",
      "- model.layers.3.self_attn.k_proj.weight\n",
      "- model.layers.3.self_attn.v_proj.weight\n",
      "- model.layers.3.self_attn.o_proj.weight\n",
      "- model.layers.3.mlp.gate_proj.weight\n",
      "- model.layers.3.mlp.up_proj.weight\n",
      "- model.layers.3.mlp.down_proj.weight\n",
      "- model.layers.3.input_layernorm.weight\n",
      "- model.layers.3.post_attention_layernorm.weight\n",
      "- model.layers.4.self_attn.q_proj.weight\n",
      "- model.layers.4.self_attn.k_proj.weight\n",
      "- model.layers.4.self_attn.v_proj.weight\n",
      "- model.layers.4.self_attn.o_proj.weight\n",
      "- model.layers.4.mlp.gate_proj.weight\n",
      "- model.layers.4.mlp.up_proj.weight\n",
      "- model.layers.4.mlp.down_proj.weight\n",
      "- model.layers.4.input_layernorm.weight\n",
      "- model.layers.4.post_attention_layernorm.weight\n",
      "- model.layers.5.self_attn.q_proj.weight\n",
      "- model.layers.5.self_attn.k_proj.weight\n",
      "- model.layers.5.self_attn.v_proj.weight\n",
      "- model.layers.5.self_attn.o_proj.weight\n",
      "- model.layers.5.mlp.gate_proj.weight\n",
      "- model.layers.5.mlp.up_proj.weight\n",
      "- model.layers.5.mlp.down_proj.weight\n",
      "- model.layers.5.input_layernorm.weight\n",
      "- model.layers.5.post_attention_layernorm.weight\n",
      "- model.layers.6.self_attn.q_proj.weight\n",
      "- model.layers.6.self_attn.k_proj.weight\n",
      "- model.layers.6.self_attn.v_proj.weight\n",
      "- model.layers.6.self_attn.o_proj.weight\n",
      "- model.layers.6.mlp.gate_proj.weight\n",
      "- model.layers.6.mlp.up_proj.weight\n",
      "- model.layers.6.mlp.down_proj.weight\n",
      "- model.layers.6.input_layernorm.weight\n",
      "- model.layers.6.post_attention_layernorm.weight\n",
      "- model.layers.7.self_attn.q_proj.weight\n",
      "- model.layers.7.self_attn.k_proj.weight\n",
      "- model.layers.7.self_attn.v_proj.weight\n",
      "- model.layers.7.self_attn.o_proj.weight\n",
      "- model.layers.7.mlp.gate_proj.weight\n",
      "- model.layers.7.mlp.up_proj.weight\n",
      "- model.layers.7.mlp.down_proj.weight\n",
      "- model.layers.7.input_layernorm.weight\n",
      "- model.layers.7.post_attention_layernorm.weight\n",
      "- model.layers.8.self_attn.q_proj.weight\n",
      "- model.layers.8.self_attn.k_proj.weight\n",
      "- model.layers.8.self_attn.v_proj.weight\n",
      "- model.layers.8.self_attn.o_proj.weight\n",
      "- model.layers.8.mlp.gate_proj.weight\n",
      "- model.layers.8.mlp.up_proj.weight\n",
      "- model.layers.8.mlp.down_proj.weight\n",
      "- model.layers.8.input_layernorm.weight\n",
      "- model.layers.8.post_attention_layernorm.weight\n",
      "- model.layers.9.self_attn.q_proj.weight\n",
      "- model.layers.9.self_attn.k_proj.weight\n",
      "- model.layers.9.self_attn.v_proj.weight\n",
      "- model.layers.9.self_attn.o_proj.weight\n",
      "- model.layers.9.mlp.gate_proj.weight\n",
      "- model.layers.9.mlp.up_proj.weight\n",
      "- model.layers.9.mlp.down_proj.weight\n",
      "- model.layers.9.input_layernorm.weight\n",
      "- model.layers.9.post_attention_layernorm.weight\n",
      "- model.layers.10.self_attn.q_proj.weight\n",
      "- model.layers.10.self_attn.k_proj.weight\n",
      "- model.layers.10.self_attn.v_proj.weight\n",
      "- model.layers.10.self_attn.o_proj.weight\n",
      "- model.layers.10.mlp.gate_proj.weight\n",
      "- model.layers.10.mlp.up_proj.weight\n",
      "- model.layers.10.mlp.down_proj.weight\n",
      "- model.layers.10.input_layernorm.weight\n",
      "- model.layers.10.post_attention_layernorm.weight\n",
      "- model.layers.11.self_attn.q_proj.weight\n",
      "- model.layers.11.self_attn.k_proj.weight\n",
      "- model.layers.11.self_attn.v_proj.weight\n",
      "- model.layers.11.self_attn.o_proj.weight\n",
      "- model.layers.11.mlp.gate_proj.weight\n",
      "- model.layers.11.mlp.up_proj.weight\n",
      "- model.layers.11.mlp.down_proj.weight\n",
      "- model.layers.11.input_layernorm.weight\n",
      "- model.layers.11.post_attention_layernorm.weight\n",
      "- model.layers.12.self_attn.q_proj.weight\n",
      "- model.layers.12.self_attn.k_proj.weight\n",
      "- model.layers.12.self_attn.v_proj.weight\n",
      "- model.layers.12.self_attn.o_proj.weight\n",
      "- model.layers.12.mlp.gate_proj.weight\n",
      "- model.layers.12.mlp.up_proj.weight\n",
      "- model.layers.12.mlp.down_proj.weight\n",
      "- model.layers.12.input_layernorm.weight\n",
      "- model.layers.12.post_attention_layernorm.weight\n",
      "- model.layers.13.self_attn.q_proj.weight\n",
      "- model.layers.13.self_attn.k_proj.weight\n",
      "- model.layers.13.self_attn.v_proj.weight\n",
      "- model.layers.13.self_attn.o_proj.weight\n",
      "- model.layers.13.mlp.gate_proj.weight\n",
      "- model.layers.13.mlp.up_proj.weight\n",
      "- model.layers.13.mlp.down_proj.weight\n",
      "- model.layers.13.input_layernorm.weight\n",
      "- model.layers.13.post_attention_layernorm.weight\n",
      "- model.layers.14.self_attn.q_proj.weight\n",
      "- model.layers.14.self_attn.k_proj.weight\n",
      "- model.layers.14.self_attn.v_proj.weight\n",
      "- model.layers.14.self_attn.o_proj.weight\n",
      "- model.layers.14.mlp.gate_proj.weight\n",
      "- model.layers.14.mlp.up_proj.weight\n",
      "- model.layers.14.mlp.down_proj.weight\n",
      "- model.layers.14.input_layernorm.weight\n",
      "- model.layers.14.post_attention_layernorm.weight\n",
      "- model.layers.15.self_attn.q_proj.weight\n",
      "- model.layers.15.self_attn.k_proj.weight\n",
      "- model.layers.15.self_attn.v_proj.weight\n",
      "- model.layers.15.self_attn.o_proj.weight\n",
      "- model.layers.15.mlp.gate_proj.weight\n",
      "- model.layers.15.mlp.up_proj.weight\n",
      "- model.layers.15.mlp.down_proj.weight\n",
      "- model.layers.15.input_layernorm.weight\n",
      "- model.layers.15.post_attention_layernorm.weight\n",
      "- model.layers.16.self_attn.q_proj.weight\n",
      "- model.layers.16.self_attn.k_proj.weight\n",
      "- model.layers.16.self_attn.v_proj.weight\n",
      "- model.layers.16.self_attn.o_proj.weight\n",
      "- model.layers.16.mlp.gate_proj.weight\n",
      "- model.layers.16.mlp.up_proj.weight\n",
      "- model.layers.16.mlp.down_proj.weight\n",
      "- model.layers.16.input_layernorm.weight\n",
      "- model.layers.16.post_attention_layernorm.weight\n",
      "- model.layers.17.self_attn.q_proj.weight\n",
      "- model.layers.17.self_attn.k_proj.weight\n",
      "- model.layers.17.self_attn.v_proj.weight\n",
      "- model.layers.17.self_attn.o_proj.weight\n",
      "- model.layers.17.mlp.gate_proj.weight\n",
      "- model.layers.17.mlp.up_proj.weight\n",
      "- model.layers.17.mlp.down_proj.weight\n",
      "- model.layers.17.input_layernorm.weight\n",
      "- model.layers.17.post_attention_layernorm.weight\n",
      "- model.layers.18.self_attn.q_proj.weight\n",
      "- model.layers.18.self_attn.k_proj.weight\n",
      "- model.layers.18.self_attn.v_proj.weight\n",
      "- model.layers.18.self_attn.o_proj.weight\n",
      "- model.layers.18.mlp.gate_proj.weight\n",
      "- model.layers.18.mlp.up_proj.weight\n",
      "- model.layers.18.mlp.down_proj.weight\n",
      "- model.layers.18.input_layernorm.weight\n",
      "- model.layers.18.post_attention_layernorm.weight\n",
      "- model.layers.19.self_attn.q_proj.weight\n",
      "- model.layers.19.self_attn.k_proj.weight\n",
      "- model.layers.19.self_attn.v_proj.weight\n",
      "- model.layers.19.self_attn.o_proj.weight\n",
      "- model.layers.19.mlp.gate_proj.weight\n",
      "- model.layers.19.mlp.up_proj.weight\n",
      "- model.layers.19.mlp.down_proj.weight\n",
      "- model.layers.19.input_layernorm.weight\n",
      "- model.layers.19.post_attention_layernorm.weight\n",
      "- model.layers.20.self_attn.q_proj.weight\n",
      "- model.layers.20.self_attn.k_proj.weight\n",
      "- model.layers.20.self_attn.v_proj.weight\n",
      "- model.layers.20.self_attn.o_proj.weight\n",
      "- model.layers.20.mlp.gate_proj.weight\n",
      "- model.layers.20.mlp.up_proj.weight\n",
      "- model.layers.20.mlp.down_proj.weight\n",
      "- model.layers.20.input_layernorm.weight\n",
      "- model.layers.20.post_attention_layernorm.weight\n",
      "- model.layers.21.self_attn.q_proj.weight\n",
      "- model.layers.21.self_attn.k_proj.weight\n",
      "- model.layers.21.self_attn.v_proj.weight\n",
      "- model.layers.21.self_attn.o_proj.weight\n",
      "- model.layers.21.mlp.gate_proj.weight\n",
      "- model.layers.21.mlp.up_proj.weight\n",
      "- model.layers.21.mlp.down_proj.weight\n",
      "- model.layers.21.input_layernorm.weight\n",
      "- model.layers.21.post_attention_layernorm.weight\n",
      "- model.layers.22.self_attn.q_proj.weight\n",
      "- model.layers.22.self_attn.k_proj.weight\n",
      "- model.layers.22.self_attn.v_proj.weight\n",
      "- model.layers.22.self_attn.o_proj.weight\n",
      "- model.layers.22.mlp.gate_proj.weight\n",
      "- model.layers.22.mlp.up_proj.weight\n",
      "- model.layers.22.mlp.down_proj.weight\n",
      "- model.layers.22.input_layernorm.weight\n",
      "- model.layers.22.post_attention_layernorm.weight\n",
      "- model.layers.23.self_attn.q_proj.weight\n",
      "- model.layers.23.self_attn.k_proj.weight\n",
      "- model.layers.23.self_attn.v_proj.weight\n",
      "- model.layers.23.self_attn.o_proj.weight\n",
      "- model.layers.23.mlp.gate_proj.weight\n",
      "- model.layers.23.mlp.up_proj.weight\n",
      "- model.layers.23.mlp.down_proj.weight\n",
      "- model.layers.23.input_layernorm.weight\n",
      "- model.layers.23.post_attention_layernorm.weight\n",
      "- model.layers.24.self_attn.q_proj.weight\n",
      "- model.layers.24.self_attn.k_proj.weight\n",
      "- model.layers.24.self_attn.v_proj.weight\n",
      "- model.layers.24.self_attn.o_proj.weight\n",
      "- model.layers.24.mlp.gate_proj.weight\n",
      "- model.layers.24.mlp.up_proj.weight\n",
      "- model.layers.24.mlp.down_proj.weight\n",
      "- model.layers.24.input_layernorm.weight\n",
      "- model.layers.24.post_attention_layernorm.weight\n",
      "- model.layers.25.self_attn.q_proj.weight\n",
      "- model.layers.25.self_attn.k_proj.weight\n",
      "- model.layers.25.self_attn.v_proj.weight\n",
      "- model.layers.25.self_attn.o_proj.weight\n",
      "- model.layers.25.mlp.gate_proj.weight\n",
      "- model.layers.25.mlp.up_proj.weight\n",
      "- model.layers.25.mlp.down_proj.weight\n",
      "- model.layers.25.input_layernorm.weight\n",
      "- model.layers.25.post_attention_layernorm.weight\n",
      "- model.layers.26.self_attn.q_proj.weight\n",
      "- model.layers.26.self_attn.k_proj.weight\n",
      "- model.layers.26.self_attn.v_proj.weight\n",
      "- model.layers.26.self_attn.o_proj.weight\n",
      "- model.layers.26.mlp.gate_proj.weight\n",
      "- model.layers.26.mlp.up_proj.weight\n",
      "- model.layers.26.mlp.down_proj.weight\n",
      "- model.layers.26.input_layernorm.weight\n",
      "- model.layers.26.post_attention_layernorm.weight\n",
      "- model.layers.27.self_attn.q_proj.weight\n",
      "- model.layers.27.self_attn.k_proj.weight\n",
      "- model.layers.27.self_attn.v_proj.weight\n",
      "- model.layers.27.self_attn.o_proj.weight\n",
      "- model.layers.27.mlp.gate_proj.weight\n",
      "- model.layers.27.mlp.up_proj.weight\n",
      "- model.layers.27.mlp.down_proj.weight\n",
      "- model.layers.27.input_layernorm.weight\n",
      "- model.layers.27.post_attention_layernorm.weight\n",
      "- model.layers.28.self_attn.q_proj.weight\n",
      "- model.layers.28.self_attn.k_proj.weight\n",
      "- model.layers.28.self_attn.v_proj.weight\n",
      "- model.layers.28.self_attn.o_proj.weight\n",
      "- model.layers.28.mlp.gate_proj.weight\n",
      "- model.layers.28.mlp.up_proj.weight\n",
      "- model.layers.28.mlp.down_proj.weight\n",
      "- model.layers.28.input_layernorm.weight\n",
      "- model.layers.28.post_attention_layernorm.weight\n",
      "- model.layers.29.self_attn.q_proj.weight\n",
      "- model.layers.29.self_attn.k_proj.weight\n",
      "- model.layers.29.self_attn.v_proj.weight\n",
      "- model.layers.29.self_attn.o_proj.weight\n",
      "- model.layers.29.mlp.gate_proj.weight\n",
      "- model.layers.29.mlp.up_proj.weight\n",
      "- model.layers.29.mlp.down_proj.weight\n",
      "- model.layers.29.input_layernorm.weight\n",
      "- model.layers.29.post_attention_layernorm.weight\n",
      "- model.layers.30.self_attn.q_proj.weight\n",
      "- model.layers.30.self_attn.k_proj.weight\n",
      "- model.layers.30.self_attn.v_proj.weight\n",
      "- model.layers.30.self_attn.o_proj.weight\n",
      "- model.layers.30.mlp.gate_proj.weight\n",
      "- model.layers.30.mlp.up_proj.weight\n",
      "- model.layers.30.mlp.down_proj.weight\n",
      "- model.layers.30.input_layernorm.weight\n",
      "- model.layers.30.post_attention_layernorm.weight\n",
      "- model.layers.31.self_attn.q_proj.weight\n",
      "- model.layers.31.self_attn.k_proj.weight\n",
      "- model.layers.31.self_attn.v_proj.weight\n",
      "- model.layers.31.self_attn.o_proj.weight\n",
      "- model.layers.31.mlp.gate_proj.weight\n",
      "- model.layers.31.mlp.up_proj.weight\n",
      "- model.layers.31.mlp.down_proj.weight\n",
      "- model.layers.31.input_layernorm.weight\n",
      "- model.layers.31.post_attention_layernorm.weight\n",
      "- model.norm.weight\n",
      "- lm_head.weight\n",
      "[NeMo W 2024-12-19 05:12:41 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "nemo_config: {'mcore_gpt': True, 'micro_batch_size': 1, 'global_batch_size': 8, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'encoder_seq_length': 131072, 'max_position_embeddings': 131072, 'num_layers': 32, 'hidden_size': 4096, 'ffn_hidden_size': 14336, 'num_attention_heads': 32, 'init_method_std': 0.02, 'use_scaled_init_method': True, 'hidden_dropout': 0.0, 'attention_dropout': 0.0, 'ffn_dropout': 0.0, 'kv_channels': None, 'apply_query_key_layer_scaling': True, 'normalization': 'rmsnorm', 'layernorm_epsilon': 1e-05, 'do_layer_norm_weight_decay': False, 'make_vocab_size_divisible_by': 1, 'pre_process': True, 'post_process': True, 'persist_layer_norm': True, 'bias': False, 'activation': 'fast-swiglu', 'headscale': False, 'transformer_block_type': 'pre_ln', 'openai_gelu': False, 'normalize_attention_scores': True, 'position_embedding_type': 'rope', 'rotary_percentage': 1.0, 'attention_type': 'multihead', 'share_embeddings_and_output_weights': False, 'overlap_p2p_comm': False, 'batch_p2p_comm': True, 'num_query_groups': 8, 'tokenizer': {'library': 'huggingface', 'type': '/work/Models/llama3-new-token', 'use_fast': True}, 'native_amp_init_scale': 4294967296, 'native_amp_growth_interval': 1000, 'hysteresis': 2, 'fp32_residual_connection': False, 'fp16_lm_cross_entropy': False, 'megatron_amp_O2': False, 'grad_allreduce_chunk_size_mb': 125, 'grad_div_ar_fusion': True, 'gradient_accumulation_fusion': False, 'bias_activation_fusion': False, 'bias_dropout_add_fusion': False, 'masked_softmax_fusion': True, 'get_attention_mask_from_fusion': True, 'apply_rope_fusion': False, 'seed': 1234, 'resume_from_checkpoint': None, 'use_cpu_initialization': True, 'onnx_safe': False, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'sync_batch_comm': False, 'activations_checkpoint_granularity': None, 'activations_checkpoint_method': None, 'activations_checkpoint_num_layers': None, 'num_micro_batches_with_partial_activation_checkpoints': None, 'activations_checkpoint_layers_per_pipeline': None, 'sequence_parallel': False, 'transformer_engine': True, 'fp8': False, 'fp8_e4m3': False, 'fp8_hybrid': True, 'fp8_margin': 0, 'fp8_interval': 1, 'fp8_amax_history_len': 1024, 'fp8_amax_compute_algo': 'max', 'reduce_amax': True, 'use_emha': False, 'data': {'index_mapping_dir': None, 'data_impl': 'mmap', 'splits_string': '900,50,50', 'seq_length': '${model.encoder_seq_length}', 'skip_warmup': True, 'num_workers': 2, 'dataloader_type': 'single', 'reset_position_ids': False, 'reset_attention_mask': False, 'eod_mask_loss': False, 'validation_drop_last': True, 'no_seqlen_plus_one_input_tokens': False, 'pad_samples_to_global_batch_size': False, 'shuffle_documents': True}, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'optim': {'name': 'fused_adam', 'lr': 0.0002, 'weight_decay': 0.01, 'betas': [0.9, 0.98], 'sched': {'name': 'CosineAnnealing', 'warmup_steps': 500, 'constant_steps': 50000, 'min_lr': 2e-05}}, 'rotary_base': 500000.0, 'seq_len_interpolation_factor': 8.0, 'scale_positional_embedding': True, 'precision': 'bf16'}\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-12-19 05:12:42 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "    \n",
      "converting layer 0\n",
      "done layer 0\n",
      "converting layer 1\n",
      "done layer 1\n",
      "converting layer 2\n",
      "done layer 2\n",
      "converting layer 3\n",
      "done layer 3\n",
      "converting layer 4\n",
      "done layer 4\n",
      "converting layer 5\n",
      "done layer 5\n",
      "converting layer 6\n",
      "done layer 6\n",
      "converting layer 7\n",
      "done layer 7\n",
      "converting layer 8\n",
      "done layer 8\n",
      "converting layer 9\n",
      "done layer 9\n",
      "converting layer 10\n",
      "done layer 10\n",
      "converting layer 11\n",
      "done layer 11\n",
      "converting layer 12\n",
      "done layer 12\n",
      "converting layer 13\n",
      "done layer 13\n",
      "converting layer 14\n",
      "done layer 14\n",
      "converting layer 15\n",
      "done layer 15\n",
      "converting layer 16\n",
      "done layer 16\n",
      "converting layer 17\n",
      "done layer 17\n",
      "converting layer 18\n",
      "done layer 18\n",
      "converting layer 19\n",
      "done layer 19\n",
      "converting layer 20\n",
      "done layer 20\n",
      "converting layer 21\n",
      "done layer 21\n",
      "converting layer 22\n",
      "done layer 22\n",
      "converting layer 23\n",
      "done layer 23\n",
      "converting layer 24\n",
      "done layer 24\n",
      "converting layer 25\n",
      "done layer 25\n",
      "converting layer 26\n",
      "done layer 26\n",
      "converting layer 27\n",
      "done layer 27\n",
      "converting layer 28\n",
      "done layer 28\n",
      "converting layer 29\n",
      "done layer 29\n",
      "converting layer 30\n",
      "done layer 30\n",
      "converting layer 31\n",
      "done layer 31\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:339] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:348] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:361] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:402] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-12-19 05:12:48 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/Models/llama3-new-token\n",
      "[NeMo I 2024-12-19 05:12:48 megatron_base_model:604] Padded vocab_size: 128263, original vocab_size: 128263, dummy tokens: 0.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:516] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: cp_comm_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-19 05:12:48 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2024-12-19 05:13:37 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n",
      "[NeMo W 2024-12-19 05:13:45 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo I 2024-12-19 05:14:35 convert_llama_hf_to_nemo:335] NeMo model saved to: /work/Models/llama3-new-token/model.nemo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nemo_ckpt_path = os.path.join(new_hf_model_path, \"model.nemo\")\n",
    "precision = \"bf16\"\n",
    "\n",
    "# Convert HF Model to NeMo\n",
    "!python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py --input_name_or_path $new_hf_model_path --output_path $nemo_ckpt_path --precision $precision --llama31 True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Convert Jsonl data to MMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-19 05:14:50 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "Searching folder for .json or .jsonl or json.gz or .jsonl.gz files...\n",
      "Found 4 .json or .jsonl or json.gz or .jsonl.gz files.\n",
      "[NeMo I 2024-12-19 05:14:51 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/Models/llama3-new-token\n",
      "Vocab size: 128263\n",
      "Output prefix: /work/Data/mmap/da_mmap\n",
      "Time to startup: 0.4700319766998291\n",
      "[NeMo I 2024-12-19 05:14:51 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/Models/llama3-new-token\n",
      "[NeMo I 2024-12-19 05:14:51 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/Models/llama3-new-token\n",
      "[NeMo I 2024-12-19 05:14:51 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/Models/llama3-new-token\n",
      "Processing file /work/Data/dapt/dapt_data2.jsonl 1/4\n",
      "[NeMo I 2024-12-19 05:14:51 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: /work/Models/llama3-new-token\n",
      "Processed 100 documents (167.433208213293 docs/s, 0.04694177107862842 MB/s).\n",
      "Processed 200 documents (330.0929017462412 docs/s, 0.09383591857196767 MB/s).\n",
      "Processed 300 documents (488.795157930148 docs/s, 0.13773530448037521 MB/s).\n",
      "Processed 400 documents (647.4787170692982 docs/s, 0.18232136096521984 MB/s).\n",
      "Processed 500 documents (798.9150476190476 docs/s, 0.22493561904761905 MB/s).\n",
      "Processed 600 documents (954.5837104381344 docs/s, 0.26743096611405165 MB/s).\n",
      "Processed 700 documents (1107.9452460073012 docs/s, 0.3100768992159133 MB/s).\n",
      "Processed 800 documents (1258.5638363910582 docs/s, 0.35327852896133627 MB/s).\n",
      "Processed 900 documents (1407.7611820070535 docs/s, 0.3947054473418155 MB/s).\n",
      "Processed 1000 documents (1553.5391382742223 docs/s, 0.4359965301621801 MB/s).\n",
      "Processing file /work/Data/dapt/dapt_data3.jsonl 2/4\n",
      "Processed 100 documents (153.6760689037359 docs/s, 0.4729465085561762 MB/s).\n",
      "Processed 200 documents (305.50465798923454 docs/s, 0.512871200588531 MB/s).\n",
      "Processed 300 documents (455.79741197412045 docs/s, 0.5512394422012021 MB/s).\n",
      "Processed 400 documents (603.7591141189718 docs/s, 0.5910062390397558 MB/s).\n",
      "Processed 500 documents (749.7840549615876 docs/s, 0.6295941949397356 MB/s).\n",
      "Processed 600 documents (895.023207958315 docs/s, 0.6672710350240828 MB/s).\n",
      "Processed 700 documents (1037.5498099482782 docs/s, 0.7049583073593624 MB/s).\n",
      "Processed 800 documents (1178.0760048310535 docs/s, 0.7423447462292504 MB/s).\n",
      "Processed 900 documents (1317.817913707441 docs/s, 0.7794216627893013 MB/s).\n",
      "Processed 1000 documents (1454.7200057712892 docs/s, 0.8156745991832793 MB/s).\n",
      "Processing file /work/Data/dapt/dapt_data0.jsonl 3/4\n",
      "Processed 100 documents (143.92149058092852 docs/s, 0.8476711388669663 MB/s).\n",
      "Processed 200 documents (286.5560697796124 docs/s, 0.8836819717782542 MB/s).\n",
      "Processed 300 documents (426.84150680210263 docs/s, 0.9172933249025073 MB/s).\n",
      "Processed 400 documents (566.3879719473273 docs/s, 0.9531791521135718 MB/s).\n",
      "Processed 500 documents (703.8273079969392 docs/s, 0.9881663556671276 MB/s).\n",
      "Processed 600 documents (840.4015361496076 docs/s, 1.0230716313240942 MB/s).\n",
      "Processed 700 documents (974.3412854412609 docs/s, 1.0560685168672144 MB/s).\n",
      "Processed 800 documents (1108.3803961168537 docs/s, 1.089236454853586 MB/s).\n",
      "Processed 900 documents (1240.915487702691 docs/s, 1.1230835432002622 MB/s).\n",
      "Processed 1000 documents (1371.1258042603158 docs/s, 1.1556556311134736 MB/s).\n",
      "Processing file /work/Data/dapt/dapt_data1.jsonl 4/4\n",
      "Processed 100 documents (135.84051135031356 docs/s, 1.182668602971563 MB/s).\n",
      "Processed 200 documents (270.24858619460997 docs/s, 1.2148423631957639 MB/s).\n",
      "Processed 300 documents (403.37357693852607 docs/s, 1.2462564342911293 MB/s).\n",
      "Processed 400 documents (535.0432044137227 docs/s, 1.2782880878917617 MB/s).\n",
      "Processed 500 documents (665.6687120926447 docs/s, 1.3105365465947023 MB/s).\n",
      "Processed 600 documents (794.9453680509708 docs/s, 1.3409696973525853 MB/s).\n",
      "Processed 700 documents (923.3487978906478 docs/s, 1.371594694415651 MB/s).\n",
      "Processed 800 documents (1049.695565927034 docs/s, 1.4004411580226035 MB/s).\n",
      "Processed 900 documents (1175.6943603089735 docs/s, 1.4302964312821789 MB/s).\n",
      "Processed 1000 documents (1300.483165482705 docs/s, 1.4596651545072243 MB/s).\n"
     ]
    }
   ],
   "source": [
    "# To train a real model, you also need to transform general purpose data.\n",
    "\n",
    "domain_data_folder = f\"{DATA_ROOT_DIR}/dapt\"\n",
    "if not os.path.exists(f\"{DATA_ROOT_DIR}/mmap\"):\n",
    "    os.mkdir(f\"{DATA_ROOT_DIR}/mmap\")\n",
    "output_folder = f\"{DATA_ROOT_DIR}/mmap/da_mmap\"\n",
    "\n",
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=$domain_data_folder \\\n",
    "--json-keys=text \\\n",
    "--tokenizer-library=huggingface \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-type $new_hf_model_path \\\n",
    "--output-prefix=$output_folder \\\n",
    "--append-eod \\\n",
    "--workers=4 --preproc-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Domain adaptive continued pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to train the model, please blend the domain-specific data with general-purpose data and use them together for training.\n",
    "Additionally, make sure to adjust the hyperparameters as needed.\n",
    "\"\"\"\n",
    "\n",
    "data_prefix = output_folder + \"_text_document\"\n",
    "output_dir = \"/work/log/megatron_llama_dapt\"\n",
    "max_steps=10 # 23200\n",
    "global_batch_size=64 # 256\n",
    "\n",
    "TP=4\n",
    "PP=2\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py  \\\n",
    "    --config-path=/opt/NeMo/examples/nlp/language_modeling/conf \\\n",
    "    --config-name=megatron_llama_config \\\n",
    "    restore_from_path=$nemo_ckpt_path \\\n",
    "    trainer.devices=8 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.max_steps=$max_steps \\\n",
    "    trainer.val_check_interval=10 \\\n",
    "    trainer.log_every_n_steps=5 \\\n",
    "    trainer.limit_val_batches=8 \\\n",
    "    trainer.limit_test_batches=8 \\\n",
    "    trainer.accumulate_grad_batches=1 \\\n",
    "    trainer.precision=bf16 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=$global_batch_size \\\n",
    "    model.tensor_model_parallel_size=$TP \\\n",
    "    model.pipeline_model_parallel_size=$PP \\\n",
    "    model.tokenizer.library=huggingface \\\n",
    "    model.tokenizer.type=$new_hf_model_path \\\n",
    "    model.tokenizer.model=null \\\n",
    "    model.megatron_amp_O2=true \\\n",
    "    model.encoder_seq_length=4096 \\\n",
    "    model.sequence_parallel=true \\\n",
    "    ++model.data.data_prefix=[1.0,$data_prefix] \\\n",
    "    model.data.num_workers=8 \\\n",
    "    model.optim.name=fused_adam \\\n",
    "    model.optim.lr=5e-6 \\\n",
    "    model.optim.betas=[0.9,0.95] \\\n",
    "    exp_manager.explicit_log_dir=$output_dir \\\n",
    "    exp_manager.resume_if_exists=true \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=true \\\n",
    "    exp_manager.create_checkpoint_callback=true \\\n",
    "    exp_manager.create_wandb_logger=true \\\n",
    "    exp_manager.wandb_logger_kwargs.project=DAPT \\\n",
    "    exp_manager.wandb_logger_kwargs.name=step1 \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=val_loss \\\n",
    "    exp_manager.checkpoint_callback_params.save_top_k=1 \\\n",
    "    exp_manager.checkpoint_callback_params.mode=min \\\n",
    "    exp_manager.checkpoint_callback_params.always_save_nemo=false \\\n",
    "    exp_manager.checkpoint_callback_params.save_nemo_on_train_end=true \\\n",
    "    ~model.optim.sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
