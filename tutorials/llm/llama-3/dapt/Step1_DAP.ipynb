{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. DAPT (Domain Adaptive Pre-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, you need to create [step 1 dummy data](./Step0_Dummy_Data.ipynb) or prepare real data ([see here](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/dapt-curation)) for the actual DAPT.\n",
    "If you plan to train the real model, make sure to prepare not only the domain-specific data but also general-purpose data to be used in the continued pretraining.\n",
    "\n",
    "We use huggingface \"meta-llama/Llama-3.1-8B\" model for practice.\n",
    "\n",
    "In this step, you will perform domain-adaptive tokenization and domain-adaptive continued pretraining (DAPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Domain-adaptive tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "MODEL_ROOT_DIR = \"/work/Models\" # change to your path\n",
    "DATA_ROOT_DIR = \"/work/Data\"\n",
    "\n",
    "all_files = glob.glob(f\"{DATA_ROOT_DIR}/dapt/*.jsonl\") # DAPT Data Path \n",
    "\n",
    "all_texts = \"\"\n",
    "for data_file in all_files:\n",
    "    with jsonlines.open(data_file) as reader:\n",
    "        for obj in reader:\n",
    "            all_texts+=obj[\"text\"]+\"\\n\"\n",
    "                \n",
    "# Write the text data into a file\n",
    "all_text_file = f\"{DATA_ROOT_DIR}/all_dapt_text.txt\"\n",
    "with open(all_text_file, 'w') as data_fp:\n",
    "  data_fp.write(all_texts)\n",
    "  \n",
    "print(f\"Save all dapt text data to {all_text_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_spe_type = \"bpe\"\n",
    "vocab_size = 100 # target vocab size for domain specific data\n",
    "\n",
    "!python /opt/NeMo/scripts/tokenizers/process_asr_text_tokenizer.py --data_file $all_text_file --data_root=$DATA_ROOT_DIR --vocab_size=$vocab_size --tokenizer=spe --spe_type=$tokenizer_spe_type  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer_dir = DATA_ROOT_DIR + f\"/tokenizer_spe_{tokenizer_spe_type}_v{vocab_size}\"\n",
    "\n",
    "! ls $custom_tokenizer_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Add domain specific token to original tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import wget\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.collections import common as nemo_common\n",
    "from omegaconf import OmegaConf\n",
    "import huggingface_hub as hf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "HF_LLM_MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "domain_tokenizer = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"sentencepiece\", tokenizer_model=custom_tokenizer_dir+\"/tokenizer.model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_LLM_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Domain-Only Token\n",
    "\n",
    "general_vocab = set(tokenizer.vocab.keys())\n",
    "domain_vocab = set(domain_tokenizer.vocab)\n",
    "domain_only_vocab = domain_vocab - general_vocab\n",
    "domain_only_vocab = list(domain_only_vocab)\n",
    "print(\"Domain Only Vocab: \", domain_only_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ori Vocab: \", len(tokenizer))\n",
    "tokenizer.add_tokens(domain_only_vocab)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"New Vocab: \", len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Reinitialize embedding matrix on LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_mean(tokens, tokenizer):\n",
    "    # get embedding initialize values\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    embedding_values = []\n",
    "    with torch.no_grad():\n",
    "        for token in tokens:\n",
    "            split_token = tokenizer.tokenize(token, add_special_tokens=False)\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(split_token)\n",
    "            embeddings = embedding_layer.weight[token_ids]\n",
    "            avg_embedding = embeddings.mean(dim=0)\n",
    "            embedding_values.append(avg_embedding)\n",
    "            \n",
    "    return embedding_values\n",
    "\n",
    "embedding_values = get_embedding_mean(domain_only_vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_embedding_value(tokens, new_tokenizer, mean_emb_values):\n",
    "    new_embedding_layer = model.get_input_embeddings()\n",
    "    output_embedding_layers = model.get_output_embeddings()\n",
    "    with torch.no_grad():\n",
    "        for i, token in enumerate(tokens):\n",
    "            token_id = new_tokenizer.convert_tokens_to_ids(token)\n",
    "            new_embedding_layer.weight[token_id] = mean_emb_values[i]\n",
    "            output_embedding_layers.weight[token_id] = torch.zeros_like(mean_emb_values[i])\n",
    "            \n",
    "\n",
    "set_embedding_value(domain_only_vocab, tokenizer, embedding_values)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Init is Okay\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "output_embedding_layer = model.get_output_embeddings()\n",
    "\n",
    "for i, token in enumerate(domain_only_vocab):\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    ori_value = embedding_values[i].data.numpy()\n",
    "    init_value = embedding_layer.weight[token_id].data.numpy()\n",
    "    out_value = output_embedding_layer.weight[token_id].data.numpy()\n",
    "    print(f\"Embedding for {token}: {init_value}\", \"Is Same: \", ori_value==init_value)\n",
    "    print(f\"Output Embedding for {token}: {out_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Converted Model\n",
    "new_hf_model_path = f\"{MODEL_ROOT_DIR}/llama3-new-token\"\n",
    "\n",
    "tokenizer.save_pretrained(new_hf_model_path)\n",
    "model.save_pretrained(new_hf_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Convert HF model to .nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nemo_ckpt_path = os.path.join(new_hf_model_path, \"model.nemo\")\n",
    "precision = \"bf16\"\n",
    "\n",
    "# Convert HF Model to NeMo\n",
    "!python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py --input_name_or_path $new_hf_model_path --output_path $nemo_ckpt_path --precision $precision --llama31 True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Convert Jsonl data to MMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a real model, you also need to transform general purpose data.\n",
    "\n",
    "domain_data_folder = f\"{DATA_ROOT_DIR}/dapt\"\n",
    "if not os.path.exists(f\"{DATA_ROOT_DIR}/mmap\"):\n",
    "    os.mkdir(f\"{DATA_ROOT_DIR}/mmap\")\n",
    "output_folder = f\"{DATA_ROOT_DIR}/mmap/da_mmap\"\n",
    "\n",
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=$domain_data_folder \\\n",
    "--json-keys=text \\\n",
    "--tokenizer-library=huggingface \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-type $new_hf_model_path \\\n",
    "--output-prefix=$output_folder \\\n",
    "--append-eod \\\n",
    "--workers=4 --preproc-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Domain adaptive continued pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to train the model, please blend the domain-specific data with general-purpose data and use them together for training.\n",
    "Additionally, make sure to adjust the hyperparameters as needed.\n",
    "\"\"\"\n",
    "\n",
    "data_prefix = output_folder + \"_text_document\"\n",
    "output_dir = \"/work/log/megatron_llama_dapt\"\n",
    "max_steps=10 # 23200\n",
    "global_batch_size=64 # 256\n",
    "\n",
    "TP=4\n",
    "PP=2\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py  \\\n",
    "    --config-path=/opt/NeMo/examples/nlp/language_modeling/conf \\\n",
    "    --config-name=megatron_llama_config \\\n",
    "    restore_from_path=$nemo_ckpt_path \\\n",
    "    trainer.devices=8 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.max_steps=$max_steps \\\n",
    "    trainer.val_check_interval=10 \\\n",
    "    trainer.log_every_n_steps=5 \\\n",
    "    trainer.limit_val_batches=8 \\\n",
    "    trainer.limit_test_batches=8 \\\n",
    "    trainer.accumulate_grad_batches=1 \\\n",
    "    trainer.precision=bf16 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=$global_batch_size \\\n",
    "    model.tensor_model_parallel_size=$TP \\\n",
    "    model.pipeline_model_parallel_size=$PP \\\n",
    "    model.tokenizer.library=huggingface \\\n",
    "    model.tokenizer.type=$new_hf_model_path \\\n",
    "    model.tokenizer.model=null \\\n",
    "    model.megatron_amp_O2=true \\\n",
    "    model.encoder_seq_length=4096 \\\n",
    "    model.sequence_parallel=true \\\n",
    "    ++model.data.data_prefix=[1.0,$data_prefix] \\\n",
    "    model.data.num_workers=8 \\\n",
    "    model.optim.name=fused_adam \\\n",
    "    model.optim.lr=5e-6 \\\n",
    "    model.optim.betas=[0.9,0.95] \\\n",
    "    exp_manager.explicit_log_dir=$output_dir \\\n",
    "    exp_manager.resume_if_exists=true \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=true \\\n",
    "    exp_manager.create_checkpoint_callback=true \\\n",
    "    exp_manager.create_wandb_logger=true \\\n",
    "    exp_manager.wandb_logger_kwargs.project=DAPT \\\n",
    "    exp_manager.wandb_logger_kwargs.name=step1 \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=val_loss \\\n",
    "    exp_manager.checkpoint_callback_params.save_top_k=1 \\\n",
    "    exp_manager.checkpoint_callback_params.mode=min \\\n",
    "    exp_manager.checkpoint_callback_params.always_save_nemo=false \\\n",
    "    exp_manager.checkpoint_callback_params.save_nemo_on_train_end=true \\\n",
    "    ~model.optim.sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
