{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2. Model Alignment (SFT, SteerLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = \"/work/Data\"\n",
    "MODEL_ROOT_DIR = \"/work/Models\"\n",
    "\n",
    "MODEL=f\"{MODEL_ROOT_DIR}/llama-new-token/model.nemo\" # step1 Pretrained model\n",
    "\n",
    "TRAIN_DS=f\"[{DATA_ROOT_DIR}/sft/sft_train_data.jsonl]\" # Blend with Domain Instruct Data & General Chat Data\n",
    "VALID_DS=f\"[{DATA_ROOT_DIR}/sft/sft_val_data.jsonl]\"\n",
    "TEST_DS=f\"[{DATA_ROOT_DIR}/sft/sft_test_data.jsonl]\"\n",
    "\n",
    "VALID_NAMES=\"[dummy-data]\"\n",
    "LOG_ROOT_DIR = \"/work/log\"\n",
    "\n",
    "NUM_DEVICES=8\n",
    "CONCAT_SAMPLING_PROBS=\"[1]\"\n",
    "TP_SIZE=8\n",
    "PP_SIZE=1\n",
    "SEQUENCE_PARALLEL=True\n",
    "\n",
    "\n",
    "model_save_dir = LOG_ROOT_DIR + \"/sft\"\n",
    "batch_size = 64 # 128\n",
    "\n",
    "!torchrun --nproc_per_node=$NUM_DEVICES \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    --config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf \\\n",
    "    --config-name=megatron_gpt_sft_config \\\n",
    "   trainer.precision=bf16 \\\n",
    "   trainer.devices=$NUM_DEVICES \\\n",
    "   trainer.num_nodes=1 \\\n",
    "   trainer.val_check_interval=0.1 \\\n",
    "   trainer.max_epochs=1 \\\n",
    "   trainer.max_steps=-1 \\\n",
    "   model.restore_from_path=$MODEL \\\n",
    "   model.micro_batch_size=1 \\\n",
    "   model.global_batch_size=$batch_size \\\n",
    "   model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "   model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "   model.megatron_amp_O2=True \\\n",
    "   model.sequence_parallel=$SEQUENCE_PARALLEL \\\n",
    "   model.activations_checkpoint_granularity=selective \\\n",
    "   model.activations_checkpoint_method=uniform \\\n",
    "   model.optim.name=distributed_fused_adam \\\n",
    "   model.optim.lr=5e-6 \\\n",
    "   model.answer_only_loss=True \\\n",
    "   model.peft.peft_scheme=none \\\n",
    "   model.data.train_ds.file_names=$TRAIN_DS \\\n",
    "   model.data.validation_ds.file_names=$VALID_DS \\\n",
    "   model.data.test_ds.file_names=$TEST_DS \\\n",
    "   model.data.train_ds.concat_sampling_probabilities=$CONCAT_SAMPLING_PROBS \\\n",
    "   model.data.train_ds.max_seq_length=4096 \\\n",
    "   model.data.validation_ds.max_seq_length=2048 \\\n",
    "   model.data.train_ds.micro_batch_size=1 \\\n",
    "   model.data.train_ds.global_batch_size=$batch_size \\\n",
    "   model.data.validation_ds.micro_batch_size=1 \\\n",
    "   model.data.validation_ds.global_batch_size=$batch_size \\\n",
    "   model.data.test_ds.micro_batch_size=1 \\\n",
    "   model.data.test_ds.global_batch_size=$batch_size \\\n",
    "   model.data.train_ds.num_workers=0 \\\n",
    "   model.data.validation_ds.num_workers=0 \\\n",
    "   model.data.test_ds.num_workers=0 \\\n",
    "   model.data.validation_ds.metric.name=loss \\\n",
    "   model.data.test_ds.metric.name=loss \\\n",
    "   exp_manager.create_wandb_logger=True \\\n",
    "   exp_manager.wandb_logger_kwargs.project=DAPT \\\n",
    "   exp_manager.wandb_logger_kwargs.name=step1_sft \\\n",
    "   exp_manager.explicit_log_dir=$model_save_dir \\\n",
    "   exp_manager.resume_if_exists=False \\\n",
    "   exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "   exp_manager.create_checkpoint_callback=True \\\n",
    "   exp_manager.checkpoint_callback_params.monitor=validation_loss \\\n",
    "   exp_manager.checkpoint_callback_params.save_best_model=False \\\n",
    "   exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train Attribute Reward Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size=64\n",
    "base_model = \"\" # base model for Attribute model\n",
    " \n",
    "!python /opt/NeMo-Aligner/examples/nlp/gpt/train_reward_model.py \\\n",
    "      trainer.num_nodes=1 \\\n",
    "      trainer.devices=8 \\\n",
    "      ++model.micro_batch_size=1 \\\n",
    "      ++model.global_batch_size=$global_batch_size \\\n",
    "      ++model.data.data_impl=jsonl \\\n",
    "      pretrained_checkpoint.restore_from_path=$base_model \\\n",
    "      \"model.data.data_prefix={train: [\"/work/Data/reg/reg_train_data.jsonl\"], validation: [\"/work/Data/reg/reg_val_data.jsonl\"], test: [\"/work/Data/reg/reg_test_data.jsonl\"]}\" \\\n",
    "      exp_manager.explicit_log_dir=/work/log/attribute \\\n",
    "      trainer.rm.val_check_interval=10 \\\n",
    "      exp_manager.create_wandb_logger=True \\\n",
    "      exp_manager.wandb_logger_kwargs.project=DAPT \\\n",
    "      exp_manager.wandb_logger_kwargs.name=steerlm \\\n",
    "      exp_manager.checkpoint_callback_params.save_top_k=1 \\\n",
    "      trainer.rm.save_interval=10 \\\n",
    "      trainer.rm.max_steps=10 \\\n",
    "      ++model.tensor_model_parallel_size=1 \\\n",
    "      ++model.pipeline_model_parallel_size=1 \\\n",
    "      ++model.activations_checkpoint_granularity=\"selective\" \\\n",
    "      model.optim.sched.constant_steps=0 \\\n",
    "      model.reward_model_type=\"regression\" \\\n",
    "      model.regression.num_attributes=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Making pseudo attribute labeled data using attribute model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "attribute_model = \"/work/log/attribute/checkpoints/megatron_gpt.nemo\"\n",
    "\n",
    "# Start Reward Server\n",
    "server_command = f\"\"\"python /opt/NeMo-Aligner/examples/nlp/gpt/serve_reward_model.py \\\n",
    "      rm_model_file={attribute_model} \\\n",
    "      trainer.num_nodes=1 \\\n",
    "      trainer.devices=8 \\\n",
    "      model.precision=bf16 \\\n",
    "      ++model.tensor_model_parallel_size=1 \\\n",
    "      ++model.pipeline_model_parallel_size=1 \\\n",
    "      ++model.reward_model_type=regression \\\n",
    "      model.forward_micro_batch_size=2 \\\n",
    "      inference.port=1424 &\"\"\"\n",
    "      \n",
    "os.system(server_command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_TRAIN_DATA = f\"{DATA_ROOT_DIR}/chat/chat_train_data.jsonl\"\n",
    "CHAT_VAL_DATA = f\"{DATA_ROOT_DIR}/chat/chat_val_data.jsonl\"\n",
    "\n",
    "CHAT_ATT_TRAIN_DATA = f\"{DATA_ROOT_DIR}/chat/chat_label_train_data.jsonl\"\n",
    "CHAT_ATT_VAL_DATA = f\"{DATA_ROOT_DIR}/chat/chat_label_val_data.jsonl\"\n",
    "CHAT_ATT_TRAIN_DATA_2EPOCH = f\"{DATA_ROOT_DIR}/chat/chat_label_train_data_2ep.jsonl\"\n",
    "\n",
    "!python /opt/NeMo-Aligner/examples/nlp/data/steerlm/attribute_annotate.py \\\n",
    "      --input-file=$CHAT_TRAIN_DATA \\\n",
    "      --output-file=$CHAT_ATT_TRAIN_DATA \\\n",
    "      --port=1424\n",
    "\n",
    "!python /opt/NeMo-Aligner/examples/nlp/data/steerlm/attribute_annotate.py \\\n",
    "      --input-file=$CHAT_VAL_DATA \\\n",
    "      --output-file=$CHAT_ATT_VAL_DATA \\\n",
    "      --port=1424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Reward Server\n",
    "!ps aux | grep \"python /opt/NeMo-Aligner/examples/nlp/gpt/serve_reward_model.py\" | grep -v grep | awk '{print $2}' | xargs -r kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!cat $CHAT_ATT_TRAIN_DATA $CHAT_ATT_TRAIN_DATA > $CHAT_ATT_TRAIN_DATA_2EPOCH\n",
    "\n",
    "!ls $CHAT_ATT_TRAIN_DATA_2EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Attribute conditioned supervised finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_MODEL = \"/work/log/sft/checkpoints/megatron_gpt_peft_none_tuning.nemo\" # Step2-(1) sft model \n",
    "\n",
    "!python /opt/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py \\\n",
    "     trainer.num_nodes=1 \\\n",
    "     trainer.devices=8 \\\n",
    "     trainer.precision=bf16 \\\n",
    "     trainer.sft.limit_val_batches=8 \\\n",
    "     trainer.sft.max_epochs=1 \\\n",
    "     trainer.sft.max_steps=-1 \\\n",
    "     trainer.sft.val_check_interval=10 \\\n",
    "     trainer.sft.save_interval=10 \\\n",
    "     model.megatron_amp_O2=True \\\n",
    "     model.restore_from_path=$SFT_MODEL \\\n",
    "     model.tensor_model_parallel_size=8 \\\n",
    "     model.pipeline_model_parallel_size=1 \\\n",
    "     model.optim.lr=5e-6 \\\n",
    "     model.optim.name=distributed_fused_adam \\\n",
    "     model.optim.weight_decay=0.01 \\\n",
    "     ~model.optim.sched \\\n",
    "     model.answer_only_loss=True \\\n",
    "     model.activations_checkpoint_granularity=selective \\\n",
    "     model.activations_checkpoint_method=uniform \\\n",
    "     model.data.chat=True \\\n",
    "     model.data.num_workers=0 \\\n",
    "     model.data.chat_prompt_tokens.system_turn_start=\\'\\<extra_id_0\\>\\' \\\n",
    "     model.data.chat_prompt_tokens.turn_start=\\'\\<extra_id_1\\>\\' \\\n",
    "     model.data.chat_prompt_tokens.label_start=\\'\\<extra_id_2\\>\\' \\\n",
    "     model.data.train_ds.max_seq_length=4096 \\\n",
    "     model.data.train_ds.micro_batch_size=1 \\\n",
    "     model.data.train_ds.global_batch_size=128 \\\n",
    "     model.data.train_ds.file_path=$CHAT_ATT_TRAIN_DATA_2EPOCH \\\n",
    "     model.data.train_ds.index_mapping_dir=/work/log/indexdir \\\n",
    "     model.data.train_ds.add_eos=False \\\n",
    "     model.data.train_ds.hf_dataset=True \\\n",
    "     model.data.validation_ds.max_seq_length=4096 \\\n",
    "     model.data.validation_ds.file_path=$CHAT_ATT_VAL_DATA \\\n",
    "     model.data.validation_ds.micro_batch_size=1 \\\n",
    "     model.data.validation_ds.global_batch_size=128 \\\n",
    "     model.data.validation_ds.index_mapping_dir=/work/log/indexdir  \\\n",
    "     model.data.validation_ds.add_eos=False \\\n",
    "     model.data.validation_ds.hf_dataset=True \\\n",
    "     exp_manager.create_wandb_logger=True \\\n",
    "     exp_manager.wandb_logger_kwargs.project=DAPT \\\n",
    "     exp_manager.wandb_logger_kwargs.name=step2_steerlm \\\n",
    "     exp_manager.explicit_log_dir=/work/log/steerlm_sft \\\n",
    "     exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (Optional) Inference Aligned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ALIGN_MODEL = \"/work/log/steerlm_sft/checkpoints/megatron_gpt_sft.nemo\"\n",
    "\n",
    "server_command = f\"\"\"python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_eval.py \\\n",
    "        gpt_model_file={ALIGN_MODEL} \\\n",
    "        pipeline_model_parallel_split_rank=0 \\\n",
    "        server=True \\\n",
    "        tensor_model_parallel_size=8 \\\n",
    "        pipeline_model_parallel_size=1 \\\n",
    "        trainer.precision=bf16 \\\n",
    "        trainer.devices=8 \\\n",
    "        trainer.num_nodes=1 \\\n",
    "        web_server=False \\\n",
    "        port=1428 &\"\"\"\n",
    "        \n",
    "os.system(server_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_answer(question, max_tokens, values, eval_port=1428):\n",
    "   prompt = (\n",
    "       \"<extra_id_0>System\\nA chat between a curious user and an artificial intelligence assistant. \"\n",
    "       \"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\"\n",
    "       \"<extra_id_1>User\\n{question}\\n<extra_id_1>Assistant\\n<extra_id_2>{values}\\n\"\n",
    "   )\n",
    "   prompts = [prompt.format(question=question, values=values)]\n",
    "   data = {\n",
    "       \"sentences\": prompts,\n",
    "       \"tokens_to_generate\": max_tokens,\n",
    "       \"top_k\": 1,\n",
    "       \"greedy\": True,\n",
    "       \"end_strings\": [\"<extra_id_1>\"],\n",
    "   }\n",
    "   url = f\"http://localhost:{eval_port}/generate\"\n",
    "   response = requests.put(url, json=data)\n",
    "   json_response = response.json()\n",
    "   response_sentence = json_response[\"sentences\"][0][len(prompt):]\n",
    "   return response_sentence\n",
    "\n",
    "def encode_labels(labels):\n",
    "   return \",\".join(f\"{key}:{value}\" for key, value in labels.items())\n",
    "\n",
    "\n",
    "values = OrderedDict(\n",
    "  [\n",
    "      (\"quality\", 4),\n",
    "      (\"toxicity\", 0),\n",
    "      (\"humor\", 0),\n",
    "      (\"creativity\", 0),\n",
    "      (\"helpfulness\", 4),\n",
    "      (\"correctness\", 4),\n",
    "      (\"coherence\", 4),\n",
    "      (\"complexity\", 4),\n",
    "      (\"verbosity\", 4),\n",
    "   ]\n",
    ")\n",
    "values = encode_labels(values)\n",
    "\n",
    "question = \"Write a poem on NVIDIA in the style of Shakespeare\"\n",
    "print(get_answer(question, 512, values))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
