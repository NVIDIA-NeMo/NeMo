{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3. Domain Adapted Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index==0.10\n",
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Convert HF model to .nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_ENCODER_MODEL = \"intfloat/e5-small-unsupervised\"\n",
    "HF_LLM_MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "DATA_ROOT_DIR = \"/work/Data\"\n",
    "MODEL_ROOT_DIR = \"/work/Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_nemo_path = \"/work/Models/e5-small.nemo\"\n",
    "\n",
    "!python /opt/NeMo/scripts/checkpoint_converters/convert_bert_hf_to_nemo.py \\\n",
    "       --input_name_or_path $HF_ENCODER_MODEL \\\n",
    "       --output_path $embed_nemo_path \\\n",
    "       --mcore True \\\n",
    "       --precision bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_nemo_path = \"/work/Models/llama3.nemo\"\n",
    "precision = \"bf16\"\n",
    "\n",
    "# Convert HF Model to NeMo\n",
    "!python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py --input_name_or_path $HF_LLM_MODEL --output_path $llm_nemo_path --precision $precision --llama31 True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Auto-Generated Domain-specific Retrieval Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dir = f\"{DATA_ROOT_DIR}/index\" # save path for vectir database\n",
    "data_dir = f\"{DATA_ROOT_DIR}/docs\" # your document directory for retrieve\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/rag/rag_indexing.py \\\n",
    "        trainer.devices=1 \\\n",
    "        trainer.precision='bf16-mixed' \\\n",
    "        indexing.embedder.model_path=$embed_nemo_path \\\n",
    "        indexing.embedder.embed_batch_size=128 \\\n",
    "        indexing.data.data_path=$data_dir \\\n",
    "        indexing.data.chunk_size=256 \\\n",
    "        indexing.data.chunk_overlap=10 \\\n",
    "        indexing.index_path=$vector_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"{DATA_ROOT_DIR}/retrieval\" # save path for auto generated data\n",
    "\n",
    "!python /opt/NeMo/tutorials/llm/llama-3/dapt/code/rag_auto_generate_sample.py \\\n",
    "    --config-path=/opt/NeMo/examples/nlp/rag/conf \\\n",
    "    --config-name=rag_generating \\\n",
    "    indexing.index_path=$vector_dir \\\n",
    "    indexing.embedder.model_path=$embed_nemo_path \\\n",
    "    generating.llm.model_path=$llm_nemo_path \\\n",
    "    ++generating.top_k=4 \\\n",
    "    ++generating.num_random=4 \\\n",
    "    ++generating.output_dir=$data_dir \\\n",
    "    ++generating.num_sample=3000 \\\n",
    "    ++generating.prefix=\"train\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Prompt Engineering \n",
    "\n",
    "If the auto-generate results do not work well with your model, \n",
    "\n",
    "debug and design an appropriate prompt, then update the prompt in ./code/rag_auto_generate_sample.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "HF_LLM_MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_LLM_MODEL)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "QUERY_PROMPT_TEMPLATE = \"\"\"You will be provided with a document or a passage. Your task is to generate a single, highly relevant and natural language query that aligns perfectly with the content of the document.\n",
    "The query should:\n",
    "    1. Reflect the main idea or a key detail from the document.\n",
    "    2. Be concise, specific, and written in natural language.\n",
    "    3. Be something a user might naturally ask to retrieve this document.\n",
    "    4. **Write only the answer, do not repeat the instructions or document.**\n",
    " \n",
    " ## Given Document:\n",
    " {document}\n",
    " \n",
    " ## Predict Query:\n",
    " \"\"\"\n",
    " \n",
    "FILTER_PROMPT_TEMPLATE = \"\"\"You will be provided with a document and a query.\n",
    "Your task is to evaluate whether the content of the document is relevant to answering the query. \n",
    "\n",
    "Return \"True\" if the document contains information directly related to the query, and \"False\" if it does not.\n",
    "**Provide only the answer: \"True\" or \"False\", without repeating the instructions, document, or query.**\n",
    "\n",
    "## Given Query:\n",
    "{query}\n",
    "\n",
    "## Given Document:\n",
    "{document}\n",
    "\n",
    "## Is Relevant (True or False):\n",
    "\"\"\"\n",
    "\n",
    "# Function to send a query and get a response\n",
    "def get_response(query, max_length=512):\n",
    "    \"\"\"\n",
    "    Sends a query to the Hugging Face model and receives a response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input text query.\n",
    "        max_length (int): Maximum length of the response.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the model.\n",
    "    \"\"\"\n",
    "    # Tokenize the input query and move tensors to the same device as the model\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,  # Set to True for sampling; False for deterministic output\n",
    "        temperature=0.7,  # Controls randomness of predictions\n",
    "        top_k=50,         # Top-k sampling\n",
    "        top_p=0.9         # Top-p sampling\n",
    "    )\n",
    "    \n",
    "    # Decode the output tokens\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example query\n",
    "document = (\n",
    "    \"NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and \"\n",
    "    \"PyTorch developers working on Large Language Models (LLMs), Multimodal Models (MMs), Automatic Speech Recognition (ASR), \"\n",
    "    \"Text to Speech (TTS), and Computer Vision (CV) domains. It is designed to help you efficiently create, customize, \"\n",
    "    \"and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints.\"\n",
    ")\n",
    "\n",
    "query = QUERY_PROMPT_TEMPLATE.format(document=document)\n",
    "response = get_response(query).split(\"## Predict Query:\")[-1]\n",
    "print(f\"1 Response: {response}\")\n",
    "\n",
    "print(\"********\")\n",
    "query2 = FILTER_PROMPT_TEMPLATE.format(query=\"What is the NeMo?\", document=document)\n",
    "response2 = get_response(query2).split(\"## Is Relevant (True or False):\")[-1]\n",
    "print(f\"2 Response: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Finetuning Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = data_dir + \"/train_data.json\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/information_retrieval/megatron_bert_embedding_finetuning.py \\\n",
    "    exp_manager.exp_dir=/work/log/retrieval \\\n",
    "    restore_from_path=$embed_nemo_path \\\n",
    "    trainer.devices=8 \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.max_epochs=1 \\\n",
    "    trainer.max_steps=-1 \\\n",
    "    trainer.val_check_interval=2 \\\n",
    "    trainer.limit_val_batches=8 \\\n",
    "    trainer.limit_test_batches=8 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=64 \\\n",
    "    model.data.data_impl=jsonl \\\n",
    "    model.hidden_size=384 \\\n",
    "    model.num_layers=12 \\\n",
    "    model.ffn_hidden_size=1536 \\\n",
    "    model.data.data_train=$data_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
