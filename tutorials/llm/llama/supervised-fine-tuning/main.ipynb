{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Author: Leo Du\n",
    "\n",
    "Email: ldu@nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOAL**\n",
    "\n",
    "given a foundation model (in thie case llama-2-7B) that was pretrained on a broad, general purpose corpus, our goal is to fine tune the model on a specific task through supervised learning approach. SFT is a general purpose to improve model performance on a specific downstream task which is usually domain specific. SFT could be directly applied to a foundational model or a domain adapted pre trained model.\n",
    "\n",
    "in this case we use open source verilog code dataset containing description of the verilog code in natural language as input and the actual verilog code as output. We demonstrate that SFT model trained on this specific dataset could be used for domain specific code generation given an input prompt, which would be very useful in developing coding copilot applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Software Requirements**\n",
    "\n",
    "1. access to latest NeMo framework NGC Containers\n",
    "2. this playbook has been tested on: nvcr.io/nvidia/nemo:25.02'. it is expected to work similarly on other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal, launch the NeMo framework container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -it -p 8080:8080 -p 8088:8088 --rm --gpus all --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal, launch Jupyter Notebook as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter notebook --allow-root --ip 0.0.0.0 --port 8088 --no-browser --NotebookApp.token=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardware Requirements**\n",
    "\n",
    "This playbook has been tested on 2xA100 80G but can be scaled to multiple GPUs as well as multiple nodes by modifying the appropriate parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1**\n",
    "\n",
    "download the llama-2-7b model from hugging face and convert it to .nemo format, remove the original download once conversion is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/meta-llama/Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Llama-2-7b-hf\n",
    "!python3 ../convert.py\n",
    "!cd ..\n",
    "!rm -rf Llama-2-7b-hf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2**\n",
    "\n",
    "download the verilog dataset, preprocess the dataset to train, validation, test split then run the supervised fine tuning step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import nemo_run as run\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "from verilog_data_module import VerilogDataModule\n",
    "\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm import Llama2Config7B\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "\n",
    "\n",
    "# configure custom dataset\n",
    "def verilog() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(VerilogDataModule, seq_length=1024, micro_batch_size=2, global_batch_size=8, num_workers=8)\n",
    "\n",
    "\n",
    "# configure trainer class similar to pytorch lightning trainer\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(nl.MegatronStrategy, tensor_model_parallel_size=2)\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        devices=2,\n",
    "        max_steps=200,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "        log_every_n_steps=40,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=20,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# configure the logger\n",
    "def logger() -> run.Config[nl.NeMoLogger]:\n",
    "    ckpt = run.Config(\n",
    "        nl.ModelCheckpoint,\n",
    "        save_last=True,\n",
    "        every_n_train_steps=40,\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    ## this is where hthe\n",
    "    return run.Config(\n",
    "        nl.NeMoLogger,\n",
    "        name=\"sft_log\",\n",
    "        log_dir=\"//workspace\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# configre the optimizer, adam with cosine annealing\n",
    "def adam_with_cosine_annealing() -> run.Config[nl.OptimizerModule]:\n",
    "    opt_cfg = run.Config(\n",
    "        OptimizerConfig,\n",
    "        optimizer=\"adam\",\n",
    "        lr=5e-5,\n",
    "        adam_beta2=0.98,\n",
    "        use_distributed_optimizer=True,\n",
    "        clip_grad=1.0,\n",
    "        bf16=True,\n",
    "    )\n",
    "    return run.Config(nl.MegatronOptimizerModule, config=opt_cfg)\n",
    "\n",
    "\n",
    "# configure the base model\n",
    "def llama2_7b() -> run.Config[pl.LightningModule]:\n",
    "    return run.Config(llm.LlamaModel, config=run.Config(llm.Llama2Config7B))\n",
    "\n",
    "\n",
    "# configure auto resume\n",
    "def resume() -> run.Config[nl.AutoResume]:\n",
    "    return run.Config(\n",
    "        nl.AutoResume,\n",
    "        restore_config=run.Config(\n",
    "            nl.RestoreConfig,\n",
    "            ## default path to save converted hf model\n",
    "            path=\"/root/.cache/nemo/models/Llama-2-7b-hf\",\n",
    "        ),\n",
    "        # requires completely saved checkpoint to resume from\n",
    "        resume_if_exists=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# with all above components created, call NeMo2.0 finetune API\n",
    "def configure_finetuning_recipe():\n",
    "    return run.Partial(\n",
    "        llm.finetune,\n",
    "        model=llama2_7b(),\n",
    "        trainer=trainer(),\n",
    "        data=verilog(),\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 2) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "    return executor\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"preprocess data!\")\n",
    "    verilog = VerilogDataModule()\n",
    "    verilog_data = verilog._download_data()\n",
    "    verilog._preprocess_and_split_data(verilog_data)\n",
    "    print(\"running supervised fine tuning!\")\n",
    "    run.run(configure_finetuning_recipe(), executor=local_executor_torchrun())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step3**\n",
    "\n",
    "once the SFT step is complete, run the inference step to generate prediction on both base and SFT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import nemo_run as run\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "from run_sft import local_executor_torchrun, trainer\n",
    "\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm import Llama2Config7B\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "\n",
    "input_data = \"/workspace/data/verilog/test.jsonl\"\n",
    "base_llama_path = \"/root/.cache/nemo/models/Llama-2-7b-hf\"\n",
    "sft_ckpt_path = str(\n",
    "    next(\n",
    "        (d for d in Path(\"/workspace/sft_log/checkpoints\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None\n",
    "    )\n",
    ")\n",
    "\n",
    "os.makedirs(\"/workspace/inference\", exist_ok=True)\n",
    "output_path_base = \"/workspace/inference/base_llama_prediction.jsonl\"\n",
    "output_path_sft = \"/workspace/inference/sft_prediction.jsonl\"\n",
    "\n",
    "\n",
    "# Configure inference to predict on base model checkpoint\n",
    "def configure_inference_base():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(base_llama_path),\n",
    "        trainer=trainer(),\n",
    "        input_dataset=input_data,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=50, top_k=1),\n",
    "        output_path=output_path_base,\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure inference to predict on trained DAPT checkpoint\n",
    "def configure_inference_sft():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        input_dataset=input_data,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=50, top_k=1),\n",
    "        output_path=output_path_sft,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"running inference on base model\")\n",
    "    run.run(configure_inference_base(), executor=local_executor_torchrun())\n",
    "    print(\"running inference on supervise fine tuned model\")\n",
    "    run.run(configure_inference_sft(), executor=local_executor_torchrun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step4**\n",
    "\n",
    "once the predictions are made, we evaluate the prediction's ROUGE scores. You should expect the SFT model's ROUGE score is much higher than that of the base model's scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/NeMo/scripts/metric_calculation/compute_rouge.py --ground-truth /workspace/data/verilog/test.jsonl --preds /workspace/inference/base_llama_prediction.jsonl --answer-field \"output\" \n",
    "!python3 /opt/NeMo/scripts/metric_calculation/compute_rouge.py --ground-truth /workspace/data/verilog/test.jsonl --preds /workspace/inference/sft_prediction.jsonl --answer-field \"output\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
