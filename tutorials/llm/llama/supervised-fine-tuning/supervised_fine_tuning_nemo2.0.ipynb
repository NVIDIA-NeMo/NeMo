{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOAL**\n",
    "\n",
    "Given a pretrained foundational model like Llama2-7b or a domain adaptive pretrained model, we can further customize the model with curated, high quality superivsed training data to align the model's performance on specific task or human preferences.\n",
    "\n",
    "In this tutorial, we use open source verilog code dataset containing description of the verilog code in natural language as input and the actual verilog code as output. We demonstrate that SFT model trained on this specific dataset could be used for domain specific code generation given an input prompt, which would be very useful in developing coding copilot applications in domain specific application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NeMo Tool and Resources**\n",
    "* [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Software Requirements**\n",
    "\n",
    "1. access to latest NeMo framework NGC Containers\n",
    "2. this playbook has been tested on: nvcr.io/nvidia/nemo:dev'. it is expected to work similarly on other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal, launch the NeMo framework container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -it -p 8080:8080 -p 8088:8088 --rm --gpus all --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal, launch Jupyter Notebook as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter notebook --allow-root --ip 0.0.0.0 --port 8088 --no-browser --NotebookApp.token=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardware Requirements**\n",
    "\n",
    "This playbook has been tested on 2xA100 80G but can be scaled to multiple GPUs as well as multiple nodes by modifying the corresponding configuration files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**\n",
    "\n",
    "this tutorial will be using the open source verilog dataset hosted on huggingface [link](https://huggingface.co/datasets/GaTech-EIC/MG-Verilog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Outline**\n",
    "* Step1: download Llama2-7b model from huggingface and convert it to .nemo format (could also use the model checkpoint from previous DAPT step)\n",
    "* Step2: implement your customized DataModule class\n",
    "* Step3: define configs needed to put together a SFT recipe\n",
    "* Step4: define a run task and actually execute the SFT step\n",
    "* Step5: define a inference run task and execute the inference step\n",
    "* Step6: evaluate the SFT model with ROUGE score and compare performance with base model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1**\n",
    "\n",
    "download the llama-2-7b model from hugging face and convert it to .nemo format, remove the original download once conversion is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/meta-llama/Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Llama-2-7b-hf\n",
    "!python3 ../convert.py\n",
    "!cd ..\n",
    "!rm -rf Llama-2-7b-hf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2**: implement customized verilog DataModule class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to use NeMo2.0 to run distributed training job on customized dataset, we need to create a customized data class inheriting the base class *FineTuningDataModule*, in the code below, we create a constructor and specify variables unique to customize Verilog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nemo.collections.llm.gpt.data.core import get_dataset_root\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.utils import logging\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from nemo.collections.common.tokenizers import TokenizerSpec\n",
    "    from nemo.collections.llm.gpt.data.packed_sequence import PackedSequenceSpecs\n",
    "\n",
    "BLOCK_COMMON = \"You only complete chats with syntax correct Verilog code. End the Verilog module code completion with 'endmodule'. Do not include module, input and output definitions.\\n    <</SYS>>\\n\\n    Implement the Verilog module based on the following block level summaries. Assume that signals are positive clock/clk edge triggered unless otherwise stated.\\nHere are block level summaries:\\n\\nblock_0:\"\n",
    "DETAILED_COMMON = \"You only complete chats with syntax correct Verilog code. End the Verilog module code completion with 'endmodule'. Do not include module, input and output definitions.\\n    <</SYS>>\\n\\n    Implement the Verilog module based on the following description. Assume that signals are positive clock/clk edge triggered unless otherwise stated.\"\n",
    "HIGH_LEVEL_COMMON = \"You only complete chats with syntax correct Verilog code. End the Verilog module code completion with 'endmodule'. Do not include module, input and output definitions.\\n    <</SYS>>\\n\\n    Implement the Verilog module based on the following description. Assume that signals are positive clock/clk edge triggered unless otherwise stated.\"\n",
    "\n",
    "\n",
    "## subclass the finetuning data module to create our own verilog data\n",
    "class VerilogDataModule(FineTuningDataModule, IOMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 1024,\n",
    "        tokenizer: Optional[\"TokenizerSpec\"] = None,\n",
    "        micro_batch_size: int = 2,\n",
    "        global_batch_size: int = 8,\n",
    "        rampup_batch_size: Optional[List[int]] = None,\n",
    "        force_redownload: bool = False,\n",
    "        delete_raw: bool = True,\n",
    "        seed: int = 12,\n",
    "        memmap_workers: int = 1,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        packed_sequence_specs: Optional[\"PackedSequenceSpecs\"] = None,\n",
    "        dataset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        self.force_redownload = force_redownload\n",
    "        self.delete_raw = delete_raw\n",
    "\n",
    "        super().__init__(\n",
    "            # where you save the train, validation, test.jsonl in nemo format\n",
    "            dataset_root=get_dataset_root(\"//workspace/data/verilog\"),\n",
    "            seq_length=seq_length,\n",
    "            tokenizer=tokenizer,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            global_batch_size=global_batch_size,\n",
    "            rampup_batch_size=rampup_batch_size,\n",
    "            seed=seed,\n",
    "            memmap_workers=memmap_workers,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            packed_sequence_specs=packed_sequence_specs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*prepare_data()* is the default function NeMo2.0 would call, hence it has to be implemented when creating customized dataset class. The function should implement logic to preprocessing the data and split the data into train, validation, test set. In the code below, *find_common_substrings()* function preprocess the downloaded raw data and get rid of the useless common substring in all data pairs. We also implement *_download_data()* function to actually download the dataset from the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def find_common_substrings(strings):\n",
    "        common_substrings = set(strings[0])\n",
    "        for string in strings[1:]:\n",
    "            common_substrings &= set(re.findall(r'\\w+', string))\n",
    "        return common_substrings\n",
    "\n",
    "    # override the base class function for data handling logic\n",
    "    def prepare_data(self) -> None:\n",
    "        # if train file is specified, no need to do anything\n",
    "        if not self.train_path.exists() or self.force_redownload:\n",
    "            dset = self._download_data()\n",
    "            self._preprocess_and_split_data(dset)\n",
    "        super().prepare_data()\n",
    "\n",
    "    def _download_data(self):\n",
    "        logging.info(f\"Downloading {self.__class__.__name__}...\")\n",
    "        return load_dataset(\n",
    "            \"GaTech-EIC/MG-Verilog\",\n",
    "            cache_dir=str(self.dataset_root),\n",
    "            download_mode=\"force_redownload\" if self.force_redownload else None,\n",
    "        )\n",
    "\n",
    "    def _preprocess_and_split_data(self, dset, train_ratio: float = 0.80, val_ratio: float = 0.15):\n",
    "        logging.info(f\"Preprocessing {self.__class__.__name__} to jsonl format and splitting...\")\n",
    "\n",
    "        test_ratio = 1 - train_ratio - val_ratio\n",
    "        save_splits = {}\n",
    "        dataset = dset.get('train')\n",
    "        split_dataset = dataset.train_test_split(test_size=val_ratio + test_ratio, seed=self.seed)\n",
    "        split_dataset2 = split_dataset['test'].train_test_split(\n",
    "            test_size=test_ratio / (val_ratio + test_ratio), seed=self.seed\n",
    "        )\n",
    "        save_splits['training'] = split_dataset['train']\n",
    "        save_splits['validation'] = split_dataset2['train']\n",
    "        save_splits['test'] = split_dataset2['test']\n",
    "\n",
    "        for split_name, dataset in save_splits.items():\n",
    "            output_file_high_level = self.dataset_root / f\"{split_name}.jsonl\"\n",
    "            with output_file_high_level.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for example in dataset:\n",
    "                    code = example[\"code\"].strip()\n",
    "                    description = example[\"description\"]\n",
    "                    high_level_global_summary = description['high_level_global_summary']\n",
    "                    high_level_global_summary = high_level_global_summary.replace(HIGH_LEVEL_COMMON, \"\")\n",
    "                    f.write(json.dumps({\"input\": high_level_global_summary, \"output\": code}) + \"\\n\")\n",
    "            logging.info(f\"{split_name} split saved to {output_file_high_level}\")\n",
    "\n",
    "        if self.delete_raw:\n",
    "            for p in self.dataset_root.iterdir():\n",
    "                if p.is_dir():\n",
    "                    shutil.rmtree(p)\n",
    "                elif '.jsonl' not in str(p.name):\n",
    "                    p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step3**: define configs needed to put together a SFT recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeMo2.0 wraps training elements into configs in order to simplify and modularize the training process. In the code snippet below, we create configs including\n",
    "* verilog(): config wrapping customized DataModule class and implemented logic \n",
    "* trainer(): config specifying training strategy, tensor parallel size, number of devices, steps, evaluation steps and so on\n",
    "* logger(): config how we want to log the intermediate results, in this case we are using validation loss and log every 40 steps\n",
    "* adam_with_cosine_annealing(): config of the adam optimizer with pre specified parameters\n",
    "* llama2-7b(): the base model you want to conduct SFT on\n",
    "* resume(): in case of training interruption, you can continue training from intermediate checkpoint rather than training from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import nemo_run as run\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "from verilog_data_module import VerilogDataModule\n",
    "\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm import Llama2Config7B\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "\n",
    "\n",
    "# configure custom dataset\n",
    "def verilog() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(VerilogDataModule, seq_length=1024, micro_batch_size=2, global_batch_size=8, num_workers=8)\n",
    "\n",
    "\n",
    "# configure trainer class similar to pytorch lightning trainer\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(nl.MegatronStrategy, tensor_model_parallel_size=2)\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        devices=2,\n",
    "        max_steps=200,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "        log_every_n_steps=40,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=20,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# configure the logger\n",
    "def logger() -> run.Config[nl.NeMoLogger]:\n",
    "    ckpt = run.Config(\n",
    "        nl.ModelCheckpoint,\n",
    "        save_last=True,\n",
    "        every_n_train_steps=40,\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    ## this is where hthe\n",
    "    return run.Config(\n",
    "        nl.NeMoLogger,\n",
    "        name=\"sft_log\",\n",
    "        log_dir=\"//workspace\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None,\n",
    "    )\n",
    "\n",
    "\n",
    "# configre the optimizer, adam with cosine annealing\n",
    "def adam_with_cosine_annealing() -> run.Config[nl.OptimizerModule]:\n",
    "    opt_cfg = run.Config(\n",
    "        OptimizerConfig,\n",
    "        optimizer=\"adam\",\n",
    "        lr=5e-5,\n",
    "        adam_beta2=0.98,\n",
    "        use_distributed_optimizer=True,\n",
    "        clip_grad=1.0,\n",
    "        bf16=True,\n",
    "    )\n",
    "    return run.Config(nl.MegatronOptimizerModule, config=opt_cfg)\n",
    "\n",
    "\n",
    "# configure the base model\n",
    "def llama2_7b() -> run.Config[pl.LightningModule]:\n",
    "    return run.Config(llm.LlamaModel, config=run.Config(llm.Llama2Config7B))\n",
    "\n",
    "\n",
    "# configure auto resume\n",
    "def resume() -> run.Config[nl.AutoResume]:\n",
    "    return run.Config(\n",
    "        nl.AutoResume,\n",
    "        restore_config=run.Config(\n",
    "            nl.RestoreConfig,\n",
    "            ## default path to save converted hf model\n",
    "            path=\"/root/.cache/nemo/models/Llama-2-7b-hf\",\n",
    "        ),\n",
    "        # requires completely saved checkpoint to resume from\n",
    "        resume_if_exists=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step4: Define run task and actually execute SFT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the above configs defined we are ready to put them together and form a finetuning recipe as show in the code below.\n",
    "in order to actually run the job, we use [NeMo_Run](https://github.com/NVIDIA/NeMo-Run/tree/main), which is a pythonic and modular way to execute a predefined run. In this case, since we are using *LocalExecutor* to conduct the run, you can also choose other executors like SlurmExecutor or SkypilotExecutor.\n",
    "Finally, we put together all the components in the main function to download, preprocess, split the data, then run the SFT.\n",
    "\n",
    "**note this process will take a while depending on your hardware, so please be patient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with all above components created, call NeMo2.0 finetune API\n",
    "def configure_finetuning_recipe():\n",
    "    return run.Partial(\n",
    "        llm.finetune,\n",
    "        model=llama2_7b(),\n",
    "        trainer=trainer(),\n",
    "        data=verilog(),\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 2) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "    return executor\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"preprocess data!\")\n",
    "    verilog = VerilogDataModule()\n",
    "    verilog_data = verilog._download_data()\n",
    "    verilog._preprocess_and_split_data(verilog_data)\n",
    "    print(\"running supervised fine tuning!\")\n",
    "    run.run(configure_finetuning_recipe(), executor=local_executor_torchrun())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step5**: define inference run task and do inference on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use the configs defined in the training steps to make sure the configs used are consistent between run and inference, otherwise you might run into error. We only need to configure the inference step which uses *llm.generate* instead of *llm.finetune* \n",
    "test data path, model checkpoints for both base model and SFT model, output prediction paths need to be specified for both base and sft model. \n",
    "\n",
    "Similar to sft, we use NeMo run to execute the inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import nemo_run as run\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "from run_sft import local_executor_torchrun, trainer\n",
    "\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.llm import Llama2Config7B\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "\n",
    "input_data = \"/workspace/data/verilog/test.jsonl\"\n",
    "base_llama_path = \"/root/.cache/nemo/models/Llama-2-7b-hf\"\n",
    "sft_ckpt_path = str(\n",
    "    next(\n",
    "        (d for d in Path(\"/workspace/sft_log/checkpoints\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None\n",
    "    )\n",
    ")\n",
    "\n",
    "os.makedirs(\"/workspace/inference\", exist_ok=True)\n",
    "output_path_base = \"/workspace/inference/base_llama_prediction.jsonl\"\n",
    "output_path_sft = \"/workspace/inference/sft_prediction.jsonl\"\n",
    "\n",
    "\n",
    "# Configure inference to predict on base model checkpoint\n",
    "def configure_inference_base():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(base_llama_path),\n",
    "        trainer=trainer(),\n",
    "        input_dataset=input_data,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=50, top_k=1),\n",
    "        output_path=output_path_base,\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure inference to predict on trained DAPT checkpoint\n",
    "def configure_inference_sft():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        input_dataset=input_data,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=50, top_k=1),\n",
    "        output_path=output_path_sft,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"running inference on base model\")\n",
    "    run.run(configure_inference_base(), executor=local_executor_torchrun())\n",
    "    print(\"running inference on supervise fine tuned model\")\n",
    "    run.run(configure_inference_sft(), executor=local_executor_torchrun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step6**: evaluate the SFT model with ROUGE score and compare performance with base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have both base_llama_predictions.jsonl and sft_prediction.jsonl files generated containing the prediction of test input for the two models. We also provide the ground truth of the test data and use ROGUE score as metrics which calculate the average n-gram overlap of the prediction and ground truth. Specify what text you want to evaluate on, in this case it is the \"output\"\n",
    "You should expect a low score for the base model and a much higher score for the sft model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/NeMo/scripts/metric_calculation/compute_rouge.py --ground-truth /workspace/data/verilog/test.jsonl --preds /workspace/inference/base_llama_prediction.jsonl --answer-field \"output\" \n",
    "!python3 /opt/NeMo/scripts/metric_calculation/compute_rouge.py --ground-truth /workspace/data/verilog/test.jsonl --preds /workspace/inference/sft_prediction.jsonl --answer-field \"output\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
