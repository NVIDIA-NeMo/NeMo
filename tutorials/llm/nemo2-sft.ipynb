{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "## Supervised Finetuning (SFT)\n",
    "\n",
    "Often we want to adapt or customize foundation models to be more performant on our specific task. Fine-tuning refers to how we can modify the weights of a pre-trained foundation model with additional custom data. Supervised fine-tuning (SFT) refers to unfreezing all the weights and layers in our model and training on a newly labeled set of examples. We can fine-tune to incorporate new, domain-specific knowledge, or teach the foundation model what type of response to provide. One specific type of SFT is also referred to as “instruction tuning” where we use SFT to teach a model to follow instructions better. In this playbook will demonstrate how to perform SFT with Llama3-8b using NeMo 2.0.\n",
    "\n",
    "## NeMo 2.0\n",
    "\n",
    "In NeMo 1.0, the main interface for configuring experiments is through YAML files. This approach allows for a declarative way to set up experiments, but it has limitations in terms of flexibility and programmatic control. NeMo 2.0 is an update on the NeMo Framework which introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.\n",
    "\n",
    "- Python-Based Configuration - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.\n",
    "\n",
    "- Modular Abstractions - By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.\n",
    "\n",
    "- Scalability - NeMo 2.0 seamlessly scales large-scale experiments across thousands of GPUs using NeMo-Run, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.\n",
    "\n",
    "By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 makes it easy for users to adapt the framework to their specific use cases and experiment with various configurations. This section offers an overview of the new features in NeMo 2.0 and includes a migration guide with step-by-step instructions for transitioning your models from NeMo 1.0 to NeMo 2.0.\n",
    "\n",
    "\n",
    "## Software Requirements\n",
    "\n",
    "1. Use the latest [NeMo Framework Training container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags). Note that you must be logged in to the container registry to view this page.\n",
    "\n",
    "2. This notebook uses the container: `nvcr.io/nvidia/nemo:dev`  \n",
    "\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "- Minimum 8xA100 80G (1 node) for SFT on 7B and 13B\n",
    "\n",
    "- SFT can be run on all (7B/13B/70B) model sizes on multiple nodes\n",
    "\n",
    "\n",
    "## Data\n",
    "Databricks-dolly-15k is an open-source dataset created by the collaborative efforts of Databricks employees. It consists of high-quality human-generated prompt/response pairs specifically designed for instruction tuning LLMs. These pairs cover a diverse range of behaviors, from brainstorming and content generation to information extraction and summarization. \n",
    "\n",
    "For more information, refer to [databricks-dolly-15k | Hugging Face](https://huggingface.co/datasets/databricks/databricks-dolly-15k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Go inside docker container\n",
    "\n",
    "You can start and enter the dev container by:\n",
    "```\n",
    "docker run --gpus device=1 --shm-size=2g --net=host --ulimit memlock=-1 --rm -it -v ${PWD}:/workspace -w /workspace -v ${PWD}/results:/results nvcr.io/nvidia/nemo:dev bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Import HuggingFace checkpoint\n",
    "First request download permission from Meta and Hugging Face. Log in through `huggingface-cli` using your Huggingface token before importing llama3 models. \n",
    "\n",
    "```\n",
    "$ huggingface-cli login\n",
    "```\n",
    "\n",
    "Once logged in, you can use the following script to import a Hugging Face model. Based on the provided model configuration (`Llama3-8b` in the example below), the `llm.import_ckpt` API will download the specified model using the \"hf://<huggingface_model_id>\" URL format. It will then convert the model into NeMo 2.0 format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[NeMo W 2024-11-15 09:57:49 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1731693…</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1731693…\u001b[0m\u001b[92m ─\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1731693470/nemo.collections.llm.api.import_ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09:57:50] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.import_ckpt for experiment </span>                     <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">660</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09:57:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.import_ckpt for experiment \u001b[0m                     \u001b]8;id=974892;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=76903;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\u001b\\\u001b[2m660\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.import_ckpt\u001b[0m                                                   \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1731693470/nemo.collections.llm.api.import_ckpt\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.import_ckpt-f0rwwn6vt74ckc\n",
      "AppStatus:\n",
      "    State: RUNNING\n",
      "    Num Restarts: 0\n",
      "    Roles: \n",
      "    Msg: <NONE>\n",
      "    Structured Error Msg: <NONE>\n",
      "    UI URL: file:///root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1731693470/nemo.collections.llm.api.import_ckpt/nemo_run/nemo.collections.llm.api.import_ckpt-f0rwwn6vt74ckc\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.import_ckpt_1731693470 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.import_ckpt_1731693470 to finish\u001b[0m\u001b[92m ─────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt_1731693470</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt_1731693470\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.import_ckpt-f0rwwn6vt74ckc\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1731693470/nemo.collections.llm.api.import_ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.import_ckpt-f0rwwn6vt74ckc\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1731693470/nemo.collections.llm.api.import_ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.import_ckpt-f0rwwn6vt74ckc to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mport_ckpt/0 [NeMo W 2024-11-15 09:57:56 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "mport_ckpt/0       cm = get_cmap(\"Set1\")\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4853.11it/s]\n",
      "mport_ckpt/0 Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_strategy:310] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:396] Rank 0 has data parallel group : [0]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:402] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:407] All data parallel group ranks with context parallel combined: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:410] Ranks 0 has data parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:418] Rank 0 has context parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:421] All context parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:422] Ranks 0 has context parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:429] Rank 0 has model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:430] All model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:439] Rank 0 has tensor model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:443] All tensor model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:444] Rank 0 has tensor model parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:464] Rank 0 has pipeline model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:476] Rank 0 has embedding group: [0]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:482] All pipeline model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:483] Rank 0 has pipeline model parallel rank 0\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:484] All embedding group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:57:59 megatron_init:485] Rank 0 has embedding rank: 0\n",
      "mport_ckpt/0 GPU available: True (cuda), used: False\n",
      "mport_ckpt/0 TPU available: False, using: 0 TPU cores\n",
      "mport_ckpt/0 HPU available: False, using: 0 HPUs\n",
      "mport_ckpt/0 [NeMo W 2024-11-15 09:57:59 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "mport_ckpt/0 ----------------------------------------------------------------------------------------------------\n",
      "mport_ckpt/0 distributed_backend=gloo\n",
      "mport_ckpt/0 All distributed processes registered. Starting with 1 processes\n",
      "mport_ckpt/0 ----------------------------------------------------------------------------------------------------\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [NeMo I 2024-11-15 09:58:00 base:44] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n",
      "mport_ckpt/0 [NeMo W 2024-11-15 09:58:00 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2024-11-15 09:58:38 megatron_strategy:324] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "mport_ckpt/0 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "mport_ckpt/0 To disable this warning, you can either:\n",
      "mport_ckpt/0 \t- Avoid using `tokenizers` before the fork if possible\n",
      "mport_ckpt/0 \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mport_ckpt/0 Converted Llama model to Nemo, model saved to /root/.cache/nemo/models/meta-llama/Meta-Llama-3-8B in torch.bfloat16.\n",
      "mport_ckpt/0 \u001b[32m $\u001b[0m\u001b[32mNEMO_MODELS_CACHE\u001b[0m\u001b[32m=\u001b[0m\u001b[32m/root/.cache/nemo/\u001b[0m\u001b[32mmodels\u001b[0m\u001b[32m \u001b[0m\n",
      "mport_ckpt/0 \u001b[32m✓ Checkpoint imported to \u001b[0m\u001b[32m/root/.cache/nemo/models/meta-llama/\u001b[0m\u001b[32mMeta-Llama-3-8B\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.import_ckpt-f0rwwn6vt74ckc finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']</span><span style=\"background-color: #272822\">                        </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt_1731693470\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1731693470\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.import_ckpt_1731693470</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.import_ckpt_1731693470 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.import_ckpt_1731693470 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1731693470\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1731693470\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1731693470\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "\n",
    "\n",
    "# llm.import_ckpt is the nemo2 API for converting Hugging Face checkpoint to NeMo format\n",
    "# example usage:\n",
    "# llm.import_ckpt(model=llm.llama3_8b.model(), source=\"hf://meta-llama/Meta-Llama-3-8B\")\n",
    "#\n",
    "# We use run.Partial to configure this function\n",
    "def configure_checkpoint_conversion():\n",
    "    return run.Partial(\n",
    "        llm.import_ckpt,\n",
    "        model=llm.llama3_8b.model(),\n",
    "        source=\"hf://meta-llama/Meta-Llama-3-8B\",\n",
    "        overwrite=False,\n",
    "    )\n",
    "\n",
    "# configure your function\n",
    "import_ckpt = configure_checkpoint_conversion()\n",
    "# define your executor\n",
    "local_executor = run.LocalExecutor()\n",
    "\n",
    "# run your experiment\n",
    "run.run(import_ckpt, executor=local_executor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data and customize DataModule\n",
    "\n",
    "We will be using Databricks-dolly-15k for this notebook. NeMo 2.0 already provides a `DollyDataModule`. Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dolly() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(llm.DollyDataModule, seq_length=2048, micro_batch_size=1, global_batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own data, you will need to create a custom `DataModule`. This involves extending the base class `FineTuningDataModule`, so that you have access to existing data handling logic such as packed sequence. Here we walk you through the process step by step using the already existing [`DollyDataModule`](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/gpt/data/dolly.py) as an example. \n",
    "\n",
    "### 1. Subclass the FineTuningDataModule\n",
    "You need to extend `FineTuningDataModule` if you're fine-tuning NeMo models. This provides access to existing data handling logic, such as packed sequences. The `data_root` parameter is where you store your generated `train/validation/test.jsonl` in NeMo format. Below is how `DollyDataModule` does it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import TYPE_CHECKING, List, Optional\n",
    "from nemo.collections.common.tokenizers import TokenizerSpec\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "import json\n",
    "from nemo.utils import logging\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "class DollyDataModule(FineTuningDataModule, IOMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 2048,\n",
    "        tokenizer: Optional[\"TokenizerSpec\"] = None,\n",
    "        micro_batch_size: int = 4,\n",
    "        global_batch_size: int = 8,\n",
    "        rampup_batch_size: Optional[List[int]] = None,\n",
    "        force_redownload: bool = False,\n",
    "        delete_raw: bool = True,\n",
    "        seed: int = 1234,\n",
    "        memmap_workers: int = 1,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        pad_to_max_length: bool = False,\n",
    "        packed_sequence_size: int = -1,\n",
    "    ):\n",
    "        self.force_redownload = force_redownload\n",
    "        self.delete_raw = delete_raw\n",
    "\n",
    "        super().__init__(\n",
    "            dataset_root=get_dataset_root(\"dolly\"),\n",
    "            seq_length=seq_length,\n",
    "            tokenizer=tokenizer,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            global_batch_size=global_batch_size,\n",
    "            rampup_batch_size=rampup_batch_size,\n",
    "            seed=seed,\n",
    "            memmap_workers=memmap_workers,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            pad_to_max_length=pad_to_max_length,\n",
    "            packed_sequence_size=packed_sequence_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Override the `prepare_data` Method\n",
    "\n",
    "The `prepare_data` method is responsible for downloading and preprocessing data if needed. If the dataset is already downloaded, you can skip this step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(self) -> None:\n",
    "    # if train file is specified, no need to do anything\n",
    "    if not self.train_path.exists() or self.force_redownload:\n",
    "        dset = self._download_data()\n",
    "        self._preprocess_and_split_data(dset)\n",
    "    super().prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement Data Download and Preprocessing Logic\n",
    "\n",
    "If your dataset requires downloading or preprocessing, implement this logic within helper methods. Skip the download part if it's not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _download_data(self):\n",
    "    logging.info(f\"Downloading {self.__class__.__name__}...\")\n",
    "    return load_dataset(\n",
    "        \"databricks/databricks-dolly-15k\",\n",
    "        cache_dir=str(self.dataset_root),\n",
    "        download_mode=\"force_redownload\" if self.force_redownload else None,\n",
    "    )\n",
    "\n",
    "def _preprocess_and_split_data(self, dset, train_ratio: float = 0.80, val_ratio: float = 0.15):\n",
    "    logging.info(f\"Preprocessing {self.__class__.__name__} to jsonl format and splitting...\")\n",
    "\n",
    "    test_ratio = 1 - train_ratio - val_ratio\n",
    "    save_splits = {}\n",
    "    dataset = dset.get('train')\n",
    "    split_dataset = dataset.train_test_split(test_size=val_ratio + test_ratio, seed=self.seed)\n",
    "    split_dataset2 = split_dataset['test'].train_test_split(\n",
    "        test_size=test_ratio / (val_ratio + test_ratio), seed=self.seed\n",
    "    )\n",
    "    save_splits['training'] = split_dataset['train']\n",
    "    save_splits['validation'] = split_dataset2['train']\n",
    "    save_splits['test'] = split_dataset2['test']\n",
    "\n",
    "    for split_name, dataset in save_splits.items():\n",
    "        output_file = self.dataset_root / f\"{split_name}.jsonl\"\n",
    "        with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for example in dataset:\n",
    "                context = example[\"context\"].strip()\n",
    "                if context != \"\":\n",
    "                    # Randomize context and instruction order.\n",
    "                    context_first = np.random.randint(0, 2) == 0\n",
    "                    if context_first:\n",
    "                        instruction = example[\"instruction\"].strip()\n",
    "                        assert instruction != \"\"\n",
    "                        _input = f\"{context}\\n\\n{instruction}\"\n",
    "                        _output = example[\"response\"]\n",
    "                    else:\n",
    "                        instruction = example[\"instruction\"].strip()\n",
    "                        assert instruction != \"\"\n",
    "                        _input = f\"{instruction}\\n\\n{context}\"\n",
    "                        _output = example[\"response\"]\n",
    "                else:\n",
    "                    _input = example[\"instruction\"]\n",
    "                    _output = example[\"response\"]\n",
    "\n",
    "                f.write(json.dumps({\"input\": _input, \"output\": _output, \"category\": example[\"category\"]}) + \"\\n\")\n",
    "\n",
    "        logging.info(f\"{split_name} split saved to {output_file}\")\n",
    "\n",
    "    if self.delete_raw:\n",
    "        for p in self.dataset_root.iterdir():\n",
    "            if p.is_dir():\n",
    "                shutil.rmtree(p)\n",
    "            elif '.jsonl' not in str(p.name):\n",
    "                p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original example in Dolly dataset looks like:\n",
    "```\n",
    "{'instruction': 'Extract all the movies from this passage and the year they were released out. Write each movie as a separate sentence', 'context': \"The genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.\", 'response': 'A Trip to the Moon was released in 1902. Metropolis came out in 1927. 2001: A Space Odyssey was released in 1968. Star Wars came out in 1977.', 'category': 'information_extraction'}\n",
    "```\n",
    "After the preprocessing logic, the data examples are transformed into NeMo format, as below:\n",
    "```\n",
    "{'input': \"Extract all the movies from this passage and the year they were released out. Write each movie as a separate sentence\\n\\nThe genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.\", 'output': 'A Trip to the Moon was released in 1902. Metropolis came out in 1927. 2001: A Space Odyssey was released in 1968. Star Wars came out in 1977.', 'category': 'information_extraction'}\n",
    "```\n",
    "Each data example is saved as a json string as one line in the `train/validation/test.jsonl` file, under `data_root` directory you specified earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run SFT with NeMo 2.0 API \n",
    "\n",
    "The following python script utilizes NeMo 2.0 API to perform SFT. In this script we are configuring the following components for training. These components are similar between SFT and PEFT. SFT and PEFT both uses `llm.finetune` API. To switch from PEFT to SFT you just need to remove `peft` parameter.\n",
    "\n",
    "### Trainer\n",
    "NeMo 2.0 Trainer works simiarly to Pytorch Lightning trainer. You can specify to use MegatronStrategy as your model parallel strategy to use NVIDIA's Megatron-LM framework and pass in configurations as below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=2\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        devices=2,\n",
    "        max_steps=20,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=2,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Logger\n",
    "Configure your training steps, output directories and logging through `NeMoLogger`. In the following example, the experiment output will be saved at `./results/nemo2_sft`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logger() -> run.Config[nl.NeMoLogger]:\n",
    "    ckpt = run.Config(\n",
    "        nl.ModelCheckpoint,\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return run.Config(\n",
    "        nl.NeMoLogger,\n",
    "        name=\"nemo2_sft\",\n",
    "        log_dir=\"./results\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Optimizer\n",
    "In the following example, we will be using distributed adam optimizer, and pass in optimizer configuration through `OptimizerConfig`: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adam_with_cosine_annealing() -> run.Config[nl.OptimizerModule]:\n",
    "    opt_cfg = run.Config(\n",
    "        OptimizerConfig,\n",
    "        optimizer=\"adam\",\n",
    "        lr=5e-6,\n",
    "        adam_beta2=0.98,\n",
    "        use_distributed_optimizer=True,\n",
    "        clip_grad=1.0,\n",
    "        bf16=True,\n",
    "    )\n",
    "    return run.Config(\n",
    "        nl.MegatronOptimizerModule,\n",
    "        config=opt_cfg\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "We will perform SFT on top of Llama3-8b so we create a `LlamaModel` to pass to finetune API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llama3_8b() -> run.Config[pl.LightningModule]:\n",
    "    return run.Config(llm.LlamaModel, config=run.Config(llm.Llama3Config8B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoResume\n",
    "In NeMo 2.0 we can directly pass in Llama3-8b's Hugging Face ID to start SFT without manually converting it into NeMo checkpoint format like in NeMo 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resume() -> run.Config[nl.AutoResume]:\n",
    "    return run.Config(\n",
    "        nl.AutoResume,\n",
    "        restore_config=run.Config(nl.RestoreConfig,\n",
    "            path=\"nemo://meta-llama/Meta-Llama-3-8B\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NeMo 2.0 finetun API\n",
    "Using all the components we created above, we can call NeMo 2.0 finetun API:\n",
    "```\n",
    "llm.finetune(\n",
    "    model=llama3_8b(),\n",
    "    data=dolly(),\n",
    "    trainer=trainer(),\n",
    "    log=logger(),\n",
    "    optim=adam_with_cosine_annealing(),\n",
    "    resume=resume(),\n",
    ")\n",
    "```\n",
    "Below is a python script that you can save as a file e.g. `nemo2-sft.py`, and run SFT training, using all components we created above and NeMo 2.0 finetune API. The script cannot be directly executed in interactive environment like a notebook. We can execute by `torchrun --nproc_per_node=<NUM_GPU> nemo2-sft.py` when multiple GPU is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1731693538</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1731693538\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1731693538/nemo.collections.llm.api.finetune\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09:58:58] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.finetune for experiment </span>                        <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">660</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.finetune</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09:58:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.finetune for experiment \u001b[0m                        \u001b]8;id=475933;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=742588;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\u001b\\\u001b[2m660\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.finetune\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1731693538/nemo.collections.llm.api.finetune\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.finetune-bsqgzflc7xzftd\n",
      "AppStatus:\n",
      "    State: RUNNING\n",
      "    Num Restarts: 0\n",
      "    Roles: \n",
      "    Msg: <NONE>\n",
      "    Structured Error Msg: <NONE>\n",
      "    UI URL: file:///root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1731693538/nemo.collections.llm.api.finetune/nemo_run/nemo.collections.llm.api.finetune-bsqgzflc7xzftd\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.finetune_1731693538 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.finetune_1731693538 to finish\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune_1731693538</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.finetune_1731693538\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.finetune-bsqgzflc7xzftd\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1731693538/nemo.collections.llm.api.finetune\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.finetune\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.finetune-bsqgzflc7xzftd\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1731693538/nemo.collections.llm.api.finetune\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.finetune-bsqgzflc7xzftd to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.finetune/0 W1115 09:58:59.485000 140737350272832 torch/distributed/run.py:778] \n",
      "i.finetune/0 W1115 09:58:59.485000 140737350272832 torch/distributed/run.py:778] *****************************************\n",
      "i.finetune/0 W1115 09:58:59.485000 140737350272832 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "i.finetune/0 W1115 09:58:59.485000 140737350272832 torch/distributed/run.py:778] *****************************************\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   min_nodes        : 1\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   max_nodes        : 1\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   nproc_per_node   : 2\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   run_id           : 9802\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : localhost:0\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   max_restarts     : 0\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   monitor_interval : 0.1\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1731693538/nemo.collections.llm.api.finetune/nemo_run/nemo.collections.llm.api.finetune-bsqgzflc7xzftd/torchelastic/nemo.collections.llm.api.finetune\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}\n",
      "i.finetune/0 I1115 09:58:59.485000 140737350272832 torch/distributed/launcher/api.py:188] \n",
      "i.finetune/0 I1115 09:58:59.488000 140737350272832 torch/distributed/elastic/agent/server/api.py:825] [default] starting workers for entrypoint: python\n",
      "i.finetune/0 I1115 09:58:59.488000 140737350272832 torch/distributed/elastic/agent/server/api.py:646] [default] Rendezvous'ing worker group\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512] [default] Rendezvous complete for workers. Result:\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   restart_count=0\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   master_addr=eos0346.eos.clusters.nvidia.com\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   master_port=51293\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   group_rank=0\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   group_world_size=1\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   local_ranks=[0, 1]\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   role_ranks=[0, 1]\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   global_ranks=[0, 1]\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   role_world_sizes=[2, 2]\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   global_world_sizes=[2, 2]\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:512] \n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/api.py:654] [default] Starting worker group\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/local_elastic_agent.py:184] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.finetune/0 I1115 09:58:59.705000 140737350272832 torch/distributed/elastic/agent/server/local_elastic_agent.py:216] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:06 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "i.finetune/0 [default0]:      cm = get_cmap(\"Set1\")\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:GPU available: True (cuda), used: True\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:07 nemo_logger:145] Experiments will be logged at results/nemo2_sft\n",
      "i.finetune/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "i.finetune/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:07 nemo_logger:123] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:07 nemo_logger:173] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to results\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:07 nemo_logger:189] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:07 megatron_strategy:310] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:396] Rank 0 has data parallel group : [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:402] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:407] All data parallel group ranks with context parallel combined: [[0], [1]]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:410] Ranks 0 has data parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:418] Rank 0 has context parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:421] All context parallel group ranks: [[0], [1]]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:422] Ranks 0 has context parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:429] Rank 0 has model parallel group: [0, 1]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:430] All model parallel group ranks: [[0, 1]]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:439] Rank 0 has tensor model parallel group: [0, 1]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:443] All tensor model parallel group ranks: [[0, 1]]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:444] Rank 0 has tensor model parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:464] Rank 0 has pipeline model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:476] Rank 0 has embedding group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:482] All pipeline model parallel group ranks: [[0], [1]]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:483] Rank 0 has pipeline model parallel rank 0\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:484] All embedding group ranks: [[0], [1]]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:08 megatron_init:485] Rank 0 has embedding rank: 0\n",
      "i.finetune/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "i.finetune/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:distributed_backend=nccl\n",
      "i.finetune/0 [default0]:All distributed processes registered. Starting with 2 processes\n",
      "i.finetune/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default1]:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:10 dolly:89] Downloading DollyDataModule...\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:11 dolly:97] Preprocessing DollyDataModule to jsonl format and splitting...\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]\n",
      "i.finetune/0 [default0]:Generating train split: 100%|██████████| 15011/15011 [00:00<00:00, 245225.62 examples/s]\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:12 dolly:134] training split saved to /root/.cache/nemo/datasets/dolly/training.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:12 dolly:134] validation split saved to /root/.cache/nemo/datasets/dolly/validation.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:12 dolly:134] test split saved to /root/.cache/nemo/datasets/dolly/test.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:12 base:44] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n",
      "i.finetune/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:13 megatron_strategy:324] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:13 num_microbatches_calculator:228] setting number of microbatches to constant 8\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:13 megatron_parallel:550]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4015263744\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:13 utils:278] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:13 utils:299] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "i.finetune/0 [default0]:    Params for bucket 1 (4015263744 elements):\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.final_layernorm.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.embedding.word_embeddings.weight\n",
      "i.finetune/0 [default0]:    \tmodule.output_layer.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:13 utils:278] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=5e-06, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.98, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:13 megatron_strategy:745] Doing selective restore from RestoreConfig(path='/root/.cache/nemo/models/meta-llama/Meta-Llama-3-8B', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:29 megatron_strategy:750] Restoring model weights from RestoreConfig(path='/root/.cache/nemo/models/meta-llama/Meta-Llama-3-8B', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 megatron_strategy:757] Finished restoring from RestoreConfig(path='/root/.cache/nemo/models/meta-llama/Meta-Llama-3-8B', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:116] Building data files\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:528] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:  | Name   | Type | Params | Mode \n",
      "i.finetune/0 [default0]:----------------------------------------\n",
      "i.finetune/0 [default0]:0 | module | DDP  | 4.0 B  | train\n",
      "i.finetune/0 [default0]:----------------------------------------\n",
      "i.finetune/0 [default0]:4.0 B     Trainable params\n",
      "i.finetune/0 [default0]:0         Non-trainable params\n",
      "i.finetune/0 [default0]:4.0 B     Total params\n",
      "i.finetune/0 [default0]:16,061.055Total estimated model params size (MB)\n",
      "i.finetune/0 [default0]:651       Modules in train mode\n",
      "i.finetune/0 [default0]:0         Modules in eval mode\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:494] Building indexing for fn = /root/.cache/nemo/datasets/dolly/training.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:506] Saving idx file = /root/.cache/nemo/datasets/dolly/training.jsonl.idx.npy\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:508] Saving metadata file = /root/.cache/nemo/datasets/dolly/training.jsonl.idx.info\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.073210\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:528] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.049917\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:158] Loading data files\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:249] Loading /root/.cache/nemo/datasets/dolly/training.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000408\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:165] Computing global indices\n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:116] Building data files\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:528] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:494] Building indexing for fn = /root/.cache/nemo/datasets/dolly/validation.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:506] Saving idx file = /root/.cache/nemo/datasets/dolly/validation.jsonl.idx.npy\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:508] Saving metadata file = /root/.cache/nemo/datasets/dolly/validation.jsonl.idx.info\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.049564\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:528] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:30 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=111` in the `DataLoader` to improve performance.\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.041159\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:158] Loading data files\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:249] Loading /root/.cache/nemo/datasets/dolly/validation.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000357\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 09:59:30 text_memmap_dataset:165] Computing global indices\n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:30 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=111` in the `DataLoader` to improve performance.\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 0/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 0 | reduced_train_loss: 2.103\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 1/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 1 | reduced_train_loss: 1.272 | consumed_samples: 16\n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:59 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('global_batch_size', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:[NeMo W 2024-11-15 09:59:59 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 2/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 2 | reduced_train_loss: 1.512 | consumed_samples: 24 | val_loss: 2.103\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 3/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 3 | reduced_train_loss: 1.811 | consumed_samples: 32 | val_loss: 2.103\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 4/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 4 | reduced_train_loss: 1.398 | consumed_samples: 40 | val_loss: 2.029\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 5/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 5 | reduced_train_loss: 1.601 | consumed_samples: 48 | val_loss: 2.029\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 6/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 6 | reduced_train_loss: 1.075 | consumed_samples: 56 | val_loss: 2.005\n",
      "i.finetune/0 [default1]:[rank1]:W1115 10:00:09.702000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default1]:[rank1]:W1115 10:00:09.702000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default1]:[rank1]:W1115 10:00:09.702000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 496, actual 512\n",
      "i.finetune/0 [default1]:[rank1]:W1115 10:00:09.702000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default1]:[rank1]:W1115 10:00:09.702000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default0]:[rank0]:W1115 10:00:09.710000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default0]:[rank0]:W1115 10:00:09.710000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default0]:[rank0]:W1115 10:00:09.710000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 496, actual 512\n",
      "i.finetune/0 [default0]:[rank0]:W1115 10:00:09.710000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default0]:[rank0]:W1115 10:00:09.710000 140737350272832 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 7/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 7 | reduced_train_loss: 1.542 | consumed_samples: 64 | val_loss: 2.005\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 8/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 8 | reduced_train_loss: 1.479 | consumed_samples: 72 | val_loss: 2.003\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 9/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 9 | reduced_train_loss: 1.671 | consumed_samples: 80 | val_loss: 2.003\n",
      "i.finetune/0 [default0]:Epoch 0, global step 9: 'reduced_train_loss' reached 1.67107 (best 1.67107), saving model to 'results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.6711-epoch=0.ckpt' as top 1\n",
      "i.finetune/0 [default1]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default1]:To disable this warning, you can either:\n",
      "i.finetune/0 [default1]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default1]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:00:35 model_checkpoint:497] Scheduled async checkpoint save for results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.6711-epoch=0.ckpt\n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default1]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default1]:To disable this warning, you can either:\n",
      "i.finetune/0 [default1]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default1]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:00:54 model_checkpoint:497] Scheduled async checkpoint save for results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.6711-epoch=0-last.ckpt\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 10/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 10 | reduced_train_loss: 1.458 | consumed_samples: 88 | val_loss: 1.958\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 11/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 11 | reduced_train_loss: 2.787 | consumed_samples: 96 | val_loss: 1.958\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 12/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 12 | reduced_train_loss: 1.427 | consumed_samples: 104 | val_loss: 1.933\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 13/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 13 | reduced_train_loss: 1.514 | consumed_samples: 112 | val_loss: 1.933\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 14/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 14 | reduced_train_loss: 1.127 | consumed_samples: 120 | val_loss: 1.925\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 15/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 15 | reduced_train_loss: 1.41 | consumed_samples: 128 | val_loss: 1.925\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 16/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 16 | reduced_train_loss: 1.075 | consumed_samples: 136 | val_loss: 1.923\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 17/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 17 | reduced_train_loss: 1.445 | consumed_samples: 144 | val_loss: 1.923\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 18/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 18 | reduced_train_loss: 1.711 | consumed_samples: 152 | val_loss: 1.929\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 19/19 | lr: 5e-06 | global_batch_size: 8 | global_step: 19 | reduced_train_loss: 1.506 | consumed_samples: 160 | val_loss: 1.929\n",
      "i.finetune/0 [default0]:Epoch 0, global step 19: 'reduced_train_loss' reached 1.50632 (best 1.50632), saving model to 'results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0.ckpt' as top 1\n",
      "i.finetune/0 [default1]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default1]:To disable this warning, you can either:\n",
      "i.finetune/0 [default1]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default1]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:01:26 model_checkpoint:497] Scheduled async checkpoint save for results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0.ckpt\n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default1]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default1]:To disable this warning, you can either:\n",
      "i.finetune/0 [default1]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default1]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "i.finetune/0 [default0]:To disable this warning, you can either:\n",
      "i.finetune/0 [default0]:\t- Avoid using `tokenizers` before the fork if possible\n",
      "i.finetune/0 [default0]:\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:01:46 model_checkpoint:497] Scheduled async checkpoint save for results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:01:46 model_checkpoint:522] Async checkpoint save for step 10 (results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.6711-epoch=0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:01:46 model_checkpoint:522] Async checkpoint save for step 10 (results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.6711-epoch=0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:01:47 dist_ckpt_io:174] Pending async checkpoint saves. Finalizing them synchronously now\n",
      "i.finetune/0 [default0]:`Trainer.fit` stopped: `max_steps=20` reached.\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:02:00 model_checkpoint:522] Async checkpoint save for step 20 (results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2024-11-15 10:02:22 model_checkpoint:522] Async checkpoint save for step 20 (results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 I1115 10:03:41.584000 140737350272832 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "i.finetune/0 I1115 10:03:41.584000 140737350272832 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "i.finetune/0 I1115 10:03:41.584000 140737350272832 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.00024819374084472656 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.finetune-bsqgzflc7xzftd finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']</span><span style=\"background-color: #272822\">                           </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune_1731693538\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune_1731693538\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.finetune_1731693538</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.finetune_1731693538 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.finetune_1731693538 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1731693538\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1731693538\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1731693538\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def configure_finetuning_recipe():\n",
    "    return run.Partial(\n",
    "        llm.finetune,\n",
    "        model=llama3_8b(),\n",
    "        trainer=trainer(),\n",
    "        data=dolly(),\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 2) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "        \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "        \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "        \"NVTE_FUSED_ATTN\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_finetuning_recipe(), executor=local_executor_torchrun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Evaluation\n",
    "\n",
    "We use the `llm.generate` API in NeMo 2.0 to generate results from the trained SFT checkpoint. Find your last saved checkpoint from your experiment dir: `results/nemo2_sft/checkpoints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will load SFT checkpoint from: results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0-last\n"
     ]
    }
   ],
   "source": [
    "sft_ckpt_path=str(next((d for d in Path(\"./results/nemo2_sft/checkpoints/\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "print(\"We will load SFT checkpoint from:\", sft_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `llm.generate` API, you can pass a data module such as dolly: `input_dataset=dolly()`. This will use the test set from the specified data module to generate predictions. In the following example, the generated predictions are saved to the `sft_predictions.txt` file. Note that while fine-tuning required `tensor_model_parallel_size=2` minimum 2 GPUs, generating predictions only requires `tensor_model_parallel_size=1`. However, using multiple GPUs can speed up the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.generate with id: nemo.collections.llm.api.generate_1731693822</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.generate with id: nemo.collections.llm.api.generate_1731693822\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1731693822/nemo.collections.llm.api.generate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10:03:42] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.generate for experiment </span>                        <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">660</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.generate</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[10:03:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.generate for experiment \u001b[0m                        \u001b]8;id=991202;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=921254;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\u001b\\\u001b[2m660\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.generate\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1731693822/nemo.collections.llm.api.generate\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.generate-lzdnjbxr7thbv\n",
      "AppStatus:\n",
      "    State: RUNNING\n",
      "    Num Restarts: 0\n",
      "    Roles: \n",
      "    Msg: <NONE>\n",
      "    Structured Error Msg: <NONE>\n",
      "    UI URL: file:///root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1731693822/nemo.collections.llm.api.generate/nemo_run/nemo.collections.llm.api.generate-lzdnjbxr7thbv\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.generate_1731693822 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.generate_1731693822 to finish\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.generate_1731693822</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.generate_1731693822\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.generate</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.generate-lzdnjbxr7thbv\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1731693822/nemo.collections.llm.api.generate\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.generate\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.generate-lzdnjbxr7thbv\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1731693822/nemo.collections.llm.api.generate\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.generate-lzdnjbxr7thbv to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   min_nodes        : 1\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   max_nodes        : 1\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   nproc_per_node   : 1\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   run_id           : 159\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : localhost:0\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   max_restarts     : 0\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   monitor_interval : 0.1\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1731693822/nemo.collections.llm.api.generate/nemo_run/nemo.collections.llm.api.generate-lzdnjbxr7thbv/torchelastic/nemo.collections.llm.api.generate\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}\n",
      "i.generate/0 I1115 10:03:43.883000 140737350272832 torch/distributed/launcher/api.py:188] \n",
      "i.generate/0 I1115 10:03:43.886000 140737350272832 torch/distributed/elastic/agent/server/api.py:825] [default] starting workers for entrypoint: python\n",
      "i.generate/0 I1115 10:03:43.886000 140737350272832 torch/distributed/elastic/agent/server/api.py:646] [default] Rendezvous'ing worker group\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512] [default] Rendezvous complete for workers. Result:\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   restart_count=0\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   master_addr=eos0346.eos.clusters.nvidia.com\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   master_port=51515\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   group_rank=0\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   group_world_size=1\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   local_ranks=[0]\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   role_ranks=[0]\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   global_ranks=[0]\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   role_world_sizes=[1]\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512]   global_world_sizes=[1]\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:512] \n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/api.py:654] [default] Starting worker group\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/local_elastic_agent.py:184] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.generate/0 I1115 10:03:44.045000 140737350272832 torch/distributed/elastic/agent/server/local_elastic_agent.py:216] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.generate/0 [default0]:/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "i.generate/0 [default0]:  warnings.warn(\n",
      "i.generate/0 [default0]:[NeMo W 2024-11-15 10:03:50 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "i.generate/0 [default0]:      cm = get_cmap(\"Set1\")\n",
      "i.generate/0 [default0]:    \n",
      "i.generate/0 [default0]:GPU available: True (cuda), used: True\n",
      "i.generate/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "i.generate/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:396] Rank 0 has data parallel group : [0]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:402] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:407] All data parallel group ranks with context parallel combined: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:410] Ranks 0 has data parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:418] Rank 0 has context parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:421] All context parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:422] Ranks 0 has context parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:429] Rank 0 has model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:430] All model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:439] Rank 0 has tensor model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:443] All tensor model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:444] Rank 0 has tensor model parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:464] Rank 0 has pipeline model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:476] Rank 0 has embedding group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:482] All pipeline model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:483] Rank 0 has pipeline model parallel rank 0\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:484] All embedding group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 megatron_init:485] Rank 0 has embedding rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:52 base:44] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n",
      "i.generate/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "i.generate/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.generate/0 [default0]:distributed_backend=nccl\n",
      "i.generate/0 [default0]:All distributed processes registered. Starting with 1 processes\n",
      "i.generate/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.generate/0 [default0]:\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:53 megatron_parallel:550]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 8030261248\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:03:53 megatron_strategy:745] Doing selective restore from RestoreConfig(path='results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0-last', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:04:04 megatron_strategy:750] Restoring model weights from RestoreConfig(path='results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0-last', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:04:04 megatron_strategy:757] Finished restoring from RestoreConfig(path='results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0-last', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "i.generate/0 [default0]:[NeMo W 2024-11-15 10:04:04 mixin:742] Could not find <root>.model.model_transform for <class 'nemo.lightning.io.pl.TrainerContext'> in results/nemo2_sft/checkpoints/nemo2_sft--reduced_train_loss=1.5063-epoch=0-last/context/io.json\n",
      "i.generate/0 [default0]:GPU available: True (cuda), used: True\n",
      "i.generate/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "i.generate/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "i.generate/0 [default0]:[NeMo I 2024-11-15 10:30:50 api:699] Predictions written to sft_prediction.jsonl\n",
      "i.generate/0 I1115 10:30:54.035000 140737350272832 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "i.generate/0 I1115 10:30:54.036000 140737350272832 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "i.generate/0 I1115 10:30:54.036000 140737350272832 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.00029087066650390625 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.generate-lzdnjbxr7thbv finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.generate']</span><span style=\"background-color: #272822\">                           </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate_1731693822\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.generate']\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate_1731693822\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.generate_1731693822</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.generate_1731693822 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.generate_1731693822 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1731693822\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1731693822\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1731693822\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "\n",
    "\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1,\n",
    "        pipeline_model_parallel_size=1,\n",
    "        context_parallel_size=1,\n",
    "        sequence_parallel=False,\n",
    "        setup_optimizers=False,\n",
    "        store_optimizer_states=False,\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        num_nodes=1,\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def configure_inference():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        input_dataset=dolly(),\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=20, top_k=1),\n",
    "        output_path=\"sft_prediction.jsonl\",\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "        \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "        \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "        \"NVTE_FUSED_ATTN\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_inference(), executor=local_executor_torchrun())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the inference is complete, you will see results similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"What is best creator's platform\", \"category\": \"brainstorming\", \"label\": \"Youtube. Youtube should be best creator platform\", \"prediction\": \" for video content creators. YouTube is best creator's platform for video content creators.\"}\n",
      "{\"input\": \"When was the last time the Raiders won the Super Bowl?\", \"category\": \"open_qa\", \"label\": \"The Raiders have won three Super Bowl championships (1977, 1981, and 1984), one American Football League (AFL) championship (1967), and four American Football Conference (AFC) titles. The most recent Super Bowl ring was won in 1984 against the Washington Redskins of the NFC.\", \"prediction\": \" 2003\"}\n",
      "{\"input\": \"Muckle Water is a long, narrow fresh water loch on Ward Hill on Rousay, Orkney, Scotland. It is the biggest loch on the island and is popular for fishing. It can be reached by a track from the roadside. The Suso Burn on the north eastern shore drains the loch into the Sound of Rousay.\\n\\nWhere is Muckle Water?\", \"category\": \"closed_qa\", \"label\": \"Muckle water is located in Rousay, Orkney, Scotland.\", \"prediction\": \" Muckle Water is a long, narrow fresh water loch on Ward Hill on Rousay,\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 3 sft_prediction.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
