{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4d51a3-4b0b-40d0-b84d-34e23a523468",
   "metadata": {},
   "source": [
    "# Train your Reasoning Model using NeMo 2.0\n",
    "\n",
    "This tutorial shows how to fine-tune Meta‚Äôs LLaMA 3‚Äì8B Instruct model using NVIDIA NeMo and supervised fine-tuning (SFT). You'll train the model on complex instruction-following and reasoning tasks using the[Llama-Nemotron-Post-Training-Data](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)\n",
    "\n",
    "### ‚úÖ What You'll Learn\n",
    "1. Load and preprocess a reasoning-focused instruction dataset.\n",
    "2. Apply SFT with NeMo 2.0.\n",
    "3. Train using NeMo's distributed, mixed-precision trainer.\n",
    "4. Save a fine-tuned checkpoint ready for evaluation or deployment.\n",
    "\n",
    "### üöÄ Ideal For\n",
    "1. Multi-turn reasoning (e.g., chain-of-thought)\n",
    "2. Domain-specific instruction following\n",
    "3. Question answering, dialogue systems, and agentic behaviors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd940e9-0c80-4021-966b-3917d6650fe7",
   "metadata": {},
   "source": [
    "## Step 1. Convert HuggingFace Checkpoint to NeMo Format\n",
    "\n",
    "Before training, we need to convert the HuggingFace LLaMA 3‚Äì8B Instruct checkpoint into NeMo format. NeMo provides a built-in utility ```llm.import_ckpt()``` to handle this conversion.\n",
    "\n",
    "### ‚ö†Ô∏è This step only needs to be run once per model.\n",
    "After conversion, the model can be loaded and fine-tuned using NeMo APIs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013764ee-95ed-44fa-85b9-4296d37e7c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">‚îÄ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1747804‚Ä¶</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ‚îÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m‚îÄ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1747804‚Ä¶\u001b[0m\u001b[92m ‚îÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1747804885/nemo.collections.llm.api.import_ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:21:25] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.import_ckpt for experiment </span>                     <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05:21:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.import_ckpt for experiment \u001b[0m                     \u001b]8;id=708539;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=374137;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.import_ckpt\u001b[0m                                                   \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1747804885/nemo.collections.llm.api.import_ckpt\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.import_ckpt-kjwcwsrkxgj37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.import_ckpt_1747804885 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.import_ckpt_1747804885 to finish\u001b[0m\u001b[92m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt_1747804885</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt_1747804885\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.import_ckpt-kjwcwsrkxgj37\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1747804885/nemo.collections.llm.api.import_ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.import_ckpt-kjwcwsrkxgj37\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1747804885/nemo.collections.llm.api.import_ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.import_ckpt-kjwcwsrkxgj37 to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_fwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_bwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_fwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_bwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_fwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_bwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_fwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "mport_ckpt/0   @custom_bwd\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 \u001b[32m $\u001b[0m\u001b[32mNEMO_MODELS_CACHE\u001b[0m\u001b[32m=\u001b[0m\u001b[32m/root/.cache/nemo/\u001b[0m\u001b[32mmodels\u001b[0m\u001b[32m \u001b[0m\n",
      "mport_ckpt/0 \u001b[32m‚úì Checkpoint imported to \u001b[0m\u001b[32m/root/.cache/nemo/models/\u001b[0m\u001b[32mMeta-Llama-3-8B-Instruct\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.import_ckpt-kjwcwsrkxgj37 finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']</span><span style=\"background-color: #272822\">                        </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt_1747804885\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1747804885\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.import_ckpt_1747804885</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.import_ckpt_1747804885 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.import_ckpt_1747804885 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1747804885\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1747804885\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1747804885\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from nemo.lightning.pytorch.optim import CosineAnnealingScheduler, MegatronOptimizerModule, PytorchOptimizerModule\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure the import from HuggingFace format to NeMo format\n",
    "def configure_checkpoint_conversion():\n",
    "    return run.Partial(\n",
    "        llm.import_ckpt,\n",
    "        model=llm.llama3_8b.model(),  # Predefined LLaMA 3 8B model structure\n",
    "        source=\"hf:///workspace/Meta-Llama-3-8B-Instruct\",  # Path to HF checkpoint (local or HF hub)\n",
    "        overwrite=False,  # Set to True if you want to overwrite an existing NeMo checkpoint\n",
    "    )\n",
    "\n",
    "# Create the configured import task\n",
    "import_ckpt = configure_checkpoint_conversion()\n",
    "\n",
    "# Define the local executor (single-node)\n",
    "local_executor = run.LocalExecutor()\n",
    "\n",
    "# Execute the checkpoint conversion\n",
    "run.run(import_ckpt, executor=local_executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148354c-af20-44f0-bbda-bf1362fd2c24",
   "metadata": {},
   "source": [
    "‚úì Checkpoint imported to /root/.cache/nemo/models/Meta-Llama-3-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1492948-5608-4596-b571-f141ec6bde9e",
   "metadata": {},
   "source": [
    "## Step 2. Prepare Data\n",
    "\n",
    "In this section, we define the configuration for loading and preprocessing an instruction-tuning dataset using NeMo‚Äôs FineTuningDataModule. The dataset is expected to be in a structured format (e.g. JSONL), stored locally as ```training.jsonl```.\n",
    "\n",
    "The training-related parameters like batch size, number of workers, memory mapping, and device count can be modified based on the size of the model, dataset size and compute resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca01f0bc-fdaa-4023-97cb-c5a26f0840a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "from nemo.collections.llm.gpt.data.core import get_dataset_root\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.collections import llm\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.utils import logging\n",
    "\n",
    "N_DEVICES = 4\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "experiment_name = \"baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers\"\n",
    "\n",
    "# Define fine-tuning dataset configuration\n",
    "finetune_config = run.Config(\n",
    "    llm.FineTuningDataModule,\n",
    "    dataset_root=\"/workspace\",       # Path to your preprocessed dataset (JSONL, etc.)\n",
    "    seq_length=8192,                 # Max sequence length for input tokens\n",
    "    micro_batch_size=1,              # Per-device batch size\n",
    "    global_batch_size=256,           # Total batch size across all devices\n",
    "    seed=1234,                       # Seed for reproducibility\n",
    "    memmap_workers=1,                # Use memory-mapped dataset format for performance\n",
    "    num_workers=8,                   # DataLoader worker threads\n",
    "    pin_memory=True,                 # Optimize data transfer to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4d9d6-0b90-4562-a9b8-7ac947ba851f",
   "metadata": {},
   "source": [
    "## Step 3. Configure SFT with the NeMo 2.0 API\n",
    "\n",
    "In this step, we'll use the modular NeMo 2.0 API to configure:\n",
    "\n",
    "* The distributed trainer\n",
    "\n",
    "* Logging and checkpointing\n",
    "\n",
    "* Optimizer with cosine annealing scheduler\n",
    "\n",
    "* Model definition and resume behavior\n",
    "\n",
    "* Final recipe assembly for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214912f1-2cd3-4db2-a295-9efb8612a043",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 3.1 Configure the Trainer\n",
    "We define the training strategy with Megatron's Distributed Training strategy using tensor model parallelism and enabling mixed precision with bf16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0abe9b85-a9b6-418a-b013-ed5459a1e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=4,\n",
    "        optimizer_cpu_offload=True\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        devices=4,\n",
    "        num_nodes=1,\n",
    "        max_steps=100,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "        log_every_n_steps=50,\n",
    "        limit_val_batches=0,\n",
    "        val_check_interval=0,\n",
    "        num_sanity_val_steps=0,\n",
    "        use_distributed_sampler=False,\n",
    "    )\n",
    "    return trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88dd49-314f-42fb-a4fb-30ad3d4b490d",
   "metadata": {},
   "source": [
    "### üìù 3.2 Configure Logging and Checkpointing\n",
    "Logs metrics and periodically saves model checkpoints during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "802508e1-8051-431f-bd54-44ffe4d292d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger() -> run.Config[nl.NeMoLogger]:\n",
    "    ckpt = run.Config(\n",
    "        nl.ModelCheckpoint,\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return run.Config(\n",
    "        nl.NeMoLogger,\n",
    "        name=f\"trained-model-checkpoints\",\n",
    "        log_dir=f\"./results-{timestamp}-{N_DEVICES}-devices-{experiment_name}\",\n",
    "        use_datetime_version=True,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489a8ab-be37-406b-a11e-b6a597604614",
   "metadata": {},
   "source": [
    "### üìà 3.3 Configure Optimizer with Cosine Annealing\n",
    "Uses the Adam optimizer with gradient clipping, distributed optimizer support, and a cosine annealing learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8badf059-c275-436c-ab5e-5db2d6c32839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron.core.optimizer import OptimizerConfig\n",
    "\n",
    "def lr_scheduler():\n",
    "    return run.Config(\n",
    "        CosineAnnealingScheduler,\n",
    "        warmup_steps=100,        \n",
    "        constant_steps=1000,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "    \n",
    "def adam_with_cosine_annealing() -> run.Config[nl.OptimizerModule]:\n",
    "    opt_cfg = run.Config(\n",
    "        OptimizerConfig,\n",
    "        optimizer=\"adam\",\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.001,\n",
    "        use_distributed_optimizer=True,\n",
    "        clip_grad=1.0,\n",
    "        bf16=True,\n",
    "    )\n",
    "    \n",
    "    return run.Config(\n",
    "        nl.MegatronOptimizerModule,\n",
    "        config=opt_cfg,\n",
    "        lr_scheduler=lr_scheduler(), \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17225cf-6fa1-48ef-95e8-54b57efcf967",
   "metadata": {},
   "source": [
    "### üß† 3.4 Define the Base Model and Resume Logic\n",
    "We use the built-in LLaMA 3 8B config from NeMo and optionally resume from a previously saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a003f8c-4947-49b8-bd24-8712fcf87532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_8b() -> run.Config[pl.LightningModule]:\n",
    "    return run.Config(llm.LlamaModel, config=run.Config(llm.Llama3Config8B))\n",
    "\n",
    "def resume() -> run.Config[nl.AutoResume]:\n",
    "    return run.Config(\n",
    "        nl.AutoResume,\n",
    "        restore_config=run.Config(\n",
    "            nl.RestoreConfig,\n",
    "            path=\"nemo://Meta-Llama-3-8B-Instruct\",  # Change to local path if needed\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe4c09-ea6d-4dd0-9953-cbdd797225ac",
   "metadata": {},
   "source": [
    "### üì¶ 3.5 Assemble the Fine-Tuning Recipe\n",
    "This ties together the model, trainer, dataset config, optimizer, and logger into a single training recipe using NeMo‚Äôs run.Partial system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c175cd8-c4fd-4f24-a206-435a79965131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_finetuning_recipe():\n",
    "    return run.Partial(\n",
    "        llm.finetune,\n",
    "        model=llama3_8b(),\n",
    "        trainer=trainer(),\n",
    "        data=finetune_config,  # From earlier step\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5592c-abfb-4489-b8a2-67b4164346b5",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Step 4: Run Supervised Fine-Tuning (SFT) with NeMo 2.0 and nemo-run\n",
    "Now that everything is configured (model, trainer, optimizer, logging, and data), it's time to launch the training job using nemo-run's LocalExecutor.\n",
    "\n",
    "This will:\n",
    "\n",
    "* Use torchrun to launch a multi-GPU job\n",
    "\n",
    "* Set environment variables for optimized NCCL behavior\n",
    "\n",
    "* Kick off the training loop with your full configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c837f7c7-ad01-4bee-87ba-2e52f919a38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1747804913</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ‚îÄ‚îÄ‚îÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m‚îÄ‚îÄ‚îÄ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1747804913\u001b[0m\u001b[92m ‚îÄ‚îÄ‚îÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1747804913/nemo.collections.llm.api.finetune\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:21:53] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.finetune for experiment </span>                        <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.finetune</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05:21:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.finetune for experiment \u001b[0m                        \u001b]8;id=681737;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=944143;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.finetune\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1747804913/nemo.collections.llm.api.finetune\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.finetune-ks9cdpj3k4r5kc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.finetune_1747804913 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.finetune_1747804913 to finish\u001b[0m\u001b[92m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune_1747804913</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.finetune_1747804913\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.finetune-ks9cdpj3k4r5kc\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1747804913/nemo.collections.llm.api.finetune\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.finetune\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.finetune-ks9cdpj3k4r5kc\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1747804913/nemo.collections.llm.api.finetune\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.finetune-ks9cdpj3k4r5kc to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.finetune/0 I0521 05:21:55.683000 6205 torch/distributed/run.py:675] Using nproc_per_node=4.\n",
      "i.finetune/0 W0521 05:21:55.684000 6205 torch/distributed/run.py:792] \n",
      "i.finetune/0 W0521 05:21:55.684000 6205 torch/distributed/run.py:792] *****************************************\n",
      "i.finetune/0 W0521 05:21:55.684000 6205 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "i.finetune/0 W0521 05:21:55.684000 6205 torch/distributed/run.py:792] *****************************************\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194] Starting elastic_operator with launch configs:\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   min_nodes        : 1\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   max_nodes        : 1\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   nproc_per_node   : 4\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   run_id           : 4715\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   rdzv_backend     : c10d\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   rdzv_endpoint    : localhost:0\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   rdzv_configs     : {'timeout': 900}\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   max_restarts     : 0\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   monitor_interval : 0.1\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1747804913/nemo.collections.llm.api.finetune/nemo_run/nemo.collections.llm.api.finetune-ks9cdpj3k4r5kc/torchelastic/nemo.collections.llm.api.finetune\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194]   metrics_cfg      : {}\n",
      "i.finetune/0 I0521 05:21:55.685000 6205 torch/distributed/launcher/api.py:194] \n",
      "i.finetune/0 I0521 05:21:55.690000 6205 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python\n",
      "i.finetune/0 I0521 05:21:55.690000 6205 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   master_addr=dgx-003.localdomain\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   master_port=37529\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0, 1, 2, 3]\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0, 1, 2, 3]\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0, 1, 2, 3]\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[4, 4, 4, 4]\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[4, 4, 4, 4]\n",
      "i.finetune/0 I0521 05:21:55.708000 6205 torch/distributed/elastic/agent/server/api.py:525] \n",
      "i.finetune/0 I0521 05:21:55.709000 6205 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group\n",
      "i.finetune/0 I0521 05:21:55.710000 6205 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True\n",
      "i.finetune/0 I0521 05:21:55.711000 6205 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.finetune/0 I0521 05:21:55.711000 6205 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_fwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_bwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_fwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_bwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_fwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_bwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_fwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_bwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_fwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_bwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_fwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default1]:  @custom_bwd\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_fwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_bwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_fwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_bwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_fwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_bwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_fwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_bwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_fwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default3]:  @custom_bwd\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_fwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default0]:  @custom_bwd\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_fwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_bwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_fwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_bwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_fwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_bwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/llm/api.py:1032: UserWarning: Setting pipeline dtype to None because pipeline model parallelism is disabled\n",
      "i.finetune/0 [default1]:  warnings.warn(\"Setting pipeline dtype to None because pipeline model parallelism is disabled\")\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_fwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "i.finetune/0 [default2]:  @custom_bwd\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Experiments will be logged at results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/llm/api.py:1032: UserWarning: Setting pipeline dtype to None because pipeline model parallelism is disabled\n",
      "i.finetune/0 [default0]:  warnings.warn(\"Setting pipeline dtype to None because pipeline model parallelism is disabled\")\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[NeMo W 2025-05-21 05:22:13 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "i.finetune/0 [default0]:[NeMo W 2025-05-21 05:22:13 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers\n",
      "i.finetune/0 [default0]:[NeMo W 2025-05-21 05:22:13 nemo_logging:405] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/llm/api.py:1032: UserWarning: Setting pipeline dtype to None because pipeline model parallelism is disabled\n",
      "i.finetune/0 [default3]:  warnings.warn(\"Setting pipeline dtype to None because pipeline model parallelism is disabled\")\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/llm/api.py:1032: UserWarning: Setting pipeline dtype to None because pipeline model parallelism is disabled\n",
      "i.finetune/0 [default2]:  warnings.warn(\"Setting pipeline dtype to None because pipeline model parallelism is disabled\")\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has model parallel group: [0, 1, 2, 3]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has tensor model parallel group: [0, 1, 2, 3]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:13 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "i.finetune/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:distributed_backend=nccl\n",
      "i.finetune/0 [default0]:All distributed processes registered. Starting with 4 processes\n",
      "i.finetune/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default2]:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "i.finetune/0 [default3]:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "i.finetune/0 [default1]:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:24 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/models/gpt/gpt_layer_specs.py:81: UserWarning: The fp8 argument in \"get_gpt_layer_with_transformer_engine_spec\" has been deprecated and will be removed soon. Please update your code accordingly.\n",
      "i.finetune/0 [default0]:  warnings.warn(\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/models/gpt/gpt_layer_specs.py:81: UserWarning: The fp8 argument in \"get_gpt_layer_with_transformer_engine_spec\" has been deprecated and will be removed soon. Please update your code accordingly.\n",
      "i.finetune/0 [default2]:  warnings.warn(\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/models/gpt/gpt_layer_specs.py:81: UserWarning: The fp8 argument in \"get_gpt_layer_with_transformer_engine_spec\" has been deprecated and will be removed soon. Please update your code accordingly.\n",
      "i.finetune/0 [default3]:  warnings.warn(\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/models/gpt/gpt_layer_specs.py:81: UserWarning: The fp8 argument in \"get_gpt_layer_with_transformer_engine_spec\" has been deprecated and will be removed soon. Please update your code accordingly.\n",
      "i.finetune/0 [default1]:  warnings.warn(\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/base.py:866: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "i.finetune/0 [default0]:  warnings.warn(\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/base.py:866: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "i.finetune/0 [default2]:  warnings.warn(\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/base.py:866: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "i.finetune/0 [default3]:  warnings.warn(\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/base.py:866: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "i.finetune/0 [default1]:  warnings.warn(\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 num_microbatches_calculator:228] setting number of microbatches to constant 256\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 2008289280\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n",
      "i.finetune/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "i.finetune/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "i.finetune/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "i.finetune/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "i.finetune/0 [default0]:    Params for bucket 1 (2008289280 elements):\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.output_layer.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.31.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.final_layernorm.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.embedding.word_embeddings.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.29.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.30.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.28.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.001, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 nemo_logging:393] Doing selective restore from RestoreConfig(path='/root/.cache/nemo/models/Meta-Llama-3-8B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:27 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7fc758af5be0> dist-ckpt load strategy.\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default0]:  warnings.warn(\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default2]:  warnings.warn(\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default3]:  warnings.warn(\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default1]:  warnings.warn(\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "i.finetune/0 [default2]:  checkpoint.load_state_dict(\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "i.finetune/0 [default2]:  device = getattr(value, \"device\", None)\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "i.finetune/0 [default3]:  checkpoint.load_state_dict(\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "i.finetune/0 [default3]:  device = getattr(value, \"device\", None)\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "i.finetune/0 [default1]:  checkpoint.load_state_dict(\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "i.finetune/0 [default1]:  device = getattr(value, \"device\", None)\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "i.finetune/0 [default0]:  checkpoint.load_state_dict(\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "i.finetune/0 [default0]:  device = getattr(value, \"device\", None)\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:52 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1747804947.439s : Time spent in load_checkpoint: 25.295s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:52 nemo_logging:393] Restoring model weights from RestoreConfig(path='/root/.cache/nemo/models/Meta-Llama-3-8B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:52 nemo_logging:393] Finished restoring from RestoreConfig(path='/root/.cache/nemo/models/Meta-Llama-3-8B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:52 nemo_logging:393] Building data files\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:52 nemo_logging:393] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:  | Name   | Type | Params | Mode \n",
      "i.finetune/0 [default0]:----------------------------------------\n",
      "i.finetune/0 [default0]:0 | module | DDP  | 2.0 B  | train\n",
      "i.finetune/0 [default0]:----------------------------------------\n",
      "i.finetune/0 [default0]:2.0 B     Trainable params\n",
      "i.finetune/0 [default0]:0         Non-trainable params\n",
      "i.finetune/0 [default0]:2.0 B     Total params\n",
      "i.finetune/0 [default0]:8,033.157 Total estimated model params size (MB)\n",
      "i.finetune/0 [default0]:651       Modules in train mode\n",
      "i.finetune/0 [default0]:0         Modules in eval mode\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:52 nemo_logging:393] Time building 0 / 1 mem-mapped files: 0:00:00.093415\n",
      "i.finetune/0 [default0]:[rank: 0] Received SIGTERM: 15\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:53 nemo_logging:393] Processing 1 data files using 1 workers\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:53 nemo_logging:393] Time building 0 / 1 mem-mapped files: 0:00:00.092113\n",
      "i.finetune/0 [default0]:[rank: 0] Received SIGTERM: 15\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:53 nemo_logging:393] Loading data files\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:53 nemo_logging:393] Loading /workspace/training.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:53 nemo_logging:393] Time loading 1 mem-mapped files: 0:00:00.001041\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:22:53 nemo_logging:393] Computing global indices\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1347: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "i.finetune/0 [default0]:  counts = torch.cuda.LongTensor([1])\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1347: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "i.finetune/0 [default2]:  counts = torch.cuda.LongTensor([1])\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1347: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "i.finetune/0 [default3]:  counts = torch.cuda.LongTensor([1])\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1347: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "i.finetune/0 [default1]:  counts = torch.cuda.LongTensor([1])\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:661: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "i.finetune/0 [default0]:  warnings.warn(\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:661: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "i.finetune/0 [default2]:  warnings.warn(\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:661: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "i.finetune/0 [default3]:  warnings.warn(\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:661: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "i.finetune/0 [default1]:  warnings.warn(\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default0]:[NeMo W 2025-05-21 05:26:11 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!\n",
      "i.finetune/0 [default0]:[NeMo W 2025-05-21 05:26:11 rerun_state_machine:211] RerunStateMachine initialized in mode RerunMode.DISABLED\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 0/99 | lr: 0 | global_batch_size: 256 | global_step: 0 | reduced_train_loss: 1.07\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 1/99 | lr: 1e-06 | global_batch_size: 256 | global_step: 1 | reduced_train_loss: 1.059 | consumed_samples: 512\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 2/99 | lr: 2e-06 | global_batch_size: 256 | global_step: 2 | reduced_train_loss: 1.058 | consumed_samples: 768\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 3/99 | lr: 3e-06 | global_batch_size: 256 | global_step: 3 | reduced_train_loss: 1.037 | consumed_samples: 1024\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 4/99 | lr: 4e-06 | global_batch_size: 256 | global_step: 4 | reduced_train_loss: 0.9848 | consumed_samples: 1280\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 5/99 | lr: 5e-06 | global_batch_size: 256 | global_step: 5 | reduced_train_loss: 0.9336 | consumed_samples: 1536\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 6/99 | lr: 6e-06 | global_batch_size: 256 | global_step: 6 | reduced_train_loss: 0.8914 | consumed_samples: 1792\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 7/99 | lr: 7e-06 | global_batch_size: 256 | global_step: 7 | reduced_train_loss: 0.9167 | consumed_samples: 2048\n",
      "i.finetune/0 [default0]:[rank0]:W0521 05:34:20.464000 6276 torch/_dynamo/convert_frame.py:915] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default0]:[rank0]:W0521 05:34:20.464000 6276 torch/_dynamo/convert_frame.py:915] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default0]:[rank0]:W0521 05:34:20.464000 6276 torch/_dynamo/convert_frame.py:915] [4/8]    last reason: 4/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 3392, actual 2944\n",
      "i.finetune/0 [default0]:[rank0]:W0521 05:34:20.464000 6276 torch/_dynamo/convert_frame.py:915] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default0]:[rank0]:W0521 05:34:20.464000 6276 torch/_dynamo/convert_frame.py:915] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default3]:[rank3]:W0521 05:34:20.463000 6279 torch/_dynamo/convert_frame.py:915] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default3]:[rank3]:W0521 05:34:20.463000 6279 torch/_dynamo/convert_frame.py:915] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default3]:[rank3]:W0521 05:34:20.463000 6279 torch/_dynamo/convert_frame.py:915] [4/8]    last reason: 4/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 3392, actual 2944\n",
      "i.finetune/0 [default3]:[rank3]:W0521 05:34:20.463000 6279 torch/_dynamo/convert_frame.py:915] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default3]:[rank3]:W0521 05:34:20.463000 6279 torch/_dynamo/convert_frame.py:915] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default1]:[rank1]:W0521 05:34:20.461000 6277 torch/_dynamo/convert_frame.py:915] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default1]:[rank1]:W0521 05:34:20.461000 6277 torch/_dynamo/convert_frame.py:915] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default1]:[rank1]:W0521 05:34:20.461000 6277 torch/_dynamo/convert_frame.py:915] [4/8]    last reason: 4/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 3392, actual 2944\n",
      "i.finetune/0 [default1]:[rank1]:W0521 05:34:20.461000 6277 torch/_dynamo/convert_frame.py:915] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default1]:[rank1]:W0521 05:34:20.461000 6277 torch/_dynamo/convert_frame.py:915] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default2]:[rank2]:W0521 05:34:20.460000 6278 torch/_dynamo/convert_frame.py:915] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "i.finetune/0 [default2]:[rank2]:W0521 05:34:20.460000 6278 torch/_dynamo/convert_frame.py:915] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "i.finetune/0 [default2]:[rank2]:W0521 05:34:20.460000 6278 torch/_dynamo/convert_frame.py:915] [4/8]    last reason: 4/0: tensor 'L['exp_logits']' size mismatch at index 0. expected 3392, actual 2944\n",
      "i.finetune/0 [default2]:[rank2]:W0521 05:34:20.460000 6278 torch/_dynamo/convert_frame.py:915] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "i.finetune/0 [default2]:[rank2]:W0521 05:34:20.460000 6278 torch/_dynamo/convert_frame.py:915] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 8/99 | lr: 8e-06 | global_batch_size: 256 | global_step: 8 | reduced_train_loss: 0.8637 | consumed_samples: 2304\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 9/99 | lr: 9e-06 | global_batch_size: 256 | global_step: 9 | reduced_train_loss: 0.8661 | consumed_samples: 2560\n",
      "i.finetune/0 [default1]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default1]:  warnings.warn(\n",
      "i.finetune/0 [default1]:\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 9: 'reduced_train_loss' reached 0.86609 (best 0.86609), saving model to 'results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.8661-epoch=0-consumed_samples=2560.0.ckpt' as top 1\n",
      "i.finetune/0 [default0]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default0]:  warnings.warn(\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default3]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default3]:  warnings.warn(\n",
      "i.finetune/0 [default3]:\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:36:35 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.\n",
      "i.finetune/0 [default2]:[WARNING  | py.warnings        ]: /opt/megatron-lm/megatron/core/transformer/transformer_layer.py:339: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "i.finetune/0 [default2]:  warnings.warn(\n",
      "i.finetune/0 [default2]:\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:39:14 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 9 : Start time: 1747805795.381s : Save duration: 158.745s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:39:14 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.8661-epoch=0-consumed_samples=2560.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:41:14 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 9 : Start time: 1747805954.330s : Save duration: 119.686s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:41:14 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.8661-epoch=0-consumed_samples=2560.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:41:19 nemo_logging:393] Successfully saved checkpoint from iteration       9 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.8661-epoch=0-consumed_samples=2560.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:41:19 nemo_logging:393] Async checkpoint save for step 10 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.8661-epoch=0-consumed_samples=2560.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:41:19 nemo_logging:393] Async finalization time took 5.453 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 10/99 | lr: 1e-05 | global_batch_size: 256 | global_step: 10 | reduced_train_loss: 0.8224 | consumed_samples: 2816\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:42:16 nemo_logging:393] Successfully saved checkpoint from iteration       9 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.8661-epoch=0-consumed_samples=2560.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:42:16 nemo_logging:393] Async checkpoint save for step 10 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.8661-epoch=0-consumed_samples=2560.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:42:16 nemo_logging:393] Async finalization time took 0.075 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 11/99 | lr: 1.1e-05 | global_batch_size: 256 | global_step: 11 | reduced_train_loss: 0.8075 | consumed_samples: 3072\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 12/99 | lr: 1.2e-05 | global_batch_size: 256 | global_step: 12 | reduced_train_loss: 0.7739 | consumed_samples: 3328\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 13/99 | lr: 1.3e-05 | global_batch_size: 256 | global_step: 13 | reduced_train_loss: 0.7434 | consumed_samples: 3584\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 14/99 | lr: 1.4e-05 | global_batch_size: 256 | global_step: 14 | reduced_train_loss: 0.7712 | consumed_samples: 3840\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 15/99 | lr: 1.5e-05 | global_batch_size: 256 | global_step: 15 | reduced_train_loss: 0.7487 | consumed_samples: 4096\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 16/99 | lr: 1.6e-05 | global_batch_size: 256 | global_step: 16 | reduced_train_loss: 0.7329 | consumed_samples: 4352\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 17/99 | lr: 1.7e-05 | global_batch_size: 256 | global_step: 17 | reduced_train_loss: 0.7146 | consumed_samples: 4608\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 18/99 | lr: 1.8e-05 | global_batch_size: 256 | global_step: 18 | reduced_train_loss: 0.7116 | consumed_samples: 4864\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 19/99 | lr: 1.9e-05 | global_batch_size: 256 | global_step: 19 | reduced_train_loss: 0.7035 | consumed_samples: 5120\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 19: 'reduced_train_loss' reached 0.70349 (best 0.70349), saving model to 'results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.7035-epoch=0-consumed_samples=5120.0.ckpt' as top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:51:38 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 19 : Start time: 1747806696.740s : Save duration: 1.304s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:51:40 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.7035-epoch=0-consumed_samples=5120.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:51:44 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 19 : Start time: 1747806700.242s : Save duration: 4.054s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:51:44 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.7035-epoch=0-consumed_samples=5120.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:51:44 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 20/99 | lr: 2e-05 | global_batch_size: 256 | global_step: 20 | reduced_train_loss: 0.686 | consumed_samples: 5376\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:52:41 nemo_logging:393] Successfully saved checkpoint from iteration      19 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.7035-epoch=0-consumed_samples=5120.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:52:41 nemo_logging:393] Async checkpoint save for step 20 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.7035-epoch=0-consumed_samples=5120.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:53:10 nemo_logging:393] Successfully saved checkpoint from iteration      19 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.7035-epoch=0-consumed_samples=5120.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:53:10 nemo_logging:393] Async checkpoint save for step 20 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.7035-epoch=0-consumed_samples=5120.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 05:53:43 nemo_logging:393] Async finalization time took 62.356 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 21/99 | lr: 2.1e-05 | global_batch_size: 256 | global_step: 21 | reduced_train_loss: 0.7065 | consumed_samples: 5632\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 22/99 | lr: 2.2e-05 | global_batch_size: 256 | global_step: 22 | reduced_train_loss: 0.7172 | consumed_samples: 5888\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 23/99 | lr: 2.3e-05 | global_batch_size: 256 | global_step: 23 | reduced_train_loss: 0.6947 | consumed_samples: 6144\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 24/99 | lr: 2.4e-05 | global_batch_size: 256 | global_step: 24 | reduced_train_loss: 0.6745 | consumed_samples: 6400\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 25/99 | lr: 2.5e-05 | global_batch_size: 256 | global_step: 25 | reduced_train_loss: 0.6614 | consumed_samples: 6656\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 26/99 | lr: 2.6e-05 | global_batch_size: 256 | global_step: 26 | reduced_train_loss: 0.6812 | consumed_samples: 6912\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 27/99 | lr: 2.7e-05 | global_batch_size: 256 | global_step: 27 | reduced_train_loss: 0.6574 | consumed_samples: 7168\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 28/99 | lr: 2.8e-05 | global_batch_size: 256 | global_step: 28 | reduced_train_loss: 0.6872 | consumed_samples: 7424\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 29/99 | lr: 2.9e-05 | global_batch_size: 256 | global_step: 29 | reduced_train_loss: 0.6731 | consumed_samples: 7680\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 29: 'reduced_train_loss' reached 0.67313 (best 0.67313), saving model to 'results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6731-epoch=0-consumed_samples=7680.0.ckpt' as top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:03:32 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 29 : Start time: 1747807411.947s : Save duration: 0.921s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:03:35 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6731-epoch=0-consumed_samples=7680.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:03:37 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 29 : Start time: 1747807415.122s : Save duration: 2.385s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:03:38 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6731-epoch=0-consumed_samples=7680.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:03:38 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 30/99 | lr: 3e-05 | global_batch_size: 256 | global_step: 30 | reduced_train_loss: 0.6734 | consumed_samples: 7936\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:05:11 nemo_logging:393] Successfully saved checkpoint from iteration      29 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6731-epoch=0-consumed_samples=7680.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:05:11 nemo_logging:393] Async checkpoint save for step 30 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6731-epoch=0-consumed_samples=7680.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:05:40 nemo_logging:393] Successfully saved checkpoint from iteration      29 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6731-epoch=0-consumed_samples=7680.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:05:40 nemo_logging:393] Async checkpoint save for step 30 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6731-epoch=0-consumed_samples=7680.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:06:14 nemo_logging:393] Async finalization time took 63.066 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 31/99 | lr: 3.1e-05 | global_batch_size: 256 | global_step: 31 | reduced_train_loss: 0.6689 | consumed_samples: 8192\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 32/99 | lr: 3.2e-05 | global_batch_size: 256 | global_step: 32 | reduced_train_loss: 0.6621 | consumed_samples: 8448\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 33/99 | lr: 3.3e-05 | global_batch_size: 256 | global_step: 33 | reduced_train_loss: 0.6509 | consumed_samples: 8704\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 34/99 | lr: 3.4e-05 | global_batch_size: 256 | global_step: 34 | reduced_train_loss: 0.6529 | consumed_samples: 8960\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 35/99 | lr: 3.5e-05 | global_batch_size: 256 | global_step: 35 | reduced_train_loss: 0.6506 | consumed_samples: 9216\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 36/99 | lr: 3.6e-05 | global_batch_size: 256 | global_step: 36 | reduced_train_loss: 0.658 | consumed_samples: 9472\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 37/99 | lr: 3.7e-05 | global_batch_size: 256 | global_step: 37 | reduced_train_loss: 0.658 | consumed_samples: 9728\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 38/99 | lr: 3.8e-05 | global_batch_size: 256 | global_step: 38 | reduced_train_loss: 0.6413 | consumed_samples: 9984\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 39/99 | lr: 3.9e-05 | global_batch_size: 256 | global_step: 39 | reduced_train_loss: 0.6564 | consumed_samples: 10240\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 39: 'reduced_train_loss' reached 0.65640 (best 0.65640), saving model to 'results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6564-epoch=0-consumed_samples=10240.0.ckpt' as top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:16:39 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 39 : Start time: 1747808198.381s : Save duration: 1.569s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:16:42 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6564-epoch=0-consumed_samples=10240.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:16:46 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 39 : Start time: 1747808202.119s : Save duration: 4.612s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:16:46 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6564-epoch=0-consumed_samples=10240.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:16:46 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 40/99 | lr: 4e-05 | global_batch_size: 256 | global_step: 40 | reduced_train_loss: 0.6631 | consumed_samples: 10496\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:17:57 nemo_logging:393] Successfully saved checkpoint from iteration      39 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6564-epoch=0-consumed_samples=10240.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:17:57 nemo_logging:393] Async checkpoint save for step 40 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6564-epoch=0-consumed_samples=10240.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:18:30 nemo_logging:393] Successfully saved checkpoint from iteration      39 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6564-epoch=0-consumed_samples=10240.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:18:30 nemo_logging:393] Async checkpoint save for step 40 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6564-epoch=0-consumed_samples=10240.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:19:00 nemo_logging:393] Async finalization time took 63.106 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 41/99 | lr: 4.1e-05 | global_batch_size: 256 | global_step: 41 | reduced_train_loss: 0.6323 | consumed_samples: 10752\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 42/99 | lr: 4.2e-05 | global_batch_size: 256 | global_step: 42 | reduced_train_loss: 0.6479 | consumed_samples: 11008\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 43/99 | lr: 4.3e-05 | global_batch_size: 256 | global_step: 43 | reduced_train_loss: 0.6536 | consumed_samples: 11264\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 44/99 | lr: 4.4e-05 | global_batch_size: 256 | global_step: 44 | reduced_train_loss: 0.6375 | consumed_samples: 11520\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 45/99 | lr: 4.5e-05 | global_batch_size: 256 | global_step: 45 | reduced_train_loss: 0.661 | consumed_samples: 11776\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 46/99 | lr: 4.6e-05 | global_batch_size: 256 | global_step: 46 | reduced_train_loss: 0.6497 | consumed_samples: 12032\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 47/99 | lr: 4.7e-05 | global_batch_size: 256 | global_step: 47 | reduced_train_loss: 0.6288 | consumed_samples: 12288\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 48/99 | lr: 4.8e-05 | global_batch_size: 256 | global_step: 48 | reduced_train_loss: 0.6359 | consumed_samples: 12544\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 49/99 | lr: 4.9e-05 | global_batch_size: 256 | global_step: 49 | reduced_train_loss: 0.6401 | consumed_samples: 12800\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 49: 'reduced_train_loss' reached 0.64007 (best 0.64007), saving model to 'results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6401-epoch=0-consumed_samples=12800.0.ckpt' as top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:30:41 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 49 : Start time: 1747809039.867s : Save duration: 1.183s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:30:43 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6401-epoch=0-consumed_samples=12800.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:30:45 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 49 : Start time: 1747809043.250s : Save duration: 1.969s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:30:47 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6401-epoch=0-consumed_samples=12800.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:30:47 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 50/99 | lr: 5e-05 | global_batch_size: 256 | global_step: 50 | reduced_train_loss: 0.6352 | consumed_samples: 13056\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:32:28 nemo_logging:393] Successfully saved checkpoint from iteration      49 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6401-epoch=0-consumed_samples=12800.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:32:28 nemo_logging:393] Async checkpoint save for step 50 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6401-epoch=0-consumed_samples=12800.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:33:01 nemo_logging:393] Successfully saved checkpoint from iteration      49 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6401-epoch=0-consumed_samples=12800.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:33:01 nemo_logging:393] Async checkpoint save for step 50 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6401-epoch=0-consumed_samples=12800.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:33:35 nemo_logging:393] Async finalization time took 66.696 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 51/99 | lr: 5.1e-05 | global_batch_size: 256 | global_step: 51 | reduced_train_loss: 0.6356 | consumed_samples: 13312\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 52/99 | lr: 5.2e-05 | global_batch_size: 256 | global_step: 52 | reduced_train_loss: 0.6342 | consumed_samples: 13568\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 53/99 | lr: 5.3e-05 | global_batch_size: 256 | global_step: 53 | reduced_train_loss: 0.6343 | consumed_samples: 13824\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 54/99 | lr: 5.4e-05 | global_batch_size: 256 | global_step: 54 | reduced_train_loss: 0.6293 | consumed_samples: 14080\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 55/99 | lr: 5.5e-05 | global_batch_size: 256 | global_step: 55 | reduced_train_loss: 0.6415 | consumed_samples: 14336\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 56/99 | lr: 5.6e-05 | global_batch_size: 256 | global_step: 56 | reduced_train_loss: 0.6396 | consumed_samples: 14592\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 57/99 | lr: 5.7e-05 | global_batch_size: 256 | global_step: 57 | reduced_train_loss: 0.6568 | consumed_samples: 14848\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 58/99 | lr: 5.8e-05 | global_batch_size: 256 | global_step: 58 | reduced_train_loss: 0.6444 | consumed_samples: 15104\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 59/99 | lr: 5.9e-05 | global_batch_size: 256 | global_step: 59 | reduced_train_loss: 0.646 | consumed_samples: 15360\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 59: 'reduced_train_loss' was not in top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:42:48 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 59 : Start time: 1747809767.126s : Save duration: 1.642s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:42:50 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6460-epoch=0-consumed_samples=15360.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:42:50 nemo_logging:393] Async finalization time took 0.002 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 60/99 | lr: 6e-05 | global_batch_size: 256 | global_step: 60 | reduced_train_loss: 0.6344 | consumed_samples: 15616\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:43:48 nemo_logging:393] Successfully saved checkpoint from iteration      59 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6460-epoch=0-consumed_samples=15360.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:43:48 nemo_logging:393] Async checkpoint save for step 60 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6460-epoch=0-consumed_samples=15360.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:44:16 nemo_logging:393] Async finalization time took 27.734 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 61/99 | lr: 6.1e-05 | global_batch_size: 256 | global_step: 61 | reduced_train_loss: 0.6389 | consumed_samples: 15872\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 62/99 | lr: 6.2e-05 | global_batch_size: 256 | global_step: 62 | reduced_train_loss: 0.6689 | consumed_samples: 16128\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 63/99 | lr: 6.3e-05 | global_batch_size: 256 | global_step: 63 | reduced_train_loss: 0.6512 | consumed_samples: 16384\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 64/99 | lr: 6.4e-05 | global_batch_size: 256 | global_step: 64 | reduced_train_loss: 0.6487 | consumed_samples: 16640\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 65/99 | lr: 6.5e-05 | global_batch_size: 256 | global_step: 65 | reduced_train_loss: 0.646 | consumed_samples: 16896\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 66/99 | lr: 6.6e-05 | global_batch_size: 256 | global_step: 66 | reduced_train_loss: 0.6345 | consumed_samples: 17152\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 67/99 | lr: 6.7e-05 | global_batch_size: 256 | global_step: 67 | reduced_train_loss: 0.6632 | consumed_samples: 17408\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 68/99 | lr: 6.8e-05 | global_batch_size: 256 | global_step: 68 | reduced_train_loss: 0.6331 | consumed_samples: 17664\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 69/99 | lr: 6.9e-05 | global_batch_size: 256 | global_step: 69 | reduced_train_loss: 0.6665 | consumed_samples: 17920\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 69: 'reduced_train_loss' was not in top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:54:47 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 69 : Start time: 1747810486.676s : Save duration: 1.037s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:54:49 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6665-epoch=0-consumed_samples=17920.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:54:49 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 70/99 | lr: 7e-05 | global_batch_size: 256 | global_step: 70 | reduced_train_loss: 0.631 | consumed_samples: 18176\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:56:03 nemo_logging:393] Successfully saved checkpoint from iteration      69 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6665-epoch=0-consumed_samples=17920.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:56:03 nemo_logging:393] Async checkpoint save for step 70 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6665-epoch=0-consumed_samples=17920.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 06:56:36 nemo_logging:393] Async finalization time took 34.569 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 71/99 | lr: 7.1e-05 | global_batch_size: 256 | global_step: 71 | reduced_train_loss: 0.6361 | consumed_samples: 18432\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 72/99 | lr: 7.2e-05 | global_batch_size: 256 | global_step: 72 | reduced_train_loss: 0.653 | consumed_samples: 18688\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 73/99 | lr: 7.3e-05 | global_batch_size: 256 | global_step: 73 | reduced_train_loss: 0.6535 | consumed_samples: 18944\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 74/99 | lr: 7.4e-05 | global_batch_size: 256 | global_step: 74 | reduced_train_loss: 0.6541 | consumed_samples: 19200\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 75/99 | lr: 7.5e-05 | global_batch_size: 256 | global_step: 75 | reduced_train_loss: 0.6424 | consumed_samples: 19456\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 76/99 | lr: 7.6e-05 | global_batch_size: 256 | global_step: 76 | reduced_train_loss: 0.6516 | consumed_samples: 19712\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 77/99 | lr: 7.7e-05 | global_batch_size: 256 | global_step: 77 | reduced_train_loss: 0.6501 | consumed_samples: 19968\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 78/99 | lr: 7.8e-05 | global_batch_size: 256 | global_step: 78 | reduced_train_loss: 0.6418 | consumed_samples: 20224\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 79/99 | lr: 7.9e-05 | global_batch_size: 256 | global_step: 79 | reduced_train_loss: 0.6431 | consumed_samples: 20480\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 79: 'reduced_train_loss' was not in top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:06:12 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 79 : Start time: 1747811171.242s : Save duration: 1.241s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:06:14 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6431-epoch=0-consumed_samples=20480.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:06:14 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 80/99 | lr: 8e-05 | global_batch_size: 256 | global_step: 80 | reduced_train_loss: 0.6434 | consumed_samples: 20736\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:07:33 nemo_logging:393] Successfully saved checkpoint from iteration      79 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6431-epoch=0-consumed_samples=20480.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:07:33 nemo_logging:393] Async checkpoint save for step 80 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6431-epoch=0-consumed_samples=20480.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:08:10 nemo_logging:393] Async finalization time took 37.047 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 81/99 | lr: 8.1e-05 | global_batch_size: 256 | global_step: 81 | reduced_train_loss: 0.6398 | consumed_samples: 20992\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 82/99 | lr: 8.2e-05 | global_batch_size: 256 | global_step: 82 | reduced_train_loss: 0.6436 | consumed_samples: 21248\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 83/99 | lr: 8.3e-05 | global_batch_size: 256 | global_step: 83 | reduced_train_loss: 0.6537 | consumed_samples: 21504\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 84/99 | lr: 8.4e-05 | global_batch_size: 256 | global_step: 84 | reduced_train_loss: 0.6625 | consumed_samples: 21760\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 85/99 | lr: 8.5e-05 | global_batch_size: 256 | global_step: 85 | reduced_train_loss: 0.6394 | consumed_samples: 22016\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 86/99 | lr: 8.6e-05 | global_batch_size: 256 | global_step: 86 | reduced_train_loss: 0.6403 | consumed_samples: 22272\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 87/99 | lr: 8.7e-05 | global_batch_size: 256 | global_step: 87 | reduced_train_loss: 0.64 | consumed_samples: 22528\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 88/99 | lr: 8.8e-05 | global_batch_size: 256 | global_step: 88 | reduced_train_loss: 0.6502 | consumed_samples: 22784\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 89/99 | lr: 8.9e-05 | global_batch_size: 256 | global_step: 89 | reduced_train_loss: 0.6653 | consumed_samples: 23040\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 89: 'reduced_train_loss' was not in top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:18:48 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 89 : Start time: 1747811926.552s : Save duration: 1.575s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:18:50 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6653-epoch=0-consumed_samples=23040.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:18:50 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 90/99 | lr: 9e-05 | global_batch_size: 256 | global_step: 90 | reduced_train_loss: 0.6313 | consumed_samples: 23296\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:20:10 nemo_logging:393] Successfully saved checkpoint from iteration      89 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6653-epoch=0-consumed_samples=23040.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:20:10 nemo_logging:393] Async checkpoint save for step 90 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6653-epoch=0-consumed_samples=23040.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:20:42 nemo_logging:393] Async finalization time took 31.633 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 91/99 | lr: 9.1e-05 | global_batch_size: 256 | global_step: 91 | reduced_train_loss: 0.6263 | consumed_samples: 23552\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 92/99 | lr: 9.2e-05 | global_batch_size: 256 | global_step: 92 | reduced_train_loss: 0.6581 | consumed_samples: 23808\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 93/99 | lr: 9.3e-05 | global_batch_size: 256 | global_step: 93 | reduced_train_loss: 0.6429 | consumed_samples: 24064\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 94/99 | lr: 9.4e-05 | global_batch_size: 256 | global_step: 94 | reduced_train_loss: 0.6661 | consumed_samples: 24320\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 95/99 | lr: 9.5e-05 | global_batch_size: 256 | global_step: 95 | reduced_train_loss: 0.6766 | consumed_samples: 24576\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 96/99 | lr: 9.6e-05 | global_batch_size: 256 | global_step: 96 | reduced_train_loss: 0.6551 | consumed_samples: 24832\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 97/99 | lr: 9.7e-05 | global_batch_size: 256 | global_step: 97 | reduced_train_loss: 0.6213 | consumed_samples: 25088\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 98/99 | lr: 9.8e-05 | global_batch_size: 256 | global_step: 98 | reduced_train_loss: 0.6609 | consumed_samples: 25344\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 99/99 | lr: 9.9e-05 | global_batch_size: 256 | global_step: 99 | reduced_train_loss: 0.6492 | consumed_samples: 25600\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 99: 'reduced_train_loss' was not in top 1\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:29:54 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 99 : Start time: 1747812592.296s : Save duration: 1.932s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:29:56 nemo_logging:393] Scheduled async checkpoint save for results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6492-epoch=0-consumed_samples=25600.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:29:56 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:29:56 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:29:56 nemo_logging:393] Pending async checkpoint saves. Finalizing them synchronously now\n",
      "i.finetune/0 [default0]:[INFO     | pytorch_lightning.utilities.rank_zero]: `Trainer.fit` stopped: `max_steps=100` reached.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:30:37 nemo_logging:393] Successfully saved checkpoint from iteration      99 to results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6492-epoch=0-consumed_samples=25600.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:30:37 nemo_logging:393] Async checkpoint save for step 100 (results-20250521-0521-4-devices-baseline-8GPUs-all-data-cleaned-shuffle-no-distrib-sampler-500k-2-workers/trained-model-checkpoints/checkpoints/trained-model-checkpoints--reduced_train_loss=0.6492-epoch=0-consumed_samples=25600.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-05-21 07:31:06 nemo_logging:393] Async finalization time took 70.086 s\n",
      "i.finetune/0 I0521 07:34:17.854000 6205 torch/distributed/elastic/agent/server/api.py:879] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "i.finetune/0 I0521 07:34:17.856000 6205 torch/distributed/elastic/agent/server/api.py:932] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "i.finetune/0 I0521 07:34:17.857000 6205 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.0005011558532714844 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.finetune-ks9cdpj3k4r5kc finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']</span><span style=\"background-color: #272822\">                           </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune_1747804913\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune_1747804913\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.finetune_1747804913</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.finetune_1747804913 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.finetune_1747804913 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1747804913\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1747804913\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1747804913\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def local_executor_torchrun(nodes: int = 1, devices: int = 4) -> run.LocalExecutor:\n",
    "    # Environment variables to optimize distributed training\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    }\n",
    "\n",
    "    return run.LocalExecutor(\n",
    "        ntasks_per_node=devices,\n",
    "        launcher=\"torchrun\",\n",
    "        env_vars=env_vars,\n",
    "    )\n",
    "\n",
    "# Execute the training run\n",
    "if __name__ == '__main__':\n",
    "    run.run(\n",
    "        configure_finetuning_recipe(),\n",
    "        executor=local_executor_torchrun()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84c0a1b3-c5f8-447e-ac0c-73a75eb1912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 21 07:34:20 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:47:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             63W /  400W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             81W /  400W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          Off |   00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   38C    P0            107W /  400W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          Off |   00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             93W /  400W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58635c3-08a9-4cec-9fe0-edd0b94036a8",
   "metadata": {},
   "source": [
    "## üéâ Tada! You Just Trained Your First Reasoning Model!\n",
    "Congratulations ‚Äî you've successfully fine-tuned LLaMA 3‚Äì8B Instruct into a domain-adapted reasoning model using NVIDIA NeMo 2.0!\n",
    "\n",
    "Your model is now ready to:\n",
    "\n",
    "* Answer questions more effectively\n",
    "* Follow domain-specific instructions\n",
    "* Support chain-of-thought reasoning in real-world applications\n",
    "\n",
    "### üöÄ Next Steps\n",
    "* üß™ Evaluate your model on reasoning benchmarks (e.g., MMLU, GSM8K)\n",
    "* ü™Ñ Add LoRA or QLoRA for even more efficient adaptation\n",
    "* ‚òÅÔ∏è Package the model for deployment or inference with Triton or vLLM\n",
    "* üì§ Optionally, upload it to HuggingFace or NGC to share with the world"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
