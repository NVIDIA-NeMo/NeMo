{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LITA Checkpoint Conversion, Finetuning and Inference Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Currently, this notebook must be run in a NeMo container (> 24.04). An example command to launch the container:\n",
    "\n",
    "```\n",
    "docker run --gpus all -it --rm -v <your_nemo_dir>:/opt/NeMo --shm-size=8g -p 8888:8888 --ulimit memlock=-1 --ulimit stack=67108864 <your_nemo_container>\n",
    "```\n",
    "For inference and finetuning, you need to increase the share memory size to avoid some OOM issue. For example,\n",
    "```\n",
    "docker run --gpus all -it --rm -v <your_nemo_dir>:/opt/NeMo -v $PWD:/ws --shm-size=128g -p 8888:8888 --ulimit memlock=-1 --ulimit stack=67108864 nvcr.io/nvidia/nemo:dev\n",
    "```\n",
    "\n",
    "By `-v $PWD:/ws`, we can mount the current local directory to `/ws/` in docker container. We may use this local directory to put the `NeMo` source code, checkpoints and dataset we will generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LITA Introduction\n",
    "\n",
    "[LITA](https://arxiv.org/pdf/2403.19046) stands for Language Instructed Temporal-Localization Assistan, which demonstrates strong performance on Reasoning Temporal Localization (RTL) task. It introduces time tokens to better help LLM understand 'When?' question in video. The below figure from [LITA paper](https://arxiv.org/pdf/2403.19046) shows a clear idea of how LITA works.\n",
    "\n",
    "<img src=\"images/LITA_arch.png\" alt=\"drawing\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and Checkpoint Conversion\n",
    "As we learned that LITA introduces `time tokens` so that timestampes of events in a video would be represented as time tokens instead of the original float point timestamps. Therefore we need to add these time tokens to the tokenizer of the backbone/LLM model. Since the backbone models (vision encoder and LLM) are huggingface LLaVA like model, we can convert them by using `convert_hf_llava_to_neva.py` under `NeMo/examples/multimodal/multimodal_llm/neva/convert_hf_llava_to_neva.py` to nemo model to do finetuning in NeMo. In this example, we take `Llama-3-VILA1.5-8B` as an example to show how to integrate LITA to a LLaVA like model. You may also use similar steps to convert other llama or LLaVA like models that have backbone LLM as llama such as [vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5) and [llava-v1.6-vicuna-13b](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-13b).\n",
    "\n",
    "Please download the huggingface `Llama-3-VILA1.5-8B` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir pretrained_models && cd pretrained_models\n",
    "! git clone https://huggingface.co/Efficient-Large-Model/Llama-3-VILA1.5-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer conversion\n",
    "Here we show how to add 100 time tokens and some nemo extra tokens to a huggingface tokenizer.\n",
    "For the definition of nemo extra tokens, please refer to `NeMo/nemo/collections/multimodal/data/neva/conversation.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the TIME_TOKEN_TEMPLATE\n",
    "TIME_TOKEN_TEMPLATE = \"<t{t}>\"\n",
    "hf_llm_model_path='/ws/pretrained_models/Llama-3-VILA1.5-8B/llm'\n",
    "tokenizer_path = '/ws/converted_models/tokenizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(hf_llm_model_path)\n",
    "DEFAULT_IM_START_TOKEN = \"<extra_id_4>\" # mark the start of the slow token\n",
    "DEFAULT_IM_END_TOKEN = \"<extra_id_5>\" # the end of the slow token\n",
    "VID_START_TOKEN = \"<extra_id_8>\" # the start of the fast token\n",
    "VID_END_TOKEN = \"<extra_id_9>\" # the end of the fast token\n",
    "num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, VID_START_TOKEN, VID_END_TOKEN], special_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # use eos token as pad token\n",
    "num_time_tokens = 100\n",
    "time_tokens = [TIME_TOKEN_TEMPLATE.format(t=x) for x in range(num_time_tokens)]\n",
    "num_new_tokens = tokenizer.add_tokens(time_tokens)\n",
    "# add the other nemo extra tokens\n",
    "extra_tokens = [\"<extra_id_0>\",\"<extra_id_1>\",\"<extra_id_2>\",\"<extra_id_3>\",\"<extra_id_6>\",\"<extra_id_7>\"]\n",
    "tokenizer.add_tokens(extra_tokens)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the tokenizer by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer\n",
    "tokenizer = get_nmt_tokenizer(library=\"huggingface\", model_name=tokenizer_path)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice if you wanna convert checkpoints trained from [LITA1.0](https://github.com/NVlabs/LITA), you should put all the extra tokens including `DEFAULT_IM_START_TOKEN` and `DEFAULT_IM_END_TOKEN` at the end of the time tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Conversion\n",
    "Now we are going to convert the huggingface LLaVA or Llama model to nemo model. For Llama model, please refer to `NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py`. For LLaVA model, please refer to `NeMo/examples/multimodal/multimodal_llm/neva/convert_hf_llava_to_neva.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cd /opt/ && git clone https://github.com/haotian-liu/LLaVA   # we only need the model structure, no need to install\n",
    "! export PYTHONPATH=/opt/LLaVA/:$PYTHONPATH\n",
    "! cd /ws  # do not run the below commands under `/opt` folder\n",
    "! config_file=vita_config.yaml  # check the config file in /opt/NeMo/examples/multimodal/multimodal_llm/neva/conf/lita_config.yaml\n",
    "! python /opt/NeMo/examples/multimodal/multimodal_llm/neva/convert_hf_llava_to_neva.py \\\n",
    "--in-file /ws/pretrained_models/Llama-3-VILA1.5-8B/llm \\\n",
    "--mm_vision_tower google/siglip-so400m-patch14-384 \\\n",
    "--mm_projector_ckpt_dir /ws/pretrained_models/Llama-3-VILA1.5-8B/mm_projector \\\n",
    "--out-file /ws/converted_models/Llama-3-VILA1.5-8B.nemo \\\n",
    "--tokenizer-model /ws/converted_models/tokenizer/ \\\n",
    "--config-file vita_config.yaml \\\n",
    "--conv-template llama_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice `mm_vision_tower` and `mm_projector_ckpt_dir` are optional."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
