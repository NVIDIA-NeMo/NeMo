{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d4e90b",
   "metadata": {},
   "source": [
    "# SDXL Int8 Quantization Solution by Ammo\n",
    "\n",
    "### Note:\n",
    "This notebook requires nvidia-ammo > 0.9.x. An example command to launch the container:\n",
    "\n",
    "```\n",
    "docker run --gpus all -it --rm -v <your_nemo_dir>:/opt/NeMo --shm-size=8g \\\n",
    "     -p 8888:8888 --ulimit memlock=-1 --ulimit \\\n",
    "      stack=67108864 <your_nemo_container>\n",
    "```\n",
    "\n",
    "This tutorial shows how to use Ammo to calibrate and quantize the UNet part of the SDXL within NeMo framework. \n",
    "\n",
    "Please note that NeMo provides users with an end-to-end training framework for SDXL, and this quantization pipeline is supposed to work with a `.nemo` checkpoint trained from their own text-image dataset. In this tutorial, a open-source checkpoint is converted to `.nemo` format for illustration purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df99d4",
   "metadata": {},
   "source": [
    "### Download SDXL checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dcfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download Unet checkpoint\n",
    "! mkdir -p /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet && wget -P /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/unet/diffusion_pytorch_model.safetensors\n",
    "## Download Vae checkpoint  \n",
    "! mkdir -p /sdxl_ckpts/stable-diffusion-xl-base-1.0/vae && wget -P /sdxl_ckpts/stable-diffusion-xl-base-1.0/vae https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/vae/diffusion_pytorch_model.safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59cb71",
   "metadata": {},
   "source": [
    "### Convert downloaded checkpoint into `.nemo` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR = '/quantization'\n",
    "! torchrun /opt/NeMo/examples/multimodal/text_to_image/convert_hf_ckpt_to_nemo.py \\\n",
    "    --model_type sdxl \\\n",
    "    --ckpt_path /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet/diffusion_pytorch_model.safetensors \\\n",
    "    --hparams_file /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/conf/sd_xl_base_train.yaml \\\n",
    "    --nemo_file_path $WORKDIR/sdxl_base.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a394e59",
   "metadata": {},
   "source": [
    "### Run quantization script with default config, and finally the script will export the quantized unet to onnx file.\n",
    "\n",
    "##### Quantization config\n",
    "\n",
    "```yaml\n",
    "quantize\n",
    "  exp_name: nemo_test\n",
    "  n_steps: 20          # number of inference steps\n",
    "  format: 'int8'       # only int8 quantization is supported now\n",
    "  percentile: 1.0      # Control quantization scaling factors (amax) collecting range, meaning that we will collect the minimum amax in the range of `(n_steps * percentile)` steps. Recommendation: 1.0\n",
    "  batch_size: 1        # batch size calling sdxl inference pipeline during calibration\n",
    "  calib_size: 32       # For SDXL, we recommend 32, 64 or 128\n",
    "  quant_level: 2.5     #Which layers to be quantized, 1: `CNNs`, 2: `CNN + FFN`, 2.5: `CNN + FFN + QKV`, 3: `CNN + Linear`. Recommendation: 2, 2.5 and 3, depending on the requirements for image quality & speedup.\n",
    "  alpha: 0.8           # A parameter in SmoothQuant, used for linear layers only. Recommendation: 0.8 for SDXL\n",
    "```\n",
    "\n",
    "##### Onnx export config\n",
    "\n",
    "```yaml\n",
    "onnx_export:\n",
    "  onnx_dir: nemo_onnx    # Path to save onnx files\n",
    "  pretrained_base: ${model.restore_from_path}  # Path to nemo checkpoint for sdxl\n",
    "  quantized_ckpt: nemo.unet.state_dict.${quantize.exp_name}.pt  # Path to save quantized unet checkpoint\n",
    "  format: int8\n",
    "```\n",
    "\n",
    "The following command restores a pre-trained sdxl model from `$WORKDIR/sdxl_base.nemo` derived from the above step.\n",
    "The quantized U-Net checkpoint is saved to `quantize.quantized_ckpt` and converted onnx file is saved to `onnx_export.onnx_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_quantize.py model.restore_from_path=$WORKDIR/sdxl_base.nemo onnx_export.onnx_dir=$WORKDIR/nemo_onnx quantize.quantized_ckpt=$WORKDIR/nemo.unet.state_dict.nemo.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0680de",
   "metadata": {},
   "source": [
    "### Now we want to build trt engine from the onnx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "! trtexec --onnx=$WORKDIR/nemo_onnx/unet.onnx --shapes=x:8x4x128x128,timesteps:8,context:8x80x2048,y:8x2816 --fp16 --int8 --builderOptimizationLevel=4 --saveEngine=$WORKDIR/nemo_unet_xl.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce2926",
   "metadata": {},
   "source": [
    "### Build end to end TRT inference pipeline\n",
    "In order to run an end to end inference with quantized U-Net engine, we need to export and build engines for the other compenents in SDXL, which includes the VAE and two CLIP encoder. The following script restores SDXL from the `nemo` checkpoint and saves the corresponding engine files to `infer.out_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_export.py model.restore_from_path=$WORKDIR/sdxl_base.nemo infer.out_path=$WORKDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643160d7",
   "metadata": {},
   "source": [
    "### Run TRT inference pipeline with original engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_trt_inference.py \\\n",
    "    out_path=$WORKDIR/trt_output_fp16 \\\n",
    "    unet_xl=$WORKDIR/plan/unet_xl.plan \\\n",
    "    vae=$WORKDIR/plan/vae.plan \\\n",
    "    clip1=$WORKDIR/plan/clip1.plan \\\n",
    "    clip2=$WORKDIR/plan/clip2.plan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966f973",
   "metadata": {},
   "source": [
    "### Run TRT inference pipeline with quantized U-Net engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_trt_inference.py \\\n",
    "    out_path=$WORKDIR/trt_output_int8 \\\n",
    "    unet_xl=$WORKDIR/nemo_unet_xl.plan \\\n",
    "    vae=$WORKDIR/plan/vae.plan \\\n",
    "    clip1=$WORKDIR/plan/clip1.plan \\\n",
    "    clip2=$WORKDIR/plan/clip2.plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd0c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
