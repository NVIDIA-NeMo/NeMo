{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/vadams/anaconda3/envs/nemo_pr/lib/python3.7/site-packages (3.2)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: unidecode in /home/vadams/anaconda3/envs/nemo_pr/lib/python3.7/site-packages (1.2.0)\n",
      "Collecting nemo_toolkit[all]\n",
      "  Cloning https://github.com/NVIDIA/NeMo.git (to revision r1.0.0rc1) to /tmp/pip-install-m6idm2gi/nemo-toolkit_4553807d996f4fbba0687f48744d0c43\n",
      "  Running command git clone -q https://github.com/NVIDIA/NeMo.git /tmp/pip-install-m6idm2gi/nemo-toolkit_4553807d996f4fbba0687f48744d0c43\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "--2021-04-06 10:55:28--  https://raw.githubusercontent.com/NVIDIA/NeMo/r1.0.0rc1/examples/nlp/entity_linking/conf/tiny_example_entity_linking_config.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-06 10:55:28 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-06 10:55:28--  https://raw.githubusercontent.com/NVIDIA/NeMo/r1.0.0rc1/examples/nlp/entity_linking/data/tiny_example_data\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-06 10:55:28 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install unidecode\n",
    "!pip install matplotlib>=3.3.2\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'r1.0.0rc1'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "## Grab the config we'll use in this example\n",
    "!mkdir conf\n",
    "!wget -P conf/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/nlp/entity_linking/conf/tiny_example_entity_linking_config.yaml\n",
    "    \n",
    "!mkdir data\n",
    "!wget -P data/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/nlp/entity_linking/data/tiny_example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
      "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Description\n",
    "[Entity linking](https://en.wikipedia.org/wiki/Entity_linking) is the process of matching concepts mentioned in natural language to their unique IDs and canonical forms stored in a knowledge base. Entity linking applications range from helping automate ingestion of large amounts of data to assisting in real time concept normalization during a conversation. \n",
    "\n",
    "Though there are a myriad of approaches to the entity linking task, within nemo and this tutorial we use the methodology described in the [Self-alignment Pre-training for Biomedical Entity Representations](https://arxiv.org/abs/2010.11784) paper. The main intution behind the approach is to reshape an initial BERT embedding space such that different descriptions of the same concept are closer togther in that space and unrealted concepts are further apart. We can then use the concept embeddings from this reshaped space to build an index of embeddings from a knowledge base. Finally, we can link query concepts to their canonical forms in the knowledge base by performing a nearest neighbor search- matching concept query embeddings to the most similar concepts embeddings in the knowledge base index. In this tutorial we will be using the [faiss](https://github.com/facebookresearch/faiss) library to build our concept index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Alignment Pretraining\n",
    "Self-Alignment pretraining is a second stage pretraining of an exsiting encoder (called second stage because the encoder model can also be further finetuned after this more general pretraining step). The dataset used during training consits of pairs of concept synonyms that map to the same ID in a knowledge base. At each training iteration, we only select *hard* examples present in the mini batch to calculate the loss and update the model weights. In this context, a hard example is an example where a concept is closer to an unrelated concept in the mini batch than it is to the synonym concept it is paired with by some margin. I encourage you to take a look at [section 2 of the paper](https://arxiv.org/pdf/2010.11784.pdf) for a more formal and indepth description of how hard examples are selected. \n",
    "\n",
    "We then use a [metric learning loss](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf) calculated from the hard examples selected. This loss takes concept representations that were incorrectly positioned in our initial embedding space and pushes embedding pairs that should be more similar together, while pulling pairs that represent distinct ideas apart. Through this training process we reshape the concept embedding space to be better suited for our entity linking task than it was originally. \n",
    "\n",
    "Now that we have idea of what's going on, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be using a tiny toy dataset to demonstrate how to use NeMo's entity linking model functionality. The dataset includes synonyms for 12 medical concepts. Here's the dataset before preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                                            CONCEPT\n",
      "0    1                                          Head ache\n",
      "1    1                                           Headache\n",
      "2    1                                           Migraine\n",
      "3    1                                   Pain in the head\n",
      "4    1                                          cephalgia\n",
      "5    1                                        cephalalgia\n",
      "6    2                                       heart attack\n",
      "7    2                              Myocardial infraction\n",
      "8    2                           necrosis of heart muscle\n",
      "9    2                                                 MI\n",
      "10   3                                                CAD\n",
      "11   3                            Coronary artery disease\n",
      "12   3                      atherosclerotic heart disease\n",
      "13   3                                      heart disease\n",
      "14   3                damage of major heart blood vessels\n",
      "15   4                                myocardial ischemia\n",
      "16   4                                   cardiac ischemia\n",
      "17   4                      reduced ability to pump blood\n",
      "18   5                    gradual loss of kidney function\n",
      "19   5                        kidneys cannot filter blood\n",
      "20   5                             chronic kidney disease\n",
      "21   5                             chronic kidney failure\n",
      "22   5                                                CKD\n",
      "23   6                              alchohol intoxication\n",
      "24   6                        acute alchohol intoxication\n",
      "25   6                                 alchohol poisoning\n",
      "26   6                                 severe drunkenness\n",
      "27   6                        over consumption of alcohol\n",
      "28   7                                  diabetes mellitus\n",
      "29   7                                           diabetes\n",
      "30   7                       inability to process glucose\n",
      "31   7                            unable to take up sugar\n",
      "32   7                                    Type 2 diabetes\n",
      "33   8                                   Hyperinsulinemia\n",
      "34   8                                   High blood sugar\n",
      "35   8                  abnormally high levels of insulin\n",
      "36   9                   Dipeptidyl peptidase-4 inhibitor\n",
      "37   9                                    dpp-4 inhibitor\n",
      "38   9                                         alogliptin\n",
      "39   9                                             Nesina\n",
      "40   9                                            Vipidia\n",
      "41  10                                       hypoglycemia\n",
      "42  10                                    low blood sugar\n",
      "43  11                                     anticoagulants\n",
      "44  11                                     blood thinners\n",
      "45  11                                           Apixaban\n",
      "46  11                                            Eliquis\n",
      "47  12                                          Ibuprofen\n",
      "48  12                                            Aspirin\n",
      "49  12  over the counter nonsteroidal anti-inflammator...\n",
      "50  12                                              NSAID\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"tiny_example_data.csv\", names=[\"ID\", \"CONCEPT\"], index_col=False)\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already paired off the concepts for this dataset with the format `ID concept_synonym1 concept_synonym2`. Here is a look at the first ten rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID      CONCEPT_SYN1      CONCEPT_SYN2\n",
      "0   1  Pain in the head         cephalgia\n",
      "1   1  Pain in the head       cephalalgia\n",
      "2   1          Migraine         cephalgia\n",
      "3   1         Head ache  Pain in the head\n",
      "4   1         Head ache          Migraine\n",
      "5   1         Head ache       cephalalgia\n",
      "6   1          Headache          Migraine\n",
      "7   1          Migraine       cephalalgia\n",
      "8   1         cephalgia       cephalalgia\n",
      "9   1          Headache  Pain in the head\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_table(\"tiny_example_train_pairs.tsv\", names=[\"ID\", \"CONCEPT_SYN1\", \"CONCEPT_SYN2\"], delimiter='\\t')\n",
    "print(training_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/index.html) dataset for full medical domain entity linking training. The data contains over 9 million entities and is a table of medical concepts with their corresponding concept IDs (CUI). After [requesting a free license and making a UMLS Terminology Services (UTS) account](https://www.nlm.nih.gov/research/umls/index.html), the [entire UMLS dataset](https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html) can be downloaded from the NIH's website. If you've cloned the NeMo repo you can run the data processing script located in `examples/nlp/entity_linking/data/umls_dataset_processing.py` on the full dataset. This script will take in the initial table of UMLS concepts and produce a .tsv file where each row is formatted as `CUI\\tconcept_synonym1\\tconcept_synonym2`. Once the UMLS dataset .RRF file is downloaded, the script can be run from the `examples/nlp/entity_linking` directory like so: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python data/umls_dataset_processing.py --cfg conf/umls_medical_entity_linking_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second stage pretrain a BERT Base encoder on the self-alignment pretraining task (SAP) for improved entity linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the config file\n",
    "cfg = OmegaConf.load(\"tiny_example_entity_linking_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-04-06 10:55:51 exp_manager:210] Experiments will be logged at SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51\n",
      "[NeMo I 2021-04-06 10:55:51 exp_manager:550] TensorboardLogger has been set up\n",
      "[NeMo I 2021-04-06 10:55:53 entity_linking_dataset:88] Loaded dataset with 63 examples\n",
      "[NeMo I 2021-04-06 10:55:53 entity_linking_dataset:88] Loaded dataset with 21 examples\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trainer and model\n",
    "trainer = Trainer(**cfg.trainer)\n",
    "exp_manager(trainer, cfg.get(\"exp_manager\", None))\n",
    "model = nemo_nlp.models.EntityLinkingModel(cfg=cfg.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-04-06 10:55:57 modelPT:685] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 3e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2021-04-06 10:55:57 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f0e48625490>\" \n",
      "    will be used during training (effective maximum steps = 16) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    min_lr: 0.0\n",
      "    last_epoch: -1\n",
      "    max_steps: 16\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | BertEncoder         | 109 M \n",
      "1 | loss  | MultiSimilarityLoss | 0     \n",
      "----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.929   Total estimated model params size (MB)\n",
      "INFO:lightning:\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | BertEncoder         | 109 M \n",
      "1 | loss  | MultiSimilarityLoss | 0     \n",
      "----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.929   Total estimated model params size (MB)\n",
      "[NeMo W 2021-04-06 10:55:59 nemo_logging:349] /home/vadams/anaconda3/envs/nemo_pr/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s][NeMo I 2021-04-06 10:56:00 entity_linking_model:107] val loss: 1.1948829889297485\n",
      "Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  9.30it/s][NeMo I 2021-04-06 10:56:00 entity_linking_model:107] val loss: 0.8535466194152832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-04-06 10:56:00 nemo_logging:349] /home/vadams/anaconda3/envs/nemo_pr/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The validation_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-04-06 10:56:00 nemo_logging:349] /home/vadams/anaconda3/envs/nemo_pr/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  15%|█▌        | 3/20 [00:00<00:01,  9.49it/s, loss=0.723, v_num=5-51, val_loss=1.020, lr=1.5e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:00 entity_linking_model:107] val loss: 1.1459448337554932\n",
      "\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  9.13it/s]\u001b[A[NeMo I 2021-04-06 10:56:00 entity_linking_model:107] val loss: 0.811598539352417\n",
      "Epoch 0:  25%|██▌       | 5/20 [00:00<00:01, 11.05it/s, loss=0.723, v_num=5-51, val_loss=1.020, lr=1.5e-5][NeMo I 2021-04-06 10:56:00 entity_linking_model:107] val loss: 0.7985554337501526\n",
      "Epoch 0:  30%|███       | 6/20 [00:00<00:01, 11.63it/s, loss=0.723, v_num=5-51, val_loss=0.919, lr=3e-5]  \n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: val_loss reached 0.91870 (best 0.91870), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.92-epoch=0.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 0, global step 1: val_loss reached 0.91870 (best 0.91870), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.92-epoch=0.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  40%|████      | 8/20 [00:04<00:06,  1.85it/s, loss=0.703, v_num=5-51, val_loss=0.919, lr=2.97e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:04 entity_linking_model:107] val loss: 1.0460283756256104\n",
      "\n",
      "Epoch 0:  50%|█████     | 10/20 [00:04<00:04,  2.26it/s, loss=0.703, v_num=5-51, val_loss=0.919, lr=2.97e-5][NeMo I 2021-04-06 10:56:04 entity_linking_model:107] val loss: 0.7007061243057251\n",
      "[NeMo I 2021-04-06 10:56:04 entity_linking_model:107] val loss: 0.7719489336013794\n",
      "Epoch 0:  60%|██████    | 12/20 [00:04<00:03,  2.66it/s, loss=0.703, v_num=5-51, val_loss=0.840, lr=2.87e-5]\n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 3: val_loss reached 0.83956 (best 0.83956), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.84-epoch=0.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 0, global step 3: val_loss reached 0.83956 (best 0.83956), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.84-epoch=0.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  70%|███████   | 14/20 [00:10<00:04,  1.29it/s, loss=0.713, v_num=5-51, val_loss=0.840, lr=2.71e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:11 entity_linking_model:107] val loss: 1.0185327529907227\n",
      "\n",
      "Epoch 0:  80%|████████  | 16/20 [00:10<00:02,  1.46it/s, loss=0.713, v_num=5-51, val_loss=0.840, lr=2.71e-5][NeMo I 2021-04-06 10:56:11 entity_linking_model:107] val loss: 0.6587757468223572\n",
      "[NeMo I 2021-04-06 10:56:11 entity_linking_model:107] val loss: 0.7208905220031738\n",
      "Epoch 0:  90%|█████████ | 18/20 [00:11<00:01,  1.62it/s, loss=0.713, v_num=5-51, val_loss=0.799, lr=2.5e-5] \n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 5: val_loss reached 0.79940 (best 0.79940), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.80-epoch=0.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 0, global step 5: val_loss reached 0.79940 (best 0.79940), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.80-epoch=0.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s, loss=0.679, v_num=5-51, val_loss=0.799, lr=2.25e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:17 entity_linking_model:107] val loss: 1.0131477117538452\n",
      "\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  7.82it/s]\u001b[A[NeMo I 2021-04-06 10:56:17 entity_linking_model:107] val loss: 0.6270858645439148\n",
      "[NeMo I 2021-04-06 10:56:18 entity_linking_model:107] val loss: 0.6414982676506042\n",
      "Epoch 0: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s, loss=0.679, v_num=5-51, val_loss=0.761, lr=1.96e-5]\n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 7: val_loss reached 0.76058 (best 0.76058), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.76-epoch=0.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 0, global step 7: val_loss reached 0.76058 (best 0.76058), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.76-epoch=0.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▌        | 3/20 [00:00<00:01, 10.87it/s, loss=0.682, v_num=5-51, val_loss=0.761, lr=1.66e-5] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:24 entity_linking_model:107] val loss: 1.0083823204040527\n",
      "\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  6.78it/s]\u001b[A[NeMo I 2021-04-06 10:56:24 entity_linking_model:107] val loss: 0.6017864346504211\n",
      "[NeMo I 2021-04-06 10:56:25 entity_linking_model:107] val loss: 0.6326455473899841\n",
      "Epoch 1:  30%|███       | 6/20 [00:00<00:01, 10.83it/s, loss=0.682, v_num=5-51, val_loss=0.748, lr=1.34e-5]\n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 9: val_loss reached 0.74760 (best 0.74760), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.75-epoch=1.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 1, global step 9: val_loss reached 0.74760 (best 0.74760), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.75-epoch=1.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▌     | 9/20 [00:07<00:08,  1.26it/s, loss=0.645, v_num=5-51, val_loss=0.748, lr=1.04e-5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:31 entity_linking_model:107] val loss: 1.002161979675293\n",
      "[NeMo I 2021-04-06 10:56:31 entity_linking_model:107] val loss: 0.5760502219200134\n",
      "\n",
      "Validating:  67%|██████▋   | 2/3 [00:00<00:00, 16.93it/s]\u001b[A[NeMo I 2021-04-06 10:56:31 entity_linking_model:107] val loss: 0.6279104948043823\n",
      "Epoch 1:  60%|██████    | 12/20 [00:07<00:04,  1.63it/s, loss=0.645, v_num=5-51, val_loss=0.735, lr=7.5e-6] \n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 11: val_loss reached 0.73537 (best 0.73537), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.74-epoch=1.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 1, global step 11: val_loss reached 0.73537 (best 0.73537), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.74-epoch=1.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▌  | 15/20 [00:13<00:04,  1.08it/s, loss=0.633, v_num=5-51, val_loss=0.735, lr=4.96e-6]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:38 entity_linking_model:107] val loss: 0.9992019534111023\n",
      "\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  6.54it/s]\u001b[A[NeMo I 2021-04-06 10:56:38 entity_linking_model:107] val loss: 0.5503721833229065\n",
      "[NeMo I 2021-04-06 10:56:38 entity_linking_model:107] val loss: 0.6585618257522583\n",
      "Epoch 1:  90%|█████████ | 18/20 [00:14<00:01,  1.27it/s, loss=0.633, v_num=5-51, val_loss=0.736, lr=2.86e-6]\n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 13: val_loss reached 0.73605 (best 0.73537), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.74-epoch=1-v1.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 1, global step 13: val_loss reached 0.73605 (best 0.73537), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.74-epoch=1-v1.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:20<00:00,  1.04s/it, loss=0.619, v_num=5-51, val_loss=0.736, lr=1.3e-6] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A[NeMo I 2021-04-06 10:56:45 entity_linking_model:107] val loss: 0.9986327290534973\n",
      "\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  8.70it/s]\u001b[A[NeMo I 2021-04-06 10:56:45 entity_linking_model:107] val loss: 0.518406867980957\n",
      "[NeMo I 2021-04-06 10:56:45 entity_linking_model:107] val loss: 0.6583847403526306\n",
      "Epoch 1: 100%|██████████| 20/20 [00:21<00:00,  1.06s/it, loss=0.619, v_num=5-51, val_loss=0.725, lr=3.28e-7]\n",
      "                                                         \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 15: val_loss reached 0.72514 (best 0.72514), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.73-epoch=1.ckpt\" as top 3\n",
      "INFO:lightning:Epoch 1, global step 15: val_loss reached 0.72514 (best 0.72514), saving model to \"/home/vadams/Projects/entity-linking-research/NeMo/tutorials/nlp/SelfAlignmentPretrainingTinyExample/2021-04-06_10-55-51/checkpoints/SelfAlignmentPretrainingTinyExample---val_loss=0.73-epoch=1.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it, loss=0.619, v_num=5-51, val_loss=0.725, lr=3.28e-7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint...\n",
      "INFO:lightning:Saving latest checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:27<00:00,  1.39s/it, loss=0.619, v_num=5-51, val_loss=0.725, lr=3.28e-7]\n"
     ]
    }
   ],
   "source": [
    "# Train and save the model\n",
    "trainer.fit(model)\n",
    "model.save_to(cfg.model.nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the script at `examples/nlp/entity_linking/self_alignment_pretraining.py` to train a model on a larger dataset. Run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python self_alignment_pretraining.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the `examples/nlp/entity_linking` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate our freashly trained model and compare its performance with a BERT Base encoder that hasn't undergone self-alignment pretraining. We first need to restore our trained model and load our BERT Base Baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-04-06 11:22:26 modelPT:133] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    data_file: tiny_example_train_pairs.tsv\n",
      "    max_seq_length: 128\n",
      "    batch_size: 8\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "    \n",
      "[NeMo W 2021-04-06 11:22:26 modelPT:140] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    data_file: tiny_example_validation_pairs.tsv\n",
      "    max_seq_length: 128\n",
      "    batch_size: 8\n",
      "    shuffle: false\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "    \n",
      "[NeMo W 2021-04-06 11:22:26 modelPT:1134] World size can only be set by PyTorch Lightning Trainer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-04-06 11:22:30 modelPT:376] Model EntityLinkingModel was successfully restored from tiny_example_sap_bert_model.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-04-06 11:22:32 modelPT:1134] World size can only be set by PyTorch Lightning Trainer.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Restore second stage pretrained model\n",
    "sap_model_cfg = cfg\n",
    "sap_model = nemo_nlp.models.EntityLinkingModel.restore_from(sap_model_cfg.model.nemo_path).to(device)\n",
    "\n",
    "# Load original model\n",
    "base_model_cfg = OmegaConf.load(\"tiny_example_entity_linking_config.yaml\")\n",
    "\n",
    "# Set train/val datasets to None to avoid lo\n",
    "base_model_cfg.model.train_ds = None\n",
    "base_model_cfg.model.validation_ds = None\n",
    "base_model_cfg.index.index_save_name = \"base_model_index\"\n",
    "base_model = nemo_nlp.models.EntityLinkingModel(base_model_cfg.model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going evaluate our model on a nearest neighbors task using top 1 and top 5 accuarcy as our metric. We will be using a tiny example test knowledge and test queries. For this evaluation we are going to be comparing every test query with every concept vector in our test set knowledge base and ranking each item in the knowledge base by its cosine similarity with the test query. We'll then compare the IDs of the predicted most similar test knowledge base concepts with our ground truth query IDs to calculate top 1 and top 5 accuarcy. For this metric higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get data embeddings\n",
    "def get_embeddings(model, dataloader):\n",
    "    embeddings, cids = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids, token_type_ids, attention_mask, batch_cids = batch\n",
    "            batch_embeddings = model.forward(input_ids=input_ids.to(device), \n",
    "                                             token_type_ids=token_type_ids.to(device), \n",
    "                                             attention_mask=attention_mask.to(device))\n",
    "\n",
    "            # Accumulate index embeddings and their corresponding IDs\n",
    "            embeddings.extend(batch_embeddings.cpu().detach().numpy())\n",
    "            cids.extend(batch_cids)\n",
    "            \n",
    "    return embeddings, cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_kb, test_queries, ks):\n",
    "    # Initialize knowledge base and query data loaders\n",
    "    test_kb_dataloader = model.setup_dataloader(test_kb, is_index_data=True)\n",
    "    test_query_dataloader = model.setup_dataloader(test_queries, is_index_data=True)\n",
    "    \n",
    "    # Get knowledge base and query embeddings\n",
    "    test_kb_embs, test_kb_cids = get_embeddings(model, test_kb_dataloader)\n",
    "    test_query_embs, test_query_cids = get_embeddings(model, test_query_dataloader)\n",
    "\n",
    "    # Calculate the cosine distance between each query and knowledge base concept\n",
    "    score_matrix = np.matmul(np.array(test_query_embs), np.array(test_kb_embs).T)\n",
    "    accs = {k : 0 for k in ks}\n",
    "    \n",
    "    # Compare the knowledge base IDs of the knowledge base entities with \n",
    "    # the smallest cosine distance from the query \n",
    "    for query_idx in tqdm(range(len(test_query_cids))):\n",
    "        query_emb = test_query_embs[query_idx]\n",
    "        query_cid = test_query_cids[query_idx]\n",
    "        query_scores = score_matrix[query_idx]\n",
    "\n",
    "        for k in ks:\n",
    "            topk_idxs = np.argpartition(query_scores, -k)[-k:]\n",
    "            topk_cids = [test_kb_cids[idx] for idx in topk_idxs]\n",
    "            \n",
    "            # If the correct query ID is amoung the top k closest kb IDs\n",
    "            # the model correctly linked the entity\n",
    "            match = int(query_cid in topk_cids)\n",
    "            accs[k] += match\n",
    "\n",
    "    for k in ks:\n",
    "        accs[k] /= len(test_query_cids)\n",
    "                \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-04-06 10:57:36 entity_linking_dataset:88] Loaded dataset with 22 examples\n",
      "[NeMo I 2021-04-06 10:57:36 entity_linking_dataset:88] Loaded dataset with 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 8133.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-04-06 10:57:36 entity_linking_dataset:88] Loaded dataset with 22 examples\n",
      "[NeMo I 2021-04-06 10:57:36 entity_linking_dataset:88] Loaded dataset with 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 9461.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 and Top 5 Accuracy Comparison:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_0f6fd_ th {\n",
       "          text-align: left;\n",
       "    }#T_0f6fd_row0_col0,#T_0f6fd_row0_col1,#T_0f6fd_row0_col2,#T_0f6fd_row1_col0,#T_0f6fd_row1_col1,#T_0f6fd_row1_col2{\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_0f6fd_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Model</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >5</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_0f6fd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_0f6fd_row0_col0\" class=\"data row0 col0\" >BERT Base Baseline</td>\n",
       "                        <td id=\"T_0f6fd_row0_col1\" class=\"data row0 col1\" >0.700000</td>\n",
       "                        <td id=\"T_0f6fd_row0_col2\" class=\"data row0 col2\" >1.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0f6fd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_0f6fd_row1_col0\" class=\"data row1 col0\" >BERT + SAP</td>\n",
       "                        <td id=\"T_0f6fd_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "                        <td id=\"T_0f6fd_row1_col2\" class=\"data row1 col2\" >1.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0e4760d850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_kb = OmegaConf.create({\n",
    "    \"data_file\": \"tiny_example_test_kb.tsv\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"batch_size\": 10,\n",
    "    \"shuffle\": False,\n",
    "})\n",
    "\n",
    "test_queries = OmegaConf.create({\n",
    "    \"data_file\": \"tiny_example_test_queries.tsv\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"batch_size\": 10,\n",
    "    \"shuffle\": False,\n",
    "})\n",
    "\n",
    "ks = [1, 5]\n",
    "\n",
    "base_accs = evaluate(base_model, test_kb, test_queries, ks)\n",
    "base_accs[\"Model\"] = \"BERT Base Baseline\"\n",
    "sap_accs = evaluate(sap_model, test_kb, test_queries, ks)\n",
    "sap_accs[\"Model\"] = \"BERT + SAP\"\n",
    "\n",
    "print(\"Top 1 and Top 5 Accuracy Comparison:\")\n",
    "results_df = pd.DataFrame([base_accs, sap_accs], columns=[\"Model\", 1, 5])\n",
    "results_df = results_df.style.set_properties(**{'text-align': 'left', }).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating a model trained on a larger dataset, you can use a nearest neighbors index to speed up the evaluation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To qualitatively observe the improvement we gain from the second stage pretraining, let's build two indices. One will be built with BERT base embeddings before self alignment pretraining and one will be built with the model we just trained. Our knowledge base in this tutorial will be in the same domain and have some over lapping concepts as the training set. This data file is formatted as `ID\\tconcept`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EntityLinkingDataset` class can load the data used for training the entity linking encoder as well as for building the index if the `is_index_data` flag is set to true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(cfg, model):\n",
    "    # Setup index dataset loader\n",
    "    index_dataloader = model.setup_dataloader(cfg.index.index_ds, is_index_data=True)\n",
    "    \n",
    "    # Get index dataset embeddings\n",
    "    embeddings, _ = get_embeddings(model, index_dataloader)\n",
    "    \n",
    "    # Train IVFFlat index using faiss\n",
    "    embeddings = np.array(embeddings)\n",
    "    quantizer = faiss.IndexFlatL2(cfg.index.dims)\n",
    "    index = faiss.IndexIVFFlat(quantizer, cfg.index.dims, cfg.index.nlist)\n",
    "    index = faiss.index_cpu_to_all_gpus(index)\n",
    "    index.train(embeddings)\n",
    "    \n",
    "    # Add concept embeddings to index\n",
    "    for i in tqdm(range(0, embeddings.shape[0], cfg.index.index_batch_size)):\n",
    "            index.add(embeddings[i:i+cfg.index.index_batch_size])\n",
    "\n",
    "    # Save index\n",
    "    faiss.write_index(faiss.index_gpu_to_cpu(index), cfg.index.index_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-04-06 11:02:36 entity_linking_dataset:88] Loaded dataset with 12 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1415.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-04-06 11:02:36 entity_linking_dataset:88] Loaded dataset with 12 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1658.16it/s]\n"
     ]
    }
   ],
   "source": [
    "build_index(sap_model_cfg, sap_model.to(device))\n",
    "build_index(base_model_cfg, base_model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Linking via Nearest Neighbor Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to query our indices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_index(cfg, model, index, queries, id2string):\n",
    "    query_embs = get_query_embedding(queries, model).cpu().detach().numpy()\n",
    "    \n",
    "    # Use query embedding to find closet concept embedding in knowledge base\n",
    "    distances, neighbors = index.search(query_embs, cfg.index.top_n)\n",
    "    neighbor_concepts = [[id2string[concept_id] for concept_id in query_neighbor] \\\n",
    "                                                for query_neighbor in neighbors]\n",
    "    \n",
    "    for query_idx in range(len(queries)):\n",
    "        print(f\"\\nThe most similar concepts to {queries[query_idx]} are:\")\n",
    "        for cid, concept, dist in zip(neighbors[query_idx], neighbor_concepts[query_idx], distances[query_idx]):\n",
    "            print(cid, concept, 1 - dist)\n",
    "\n",
    "    \n",
    "def get_query_embedding(queries, model):\n",
    "    model_input =  model.tokenizer(queries,\n",
    "                                   add_special_tokens = True,\n",
    "                                   padding = True,\n",
    "                                   truncation = True,\n",
    "                                   max_length = 512,\n",
    "                                   return_token_type_ids = True,\n",
    "                                   return_attention_mask = True)\n",
    "\n",
    "    query_emb =  model.forward(input_ids=torch.LongTensor(model_input[\"input_ids\"]).to(device),\n",
    "                               token_type_ids=torch.LongTensor(model_input[\"token_type_ids\"]).to(device),\n",
    "                               attention_mask=torch.LongTensor(model_input[\"attention_mask\"]).to(device))\n",
    "    \n",
    "    return query_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indices\n",
    "sap_index = faiss.read_index(sap_model_cfg.index.index_save_name)\n",
    "base_index = faiss.read_index(base_model_cfg.index.index_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map concept IDs to one canonical string\n",
    "index_data = open(sap_model_cfg.index.index_ds.data_file, \"r\", encoding='utf-8-sig')\n",
    "id2string = {}\n",
    "\n",
    "for line in index_data:\n",
    "    cid, concept = line.split(\"\\t\")\n",
    "    id2string[int(cid) - 1] = concept.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Headache',\n",
       " 1: 'Myocardial infraction',\n",
       " 2: 'Coronary artery disease',\n",
       " 3: 'myocardial ischemia',\n",
       " 4: 'chronic kidney disease',\n",
       " 5: 'alchohol intoxication',\n",
       " 6: 'diabetes',\n",
       " 7: 'Hyperinsulinemia',\n",
       " 8: 'Nesina',\n",
       " 9: 'hypoglycemia',\n",
       " 10: 'anticoagulants',\n",
       " 11: 'Ibuprofen'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Base output before Self Alignment Pretraining:\n",
      "\n",
      "The most similar concepts to head pain are:\n",
      "1 Myocardial infraction 0.7848673164844513\n",
      "4 chronic kidney disease 0.766732469201088\n",
      "3 myocardial ischemia 0.761662483215332\n",
      "\n",
      "The most similar concepts to high blood sugar are:\n",
      "6 diabetes 0.9095035269856453\n",
      "0 Headache 0.9046077728271484\n",
      "8 Nesina 0.8512845933437347\n",
      "--------------------------------------------------\n",
      "BERT Base output after Self Alignment Pretraining:\n",
      "\n",
      "The most similar concepts to head pain are:\n",
      "0 Headache 0.6238322257995605\n",
      "8 Nesina -0.0450282096862793\n",
      "5 alchohol intoxication -0.1438838243484497\n",
      "\n",
      "The most similar concepts to high blood sugar are:\n",
      "7 Hyperinsulinemia 0.2721104621887207\n",
      "6 diabetes 0.21312189102172852\n",
      "9 hypoglycemia 0.10792684555053711\n"
     ]
    }
   ],
   "source": [
    "# Query both indices\n",
    "queries = [\"head pain\", \"high blood sugar\"]\n",
    "print(\"BERT Base output before Self Alignment Pretraining:\")\n",
    "query_index(base_model_cfg, base_model, base_index, queries, id2string)\n",
    "print(\"-\" * 50)\n",
    "print(\"BERT Base output after Self Alignment Pretraining:\")\n",
    "query_index(sap_model_cfg, sap_model, sap_index, queries, id2string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger knowledge bases keeping the default embedding size might be too large and cause out of memory issues. You can apply PCA or some other dimensionality reduction method to your data to reduce its memory footprint. Code for creating a text file of all the UMLS entities in the correct format needed to build an index and creating a dictionary mapping concept ids to canonical concept strings can be found here `examples/nlp/entity_linking/data/umls_dataset_processing.py`. \n",
    "\n",
    "The code for extracting knowledge base concept embeddings, training and applying a pca transformation to the embeddings, builing a faiss index and querying the index from the command line is located at `examples/nlp/entity_linking/build_and_query_index.py`. \n",
    "\n",
    "If you've cloned the NeMo repo, both of these steps can be run as follows on the commandline from the `examples/nlp/entity_linking/` directory."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python data/umls_dataset_processing.py --index --cfg /conf/medical_entity_linking_config.yaml\n",
    "python build_and_query_index.py --restore --cfg conf/medical_entity_linking_config.yaml --top_n 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermidate steps of the index building process are saved, so in the occurance of an error, previously completed steps do not need to be rerun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recap of the commands and steps to repeat this process on the full UMLS dataset. \n",
    "\n",
    "1) Download the UMLS datset file `MRCONSO.RRF` from the NIH website and place it in the `examples/nlp/entity_linking/data` directory.\n",
    "\n",
    "2) Run the following commands from the `examples/nlp/entity_linking` directory\n",
    "```\n",
    "python data/umls_dataset_processing.py --cfg conf/umls_medical_entity_linking_config.yaml\n",
    "python self_alignment_pretraining.py\n",
    "python data/umls_dataset_processing.py --index --cfg conf/umls_medical_entity_linking_config.yaml\n",
    "python build_and_query_index.py --restore --cfg conf/umls_medical_entity_linking_config.yaml --top_n 5\n",
    "```\n",
    "The model will take ~24hrs to train on two GPUs and ~48hrs to train on one GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
