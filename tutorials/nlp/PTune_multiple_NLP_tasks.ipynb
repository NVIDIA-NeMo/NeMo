{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a434f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH='main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell\n",
    "\n",
    "# install NeMo\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "challenging-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-ethiopia",
   "metadata": {},
   "source": [
    "In the era of super large language models, the traditional \"pre-train, fine-tune\" procedure is replaced by \"pre-train, prompt, and predict\" method as shown in the [survey paper](https://arxiv.org/pdf/2107.13586.pdf). The prompt method is versatile enough to support all kinds of NLP tasks as shown in the following table: \n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>Task</th>\n",
    "            <th>Input ([X])</th>\n",
    "            <th>Template</th>\n",
    "            <th>Answer([Y])</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=3>Text CLS</td>\n",
    "            <td>Sentiment</td>\n",
    "            <td>I love this movie.</td>\n",
    "            <td>[X] The movie is [Y]</td>\n",
    "            <td>great<br>fantastic<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Topics</td>\n",
    "            <td>He prompted the LM.</td>\n",
    "            <td>[X] The text is about [Y]</td>\n",
    "            <td>sports<br>science<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Intention</td>\n",
    "            <td>What is taxi fare to Denver?</td>\n",
    "            <td>[X] The question is about [Y]</td>\n",
    "            <td>quantity<br>city<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=1>Text-span CLS</td>\n",
    "            <td>Aspect<br>Sentiment</td>\n",
    "            <td>Poor service but good food.</td>\n",
    "            <td>[X] What about service? [Y]</td>\n",
    "            <td>Bad<br>Terrible<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=1>Text-pair CLS</td>\n",
    "            <td>NLI</td>\n",
    "            <td>[X1]: An old man with ...<br>[X2]: A man walks ...</td>\n",
    "            <td>Hypothesis: [X1], Premise: [X2], Answer: [Y]</td>\n",
    "            <td>Contradiction<br>Entailment<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=1>Tagging</td>\n",
    "            <td>NER</td>\n",
    "            <td>[X1]: Mike went to Paris.<br>[X2]: Paris</td>\n",
    "            <td>[X1] [X2] is a [Y]</td>\n",
    "            <td>Yes<br>No<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=2>Text Generation</td>\n",
    "            <td>Summarization</td>\n",
    "            <td>Las Vegas police ...</td>\n",
    "            <td>[X] TL;DR: [Y]</td>\n",
    "            <td>The victim ...<br>A woman ...<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Translation</td>\n",
    "            <td>Je vous aime.</td>\n",
    "            <td>French [X] English: [Y]</td>\n",
    "            <td>I love you.<br>I fancy you.<br>...</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "In this tutorial, we are going to describe how to use [P-Tuning method](https://arxiv.org/pdf/2103.10385.pdf) , which is one of the prompt engineering methods, to find good prompts for large GPT models. We show it can solve multiple downstream NLP tasks with good performance. P-Tuning leverages few continuous free parameters to serve as prompts fed as the input to the pre-trained language models. Freezing the large language model weights, P-Tuning model can be trained efficiently while delivering stats of art performance. \n",
    "\n",
    "Large Language Model can be trained with [NeMo Megatron](https://github.com/NVIDIA/NeMo/tree/main/examples/nlp/language_modeling), up to multi-billion parameters. In this notebook, we will use the pre-trained 344M GPT model released from NGC.\n",
    "\n",
    "# Task Description\n",
    "P-Tuning method can be applied to solve various NLP tasks. Without losing generality, in this notebook, we are going to use P-Tuning method to solve two NLP tasks: **Sentiment Analysis** task and **Question and Answer** task. \n",
    "\n",
    "**Sentiment Analysis** task is also known as opinion mining or emotion AI. It is a sub-field of NLP that tries to identify and extract opinions within a given text across blogs, reviews, social media, forums, news etc. \n",
    "\n",
    "For instance, **given sentences from news title, is it a good or bad news?**<br>\n",
    "\n",
    "**Question and Answer** task is to find the answer to a question given the context text. \n",
    "\n",
    "For instance, \n",
    "```\n",
    "Context: \n",
    "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
    "Question:\n",
    "How many Grammy awards did Beyoncé win for her first solo album?\n",
    "```\n",
    "\n",
    "# Dataset\n",
    "We will use [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) for sentiment analysis task and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) for question and answer task.\n",
    "\n",
    "The [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) contains the sentiments for financial news headlines from the perspective of a retail investor. Further details about the dataset can be found in: Malo, P., Sinha, A., Takala, P., Korhonen, P. and Wallenius, J. (2014): “Good debt or bad debt: Detecting semantic orientations in economic texts.” Journal of the American Society for Information Science and Technology.\n",
    "\n",
    "Here's an example of what an annotated abstract from the corpus looks like:\n",
    "\n",
    "```\n",
    "HELSINKI Thomson Financial - Shares in Cargotec fell sharply in early afternoon trade after the cargo handling group posted a surprise drop in April-June profits , which overshadowed the large number of new orders received during the three months .@negative\n",
    "LONDON MarketWatch -- Share prices ended lower in London Monday as a rebound in bank stocks failed to offset broader weakness for the FTSE 100 .@negative\n",
    "Operating profit fell to EUR 35.4 mn from EUR 68.8 mn in 2007 , including vessel sales gain of EUR 12.3 mn .@negative\n",
    "Sales in Finland decreased by 10.5 % in January , while sales outside Finland dropped by 17 % .@negative\n",
    "```\n",
    "\n",
    "The [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "Let's download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "federal-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"DATA_DIR\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e1b08",
   "metadata": {},
   "source": [
    "## Downloading Financial Phrase Bank Dataset\n",
    "\n",
    "The datase is collected by Malo et al. 2014, and can be downloaded from this [link](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip). The zip file for the Financial Phrase Bank Dataset has been provided for ease of download and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ad03fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-07 20:38:46--  https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip\n",
      "Resolving www.researchgate.net (www.researchgate.net)... 104.17.32.105, 104.17.33.105, 2606:4700::6811:2169, ...\n",
      "Connecting to www.researchgate.net (www.researchgate.net)|104.17.32.105|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip [following]\n",
      "--2022-02-07 20:38:46--  https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip\n",
      "Reusing existing connection to www.researchgate.net:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681890 (666K) [application/zip]\n",
      "Saving to: ‘FinancialPhraseBank-v10.zip.2’\n",
      "\n",
      "FinancialPhraseBank 100%[===================>] 665.91K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-02-07 20:38:47 (10.4 MB/s) - ‘FinancialPhraseBank-v10.zip.2’ saved [681890/681890]\n",
      "\n",
      "Archive:  FinancialPhraseBank-v10.zip\n",
      "   creating: DATA_DIR/FinancialPhraseBank-v1.0/\n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/License.txt  \n",
      "   creating: DATA_DIR/__MACOSX/\n",
      "   creating: DATA_DIR/__MACOSX/FinancialPhraseBank-v1.0/\n",
      "  inflating: DATA_DIR/__MACOSX/FinancialPhraseBank-v1.0/._License.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/README.txt  \n",
      "  inflating: DATA_DIR/__MACOSX/FinancialPhraseBank-v1.0/._README.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_50Agree.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_66Agree.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_75Agree.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip\n",
    "!unzip FinancialPhraseBank-v10.zip -d {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "radical-castle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\n"
     ]
    }
   ],
   "source": [
    "# If you want to see more examples, you can explore the text of the corpus using the file browser to the left, or open files directly, for example typing a command like the following in a code-cell:\n",
    "\n",
    "! head -1 $DATA_DIR/FinancialPhraseBank-v1.0/Sentences_50Agree.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f69a9",
   "metadata": {},
   "source": [
    "## Download the SQuDA dataset\n",
    "\n",
    "Download a copy of the dataset (distributed under the CC BY-SA 4.0 license):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27e91c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-07 20:48:05--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.108.153, 185.199.109.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42123633 (40M) [application/json]\n",
      "Saving to: ‘train-v2.0.json.1’\n",
      "\n",
      "train-v2.0.json.1   100%[===================>]  40.17M  1.24MB/s    in 29s     \n",
      "\n",
      "2022-02-07 20:48:34 (1.38 MB/s) - ‘train-v2.0.json.1’ saved [42123633/42123633]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "!mv train-v2.0.json {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-numbers",
   "metadata": {},
   "source": [
    "## Pre-process Financial Phrase Bank Dataset\n",
    "\n",
    "In this pre-process step, we are going to convert the downloaded dataset into the format that can be used for P-Tuning dataloader. The data is split into 10 folds so we can do 10-fold cross validation. In this notebook, we will use the first fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "198287d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "files = ['Sentences_50Agree.txt', 'Sentences_66Agree.txt', 'Sentences_75Agree.txt', 'Sentences_AllAgree.txt']\n",
    "base_dir = DATA_DIR + '/FinancialPhraseBank-v1.0/'\n",
    "files = [base_dir + f for f in files]\n",
    "\n",
    "alllines = []\n",
    "for fn in files:\n",
    "    with open(fn, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        alllines.extend(f.readlines())\n",
    "\n",
    "random.shuffle(alllines)\n",
    "fold = 10\n",
    "fold_size = len(alllines) // fold\n",
    "\n",
    "chunk_start = list(range(0, 14780, 1478))\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for start_id in chunk_start:\n",
    "    chunks.append(alllines[start_id:start_id+fold_size])\n",
    "\n",
    "def gen_file(data, fold_id, split_type):\n",
    "    filename = \"{}/{}_{}.txt\".format(base_dir, split_type, fold_id)\n",
    "    with open(filename, 'w') as f:\n",
    "        obj = {}\n",
    "        for line in data:\n",
    "            splits = line.split('@')\n",
    "            part1 = splits[0].strip()\n",
    "            part2 = splits[1].strip()\n",
    "            obj['sentence'] = part1\n",
    "            obj['label'] = part2\n",
    "            obj['prompt_tag'] = 'sentiment-task'\n",
    "            f.write(json.dumps(obj)+'\\n')\n",
    "\n",
    "\n",
    "def gen_fold(fold_number):\n",
    "    lists = list(range(fold))\n",
    "    test_id = (fold_number + fold) % fold\n",
    "    val_id = (fold_number + fold - 1) % fold\n",
    "    test_set = chunks[test_id]\n",
    "    val_set = chunks[val_id]\n",
    "    lists.remove(test_id)\n",
    "    lists.remove(val_id)\n",
    "    train_set = []\n",
    "    for idd in lists:\n",
    "        train_set += chunks[idd]\n",
    "    gen_file(train_set, fold_number, 'train')\n",
    "    gen_file(val_set, fold_number, 'validation')\n",
    "    gen_file(test_set, fold_number, 'test')\n",
    "\n",
    "gen_fold(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-debate",
   "metadata": {},
   "source": [
    "The data is converted to the loss json file. Each line has three keys \"sentence\", \"label\" and \"prompt-tag\". \n",
    "Here are the first two lines of converted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sound-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sentence\": \"The contract includes heating plant equipment and associated installation work .\", \"label\": \"neutral\", \"prompt-tag\": \"sentiment-task\"}\n",
      "{\"sentence\": \"The utility will also provide services related to electricity management , such as hedging trades and risk management and reporting .\", \"label\": \"neutral\", \"prompt-tag\": \"sentiment-task\"}\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 $DATA_DIR/FinancialPhraseBank-v1.0/train_0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71556cbe",
   "metadata": {},
   "source": [
    "### Preprocess SQuAD Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "spectacular-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = DATA_DIR + '/train-v2.0.json'\n",
    "with open(file_name, 'r') as f:\n",
    "    data_obj = json.load(f)\n",
    "\n",
    "articles = data_obj['data']\n",
    "test_len = 40\n",
    "validation_len = 40\n",
    "train_len = len(articles) - test_len - validation_len\n",
    "train_records = []\n",
    "validation_records = []\n",
    "test_records = []\n",
    "\n",
    "\n",
    "def get_records(sub_articals, records):\n",
    "    for article in sub_articals:\n",
    "        paragraphs = article['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph['qas']\n",
    "            context = paragraph['context']\n",
    "            for qa in qas:\n",
    "                record = {}\n",
    "                record['question'] = qa['question']\n",
    "                record['context'] = context\n",
    "                if qa['is_impossible']:\n",
    "                    record['label'] = 'NA'\n",
    "                else:\n",
    "                    record['label'] = qa['answers'][0]['text']\n",
    "                record['prompt_tag'] = 'qa-task'\n",
    "                records.append(json.dumps(record))\n",
    "get_records(articles[:train_len], train_records)\n",
    "get_records(articles[train_len:train_len+validation_len], validation_records)\n",
    "get_records(articles[train_len+validation_len:], test_records)\n",
    "random.shuffle(train_records)\n",
    "random.shuffle(validation_records)\n",
    "random.shuffle(test_records)\n",
    "squad_dir = \"DATA_DIR/squad\"\n",
    "os.makedirs(squad_dir, exist_ok=True)\n",
    "with open(squad_dir+'/train.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(train_records))\n",
    "with open(squad_dir+'/validation.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(validation_records))\n",
    "with open(squad_dir+'/test.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_records))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6bcd460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"Where does the Downeaster service to maine start?\", \"context\": \"Amtrak's Northeast Corridor and Chicago lines originate at South Station, which serves as a major intermodal transportation hub, and stop at Back Bay. Fast Northeast Corridor trains, which serve New York City, Washington, D.C., and points in between, also stop at Route 128 Station in the southwestern suburbs of Boston. Meanwhile, Amtrak's Downeaster service to Maine originates at North Station, despite the current lack of a dedicated passenger rail link between the two railhubs, other than the \\\"T\\\" subway lines.\", \"label\": \"North Station\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"What period do Italian historians believe came immediately after the High Period of the Middle Ages?\", \"context\": \"The changes brought about by these developments have led many scholars to view this period as the end of the Middle Ages and beginning of modern history and early modern Europe. However, the division is somewhat artificial, since ancient learning was never entirely absent from European society. As a result there was developmental continuity between the ancient age (via classical antiquity) and the modern age. Some historians, particularly in Italy, prefer not to speak of the late Middle Ages at all, but rather see the high period of the Middle Ages transitioning to the Renaissance and the modern era.\", \"label\": \"the Renaissance\", \"prompt_tag\": \"qa-task\"}\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {squad_dir}/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba98b18",
   "metadata": {},
   "source": [
    "### Combine the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a30f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_data_dir = f\"{DATA_DIR}/mix\"\n",
    "os.makedirs(mix_data_dir, exist_ok=True)\n",
    "!cat $DATA_DIR/FinancialPhraseBank-v1.0/train_0.txt {squad_dir}/train.txt | shuf > {mix_data_dir}/train.txt\n",
    "!cat $DATA_DIR/FinancialPhraseBank-v1.0/validation_0.txt {squad_dir}/validation.txt | shuf > {mix_data_dir}/validation.txt\n",
    "!cat $DATA_DIR/FinancialPhraseBank-v1.0/test_0.txt {squad_dir}/test.txt | shuf > {mix_data_dir}/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e373c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"Did Hampshire Constabulary record fewer or more crime incidents in 2009/10 than the year before?\", \"context\": \"According to Hampshire Constabulary figures, Southampton is currently safer than it has ever been before, with dramatic reductions in violent crime year on year for the last three years. Data from the Southampton Safer City Partnership shows there has been a reduction in all crimes in recent years and an increase in crime detection rates. According to government figures Southampton has a higher crime rate than the national average. There is some controversy regarding comparative crime statisitics due to inconsistencies between different police forces recording methodologies. For example, in Hampshire all reported incidents are recorded and all records then retained. However, in neighbouring Dorset crimes reports withdrawn or shown to be false are not recorded, reducing apparent crime figures. In the violence against the person category, the national average is 16.7 per 1000 population while Southampton is 42.4 per 1000 population. In the theft from a vehicle category, the national average is 7.6 per 1000 compared to Southampton's 28.4 per 1000. Overall, for every 1,000 people in the city, 202 crimes are recorded. Hampshire Constabulary's figures for 2009/10 show fewer incidents of recorded crime in Southampton than the previous year.\", \"label\": \"fewer\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"What languages are grouped into northern and southern branches?\", \"context\": \"Conventionally, Iranian languages are grouped in \\\"western\\\" and \\\"eastern\\\" branches. These terms have little meaning with respect to Old Avestan as that stage of the language may predate the settling of the Iranian peoples into western and eastern groups. The geographic terms also have little meaning when applied to Younger Avestan since it isn't known where that dialect (or dialects) was spoken either. Certain is only that Avestan (all forms) and Old Persian are distinct, and since Old Persian is \\\"western\\\", and Avestan was not Old Persian, Avestan acquired a default assignment to \\\"eastern\\\". Confusing the issue is the introduction of a western Iranian substrate in later Avestan compositions and redactions undertaken at the centers of imperial power in western Iran (either in the south-west in Persia, or in the north-west in Nisa/Parthia and Ecbatana/Media).\", \"label\": \"NA\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"When did United States begin to provide foreign aid to Israel?\", \"context\": \"In July 2007, American business magnate and investor Warren Buffett's holding company Berkshire Hathaway bought an Israeli company, Iscar, its first non-U.S. acquisition, for $4 billion. Since the 1970s, Israel has received military aid from the United States, as well as economic assistance in the form of loan guarantees, which now account for roughly half of Israel's external debt. Israel has one of the lowest external debts in the developed world, and is a net lender in terms of net external debt (the total value of assets vs. liabilities in debt instruments owed abroad), which in December 2015[update] stood at a surplus of US$118 billion.\", \"label\": \"1970s\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"When did the Ottoman Empire disband?\", \"context\": \"In the 16th century the northern fringe of the Sahara, such as coastal regencies in present-day Algeria and Tunisia, as well as some parts of present-day Libya, together with the semi-autonomous kingdom of Egypt, were occupied by the Ottoman Empire. From 1517 Egypt was a valued part of the Ottoman Empire, ownership of which provided the Ottomans with control over the Nile Valley, the east Mediterranean and North Africa. The benefit of the Ottoman Empire was the freedom of movement for citizens and goods. Trade exploited the Ottoman land routes to handle the spices, gold and silk from the East, manufactures from Europe, and the slave and gold traffic from Africa. Arabic continued as the local language and Islamic culture was much reinforced. The Sahel and southern Sahara regions were home to several independent states or to roaming Tuareg clans.\", \"label\": \"NA\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"What is the Boardwalk Hall?\", \"context\": \"Boardwalk Hall, formally known as the \\\"Historic Atlantic City Convention Hall\\\", is an arena in Atlantic City along the boardwalk. Boardwalk Hall was Atlantic City's primary convention center until the opening of the Atlantic City Convention Center in 1997. The Atlantic City Convention Center includes 500,000 sq ft (46,000 m2) of showroom space, 5 exhibit halls, 45 meeting rooms with 109,000 sq ft (10,100 m2) of space, a garage with 1,400 parking spaces, and an adjacent Sheraton hotel. Both the Boardwalk Hall and Convention Center are operated by the Atlantic City Convention & Visitors Authority.\", \"label\": \"an arena\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"Why did UNFPA never change their name?\", \"context\": \"UNFPA began operations in 1969 as the United Nations Fund for Population Activities (the name was changed in 1987) under the administration of the United Nations Development Fund. In 1971 it was placed under the authority of the United Nations General Assembly.\", \"label\": \"NA\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"What is Nyaya uninterested in?\", \"context\": \"In its metaphysics, Ny\\u0101ya school is closer to the Vai\\u015be\\u1e63ika school than others. It holds that human suffering results from mistakes/defects produced by activity under wrong knowledge (notions and ignorance). Moksha (liberation), it states, is gained through right knowledge. This premise led Ny\\u0101ya to concern itself with epistemology, that is the reliable means to gain correct knowledge and to remove wrong notions. False knowledge is not merely ignorance to Naiyayikas, it includes delusion. Correct knowledge is discovering and overcoming one's delusions, and understanding true nature of soul, self and reality. The Ny\\u0101ya S\\u016btras begin:\", \"label\": \"NA\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"Who designed the New York Life Building?\", \"context\": \"Heading east, 27th Street passes through Chelsea Park between Tenth and Ninth Avenues, with the Fashion Institute of Technology (FIT) on the corner of Eighth. On Madison Avenue between 26th and 27th streets, on the site of the old Madison Square Garden, is the New York Life Building, built in 1928 and designed by Cass Gilbert, with a square tower topped by a striking gilded pyramid. Twenty-Seventh Street passes one block north of Madison Square Park and culminates at Bellevue Hospital Center on First Avenue.\", \"label\": \"Cass Gilbert\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"Oltenia is also known as what?\", \"context\": \"After the Austro-Turkish War of 1716\\u20131718 the Treaty of Passarowitz confirmed the loss of the Banat, Serbia and \\\"Little Walachia\\\" (Oltenia) to Austria. The Treaty also revealed that the Ottoman Empire was on the defensive and unlikely to present any further aggression in Europe. The Austro-Russian\\u2013Turkish War, which was ended by the Treaty of Belgrade in 1739, resulted in the recovery of Serbia and Oltenia, but the Empire lost the port of Azov, north of the Crimean Peninsula, to the Russians. After this treaty the Ottoman Empire was able to enjoy a generation of peace, as Austria and Russia were forced to deal with the rise of Prussia.\", \"label\": \"Little Walachia\", \"prompt_tag\": \"qa-task\"}\n",
      "{\"question\": \"Which was the first new indoor mall in Melbourne?\", \"context\": \"Height limits in the Melbourne CBD were lifted in 1958, after the construction of ICI House, transforming the city's skyline with the introduction of skyscrapers. Suburban expansion then intensified, serviced by new indoor malls beginning with Chadstone Shopping Centre. The post-war period also saw a major renewal of the CBD and St Kilda Road which significantly modernised the city. New fire regulations and redevelopment saw most of the taller pre-war CBD buildings either demolished or partially retained through a policy of facadism. Many of the larger suburban mansions from the boom era were also either demolished or subdivided.\", \"label\": \"Chadstone Shopping Centre\", \"prompt_tag\": \"qa-task\"}\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 {mix_data_dir}/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e395029",
   "metadata": {},
   "source": [
    "## Add the Data Processor to generate the prompted input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7347628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.data.glue_benchmark.gpt_ptune_dataset import DataProcessor, register_taskdata_processor\n",
    "from typing import Dict, List\n",
    "from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n",
    "\n",
    "\n",
    "class SentimentProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the sentiment analysis data set.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_ptune_query(\n",
    "        self, content: Dict, prompt_token_id: int, max_seq_len: int, templates: List[int], tokenizer: TokenizerSpec,\n",
    "    ):\n",
    "        text_a = content['sentence']\n",
    "        sentence_a = f\" Sentence: {text_a}\"\n",
    "        sentence_b = f\" Sentiment:\"\n",
    "        a_input_token_ids = tokenizer.text_to_ids(sentence_a)\n",
    "        b_input_token_ids = tokenizer.text_to_ids(sentence_b)\n",
    "        cut = 0\n",
    "        total_num_ids = len(a_input_token_ids) + len(b_input_token_ids) + sum(templates)\n",
    "        if total_num_ids > max_seq_len:\n",
    "            cut = total_num_ids - max_seq_len\n",
    "        return (\n",
    "            [prompt_token_id] * (templates[0] + templates[1])\n",
    "            + a_input_token_ids[cut:]\n",
    "            + [prompt_token_id] * templates[2]\n",
    "            + b_input_token_ids\n",
    "        )\n",
    "\n",
    "    def label2string(self, label):\n",
    "        return ' ' + label\n",
    "\n",
    "class QAProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the question and answer data set.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_ptune_query(\n",
    "        self, content: Dict, prompt_token_id: int, max_seq_len: int, templates: List[int], tokenizer: TokenizerSpec,\n",
    "    ):\n",
    "        text_a = content['context']\n",
    "        text_b = content['question']\n",
    "\n",
    "        sentence_a = f\" Context: {text_a}\"\n",
    "        sentence_b = f\" Question: {text_b}?\"\n",
    "        sentence_c = f\" Answer:\"\n",
    "        a_input_token_ids = tokenizer.text_to_ids(sentence_a)\n",
    "        b_input_token_ids = tokenizer.text_to_ids(sentence_b)\n",
    "        c_input_token_ids = tokenizer.text_to_ids(sentence_c)\n",
    "        cut = 0\n",
    "        total_num_ids = len(a_input_token_ids) + len(b_input_token_ids) + sum(templates)\n",
    "        if total_num_ids > max_seq_len:\n",
    "            cut = total_num_ids - max_seq_len\n",
    "        return (\n",
    "            [prompt_token_id] * templates[0]\n",
    "            + a_input_token_ids[cut:]\n",
    "            + [prompt_token_id] * templates[1]\n",
    "            + b_input_token_ids\n",
    "            + [prompt_token_id] * templates[2]\n",
    "            + c_input_token_ids\n",
    "        )\n",
    "\n",
    "    def label2string(self, label):\n",
    "        return ' ' + label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70d5a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_taskdata_processor(\"qa-task\", QAProcessor())\n",
    "register_taskdata_processor(\"sentiment-task\", SentimentProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813cc36",
   "metadata": {},
   "source": [
    "## Convert the Megatron-LM Weights to Nemo file\n",
    "\n",
    "P-Tuning method works the best with large GPT lanague models. From our experiences, models of size 5B or above give good performance. If you already have a large GPT model ready, skip this section. \n",
    "\n",
    "In this example, we will use the pretrained 344M NeMo Megatron GPT model from [Megatron-LM project](https://github.com/NVIDIA/Megatron-LM). To load it in NeMo Megatron, We first need to convert the Megatron-LM checkpoint to the `.nemo` file. Let's download the pretrained model weights and vocabulary file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82b8e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "gpt_file = 'megatron_lm_345m_v0.0.zip'\n",
    "vocab_file = 'gpt2-vocab.json'\n",
    "merge_file = 'gpt2-merge.txt'\n",
    "checkpoint_filename = 'model_optim_rng.pt'\n",
    "\n",
    "if not pathlib.Path(gpt_file).exists():\n",
    "    !wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O $gpt_file\n",
    "    !unzip -f $gpt_file\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/$vocab_file -O $vocab_file \n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -O $merge_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b00ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is already exists\n"
     ]
    }
   ],
   "source": [
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "# Prepare the model parameters \n",
    "# download the model's configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "MODEL_CONFIG = \"megatron_gpt_config.yaml\"\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ae5a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR/configs/megatron_gpt_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "config.model.num_layers = 24\n",
    "config.model.hidden_size = 1024\n",
    "config.model.ffn_hidden_size = 4096\n",
    "config.model.num_attention_heads = 16\n",
    "config.model.tokenizer.vocab_file = vocab_file\n",
    "config.model.tokenizer.merge_file = merge_file\n",
    "config.model.tensor_model_parallel_size = 1\n",
    "config.model.data.data_prefix = ''\n",
    "config.model.max_position_embeddings = 1024\n",
    "config.model.data.seq_length = 1024\n",
    "config.model.encoder_seq_length = 1024\n",
    "config.cfg = {}\n",
    "config.cfg.cfg = config.model\n",
    "with open('hparams.yaml', 'w') as f:\n",
    "    f.write(OmegaConf.to_yaml(config.cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1beda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PWD = os.getcwd()\n",
    "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/megatron_lm_ckpt_to_nemo.py')\n",
    "!python -m torch.distributed.run --nproc_per_node=1 megatron_lm_ckpt_to_nemo.py --checkpoint_folder=$PWD/release/mp_rank_00/ --checkpoint_name=$checkpoint_filename --hparams_file=$PWD/hparams.yaml --nemo_file_path=$PWD/gpt_344m.nemo --model_type=gpt --tensor_model_parallel_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b455a6",
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "\n",
    "Our P-Tuning text classification model is comprised of the pretrained GPT LM model followed by a prompt encoder layer.\n",
    "\n",
    "The model is defined in a config file which declares multiple important sections. They are:\n",
    "- **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "speaking-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = \"megatron_ptune_gpt.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model's configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "criminal-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR/configs/megatron_ptune_gpt.yaml\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "# Note: these are small batch-sizes - increase as appropriate to available GPU capacity\n",
    "config.model.data.train_ds.batch_size=8\n",
    "config.model.data.validation_ds.batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-effort",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Setting up Data within the config\n",
    "\n",
    "Among other things, the config file contains dictionaries called train_ds, validation_ds and test_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "informed-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this tutorial train and dev datasets are located in the same folder, so it is enough to add the path of the data directory to the config\n",
    "#config.model.dataset.classes = ['positive', 'neutral', 'negative']\n",
    "config.model.data.train_ds.file_path = DATA_DIR+'/mix/train.txt'\n",
    "config.model.data.validation_ds.file_path = DATA_DIR+'/mix/validation.txt'\n",
    "config.model.data.test_ds.file_path = DATA_DIR+'/mix/test.txt'\n",
    "\n",
    "\n",
    "# if you want to decrease the size of your datasets, uncomment the lines below:\n",
    "# NUM_SAMPLES = 1000\n",
    "# config.model.train_ds.num_samples = NUM_SAMPLES\n",
    "# config.model.validation_ds.num_samples = NUM_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "divine-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: megatron_ptune_gpt\n",
      "trainer:\n",
      "  gpus: 2\n",
      "  num_nodes: 1\n",
      "  precision: 16\n",
      "  logger: false\n",
      "  checkpoint_callback: false\n",
      "  replace_sampler_ddp: false\n",
      "  max_epochs: 3\n",
      "  max_steps: null\n",
      "  log_every_n_steps: 10\n",
      "  val_check_interval: 300\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 1.0\n",
      "  resume_from_checkpoint: null\n",
      "exp_manager:\n",
      "  explicit_log_dir: null\n",
      "  exp_dir: null\n",
      "  name: megatron_ptune_gpt\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    project: null\n",
      "    name: null\n",
      "  resume_if_exists: true\n",
      "  resume_ignore_no_checkpoint: true\n",
      "  create_checkpoint_callback: true\n",
      "  checkpoint_callback_params:\n",
      "    monitor: val_acc\n",
      "    save_top_k: 2\n",
      "    mode: max\n",
      "    always_save_nemo: false\n",
      "    filename: megatron_gpt--{val_acc:.3f}-{step}\n",
      "    model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "    save_best_model: true\n",
      "model:\n",
      "  tensor_model_parallel_size: 1\n",
      "  seed: 1234\n",
      "  nemo_path: null\n",
      "  use_lm_finetune: false\n",
      "  pseudo_token: '[PROMPT]'\n",
      "  max_decode_length: null\n",
      "  language_model:\n",
      "    nemo_file: null\n",
      "  prompt_encoder:\n",
      "    template:\n",
      "    - 3\n",
      "    - 3\n",
      "    - 3\n",
      "    dropout: 0.0\n",
      "    num_layers: 2\n",
      "    task_dependent: true\n",
      "  data:\n",
      "    train_ds:\n",
      "      file_path: DATA_DIR/mix/train.txt\n",
      "      batch_size: 8\n",
      "      shuffle: true\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    validation_ds:\n",
      "      file_path: DATA_DIR/mix/validation.txt\n",
      "      batch_size: 8\n",
      "      shuffle: false\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    test_ds:\n",
      "      file_path: DATA_DIR/mix/test.txt\n",
      "      batch_size: 32\n",
      "      shuffle: false\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "  optim:\n",
      "    name: adam\n",
      "    lr: 1.0e-05\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.999\n",
      "    weight_decay: 0.0005\n",
      "    sched:\n",
      "      name: WarmupAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      last_epoch: -1\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "  infer_samples:\n",
      "  - prompt_tag: boolq-full-text\n",
      "    sentence: 'Slave states and free states -- In the 1770s, blacks throughout New\n",
      "      England began sending petitions to northern legislatures demanding freedom.\n",
      "      Five of the Northern self-declared states adopted policies to at least gradually\n",
      "      abolish slavery: Pennsylvania (1780), New Hampshire and Massachusetts (1783),\n",
      "      Connecticut and Rhode Island (1784). Vermont had abolished slavery in 1777,\n",
      "      while it was still independent, and when it joined the United States as the\n",
      "      14th state in 1791, it was the first state to join untainted by slavery. These\n",
      "      state jurisdictions thus enacted the first abolition laws in the Americas. By\n",
      "      1804 (including, New York (1799), New Jersey (1804)), all of the northern states\n",
      "      had abolished slavery or set measures in place to gradually abolish it.'\n",
      "    question: were the new england states free states in 1854\n",
      "  - prompt_tag: boolq-full-text\n",
      "    sentence: Titius–Bode law -- Recent astronomical research suggests that planetary\n",
      "      systems around some other stars may follow Titius--Bode-like laws. Bovaird and\n",
      "      Lineweaver applied a generalized Titius--Bode relation to 68 exoplanet systems\n",
      "      that contain four or more planets. They showed that 96% of these exoplanet systems\n",
      "      adhere to a generalized Titius--Bode relation to a similar or greater extent\n",
      "      than the Solar System does. The locations of potentially undetected exoplanets\n",
      "      are predicted in each system.\n",
      "    question: do exoplanetary systems follow the titus bode rule\n",
      "  - prompt_tag: boolq-full-text\n",
      "    sentence: Bipolar disorder -- Bipolar disorder, previously known as manic depression,\n",
      "      is a mental disorder that causes periods of depression and periods of abnormally\n",
      "      elevated mood. The elevated mood is significant and is known as mania or hypomania,\n",
      "      depending on its severity, or whether symptoms of psychosis are present. During\n",
      "      mania, an individual behaves or feels abnormally energetic, happy, or irritable.\n",
      "      Individuals often make poorly thought out decisions with little regard to the\n",
      "      consequences. The need for sleep is usually reduced during manic phases. During\n",
      "      periods of depression, there may be crying, a negative outlook on life, and\n",
      "      poor eye contact with others. The risk of suicide among those with the illness\n",
      "      is high at greater than 6 percent over 20 years, while self-harm occurs in 30--40\n",
      "      percent. Other mental health issues such as anxiety disorders and substance\n",
      "      use disorder are commonly associated.\n",
      "    question: is manic depression the same as bi polar\n",
      "  - prompt_tag: boolq-full-text\n",
      "    sentence: SS Politician -- SS Politician was an 8000-ton cargo ship owned by T\n",
      "      & J Harrison of Liverpool. It left Liverpool on 3 February 1941, bound for Kingston,\n",
      "      Jamaica and New Orleans with a cargo including 28,000 cases of malt whisky.\n",
      "      The ship sank off the north coast of Eriskay in the Outer Hebrides, off the\n",
      "      west coast of Scotland, and much of the wreck's cargo was salvaged by the island's\n",
      "      inhabitants. The story of the wreck and looting was the basis for the book and\n",
      "      film Whisky Galore!.\n",
      "    question: was whiskey galore based on a true story\n",
      "  - prompt_tag: boolq-full-text\n",
      "    sentence: Plants in space -- Plant research continued on the International Space\n",
      "      Station. Biomass Production System was used on the ISS Expedition 4. The Vegetable\n",
      "      Production System (Veggie) system was later used aboard ISS. Plants tested in\n",
      "      Veggie before going into space included lettuce, Swiss chard, radishes, Chinese\n",
      "      cabbage and peas. Red Romaine lettuce was grown in space on Expedition 40 which\n",
      "      were harvested when mature, frozen and tested back on Earth. Expedition 44 members\n",
      "      became the first American astronauts to eat plants grown in space on 10 August\n",
      "      2015, when their crop of Red Romaine was harvested. Since 2003 Russian cosmonauts\n",
      "      have been eating half of their crop while the other half goes towards further\n",
      "      research. In 2012, a sunflower bloomed aboard the ISS under the care of NASA\n",
      "      astronaut Donald Pettit. In January 2016, US astronauts announced that a zinnia\n",
      "      had blossomed aboard the ISS.\n",
      "    question: are there plants on the international space station\n",
      "  - prompt_tag: boolq-full-text\n",
      "    sentence: Goal (ice hockey) -- In ice hockey, a goal is scored when the puck entirely\n",
      "      crosses the goal line between the two goal posts and below the goal crossbar.\n",
      "      A goal awards one point to the team attacking the goal scored upon, regardless\n",
      "      of which team the player who actually deflected the puck into the goal belongs\n",
      "      to (see also own goal). Typically, a player on the team attempting to score\n",
      "      shoots the puck with his/her stick towards the goal net opening, and a player\n",
      "      on the opposing team called a goaltender tries to block the shot to prevent\n",
      "      a goal from being scored against his/her team.\n",
      "    question: does the hockey puck have to cross the line to be a goal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-mauritius",
   "metadata": {},
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
    "\n",
    "Let's first instantiate a Trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "computational-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer config - \n",
      "\n",
      "gpus: 2\n",
      "num_nodes: 1\n",
      "precision: 16\n",
      "logger: false\n",
      "checkpoint_callback: false\n",
      "replace_sampler_ddp: false\n",
      "max_epochs: 3\n",
      "max_steps: null\n",
      "log_every_n_steps: 10\n",
      "val_check_interval: 300\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 1.0\n",
      "resume_from_checkpoint: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "unique-genre",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPPlugin\n",
    "\n",
    "\n",
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "cuda = 1 if torch.cuda.is_available() else 0\n",
    "config.trainer.gpus = cuda\n",
    "config.trainer.max_epochs = 100\n",
    "config.trainer.val_check_interval=95230\n",
    "# for PyTorch Native AMP set precision=16\n",
    "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "\n",
    "# remove distributed training flags\n",
    "config.trainer.accelerator = None\n",
    "\n",
    "trainer = pl.Trainer(plugins=[NLPDDPPlugin()], **config.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-literature",
   "metadata": {},
   "source": [
    "## Setting up a NeMo Experiment\n",
    "\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "mathematical-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-07 21:39:49 exp_manager:558] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2022-02-07 21:39:49 exp_manager:411] There was no checkpoint folder at checkpoint_dir :/NeMo/tutorials/nlp/nemo_experiments/megatron_ptune_gpt/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-07 21:39:49 exp_manager:283] Experiments will be logged at /NeMo/tutorials/nlp/nemo_experiments/megatron_ptune_gpt\n",
      "[NeMo I 2022-02-07 21:39:49 exp_manager:648] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-07 21:39:49 exp_manager:901] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/NeMo/tutorials/nlp/nemo_experiments/megatron_ptune_gpt'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ea6cd",
   "metadata": {},
   "source": [
    "We will use the converted `.nemo` file as our LM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "compact-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "# config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL\n",
    "config.model.language_model.nemo_file = 'gpt_344m.nemo'\n",
    "config.model.tensor_model_parallel_size = 1\n",
    "config.exp_manager.checkpoint_callback_params.save_top_k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-geometry",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders we'll be prepared for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "indoor-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-07 21:41:55 tokenizer_utils:193] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m and custom vocab file: /tmp/tmpre5q9okj/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json\n",
      "[NeMo I 2022-02-07 21:41:55 tokenizer_utils:125] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpre5q9okj/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-07 21:42:02 megatron_gpt_model:763] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2022-02-07 21:42:05 save_restore_connector:154] Model MegatronGPTModel was successfully restored from /NeMo/tutorials/nlp/gpt_344m.nemo.\n",
      "[NeMo I 2022-02-07 21:42:05 auto_tokenizer:171] 1 special tokens added, resize your model accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.nlp.models.language_modeling.megatron_ptune_gpt_model import MegatronGPTPTuneModel\n",
    "model_ptune = MegatronGPTPTuneModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-pipeline",
   "metadata": {},
   "source": [
    "## Monitoring training progress\n",
    "Optionally, you can create a Tensorboard visualization to monitor training progress.\n",
    "If you're not using Colab, refer to [https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) if you're facing issues with running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "changed-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use tensorboard, please use this notebook in a Google Colab environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google import colab\n",
    "    COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "applied-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-07 21:48:49 megatron_ptune_gpt_model:332] Building P-Tune datasets.\n",
      "[NeMo I 2022-02-07 21:48:49 gpt_ptune_dataset:201] Processing DATA_DIR/mix/test.txt\n",
      "[NeMo I 2022-02-07 21:48:50 gpt_ptune_dataset:331] Writing example 0 of 13538\n",
      "[NeMo I 2022-02-07 21:49:01 gpt_ptune_dataset:331] Writing example 10000 of 13538\n",
      "[NeMo I 2022-02-07 21:49:04 gpt_ptune_dataset:201] Processing DATA_DIR/mix/train.txt\n",
      "[NeMo I 2022-02-07 21:49:12 gpt_ptune_dataset:331] Writing example 0 of 118101\n",
      "[NeMo I 2022-02-07 21:49:23 gpt_ptune_dataset:331] Writing example 10000 of 118101\n",
      "[NeMo I 2022-02-07 21:49:33 gpt_ptune_dataset:331] Writing example 20000 of 118101\n",
      "[NeMo I 2022-02-07 21:49:43 gpt_ptune_dataset:331] Writing example 30000 of 118101\n",
      "[NeMo I 2022-02-07 21:49:53 gpt_ptune_dataset:331] Writing example 40000 of 118101\n",
      "[NeMo I 2022-02-07 21:50:04 gpt_ptune_dataset:331] Writing example 50000 of 118101\n",
      "[NeMo I 2022-02-07 21:50:14 gpt_ptune_dataset:331] Writing example 60000 of 118101\n",
      "[NeMo I 2022-02-07 21:50:23 gpt_ptune_dataset:331] Writing example 70000 of 118101\n",
      "[NeMo I 2022-02-07 21:50:33 gpt_ptune_dataset:331] Writing example 80000 of 118101\n",
      "[NeMo I 2022-02-07 21:50:43 gpt_ptune_dataset:331] Writing example 90000 of 118101\n",
      "[NeMo I 2022-02-07 21:50:53 gpt_ptune_dataset:331] Writing example 100000 of 118101\n",
      "[NeMo I 2022-02-07 21:51:02 gpt_ptune_dataset:331] Writing example 110000 of 118101\n",
      "[NeMo I 2022-02-07 21:51:10 gpt_ptune_dataset:201] Processing DATA_DIR/mix/validation.txt\n",
      "[NeMo I 2022-02-07 21:51:11 gpt_ptune_dataset:331] Writing example 0 of 13460\n",
      "[NeMo I 2022-02-07 21:51:21 gpt_ptune_dataset:331] Writing example 10000 of 13460\n",
      "[NeMo I 2022-02-07 21:51:26 megatron_ptune_gpt_model:369] Length of train dataset: 118101\n",
      "[NeMo I 2022-02-07 21:51:26 megatron_ptune_gpt_model:370] Length of val dataset: 13460\n",
      "[NeMo I 2022-02-07 21:51:26 megatron_ptune_gpt_model:371] Length of test dataset: 13538\n",
      "[NeMo I 2022-02-07 21:51:26 megatron_ptune_gpt_model:372] Finished building T5 datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-07 21:51:26 modelPT:577] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 1e-05\n",
      "        weight_decay: 0.0005\n",
      "    )\n",
      "[NeMo I 2022-02-07 21:51:26 lr_scheduler:833] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f9dc58ae730>\" \n",
      "    will be used during training (effective maximum steps = 1476300) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 1476300\n",
      "    )\n",
      "[NeMo I 2022-02-07 21:51:28 nlp_overrides:94] Configuring DDP for model parallelism.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type                   | Params\n",
      "----------------------------------------------------------\n",
      "0 | model          | MegatronGPTModel       | 354 M \n",
      "1 | embeddings     | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_encoder | PromptEncoder          | 14.7 M\n",
      "----------------------------------------------------------\n",
      "14.7 M    Trainable params\n",
      "354 M     Non-trainable params\n",
      "369 M     Total params\n",
      "739.158   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74d18c49f814f908d2cf3bace68dbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-07 21:51:41 megatron_ptune_gpt_model:318] Validation loss: 3.0266573429107666\n",
      "[NeMo I 2022-02-07 21:51:41 megatron_ptune_gpt_model:319] Validation accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea016c55b78459c838b5843dc897e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:220: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "I0207 21:51:43.998908 140320770778944 distributed.py:902] Reducer buckets have been rebuilt in this iteration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d31916902a4845964aeb471c14cfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# start model training\n",
    "trainer.fit(model_ptune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-michael",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "To see how the model performs, we can run model in the inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first create a subset of our dev data\n",
    "query_examples = [\n",
    "\n",
    "]\n",
    "results = model_ptune.cuda().ptune_inference(queries=query_examples, batch_size=1, decode_token_len=15)\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for query, result in zip(query_examples, results):\n",
    "    print(f'Query : {query}')\n",
    "    print(f'Predicted label: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-typing",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "If you have NeMo installed locally, you can also train the model with `examples/nlp/text_classification/ptune_text_classification.py`.\n",
    "\n",
    "To run training script, use:\n",
    "```\n",
    "python examples/nlp/language_modeling/megatron_gpt_ptune.py \\\n",
    "    trainer.gpus=1 \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.language_model.nemo_file=gpt_344m.nemo \\\n",
    "    model.train_ds.file_path=TRAIN_FILE \\\n",
    "    model.prompt_encoder.template=[3,3,3] \\\n",
    "    model.train_ds.batch_size=8 \\\n",
    "    model.validation_ds.file_path=VAL_FILE \\\n",
    "    model.test_ds.file_path=TEST_FILE \\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb3960",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
