{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a434f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook is currently being upated to work with the ptuning/prompt-tuning refactor. Please use NeMo r1.8.0 instead of main in the mean time. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BRANCH='main'\n",
    "\n",
    "\"\"\"\n",
    "This notebook is currently being upated to work with the ptuning/prompt-tuning refactor. Please use NeMo r1.8.0 instead of main in the mean time. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developmental-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\\n\\nInstructions for setting up Colab are as follows:\\n1. Open a new Python 3 notebook.\\n2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\\n3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\\n4. Run this cell to set up dependencies.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell\n",
    "\n",
    "# install NeMo\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "challenging-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-15 06:08:22 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-04-15 06:08:22 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-ethiopia",
   "metadata": {},
   "source": [
    "In the era of super large language models, the traditional \"pre-train, fine-tune\" procedure is replaced by \"pre-train, prompt, and predict\" method as shown in the [survey paper](https://arxiv.org/pdf/2107.13586.pdf). The prompt method is versatile enough to support all kinds of NLP tasks as shown in the following table: \n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>Task</th>\n",
    "            <th>Input ([X])</th>\n",
    "            <th>Template</th>\n",
    "            <th>Answer([Y])</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=3>Text CLS</td>\n",
    "            <td>Sentiment</td>\n",
    "            <td>I love this movie.</td>\n",
    "            <td>[X] The movie is [Y]</td>\n",
    "            <td>great<br>fantastic<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Topics</td>\n",
    "            <td>He prompted the LM.</td>\n",
    "            <td>[X] The text is about [Y]</td>\n",
    "            <td>sports<br>science<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Intention</td>\n",
    "            <td>What is taxi fare to Denver?</td>\n",
    "            <td>[X] The question is about [Y]</td>\n",
    "            <td>quantity<br>city<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=1>Text-span CLS</td>\n",
    "            <td>Aspect<br>Sentiment</td>\n",
    "            <td>Poor service but good food.</td>\n",
    "            <td>[X] What about service? [Y]</td>\n",
    "            <td>Bad<br>Terrible<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=1>Text-pair CLS</td>\n",
    "            <td>NLI</td>\n",
    "            <td>[X1]: An old man with ...<br>[X2]: A man walks ...</td>\n",
    "            <td>Hypothesis: [X1], Premise: [X2], Answer: [Y]</td>\n",
    "            <td>Contradiction<br>Entailment<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=1>Tagging</td>\n",
    "            <td>NER</td>\n",
    "            <td>[X1]: Mike went to Paris.<br>[X2]: Paris</td>\n",
    "            <td>[X1] [X2] is a [Y]</td>\n",
    "            <td>Yes<br>No<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=2>Text Generation</td>\n",
    "            <td>Summarization</td>\n",
    "            <td>Las Vegas police ...</td>\n",
    "            <td>[X] TL;DR: [Y]</td>\n",
    "            <td>The victim ...<br>A woman ...<br>...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Translation</td>\n",
    "            <td>Je vous aime.</td>\n",
    "            <td>French [X] English: [Y]</td>\n",
    "            <td>I love you.<br>I fancy you.<br>...</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "In this tutorial, we are going to describe how to use [P-Tuning method](https://arxiv.org/pdf/2103.10385.pdf) , which is one of the prompt engineering methods, to find good prompts for large GPT models. We show it can solve multiple downstream NLP tasks with good performance. P-Tuning leverages few continuous free parameters to serve as prompts fed as the input to the pre-trained language models. Freezing the large language model weights, P-Tuning model can be trained efficiently while delivering stats of art performance. \n",
    "\n",
    "Large Language Model can be trained with [NeMo Megatron](https://github.com/NVIDIA/NeMo/tree/main/examples/nlp/language_modeling), up to multi-billion parameters. In this notebook, we will use the pre-trained 344M GPT model released from NGC.\n",
    "\n",
    "# Task Description\n",
    "P-Tuning method can be applied to solve various NLP tasks. Without losing generality, in this notebook, we are going to use P-Tuning method to solve two NLP tasks: **Sentiment Analysis** task and **Question and Answer** task. \n",
    "\n",
    "**Sentiment Analysis** task is also known as opinion mining or emotion AI. It is a sub-field of NLP that tries to identify and extract opinions within a given text across blogs, reviews, social media, forums, news etc. \n",
    "\n",
    "For instance, **given sentences from news title, is it a good or bad news?**<br>\n",
    "\n",
    "**Question and Answer** task is to find the answer to a question given the context text. \n",
    "\n",
    "For instance, \n",
    "```\n",
    "Context: \n",
    "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
    "Question:\n",
    "How many Grammy awards did Beyoncé win for her first solo album?\n",
    "```\n",
    "\n",
    "# Dataset\n",
    "We will use [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) for sentiment analysis task and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) for question and answer task.\n",
    "\n",
    "The [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) contains the sentiments for financial news headlines from the perspective of a retail investor. Further details about the dataset can be found in: Malo, P., Sinha, A., Takala, P., Korhonen, P. and Wallenius, J. (2014): “Good debt or bad debt: Detecting semantic orientations in economic texts.” Journal of the American Society for Information Science and Technology.\n",
    "\n",
    "Here's an example of what an annotated abstract from the corpus looks like:\n",
    "\n",
    "```\n",
    "HELSINKI Thomson Financial - Shares in Cargotec fell sharply in early afternoon trade after the cargo handling group posted a surprise drop in April-June profits , which overshadowed the large number of new orders received during the three months .@negative\n",
    "LONDON MarketWatch -- Share prices ended lower in London Monday as a rebound in bank stocks failed to offset broader weakness for the FTSE 100 .@negative\n",
    "Operating profit fell to EUR 35.4 mn from EUR 68.8 mn in 2007 , including vessel sales gain of EUR 12.3 mn .@negative\n",
    "Sales in Finland decreased by 10.5 % in January , while sales outside Finland dropped by 17 % .@negative\n",
    "```\n",
    "\n",
    "The [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "Let's download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "federal-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"DATA_DIR\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e1b08",
   "metadata": {},
   "source": [
    "## Downloading Financial Phrase Bank Dataset\n",
    "\n",
    "The dataset is collected by Malo et al. 2014, and can be downloaded from this [link](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip). The zip file for the Financial Phrase Bank Dataset has been provided for ease of download and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad03fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-15 06:08:23--  https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip\n",
      "Resolving www.researchgate.net (www.researchgate.net)... 104.17.32.105, 104.17.33.105, 2606:4700::6811:2069, ...\n",
      "Connecting to www.researchgate.net (www.researchgate.net)|104.17.32.105|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip [following]\n",
      "--2022-04-15 06:08:23--  https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip\n",
      "Reusing existing connection to www.researchgate.net:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681890 (666K) [application/zip]\n",
      "Saving to: ‘FinancialPhraseBank-v10.zip’\n",
      "\n",
      "FinancialPhraseBank 100%[===================>] 665.91K  1.73MB/s    in 0.4s    \n",
      "\n",
      "2022-04-15 06:08:23 (1.73 MB/s) - ‘FinancialPhraseBank-v10.zip’ saved [681890/681890]\n",
      "\n",
      "Archive:  FinancialPhraseBank-v10.zip\n",
      "   creating: DATA_DIR/FinancialPhraseBank-v1.0/\n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/License.txt  \n",
      "   creating: DATA_DIR/__MACOSX/\n",
      "   creating: DATA_DIR/__MACOSX/FinancialPhraseBank-v1.0/\n",
      "  inflating: DATA_DIR/__MACOSX/FinancialPhraseBank-v1.0/._License.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/README.txt  \n",
      "  inflating: DATA_DIR/__MACOSX/FinancialPhraseBank-v1.0/._README.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_50Agree.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_66Agree.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_75Agree.txt  \n",
      "  inflating: DATA_DIR/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip\n",
    "!unzip FinancialPhraseBank-v10.zip -d {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "radical-castle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# If you want to see more examples, you can explore the text of the corpus using the file browser to the left, or open files directly, for example typing a command like the following in a code-cell:\n",
    "\n",
    "! head -1 $DATA_DIR/FinancialPhraseBank-v1.0/Sentences_50Agree.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f69a9",
   "metadata": {},
   "source": [
    "## Download the SQuAD dataset\n",
    "\n",
    "Download a copy of the dataset (distributed under the CC BY-SA 4.0 license):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e91c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-15 06:08:24--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.111.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42123633 (40M) [application/json]\n",
      "Saving to: ‘train-v2.0.json’\n",
      "\n",
      "train-v2.0.json     100%[===================>]  40.17M   108MB/s    in 0.4s    \n",
      "\n",
      "2022-04-15 06:08:25 (108 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "!mv train-v2.0.json {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-numbers",
   "metadata": {},
   "source": [
    "## Pre-process Financial Phrase Bank Dataset\n",
    "\n",
    "In this pre-process step, we are going to convert the downloaded dataset into the format that can be used for P-Tuning dataloader. The data is split into 10 folds so we can do 10-fold cross validation. In this notebook, we will use the first fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "198287d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "files = ['Sentences_50Agree.txt', 'Sentences_66Agree.txt', 'Sentences_75Agree.txt', 'Sentences_AllAgree.txt']\n",
    "base_dir = DATA_DIR + '/FinancialPhraseBank-v1.0/'\n",
    "files = [base_dir + f for f in files]\n",
    "\n",
    "alllines = []\n",
    "for fn in files:\n",
    "    with open(fn, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        alllines.extend(f.readlines())\n",
    "\n",
    "random.shuffle(alllines)\n",
    "fold = 10\n",
    "fold_size = len(alllines) // fold\n",
    "\n",
    "chunk_start = list(range(0, 14780, 1478))\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for start_id in chunk_start:\n",
    "    chunks.append(alllines[start_id:start_id+fold_size])\n",
    "\n",
    "def gen_file(data, fold_id, split_type):\n",
    "    filename = \"{}/{}_{}.txt\".format(base_dir, split_type, fold_id)\n",
    "    with open(filename, 'w') as f:\n",
    "        obj = {}\n",
    "        for line in data:\n",
    "            splits = line.split('@')\n",
    "            part1 = splits[0].strip()\n",
    "            part2 = splits[1].strip()\n",
    "            obj['sentence'] = part1\n",
    "            obj['label'] = part2\n",
    "            obj['taskname'] = 'sentiment-task'\n",
    "            f.write(json.dumps(obj)+'\\n')\n",
    "\n",
    "\n",
    "def gen_fold(fold_number):\n",
    "    lists = list(range(fold))\n",
    "    test_id = (fold_number + fold) % fold\n",
    "    val_id = (fold_number + fold - 1) % fold\n",
    "    test_set = chunks[test_id]\n",
    "    val_set = chunks[val_id]\n",
    "    lists.remove(test_id)\n",
    "    lists.remove(val_id)\n",
    "    train_set = []\n",
    "    for idd in lists:\n",
    "        train_set += chunks[idd]\n",
    "    gen_file(train_set, fold_number, 'train')\n",
    "    gen_file(val_set, fold_number, 'validation')\n",
    "    gen_file(test_set, fold_number, 'test')\n",
    "\n",
    "gen_fold(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-debate",
   "metadata": {},
   "source": [
    "The data is converted to the loss json file. Each line has three keys \"sentence\", \"label\" and \"prompt_tag\". \n",
    "Here are the first two lines of converted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sound-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sentence\": \"The contract includes heating plant equipment and associated installation work .\", \"label\": \"neutral\", \"taskname\": \"sentiment-task\"}\r\n",
      "{\"sentence\": \"The utility will also provide services related to electricity management , such as hedging trades and risk management and reporting .\", \"label\": \"neutral\", \"taskname\": \"sentiment-task\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 $DATA_DIR/FinancialPhraseBank-v1.0/train_0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71556cbe",
   "metadata": {},
   "source": [
    "### Preprocess SQuAD Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spectacular-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = DATA_DIR + '/train-v2.0.json'\n",
    "with open(file_name, 'r') as f:\n",
    "    data_obj = json.load(f)\n",
    "\n",
    "articles = data_obj['data']\n",
    "test_len = 40\n",
    "validation_len = 40\n",
    "train_len = len(articles) - test_len - validation_len\n",
    "train_records = []\n",
    "validation_records = []\n",
    "test_records = []\n",
    "\n",
    "\n",
    "def get_records(sub_articals, records):\n",
    "    for article in sub_articals:\n",
    "        paragraphs = article['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph['qas']\n",
    "            context = paragraph['context'].strip()\n",
    "            for qa in qas:\n",
    "                record = {}\n",
    "                record['question'] = qa['question'].strip()\n",
    "                record['context'] = context\n",
    "                if qa['is_impossible']:\n",
    "                    record['label'] = 'NA'\n",
    "                else:\n",
    "                    record['label'] = qa['answers'][0]['text'].strip()\n",
    "                record['taskname'] = 'qa-task'\n",
    "                records.append(json.dumps(record))\n",
    "get_records(articles[:train_len], train_records)\n",
    "get_records(articles[train_len:train_len+validation_len], validation_records)\n",
    "get_records(articles[train_len+validation_len:], test_records)\n",
    "random.shuffle(train_records)\n",
    "random.shuffle(validation_records)\n",
    "random.shuffle(test_records)\n",
    "squad_dir = \"DATA_DIR/squad\"\n",
    "os.makedirs(squad_dir, exist_ok=True)\n",
    "with open(squad_dir+'/train.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(train_records))\n",
    "with open(squad_dir+'/validation.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(validation_records))\n",
    "with open(squad_dir+'/test.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(test_records))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e4788",
   "metadata": {},
   "source": [
    "The data is converted to the loss json file. Each line has three keys \"question\", \"context\", \"label\" and \"prompt_tag\". \n",
    "Here are the first two lines of converted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bcd460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"What book did the New York Times publish excerpts from?\", \"context\": \"On July 8, 2007 The Washington Post published excerpts from UCLA Professor Amy Zegart's book Spying Blind: The CIA, the FBI, and the Origins of 9/11. The Post reported from Zegart's book that government documents show the CIA and FBI missed 23 potential chances to disrupt the terrorist attacks of September 11, 2001. The primary reasons for the failures included: agency cultures resistant to change and new ideas; inappropriate incentives for promotion; and a lack of cooperation between the FBI, CIA and the rest of the United States Intelligence Community. The book blamed the FBI's decentralized structure, which prevented effective communication and cooperation among different FBI offices. The book suggested that the FBI has not evolved into an effective counter-terrorism or counter-intelligence agency, due in large part to deeply ingrained agency cultural resistance to change. For example, FBI personnel practices continue to treat all staff other than special agents as support staff, classifying intelligence analysts alongside the FBI's auto mechanics and janitors.\", \"label\": \"NA\", \"taskname\": \"qa-task\"}\r\n",
      "{\"question\": \"What government had exploitation rights for bitumen extraction?\", \"context\": \"The Albanian bitumen extraction has a long history and was practiced in an organized way by the Romans. After centuries of silence, the first mentions of Albanian bitumen appeared only in 1868, when the Frenchman Coquand published the first geological description of the deposits of Albanian bitumen. In 1875, the exploitation rights were granted to the Ottoman government and in 1912, they were transferred to the Italian company Simsa. Since 1945, the mine was exploited by the Albanian government and from 2001 to date, the management passed to a French company, which organized the mining process for the manufacture of the natural bitumen on an industrial scale.\", \"label\": \"Ottoman\", \"taskname\": \"qa-task\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {squad_dir}/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba98b18",
   "metadata": {},
   "source": [
    "### Combine the two datasets\n",
    "\n",
    "The P-tune model includes a prompt encoder which is used to generate virtual tokens. Its output can be conditioned on the task tags so the P-tune model supports multiple tasks simultaneously. We are going to mix the Financial phrase bank dataset and SQuAD dataset together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a30f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_data_dir = f\"{DATA_DIR}/mix\"\n",
    "os.makedirs(mix_data_dir, exist_ok=True)\n",
    "!cat $DATA_DIR/FinancialPhraseBank-v1.0/train_0.txt {squad_dir}/train.txt | shuf > {mix_data_dir}/train.txt\n",
    "!cat $DATA_DIR/FinancialPhraseBank-v1.0/validation_0.txt {squad_dir}/validation.txt | shuf > {mix_data_dir}/validation.txt\n",
    "!cat $DATA_DIR/FinancialPhraseBank-v1.0/test_0.txt {squad_dir}/test.txt | shuf > {mix_data_dir}/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7d704",
   "metadata": {},
   "source": [
    "Here are the first two lines of converted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e373c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"What other name does the KInsey scale go by?\", \"context\": \"The Kinsey scale, also called the Heterosexual-Homosexual Rating Scale, was first published in Sexual Behavior in the Human Male (1948) by Alfred Kinsey, Wardell Pomeroy, and Clyde Martin and also featured in Sexual Behavior in the Human Female (1953). The scale was developed to combat the assumption at the time that people are either heterosexual or homosexual and that these two types represent antitheses in the sexual world. Recognizing that a large portion of population is not completely heterosexual or homosexual and people can experience both heterosexual and homosexual behavior and psychic responses, Kinsey et al., stated:\", \"label\": \"Heterosexual-Homosexual Rating Scale\", \"taskname\": \"qa-task\"}\r\n",
      "{\"question\": \"What did the Washington Naval Treaty of 1920 limit?\", \"context\": \"The development of flattop vessels produced the first large fleet ships. In 1918, HMS Argus became the world's first carrier capable of launching and recovering naval aircraft. As a result of the Washington Naval Treaty of 1922, which limited the construction of new heavy surface combat ships, most early aircraft carriers were conversions of ships that were laid down (or had served) as different ship types: cargo ships, cruisers, battlecruisers, or battleships. These conversions gave rise to the Lexington-class aircraft carriers (1927), Akagi and Courageous class. Specialist carrier evolution was well underway, with several navies ordering and building warships that were purposefully designed to function as aircraft carriers by the mid-1920s, resulting in the commissioning of ships such as H\\u014dsh\\u014d (1922), HMS Hermes (1924), and B\\u00e9arn (1927). During World War II, these ships would become known as fleet carriers.[citation needed]\", \"label\": \"NA\", \"taskname\": \"qa-task\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {mix_data_dir}/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813cc36",
   "metadata": {},
   "source": [
    "## Convert the Megatron-LM Weights to Nemo file\n",
    "\n",
    "P-Tuning method works the best with large GPT language models. From our experiences, models of size 5B or above give good performance. If you already have a large GPT model ready, skip this section. \n",
    "\n",
    "In this example, we will use the pretrained 344M NeMo Megatron GPT model from [Megatron-LM project](https://github.com/NVIDIA/Megatron-LM). To load it in NeMo Megatron, We first need to convert the Megatron-LM checkpoint to the `.nemo` file. Let's download the pretrained model weights and vocabulary file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82b8e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-15 06:08:29--  https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 54.193.81.248, 54.177.228.217\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|54.193.81.248|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 \n",
      "Location: https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/models/megatron_lm_345m/versions/v0.0/files.zip?response-content-disposition=attachment%3B%20filename%3D%22files.zip%22&response-content-type=application%2Fzip&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEN7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJHMEUCIQCd2aNobSx1vNwpWsVtFi2FL10p%2F2bwVkkJMJubSmXwSwIgNiM5TEuUdLEly5ikoi0ClCM4%2BtPYOeWhJor%2FeB0gZwgqgwQIh%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgw3ODkzNjMxMzUwMjciDC0gGuFmPuYRQIq%2FeCrXA2pI6s8ZQEPoHs91RxflkQLYJPxXOCRyWgfxGBN7t8q02wpN00qTudRR8dkQFRrCToFs64pbs3ubKs5UVG639sRolnTL7zocyrau9VuLFOcWq5sR%2FcBClxN3LuqZYSFmUWf4uNNX7%2FJq93RE07pHBLIZRkkLdXCLzAwrSGgb0knwdyxkVp4kBt%2BLopsT%2BNW2k%2FA4oDpWYRW0i1Dukb6z0sjDXa%2B2F5QN7fP9Hl02eeRWuVDBQKMp69EMfYV8%2FcFQb%2Bp7qSwqpOpGyRMy9hRan%2FF0UEg103B4H69xaH%2Fgp9W1o2pGcJBJdnmSIlRuATHQxteve0YyAFwdrvkVn1fU4%2FIg0ZwDDtwyKqIRLFIyHueKQwXMGPFoJ2st1dzq7HU1T29C5Zimbf4DyZkRi163uBkm9MzAgk%2BvnywciJ0cDGb9mY1NYuL3SDZyEe0iyXAQ42rPpQf8sbOxJ28YDW3CWShMtqRZKzXU18%2F8oH6DBuCBcB%2FIw8JEAcuGLS5P7T8ulIOu9UcjDwltLLA9LWciQZfo%2Fry8CLiDD2mocIiIj%2BghlAlbXys%2FHGgcpuD3ADfQc28Eo9IErc9rj%2Bjp3fTOoQHXIjlcol1aCv5AsfUNRR8FJfaKHsBIBTDmluSSBjqlAfYI%2FftR%2FmkpUcJ1AhK0cC6TzabG%2FMF%2B5t26pNByqyRjgNvL%2B%2FSI%2FJAlakO4Ba0DGCvRGAe2Y31AEAB3EwWmP2m%2BJ9YMO1YFouIlhFd%2FsgVKUnLzXwmI06LcsSHjMJoeXegMFQ4JY%2FksFA4lQBh6HjXXssWYcXQdsf7wSvbvRXpL%2FbSobS8XxJsvz8Y1f8i4xlMyJolSjepkkXyvE0p47kYm1npozw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220415T060826Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3599&X-Amz-Credential=ASIA3PSNVSIZSS67KFDC%2F20220415%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=48ed0826e55558c8d22ab46ad7cdd89a3e88d703218c886657d2b93afb16937f [following]\n",
      "--2022-04-15 06:08:30--  https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/models/megatron_lm_345m/versions/v0.0/files.zip?response-content-disposition=attachment%3B%20filename%3D%22files.zip%22&response-content-type=application%2Fzip&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEN7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJHMEUCIQCd2aNobSx1vNwpWsVtFi2FL10p%2F2bwVkkJMJubSmXwSwIgNiM5TEuUdLEly5ikoi0ClCM4%2BtPYOeWhJor%2FeB0gZwgqgwQIh%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgw3ODkzNjMxMzUwMjciDC0gGuFmPuYRQIq%2FeCrXA2pI6s8ZQEPoHs91RxflkQLYJPxXOCRyWgfxGBN7t8q02wpN00qTudRR8dkQFRrCToFs64pbs3ubKs5UVG639sRolnTL7zocyrau9VuLFOcWq5sR%2FcBClxN3LuqZYSFmUWf4uNNX7%2FJq93RE07pHBLIZRkkLdXCLzAwrSGgb0knwdyxkVp4kBt%2BLopsT%2BNW2k%2FA4oDpWYRW0i1Dukb6z0sjDXa%2B2F5QN7fP9Hl02eeRWuVDBQKMp69EMfYV8%2FcFQb%2Bp7qSwqpOpGyRMy9hRan%2FF0UEg103B4H69xaH%2Fgp9W1o2pGcJBJdnmSIlRuATHQxteve0YyAFwdrvkVn1fU4%2FIg0ZwDDtwyKqIRLFIyHueKQwXMGPFoJ2st1dzq7HU1T29C5Zimbf4DyZkRi163uBkm9MzAgk%2BvnywciJ0cDGb9mY1NYuL3SDZyEe0iyXAQ42rPpQf8sbOxJ28YDW3CWShMtqRZKzXU18%2F8oH6DBuCBcB%2FIw8JEAcuGLS5P7T8ulIOu9UcjDwltLLA9LWciQZfo%2Fry8CLiDD2mocIiIj%2BghlAlbXys%2FHGgcpuD3ADfQc28Eo9IErc9rj%2Bjp3fTOoQHXIjlcol1aCv5AsfUNRR8FJfaKHsBIBTDmluSSBjqlAfYI%2FftR%2FmkpUcJ1AhK0cC6TzabG%2FMF%2B5t26pNByqyRjgNvL%2B%2FSI%2FJAlakO4Ba0DGCvRGAe2Y31AEAB3EwWmP2m%2BJ9YMO1YFouIlhFd%2FsgVKUnLzXwmI06LcsSHjMJoeXegMFQ4JY%2FksFA4lQBh6HjXXssWYcXQdsf7wSvbvRXpL%2FbSobS8XxJsvz8Y1f8i4xlMyJolSjepkkXyvE0p47kYm1npozw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220415T060826Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3599&X-Amz-Credential=ASIA3PSNVSIZSS67KFDC%2F20220415%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=48ed0826e55558c8d22ab46ad7cdd89a3e88d703218c886657d2b93afb16937f\n",
      "Resolving prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com)... 52.218.136.49\n",
      "Connecting to prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com)|52.218.136.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 655013166 (625M) [application/zip]\n",
      "Saving to: ‘megatron_lm_345m_v0.0.zip’\n",
      "\n",
      "megatron_lm_345m_v0 100%[===================>] 624.67M  44.8MB/s    in 11s     \n",
      "\n",
      "2022-04-15 06:08:41 (55.9 MB/s) - ‘megatron_lm_345m_v0.0.zip’ saved [655013166/655013166]\n",
      "\n",
      "Archive:  megatron_lm_345m_v0.0.zip\n",
      "  inflating: latest_checkpointed_iteration.txt  \n",
      "  inflating: release/mp_rank_00/model_optim_rng.pt  \n",
      "--2022-04-15 06:08:47--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.138.224\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.138.224|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘gpt2-vocab.json’\n",
      "\n",
      "gpt2-vocab.json     100%[===================>]   1018K  2.35MB/s    in 0.4s    \n",
      "\n",
      "2022-04-15 06:08:48 (2.35 MB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]\n",
      "\n",
      "--2022-04-15 06:08:48--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.138.224\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.138.224|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘gpt2-merge.txt’\n",
      "\n",
      "gpt2-merge.txt      100%[===================>] 445.62K  1.52MB/s    in 0.3s    \n",
      "\n",
      "2022-04-15 06:08:48 (1.52 MB/s) - ‘gpt2-merge.txt’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "gpt_file = 'megatron_lm_345m_v0.0.zip'\n",
    "vocab_file = 'gpt2-vocab.json'\n",
    "merge_file = 'gpt2-merge.txt'\n",
    "checkpoint_filename = 'model_optim_rng.pt'\n",
    "\n",
    "if not pathlib.Path(gpt_file).exists():\n",
    "    !wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O $gpt_file\n",
    "    !unzip -o $gpt_file\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/$vocab_file -O $vocab_file \n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -O $merge_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b00ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is already exists\n"
     ]
    }
   ],
   "source": [
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "# Prepare the model parameters \n",
    "# download the model's configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "MODEL_CONFIG = \"megatron_gpt_config.yaml\"\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae5a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR/configs/megatron_gpt_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "config.model.num_layers = 24\n",
    "config.model.hidden_size = 1024\n",
    "config.model.ffn_hidden_size = 4096\n",
    "config.model.num_attention_heads = 16\n",
    "config.model.tokenizer.vocab_file = vocab_file\n",
    "config.model.tokenizer.merge_file = merge_file\n",
    "config.model.tensor_model_parallel_size = 1\n",
    "config.model.data.data_prefix = ''\n",
    "config.model.max_position_embeddings = 1024\n",
    "config.model.data.seq_length = 1024\n",
    "config.model.encoder_seq_length = 1024\n",
    "config.cfg = {}\n",
    "config.cfg.cfg = config.model\n",
    "with open('hparams.yaml', 'w') as f:\n",
    "    f.write(OmegaConf.to_yaml(config.cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1beda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [.........................................................] 20898 / 20898[NeMo W 2022-04-15 06:08:53 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-04-15 06:08:53 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo I 2022-04-15 06:08:53 distributed:31] Initializing torch.distributed with local_rank: 0, rank: 0, world_size: 1\n",
      "[NeMo W 2022-04-15 06:08:55 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_lm_ckpt_to_nemo:387] loading checkpoint /prompt-tuning/refactor/NeMo/tutorials/nlp/release/mp_rank_00/model_optim_rng.pt\n",
      "converted 354.87M parameters\n",
      "[NeMo W 2022-04-15 06:08:55 megatron_lm_ckpt_to_nemo:347] the checkpoint version is 0\n",
      "[NeMo W 2022-04-15 06:08:55 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:191] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:194] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:195] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:203] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:204] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:214] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:218] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:219] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:233] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:245] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:251] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:252] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:253] All embedding group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:08:55 megatron_init:254] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2022-04-15 06:08:55 tokenizer_utils:201] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m and custom vocab file: /prompt-tuning/refactor/NeMo/tutorials/nlp/gpt2-vocab.json\n",
      "[NeMo I 2022-04-15 06:08:55 tokenizer_utils:129] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /prompt-tuning/refactor/NeMo/tutorials/nlp/gpt2-vocab.json, special_tokens_dict: {}, and use_fast: False\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo I 2022-04-15 06:08:58 megatron_gpt_model:783] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 354871296\n",
      "[NeMo W 2022-04-15 06:08:58 modelPT:215] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2022-04-15 06:09:01 megatron_lm_ckpt_to_nemo:460] NeMo model saved to: /prompt-tuning/refactor/NeMo/tutorials/nlp/gpt_344m.nemo\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PWD = os.getcwd()\n",
    "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/megatron_lm_ckpt_to_nemo.py')\n",
    "!python -m torch.distributed.run --nproc_per_node=1 megatron_lm_ckpt_to_nemo.py --checkpoint_folder=$PWD/release/mp_rank_00/ --checkpoint_name=$checkpoint_filename --hparams_file=$PWD/hparams.yaml --nemo_file_path=$PWD/gpt_344m.nemo --model_type=gpt --tensor_model_parallel_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b455a6",
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "\n",
    "Our P-Tuning text classification model is comprised of the pretrained GPT LM model followed by a prompt encoder layer.\n",
    "\n",
    "The model is defined in a config file which declares multiple important sections. They are:\n",
    "- **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "speaking-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = \"megatron_gpt_prompt_learning_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "demanding-ballet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is already exists\n"
     ]
    }
   ],
   "source": [
    "# download the model's configuration file \n",
    "BRANCH=\"continuous_prompt_refactor\"\n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "criminal-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR/configs/megatron_gpt_prompt_learning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "# Note: these are small batch-sizes - increase as appropriate to available GPU capacity\n",
    "config.model.batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-effort",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Setting up Data within the config\n",
    "\n",
    "Among other things, the config file contains dictionaries called train_ds, validation_ds and test_ds. These are configurations used to setup the Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "informed-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this tutorial train and dev datasets are located in the same folder, so it is enough to add the path of the data directory to the config\n",
    "config.model.data.train_ds = [DATA_DIR+'/mix/train.txt',]\n",
    "config.model.data.validation_ds = [DATA_DIR+'/mix/validation.txt',]\n",
    "config.model.data.test_ds = [DATA_DIR+'/mix/test.txt',]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304d29b",
   "metadata": {},
   "source": [
    "## Add the Data Processors to Generate the Prompts\n",
    "\n",
    "To customize different prompts for different tasks, we simply need to specify the prompt task template in the config file. The virtual token markers `<|VIRTUAL_PROMPT_#|>` signify where you want virtual tokens to be placed in the the template string. `<|VIRTUAL_PROMPT_0|>`, `<|VIRTUAL_PROMPT_1|>`, and `<|VIRTUAL_PROMPT_2|>` indicate where a number of virtual tokens matching the values given at `virtual_token_splits[0]`, `virtual_token_splits[1]` and `virtual_token_splits[2]` will be placed. The other variable fields `{var}` refer to the variables in the data record. For example:\n",
    "\n",
    "Given the data record **{\"sentence1\": \"And he said, Mama, I'm home.\", \"sentence2\": \"He didn't say a word.\"}**, along with `virtual_token_splits = [3, 3, 3]` and `prompt_template = \"<|VIRTUAL_PROMPT_0|> Hypothesis: [sentence1], <|VIRTUAL_PROMPT_1|> Premise: [sentence2] <|VIRTUAL_PROMPT_2|> Answer:\"`, the input will be translated into **<span style=\"color:red\">VVV</span> Hypothesis: And he said, Mama, I'm home.<span style=\"color:red\">VVV</span> Premise: He didn't say a word.<span style=\"color:red\">VVV</span> Answer:**, where <span style=\"color:red\">VVV</span> are three virtual tokens.\n",
    "\n",
    "Let's configure the proper template for the two dataset we prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5ff646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  config.model.task_templates = [\n",
    "    {\n",
    "      \"taskname\": \"qa-task\",\n",
    "      \"prompt_template\": \"<|VIRTUAL_PROMPT_0|> Context: {context} <|VIRTUAL_PROMPT_1|> Question: {question}? <|VIRTUAL_PROMPT_2|> Answer: {label}\",\n",
    "      \"total_virtual_tokens\": 9,\n",
    "      \"virtual_token_splits\":[3, 3, 3],\n",
    "      \"truncate_field\": \"content\",\n",
    "    },\n",
    "    {\n",
    "      \"taskname\": \"sentiment-task\",\n",
    "      \"prompt_template\": \"<|VIRTUAL_PROMPT_0|> Sentence: {sentence} <|VIRTUAL_PROMPT_1|> Sentiment: {label}\",\n",
    "      \"total_virtual_tokens\": 9,\n",
    "      \"virtual_token_splits\":[6, 3],\n",
    "      \"truncate_field\": \"sentence\",\n",
    "    },\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f0d09",
   "metadata": {},
   "source": [
    "Note each `task_template` item has 5 fields. Besides the `prompt_template` string, the `taskname` refers to the `taskname` in the data record. The `truncate_field` specifies which field in the data is going to be cut if the length of the input exceeds the maximum sequence length of the model.`total_virtual_tokens` specifies the total number of virtual tokens that will be inserted into the model prompt. `virtual_token_splits` specifies the number of virtual tokens that belong at each `<|VIRTUAL_PROMPT_#|>` marker. `virtual_token_splits` values should add up to `total_virtual_tokens`. The number of `virtual_token_splits` should match the number of `<|VIRTUAL_PROMPT_#|>` markers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98aacf",
   "metadata": {},
   "source": [
    "After you p-tune your model this time, you can always go back and p-tune your model on more tasks without over writting the virtual prompts who've trained this time. You can also use a different number of `total_virtual_tokens` between each training session as long as tasks ptuned at the same time have the same number of `total_virtual_tokens`. For this reason, you ptune on a new task, you need to tell your model which of your tasks are new and which ones already exist (and thus you don't want to tune them). \n",
    "\n",
    "You do this by setting the `new_tasks` and `existing_tasks` values in the config file. Because we are ptuning a model with no existing tasks, you should set `existing_tasks=[]` and `new_tasks=['qa-task', 'sentiment-task']` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2122d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.new_tasks = ['qa-task', 'sentiment-task']\n",
    "config.model.existing_tasks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770e505",
   "metadata": {},
   "source": [
    "After ptuning is complete, you can run inference on all tasks at the same time, regradless of their `total_virtual_tokens` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "divine-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1234\n",
      "nemo_path: ${name}.nemo\n",
      "lm_finetune: false\n",
      "pseudo_token_base: PROMPT_\n",
      "virtual_prompt_style: p-tuning\n",
      "encoder_seq_length: 2048\n",
      "tensor_model_parallel_size: 1\n",
      "pipeline_model_parallel_size: 1\n",
      "batch_size: 8\n",
      "restore_path: null\n",
      "language_model_path: models/megatron_125M_gpt.nemo\n",
      "existing_tasks: []\n",
      "new_tasks:\n",
      "- qa-task\n",
      "- sentiment-task\n",
      "task_templates:\n",
      "- taskname: qa-task\n",
      "  prompt_template: '<|VIRTUAL_PROMPT_0|> Context: {context} <|VIRTUAL_PROMPT_1|> Question:\n",
      "    {question}? <|VIRTUAL_PROMPT_2|> Answer: {label}'\n",
      "  total_virtual_tokens: 9\n",
      "  virtual_token_splits:\n",
      "  - 3\n",
      "  - 3\n",
      "  - 3\n",
      "  truncate_field: content\n",
      "- taskname: sentiment-task\n",
      "  prompt_template: '<|VIRTUAL_PROMPT_0|> Sentence: {sentence} <|VIRTUAL_PROMPT_1|>\n",
      "    Sentiment: {label}'\n",
      "  total_virtual_tokens: 9\n",
      "  virtual_token_splits:\n",
      "  - 6\n",
      "  - 3\n",
      "  truncate_field: sentence\n",
      "prompt_tuning:\n",
      "  new_prompt_init_methods:\n",
      "  - text\n",
      "  new_prompt_init_text:\n",
      "  - some init text goes here\n",
      "p_tuning:\n",
      "  dropout: 0.0\n",
      "  num_layers: 2\n",
      "  save_tuned_prompts_to_prompt_table: true\n",
      "data:\n",
      "  train_ds:\n",
      "  - DATA_DIR/mix/train.txt\n",
      "  validation_ds:\n",
      "  - DATA_DIR/mix/validation.txt\n",
      "  add_eos: true\n",
      "  shuffle: true\n",
      "  num_workers: 1\n",
      "  pin_memory: true\n",
      "  test_ds:\n",
      "  - DATA_DIR/mix/test.txt\n",
      "optim:\n",
      "  name: fused_adam\n",
      "  lr: 0.0001\n",
      "  weight_decay: 0.01\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.98\n",
      "  sched:\n",
      "    name: CosineAnnealing\n",
      "    warmup_steps: 50\n",
      "    constant_steps: 10\n",
      "    min_lr: 1.0e-06\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-mauritius",
   "metadata": {},
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
    "\n",
    "Let's first instantiate a Trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unique-genre",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-15 06:38:43 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py:20: LightningDeprecationWarning: The `pl.plugins.training_type.ddp.DDPPlugin` is deprecated in v1.6 and will be removed in v1.8. Use `pl.strategies.ddp.DDPStrategy` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-04-15 06:38:43 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:312: LightningDeprecationWarning: Passing <nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7f3b9dc19100> `strategy` to the `plugins` flag in Trainer has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy=<nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7f3b9dc19100>)` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7f3b9dc19100>\n",
      "<pytorch_lightning.strategies.launchers.subprocess_script._SubprocessScriptLauncher object at 0x7f3b9dac6280>\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29188/2897975853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#trainer = pl.Trainer(**config.trainer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_overrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLPDDPPlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNLPDDPPlugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainer config - \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# 6. Instantiate Strategy - Part 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_init_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_deterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m    786\u001b[0m                 \u001b[0;34mf\"`Trainer(strategy={self.strategy.strategy_name!r})` or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;34mf\" `Trainer(accelerator={self.strategy.strategy_name!r})` is not compatible with an interactive\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "config.trainer.accelerator = accelerator\n",
    "config.trainer.devices = 1\n",
    "config.trainer.max_epochs = 3\n",
    "config.trainer.val_check_interval = 1.0\n",
    "\n",
    "# for PyTorch Native AMP set precision=16\n",
    "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "\n",
    "# remove distributed training flags\n",
    "config.trainer.strategy = None\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)\n",
    "#from nemo.collections.nlp.parts.nlp_overrides import NLPDDPPlugin\n",
    "#trainer = pl.Trainer(plugins=[NLPDDPPlugin()], **config.trainer)\n",
    "\n",
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-literature",
   "metadata": {},
   "source": [
    "## Setting up a NeMo Experiment\n",
    "\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mathematical-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-15 06:09:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-04-15 06:09:07 exp_manager:557] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2022-04-15 06:09:07 exp_manager:409] There was no checkpoint folder at checkpoint_dir :/prompt-tuning/refactor/NeMo/tutorials/nlp/nemo_experiments/megatron_virtual_prompt_gpt/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:09:07 exp_manager:281] Experiments will be logged at /prompt-tuning/refactor/NeMo/tutorials/nlp/nemo_experiments/megatron_virtual_prompt_gpt\n",
      "[NeMo I 2022-04-15 06:09:07 exp_manager:647] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-15 06:09:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\n",
      "      rank_zero_deprecation(\"`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\")\n",
      "    \n",
      "[NeMo W 2022-04-15 06:09:07 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/prompt-tuning/refactor/NeMo/tutorials/nlp/nemo_experiments/megatron_virtual_prompt_gpt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ea6cd",
   "metadata": {},
   "source": [
    "We will use the converted `.nemo` file as our LM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "compact-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "config.model.language_model_path = 'gpt_344m.nemo'\n",
    "config.model.tensor_model_parallel_size = 1\n",
    "config.exp_manager.checkpoint_callback_params.save_top_k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff09a1d",
   "metadata": {},
   "source": [
    "Your final config looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80531712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config - \n",
      "\n",
      "name: megatron_virtual_prompt_gpt\n",
      "trainer:\n",
      "  devices: 1\n",
      "  accelerator: gpu\n",
      "  num_nodes: 1\n",
      "  precision: 16\n",
      "  logger: false\n",
      "  enable_checkpointing: false\n",
      "  replace_sampler_ddp: false\n",
      "  max_epochs: 3\n",
      "  max_steps: null\n",
      "  log_every_n_steps: 10\n",
      "  val_check_interval: 1.0\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 1.0\n",
      "  resume_from_checkpoint: null\n",
      "  strategy: null\n",
      "exp_manager:\n",
      "  explicit_log_dir: null\n",
      "  exp_dir: null\n",
      "  name: ${name}\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    project: null\n",
      "    name: null\n",
      "  resume_if_exists: true\n",
      "  resume_ignore_no_checkpoint: true\n",
      "  create_checkpoint_callback: true\n",
      "  checkpoint_callback_params:\n",
      "    monitor: val_loss\n",
      "    save_top_k: 1\n",
      "    mode: min\n",
      "    save_nemo_on_train_end: true\n",
      "    filename: megatron_gpt_prompt_tune--{val_loss:.3f}-{step}\n",
      "    model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "    save_best_model: true\n",
      "model:\n",
      "  seed: 1234\n",
      "  nemo_path: ${name}.nemo\n",
      "  lm_finetune: false\n",
      "  pseudo_token_base: PROMPT_\n",
      "  virtual_prompt_style: p-tuning\n",
      "  encoder_seq_length: 2048\n",
      "  tensor_model_parallel_size: 1\n",
      "  pipeline_model_parallel_size: 1\n",
      "  batch_size: 8\n",
      "  restore_path: null\n",
      "  language_model_path: gpt_344m.nemo\n",
      "  existing_tasks: []\n",
      "  new_tasks:\n",
      "  - qa-task\n",
      "  - sentiment-task\n",
      "  task_templates:\n",
      "  - taskname: qa-task\n",
      "    prompt_template: '<|VIRTUAL_PROMPT_0|> Context: {context} <|VIRTUAL_PROMPT_1|>\n",
      "      Question: {question}? <|VIRTUAL_PROMPT_2|> Answer: {label}'\n",
      "    total_virtual_tokens: 9\n",
      "    virtual_token_splits:\n",
      "    - 3\n",
      "    - 3\n",
      "    - 3\n",
      "    truncate_field: content\n",
      "  - taskname: sentiment-task\n",
      "    prompt_template: '<|VIRTUAL_PROMPT_0|> Sentence: {sentence} <|VIRTUAL_PROMPT_1|>\n",
      "      Sentiment: {label}'\n",
      "    total_virtual_tokens: 9\n",
      "    virtual_token_splits:\n",
      "    - 6\n",
      "    - 3\n",
      "    truncate_field: sentence\n",
      "  prompt_tuning:\n",
      "    new_prompt_init_methods:\n",
      "    - text\n",
      "    new_prompt_init_text:\n",
      "    - some init text goes here\n",
      "  p_tuning:\n",
      "    dropout: 0.0\n",
      "    num_layers: 2\n",
      "    save_tuned_prompts_to_prompt_table: true\n",
      "  data:\n",
      "    train_ds:\n",
      "    - DATA_DIR/mix/train.txt\n",
      "    validation_ds:\n",
      "    - DATA_DIR/mix/validation.txt\n",
      "    add_eos: true\n",
      "    shuffle: true\n",
      "    num_workers: 1\n",
      "    pin_memory: true\n",
      "    test_ds:\n",
      "    - DATA_DIR/mix/test.txt\n",
      "  optim:\n",
      "    name: fused_adam\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.01\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.98\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_steps: 50\n",
      "      constant_steps: 10\n",
      "      min_lr: 1.0e-06\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model config - \\n\")\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-geometry",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders we'll be prepared for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "indoor-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-15 06:09:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:09:07 megatron_init:191] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:194] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:195] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:203] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:204] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:214] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:218] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:219] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:233] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:245] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:251] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:252] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:253] All embedding group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:07 megatron_init:254] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:191] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:194] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:195] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:203] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:204] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:214] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:218] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:219] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:233] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:245] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:251] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:252] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:253] All embedding group ranks: [[0]]\n",
      "[NeMo I 2022-04-15 06:09:08 megatron_init:254] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2022-04-15 06:09:08 tokenizer_utils:201] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m and custom vocab file: /tmp/tmp_zd42fe5/43f5242c4f4a495b8f398af238f696c7_gpt2-vocab.json\n",
      "[NeMo I 2022-04-15 06:09:08 tokenizer_utils:129] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmp_zd42fe5/43f5242c4f4a495b8f398af238f696c7_gpt2-vocab.json, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:09:11 megatron_gpt_model:783] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2022-04-15 06:09:13 nlp_overrides:404] Model MegatronGPTModel was successfully restored from /prompt-tuning/refactor/NeMo/tutorials/nlp/gpt_344m.nemo.\n",
      "[NeMo I 2022-04-15 06:09:13 auto_tokenizer:171] 9 special tokens added, resize your model accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_prompt_learning_model import MegatronGPTPromptLearningModel\n",
    "model_ptune = MegatronGPTPromptLearningModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-pipeline",
   "metadata": {},
   "source": [
    "## Monitoring training progress\n",
    "Optionally, you can create a Tensorboard visualization to monitor training progress.\n",
    "If you're not using Colab, refer to [https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) if you're facing issues with running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "changed-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use tensorboard, please use this notebook in a Google Colab environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google import colab\n",
    "    COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "applied-quality",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:09:14 gpt_prompt_learning_dataset:60] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13538it [00:14, 939.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:09:28 gpt_prompt_learning_dataset:134] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "[NeMo I 2022-04-15 06:09:28 gpt_prompt_learning_dataset:60] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "118101it [01:57, 1004.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:11:26 gpt_prompt_learning_dataset:134] Skipped 0 sentences, sequence length too short or too long even after truncation\n",
      "[NeMo I 2022-04-15 06:11:26 gpt_prompt_learning_dataset:60] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "13460it [00:13, 1002.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:11:39 gpt_prompt_learning_dataset:134] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo W 2022-04-15 06:11:40 modelPT:496] The lightning trainer received accelerator: <pytorch_lightning.accelerators.gpu.GPUAccelerator object at 0x7f3b9dc3ddc0>. We recommend to use 'ddp' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-15 06:11:40 modelPT:587] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2022-04-15 06:11:40 lr_scheduler:833] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f3b9dabbf40>\" \n",
      "    will be used during training (effective maximum steps = 44286) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    constant_steps: 10\n",
      "    min_lr: 1.0e-06\n",
      "    max_steps: 44286\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model           | MegatronGPTModel       | 354 M \n",
      "1 | word_embeddings | VocabParallelEmbedding | 51.5 M\n",
      "2 | prompt_table    | PromptTable            | 0     \n",
      "3 | prompt_encoder  | PromptEncoder          | 14.7 M\n",
      "-----------------------------------------------------------\n",
      "14.7 M    Trainable params\n",
      "354 M     Non-trainable params\n",
      "369 M     Total params\n",
      "739.158   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ec068574614be49d2fea5d6b482466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-15 06:11:40 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257, 50258, 50259,  ..., 50256, 50256, 50256],\n",
      "        [50257, 50258, 50259,  ..., 50256, 50256, 50256],\n",
      "        [50257, 50258, 50259,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [50257, 50258, 50259,  ..., 50256, 50256, 50256],\n",
      "        [50257, 50258, 50259,  ..., 50256, 50256, 50256],\n",
      "        [50257, 50258, 50259,  ..., 50256, 50256, 50256]], device='cuda:0')\n",
      "tensor([[20402,    12, 35943, 50256],\n",
      "        [20402,    12, 35943, 50256],\n",
      "        [20402,    12, 35943, 50256],\n",
      "        [20402,    12, 35943, 50256],\n",
      "        [34086,  3681,    12, 35943],\n",
      "        [20402,    12, 35943, 50256],\n",
      "        [20402,    12, 35943, 50256],\n",
      "        [20402,    12, 35943, 50256]], device='cuda:0')\n",
      "tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n",
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        [  0, 196],\n",
      "        [  0, 197],\n",
      "        [  0, 198],\n",
      "        [  0, 210],\n",
      "        [  0, 211],\n",
      "        [  0, 212],\n",
      "        [  1,   0],\n",
      "        [  1,   1],\n",
      "        [  1,   2],\n",
      "        [  1, 100],\n",
      "        [  1, 101],\n",
      "        [  1, 102],\n",
      "        [  1, 118],\n",
      "        [  1, 119],\n",
      "        [  1, 120],\n",
      "        [  2,   0],\n",
      "        [  2,   1],\n",
      "        [  2,   2],\n",
      "        [  2, 120],\n",
      "        [  2, 121],\n",
      "        [  2, 122],\n",
      "        [  2, 140],\n",
      "        [  2, 141],\n",
      "        [  2, 142],\n",
      "        [  3,   0],\n",
      "        [  3,   1],\n",
      "        [  3,   2],\n",
      "        [  3, 283],\n",
      "        [  3, 284],\n",
      "        [  3, 285],\n",
      "        [  3, 298],\n",
      "        [  3, 299],\n",
      "        [  3, 300],\n",
      "        [  4,   0],\n",
      "        [  4,   1],\n",
      "        [  4,   2],\n",
      "        [  4,   3],\n",
      "        [  4,   4],\n",
      "        [  4,   5],\n",
      "        [  4,  39],\n",
      "        [  4,  40],\n",
      "        [  4,  41],\n",
      "        [  5,   0],\n",
      "        [  5,   1],\n",
      "        [  5,   2],\n",
      "        [  5, 173],\n",
      "        [  5, 174],\n",
      "        [  5, 175],\n",
      "        [  5, 187],\n",
      "        [  5, 188],\n",
      "        [  5, 189],\n",
      "        [  6,   0],\n",
      "        [  6,   1],\n",
      "        [  6,   2],\n",
      "        [  6, 197],\n",
      "        [  6, 198],\n",
      "        [  6, 199],\n",
      "        [  6, 209],\n",
      "        [  6, 210],\n",
      "        [  6, 211],\n",
      "        [  7,   0],\n",
      "        [  7,   1],\n",
      "        [  7,   2],\n",
      "        [  7, 140],\n",
      "        [  7, 141],\n",
      "        [  7, 142],\n",
      "        [  7, 162],\n",
      "        [  7, 163],\n",
      "        [  7, 164]], device='cuda:0')\n",
      "torch.Size([72, 2])\n",
      "tensor([[[-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         ...,\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242]],\n",
      "\n",
      "        [[-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         ...,\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242]],\n",
      "\n",
      "        [[-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         ...,\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         ...,\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242]],\n",
      "\n",
      "        [[-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         ...,\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242]],\n",
      "\n",
      "        [[-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         ...,\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242],\n",
      "         [-0.0364,  0.0080, -0.0287,  ..., -0.0300, -0.0843, -0.0242]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[[ 0.0210,  0.0164, -0.0244,  ...,  0.0196, -0.0207,  0.0128],\n",
      "         [ 0.0209,  0.0182, -0.0248,  ...,  0.0182, -0.0196,  0.0145],\n",
      "         [ 0.0213,  0.0195, -0.0240,  ...,  0.0153, -0.0194,  0.0163],\n",
      "         ...,\n",
      "         [ 0.0146,  0.0173, -0.0256,  ...,  0.0133, -0.0174, -0.0026],\n",
      "         [ 0.0028,  0.0224, -0.0262,  ...,  0.0077, -0.0280,  0.0124],\n",
      "         [ 0.0116,  0.0158, -0.0241,  ...,  0.0164, -0.0238,  0.0137]],\n",
      "\n",
      "        [[ 0.0210,  0.0164, -0.0244,  ...,  0.0196, -0.0207,  0.0128],\n",
      "         [ 0.0209,  0.0182, -0.0248,  ...,  0.0182, -0.0196,  0.0145],\n",
      "         [ 0.0213,  0.0195, -0.0240,  ...,  0.0153, -0.0194,  0.0163],\n",
      "         ...,\n",
      "         [ 0.0146,  0.0173, -0.0256,  ...,  0.0133, -0.0174, -0.0026],\n",
      "         [ 0.0028,  0.0224, -0.0262,  ...,  0.0077, -0.0280,  0.0124],\n",
      "         [ 0.0116,  0.0158, -0.0241,  ...,  0.0164, -0.0238,  0.0137]],\n",
      "\n",
      "        [[ 0.0210,  0.0164, -0.0244,  ...,  0.0196, -0.0207,  0.0128],\n",
      "         [ 0.0209,  0.0182, -0.0248,  ...,  0.0182, -0.0196,  0.0145],\n",
      "         [ 0.0213,  0.0195, -0.0240,  ...,  0.0153, -0.0194,  0.0163],\n",
      "         ...,\n",
      "         [ 0.0146,  0.0173, -0.0256,  ...,  0.0133, -0.0174, -0.0026],\n",
      "         [ 0.0028,  0.0224, -0.0262,  ...,  0.0077, -0.0280,  0.0124],\n",
      "         [ 0.0116,  0.0158, -0.0241,  ...,  0.0164, -0.0238,  0.0137]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0210,  0.0164, -0.0244,  ...,  0.0196, -0.0207,  0.0128],\n",
      "         [ 0.0209,  0.0182, -0.0248,  ...,  0.0182, -0.0196,  0.0145],\n",
      "         [ 0.0213,  0.0195, -0.0240,  ...,  0.0153, -0.0194,  0.0163],\n",
      "         ...,\n",
      "         [ 0.0146,  0.0173, -0.0256,  ...,  0.0133, -0.0174, -0.0026],\n",
      "         [ 0.0028,  0.0224, -0.0262,  ...,  0.0077, -0.0280,  0.0124],\n",
      "         [ 0.0116,  0.0158, -0.0241,  ...,  0.0164, -0.0238,  0.0137]],\n",
      "\n",
      "        [[ 0.0210,  0.0164, -0.0244,  ...,  0.0196, -0.0207,  0.0128],\n",
      "         [ 0.0209,  0.0182, -0.0248,  ...,  0.0182, -0.0196,  0.0145],\n",
      "         [ 0.0213,  0.0195, -0.0240,  ...,  0.0153, -0.0194,  0.0163],\n",
      "         ...,\n",
      "         [ 0.0146,  0.0173, -0.0256,  ...,  0.0133, -0.0174, -0.0026],\n",
      "         [ 0.0028,  0.0224, -0.0262,  ...,  0.0077, -0.0280,  0.0124],\n",
      "         [ 0.0116,  0.0158, -0.0241,  ...,  0.0164, -0.0238,  0.0137]],\n",
      "\n",
      "        [[ 0.0210,  0.0164, -0.0244,  ...,  0.0196, -0.0207,  0.0128],\n",
      "         [ 0.0209,  0.0182, -0.0248,  ...,  0.0182, -0.0196,  0.0145],\n",
      "         [ 0.0213,  0.0195, -0.0240,  ...,  0.0153, -0.0194,  0.0163],\n",
      "         ...,\n",
      "         [ 0.0146,  0.0173, -0.0256,  ...,  0.0133, -0.0174, -0.0026],\n",
      "         [ 0.0028,  0.0224, -0.0262,  ...,  0.0077, -0.0280,  0.0124],\n",
      "         [ 0.0116,  0.0158, -0.0241,  ...,  0.0164, -0.0238,  0.0137]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "intra_layer_model parallel group is not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29188/3447343327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ptune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         )\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m                 \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataloader_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prompt-tuning/refactor/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_prompt_learning_model.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaskname_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prompt-tuning/refactor/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_prompt_learning_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, position_ids, attention_mask, taskname_ids, labels, inference, set_inference_key_value_memory, inference_max_sequence_len)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 output = self.model.model(\n\u001b[0m\u001b[1;32m    304\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                     \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prompt-tuning/refactor/NeMo/nemo/collections/nlp/models/language_modeling/megatron/gpt_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, position_ids, attention_mask, labels, token_type_ids, layer_past, get_key_value, forward_method_parallel_output, encoder_input, set_inference_key_value_memory, inference_max_sequence_len)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             return post_language_model_processing(\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mlm_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prompt-tuning/refactor/NeMo/nemo/collections/nlp/models/language_modeling/megatron/gpt_model.py\u001b[0m in \u001b[0;36mpost_language_model_processing\u001b[0;34m(lm_output, labels, logit_weights, get_key_value, parallel_output, forward_method_parallel_output, fp16_lm_cross_entropy, return_logits)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_parallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_parallel_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_parallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_parallel_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_logits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/cross_entropy.py\u001b[0m in \u001b[0;36mvocab_parallel_cross_entropy\u001b[0;34m(vocab_parallel_logits, target)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvocab_parallel_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_parallel_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"\"\"Helper function for the cross entropy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VocabParallelCrossEntropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_parallel_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/cross_entropy.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, vocab_parallel_logits, target)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlogits_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_parallel_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         torch.distributed.all_reduce(\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mlogits_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_tensor_model_parallel_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Subtract the maximum value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/apex/transformer/parallel_state.py\u001b[0m in \u001b[0;36mget_tensor_model_parallel_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tensor_model_parallel_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;34m\"\"\"Get the tensor model parallel group the caller rank belongs to.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     assert (\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0m_TENSOR_MODEL_PARALLEL_GROUP\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     ), \"intra_layer_model parallel group is not initialized\"\n",
      "\u001b[0;31mAssertionError\u001b[0m: intra_layer_model parallel group is not initialized"
     ]
    }
   ],
   "source": [
    "# start model training\n",
    "trainer.fit(model_ptune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-michael",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "To see how the model performs, we can run model in the inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first select a subset of our dev data\n",
    "query_examples = [\n",
    "    {\"taskname\": \"sentiment-task\", \"sentence\": \"The Finland-based company says it will move into an existing 260,000-square-foot facility in September .\"},\n",
    "    {\"taskname\": \"qa-task\", \"question\": \"What are the closest relatives of the deinonychosaurs?\", \"context\": \"The consensus view in contemporary paleontology is that the flying theropods, or avialans, are the closest relatives of the deinonychosaurs, which include dromaeosaurids and troodontids. Together, these form a group called Paraves. Some basal members of this group, such as Microraptor, have features which may have enabled them to glide or fly. The most basal deinonychosaurs were very small. This evidence raises the possibility that the ancestor of all paravians may have been arboreal, have been able to glide, or both. Unlike Archaeopteryx and the non-avialan feathered dinosaurs, who primarily ate meat, recent studies suggest that the first avialans were omnivores.\", \"label\": \"flying theropods\"}\n",
    "]\n",
    "response = model_ptune.generate(inputs=query_examples)\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for result in response['sentences']:\n",
    "    print(result)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-typing",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "If you have NeMo installed locally, you can also train the model with `examples/nlp/language_modeling/megatron_gpt_prompt_learning.py`.\n",
    "\n",
    "To run training script, first change the values in `examples/nlp/language_modeling/conf/megatron_gpt_prompt_learning_config.yaml` to your desired values, then run:\n",
    "```\n",
    "python examples/nlp/language_modeling/megatron_gpt_prompt_learning.py \\\n",
    "    --config-name=prompt_learning_megatron_gpt_inference.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
