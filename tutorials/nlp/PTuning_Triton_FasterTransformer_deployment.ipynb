{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caaac21",
   "metadata": {},
   "source": [
    "# P-tuning High-Performance Inference Deployment with FasterTransformer and Triton Inference Server\n",
    "\n",
    "This notebook walks you through the process of deploying a p-tuned model trained by NeMo using FasterTransformer and the Triton inference server for high-performance inference.\n",
    "\n",
    "\n",
    "## Pre-requisite\n",
    "\n",
    "1. A p-tuning model trained with NeMo. Refer to [Multitask_Prompt_and_PTuning.ipynb](Multitask_Prompt_and_PTuning.ipynb) and our blog [How to Create a Custom Language Model](https://developer.nvidia.com/blog/how-to-create-a-custom-language-model/) for examples and walk-through guidance.\n",
    "2. Access to a compatible NeMo GPT3 checkpoint. There are currently several public NeMo GPT3 checkpoints on HuggingFace:\n",
    "\n",
    "- 5B: https://huggingface.co/nvidia/nemo-megatron-gpt-5B\n",
    "- 20B: https://huggingface.co/nvidia/nemo-megatron-gpt-20B\n",
    "\n",
    "You should ensure the same model is used both for training and inference. \n",
    "\n",
    "3. You will need 2x NVIDIA Volta, Ampere or Hopper GPUs to work with the 5B model, and  4x NVIDIA Volta Ampere or Hopper GPUs to work with the 20B model.\n",
    "4. This notebook was tested with the NeMo 23.02 container, but you can also try later releases should they become available. Download and run this container with:\n",
    "\n",
    "```\n",
    "docker run --gpus=all -u $(id -u ${USER}):$(id -g ${USER}) --rm -it --net=host nvcr.io/nvidia/nemo:23.02 bash\n",
    "```\n",
    "\n",
    "Then from within the container interactive bash environment, start Jupyter lab:\n",
    "```\n",
    "cd /myworkspace\n",
    "jupyter lab --ip 0.0.0.0 --allow-root --port=8888\n",
    "```\n",
    "\n",
    "From within the Jupyter lab environment, you can upload this notebook.\n",
    "\n",
    "## P-tuning customization recap\n",
    "\n",
    "p-tuning is a parameter efficient customization technique, during which a small auxiliary model (usually LSTM or MLP) is used to learn a set of “soft tokens”. These soft tokens (aka. virtual tokens) are usually prepended  to the user’s prompt to form the complete input to a base LLM. During the training, the base LLM weights are fixed, and only the prompt encoder weights are learned. After training, the prompt encoder is discarded and only the final soft prompt tokens are retained for deployment. Think of the soft prompt tokens akin to an instruction, such as “Summarize the following article”, except that these prompt tokens are not in natural language but in a continuous space, and are learned and optimized for the task at hand. \n",
    "\n",
    "\n",
    "The nature of p-tuning technique implies that the same base model can be used to serve both customized use cases and non-customized use cases. It can serve two type of requests in the same fashion.\n",
    "- Client 1 sends a regular request with a tokenized prompt, in the form of integer indices. The model uses these indices and its embedding table to map tokens to a continuous input. \n",
    "- Client 2 sends a request for a p-tuned model. The p-tuned model in fact is not deployed at the server side. Instead, the client sends in a prompt which contain the prepended virtual token IDs, together with a mini embedding table containing the learned embedding of those soft tokens. \n",
    "\n",
    "With this paradigm, the p-tuning inference logic is handled primarily at the client side, or a middleman server. At the backend, the base model mostly just does “business as usual”, serving both customized and non-customized requests in an almost identical manner. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdc83a",
   "metadata": {},
   "source": [
    "## 1. Download public NeMo-Megatron 5B GPT model from HuggingFace\n",
    "\n",
    "The below code downloads the the 5B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt update && apt install git-lfs\n",
    "git lfs install\n",
    "rm -rf nemo-megatron-gpt-5B\n",
    "git clone https://huggingface.co/nvidia/nemo-megatron-gpt-5B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74665b",
   "metadata": {},
   "source": [
    "## 2. Deploy foundation model with Triton\n",
    "\n",
    "The first step is to build a Triton container with the FasterTransformer backend. \n",
    "\n",
    "### 2.1 Build Triton FT backend\n",
    "\n",
    "Follow the instruction at `https://github.com/triton-inference-server/fastertransformer_backend` to build a Triton container with the FasterTransformer backend. \n",
    "\n",
    "The below command should be executed from the base OS environment, and not from within a docker environment.\n",
    "\n",
    "```\n",
    "git clone https://github.com/triton-inference-server/fastertransformer_backend\n",
    "\n",
    "export WORKSPACE=$(pwd)\n",
    "export CONTAINER_VERSION=22.12\n",
    "export TRITON_DOCKER_IMAGE=triton_with_ft:${CONTAINER_VERSION}\n",
    "\n",
    "cd fastertransformer_backend\n",
    "python3 docker/create_dockerfile_and_build.py --triton-version 22.12\n",
    "```\n",
    "\n",
    "Upon success, a docker image named `tritonserver_with_ft:latest` will be created. \n",
    "\n",
    "Once ready, execute the following command, also from the base OS environment, to convert .nemo checkpoint to a FasterTransformer compatible format. \n",
    "\n",
    "We first clone the FasterTransformer library.\n",
    "```\n",
    "git clone https://github.com/NVIDIA/FasterTransformer\n",
    "```\n",
    "\n",
    "Next, use FasterTransformer code to convert the model.\n",
    "\n",
    "```\n",
    "mkdir -p <Path_to_model_repository>\n",
    "\n",
    "docker run --rm \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    -v <Path_to_FasterTransformer>:/FasterTransformer \\\n",
    "    -v <Path_to_nemo-megatron-gpt>:/checkpoints \\\n",
    "    -v <Path_to_model_repository>:/model_repository \\\n",
    "   tritonserver_with_ft:latest \\\n",
    "    bash -c \"export PYTHONPATH=/FasterTransformer:${PYTHONPATH} && pip install nemo_toolkit['all'] && wget https://raw.githubusercontent.com/triton-inference-server/fastertransformer_backend/main/all_models/gpt/fastertransformer/config.pbtxt && \\\n",
    "    python3 /FasterTransformer/examples/pytorch/gpt/utils/nemo_ckpt_convert.py \\\n",
    "        --in-file /checkpoints/nemo_gpt5B_bf16_tp2.nemo \\\n",
    "        --infer-gpu-num 2 \\\n",
    "        --saved-dir /model_repository/gpt3_5b \\\n",
    "        --weight-data-type fp16 \\\n",
    "        --load-checkpoints-to-cpu 0\"\n",
    "\n",
    "```\n",
    "\n",
    "Herein you should replace the following path with absolute paths:\n",
    "- `<Path_to_FasterTransformer>`: absolute path to the FasterTransformer directory\n",
    "- `<Path_to_nemo-megatron-gpt>`: absolute path to the NeMo Megatron GPT model\n",
    "- `<Path_to_model_repository>`: absolute path to an empty directory which store the output of the conversion. Note: ensure that the container can write to this directory.\n",
    "\n",
    "\n",
    "In addition, if you are using the GPT3 20B model, you should also replace `--infer-gpu-num` to equal the TP degree (Typically, TP=4 for the 20B model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6138750",
   "metadata": {},
   "source": [
    "### 2.2 Fix configuration\n",
    "\n",
    "For the 5B model, overwrite the model configuration with  the following config file. Note `<Path_to_model_repository>` is the absolute path to the directory which store the output of the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile <Path_to_model_repository>/gpt3_5b/config.pbtxt\n",
    "name: \"gpt3_5b\"\n",
    "max_batch_size: 256\n",
    "input {\n",
    "  name: \"input_ids\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: -1\n",
    "}\n",
    "input {\n",
    "  name: \"input_lengths\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "}\n",
    "input {\n",
    "  name: \"request_output_len\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: -1\n",
    "}\n",
    "input {\n",
    "  name: \"runtime_top_k\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"runtime_top_p\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"beam_search_diversity_rate\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"temperature\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"len_penalty\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"repetition_penalty\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"random_seed\"\n",
    "  data_type: TYPE_UINT64\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"is_return_log_probs\"\n",
    "  data_type: TYPE_BOOL\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"is_return_context_embeddings\"\n",
    "  data_type: TYPE_BOOL\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"beam_width\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"start_id\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"end_id\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"stop_words_list\"\n",
    "  data_type: TYPE_INT32\n",
    "  dims: 2\n",
    "  dims: -1\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"bad_words_list\"\n",
    "  data_type: TYPE_INT32\n",
    "  dims: 2\n",
    "  dims: -1\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"prompt_learning_task_name_ids\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"request_prompt_embedding\"\n",
    "  data_type: TYPE_FP16\n",
    "  dims: -1\n",
    "  dims: -1\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"request_prompt_lengths\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "input {\n",
    "  name: \"request_prompt_type\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: 1\n",
    "  reshape {\n",
    "  }\n",
    "  optional: true\n",
    "}\n",
    "output {\n",
    "  name: \"output_ids\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: -1\n",
    "  dims: -1\n",
    "}\n",
    "output {\n",
    "  name: \"sequence_length\"\n",
    "  data_type: TYPE_UINT32\n",
    "  dims: -1\n",
    "}\n",
    "output {\n",
    "  name: \"cum_log_probs\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: -1\n",
    "}\n",
    "output {\n",
    "  name: \"output_log_probs\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: -1\n",
    "  dims: -1\n",
    "}\n",
    "output {\n",
    "  name: \"context_embeddings\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: -1\n",
    "  dims: -1\n",
    "}\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_CPU\n",
    "}\n",
    "default_model_filename: \"2-gpu\"\n",
    "parameters {\n",
    "  key: \"data_type\"\n",
    "  value {\n",
    "    string_value: \"fp16\"\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  key: \"enable_custom_all_reduce\"\n",
    "  value {\n",
    "    string_value: \"0\"\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  key: \"int8_mode\"\n",
    "  value {\n",
    "    string_value: \"0\"\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  key: \"model_checkpoint_path\"\n",
    "  value {\n",
    "    string_value: \"/model_repository/gpt3_5b/2-gpu\"\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  key: \"model_type\"\n",
    "  value {\n",
    "    string_value: \"GPT\"\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  key: \"pipeline_para_size\"\n",
    "  value {\n",
    "    string_value: \"1\"\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  key: \"tensor_para_size\"\n",
    "  value {\n",
    "    string_value: \"2\"\n",
    "  }\n",
    "}\n",
    "backend: \"fastertransformer\"\n",
    "model_transaction_policy {\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ed5d9",
   "metadata": {},
   "source": [
    "### 2.3 Deploy model\n",
    "\n",
    "Once converted, run the following command to deploy the model from the OS environment.\n",
    "\n",
    "```\n",
    "docker run --rm \\\n",
    "    --name triton-inference-server \\\n",
    "    --gpus all \\\n",
    "    -p 8000-8002:8000-8002 \\\n",
    "    -v <Path_to_model_repository>:/model_repository \\\n",
    "     tritonserver_with_ft:latest \\\n",
    "    bash -c 'export CUDA_VISIBLE_DEVICES=0,1 && \\\n",
    "    tritonserver --model-repository /model_repository'\n",
    "```\n",
    "\n",
    "Herein you should replace the following path with absolute paths:\n",
    "- `<Path_to_model_repository>`: absolute path to the model repository\n",
    "\n",
    "\n",
    "Upon successfull deployment, you should observed the model loaded and ready to serve:\n",
    "\n",
    "```\n",
    "│I0309 06:56:20.186185 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\n",
    "│I0309 06:56:20.186468 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\n",
    "│I0309 06:56:20.227948 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e984fd",
   "metadata": {},
   "source": [
    "## 3. Preprocessing at client side\n",
    "\n",
    "Now the Triton inference server is up and ready to serve the base model, it's time to look at setting up the p-tuning logic at the client side. By and large, for each p-tuning task, we want to read the virtual tokens and append them to each of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887a270",
   "metadata": {},
   "source": [
    "### 3.1 Read the virtual prompt config file and weights\n",
    "\n",
    "First, we extract the NeMo model file, and read the config and the weights. By unpacking the .nemo file you should be able to see its two main components:\n",
    "- model_config.yaml: contains the model configuration\n",
    "- model_weights.ckpt: contains the model weights, in one of the subdirectory named `mp_rank_xx`.\n",
    "\n",
    "Here, we assume a `squad.nemo` model was trained according to [Multitask_Prompt_and_PTuning.ipynb](Multitask_Prompt_and_PTuning.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir squad-ptune-model\n",
    "!tar -xvf 'squad.nemo' -C squad-ptune-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omegaconf torch tritonclient[http] transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ecdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.load(\"squad-ptune-model/model_config.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe479365",
   "metadata": {},
   "source": [
    "NeMo framework support multi-task p-tuning, hence the configuration file can contain multiple tasks. We are only interested in the \"squad\" task here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5e2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = \"squad\"\n",
    "\n",
    "for t in config['task_templates']:\n",
    "    if t['taskname'] == taskname:\n",
    "        template = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a6a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'taskname': 'squad', 'prompt_template': '<|VIRTUAL_PROMPT_0|> Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:{answer}', 'total_virtual_tokens': 10, 'virtual_token_splits': [10], 'truncate_field': None, 'answer_only_loss': False, 'answer_field': 'answer'}\n"
     ]
    }
   ],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9b200",
   "metadata": {},
   "source": [
    "#### Read the virtual embedding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7e6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "prompt_table = torch.load(\"squad-ptune-model/mp_rank_00/model_weights.ckpt\", map_location=torch.device('cpu'))['prompt_table']\n",
    "fp16_dtype = prompt_table[f'prompt_table.{taskname}.prompt_embeddings.weight'].to(torch.float16)\n",
    "prompt_table[f'prompt_table.{taskname}.prompt_embeddings.weight'] = fp16_dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0bb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embedding = prompt_table[f'prompt_table.{taskname}.prompt_embeddings.weight']\n",
    "prompt_length = prompt_embedding.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0fecc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4096])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7703cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TYPE = 3\n",
    "request_prompt_lengths = prompt_length * np.ones([1, 1]).astype(np.uint32)\n",
    "request_prompt_embedding = np.expand_dims(prompt_embedding, axis=0)\n",
    "request_prompt_type = PROMPT_TYPE * np.ones([1, 1]).astype(np.uint32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68f0f5",
   "metadata": {},
   "source": [
    "### 3.2 Patch tokenizer\n",
    "\n",
    "We take the GPT2 tokenizer, which is used in the NeMo Megatron model. We then patch this tokenizer with additional special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8bda188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "pseudo_tokens = [\n",
    "    f'<prompt_{str(num)}>' for num in range(prompt_length)\n",
    "]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': pseudo_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b39781d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {'taskname': 'squad',\n",
    " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
    " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
    " 'answer': 'Saint Bernadette Soubirous'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8ebea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<prompt_0><prompt_1><prompt_2><prompt_3><prompt_4><prompt_5><prompt_6><prompt_7><prompt_8><prompt_9> Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_template = template['prompt_template']\n",
    "prompt = prompt.replace('<|VIRTUAL_PROMPT_0|>', ''.join(pseudo_tokens))\n",
    "\n",
    "prompt = prompt.replace(\"{context}\", example['context'])\n",
    "prompt = prompt.replace(\"{question}\", example['question'])\n",
    "prompt = prompt.replace(\"{answer}\", \"\")\n",
    "\n",
    "print(prompt)\n",
    "input_tokens = tokenizer.tokenize(prompt)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c45f2e",
   "metadata": {},
   "source": [
    "As we can see, the extra virtual token are appended at the beginning of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c644e59",
   "metadata": {},
   "source": [
    "### 3.3 Send to Triton\n",
    "\n",
    "Finally, with everything ready, we can now put the prompt and other hyperparameter into a proper Triton request, and send to the Triton inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "518a7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import np_to_triton_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83fb51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_input(input_name: str, data) -> httpclient.InferInput:\n",
    "    \"\"\"\n",
    "    Converts an input from a numpy array to a Triton compatible data type.\n",
    "\n",
    "    Args:\n",
    "        input_name: The name of the input parameter to send to Triton.\n",
    "        data: The full data field as a numpy array.\n",
    "\n",
    "    Returns:\n",
    "        Returns the converted field as a Triton input.\n",
    "    \"\"\"\n",
    "    infer_input = httpclient.InferInput(\n",
    "        input_name,\n",
    "        data.shape,\n",
    "        np_to_triton_dtype(data.dtype)\n",
    "    )\n",
    "    infer_input.set_data_from_numpy(data)\n",
    "    return infer_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f93dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM hyper parameters\n",
    "\n",
    "topk=1\n",
    "topp=0.9\n",
    "temperature = 1.0\n",
    "len_penalty = 1.0\n",
    "repetition_penalty = 1.0\n",
    "random_seed = 0\n",
    "beam_width = 1\n",
    "max_output_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "908ddc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_start_ids = np.array([input_ids]).astype(np.uint32)\n",
    "input_length = np.array([[len(input_start_ids[0])]]).astype(np.uint32)\n",
    "output_len = np.ones_like(input_length).astype(np.uint32) * max_output_len\n",
    "\n",
    "runtime_top_k = np.array([[topk]]).astype(np.uint32)\n",
    "runtime_top_p = np.array([[topp]]).astype(np.float32)\n",
    "beam_search_diversity_rate = np.array([[0.0]]).astype(np.float32)\n",
    "temperature = np.array([[temperature]]).astype(np.float32)\n",
    "len_penalty = np.array([[len_penalty]]).astype(np.float32)\n",
    "repetition_penalty = np.array([[repetition_penalty]]).astype(np.float32)\n",
    "random_seed = np.array([[random_seed]]).astype(np.uint64)\n",
    "is_return_log_probs = np.array([[True]]).astype(bool)\n",
    "beam_width = np.array([[beam_width]]).astype(np.uint32)\n",
    "start_ids = np.array([[50256]]).astype(np.uint32)\n",
    "end_ids = np.array([[50256]]).astype(np.uint32)\n",
    "bad_words_list = np.concatenate([np.zeros([input_start_ids.shape[0], 1, 1]).astype(\n",
    "    np.int32), (-1 * np.ones([input_start_ids.shape[0], 1, 1])).astype(np.int32)], axis=1)\n",
    "stop_word_list = np.concatenate([np.zeros([input_start_ids.shape[0], 1, 1]).astype(\n",
    "    np.int32), (-1 * np.ones([input_start_ids.shape[0], 1, 1])).astype(np.int32)], axis=1)\n",
    "\n",
    "inputs = [\n",
    "    fill_input(\"input_ids\", input_start_ids),\n",
    "    fill_input(\"input_lengths\", input_length),\n",
    "    fill_input(\"request_output_len\", output_len),\n",
    "    fill_input(\"runtime_top_k\", runtime_top_k),\n",
    "    fill_input(\"runtime_top_p\", runtime_top_p),\n",
    "    fill_input(\"beam_search_diversity_rate\", beam_search_diversity_rate),\n",
    "    fill_input(\"temperature\", temperature),\n",
    "    fill_input(\"len_penalty\", len_penalty),\n",
    "    fill_input(\"repetition_penalty\", repetition_penalty),\n",
    "    fill_input(\"random_seed\", random_seed),\n",
    "    fill_input(\"is_return_log_probs\", is_return_log_probs),\n",
    "    fill_input(\"beam_width\", beam_width),\n",
    "    fill_input(\"start_id\", start_ids),\n",
    "    fill_input(\"end_id\", end_ids),\n",
    "    fill_input(\"bad_words_list\", bad_words_list),\n",
    "    fill_input(\"stop_words_list\", stop_word_list),\n",
    "    fill_input(\"request_prompt_embedding\", request_prompt_embedding),\n",
    "    fill_input(\"request_prompt_lengths\", request_prompt_lengths),\n",
    "    fill_input(\"request_prompt_type\", request_prompt_type)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cd60e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    result = client.infer(\"gpt3_5b\", inputs)\n",
    "    output = result.as_numpy('output_ids').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6034f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21947,    25, 17340, 20221,    11,   262,  1524,   468,   257,\n",
       "        7835,  2095,    13,  1629,   404,   262,  8774, 11819,   338,\n",
       "        3869, 29500,   318,   257, 10861, 15207,   286,   262,  5283,\n",
       "        5335,    13, 34528,   287,  2166,   286,   262,  8774, 11819,\n",
       "         290,  6476,   340,    11,   318,   257, 15317, 15207,   286,\n",
       "        1951,   351,  5101,   510, 49309,   351,   262,  8177,   366,\n",
       "       37522,   578,  1215,  2185, 16543,  2516,  1911,  7406,   284,\n",
       "         262,  8774, 11819,   318,   262, 32520,  3970,   286,   262,\n",
       "       17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
       "         262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,\n",
       "         290, 14580,    13,   632,   318,   257, 30069,   286,   262,\n",
       "        7128, 33955,   379,   406,   454,  8906,    11,  4881,   810,\n",
       "         262,  5283,  5335,  1128,  7241,   306,  4120,   284,  9281,\n",
       "        6206,   324,  5857,   311, 12944,   343,   516,   287,  1248,\n",
       "        3365,    13,  1629,   262,   886,   286,   262,  1388,  3708,\n",
       "         357,   392,   287,   257,  1277,  1627,   326, 20417,   832,\n",
       "         513, 25827,   290,   262,  3561, 31390,   828,   318,   257,\n",
       "        2829,    11,  3660,  7815, 15207,   286,  5335,    13,   198,\n",
       "         198, 24361,    25,  1675,  4150,   750,   262,  5283,  5335,\n",
       "        7910,  1656,   287,  1248,  3365,   287,   406,   454,  8906,\n",
       "        4881,    30,   198,   198, 33706,    25, 48615,  6206,   324,\n",
       "        5857,   311, 12944,   343,   516, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256], dtype=uint32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4727f19",
   "metadata": {},
   "source": [
    "Lastly, we use the tokenizer to decode the Triton output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e5fc5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context: Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\\n\\nQuestion: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\\n\\nAnswer:Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.decode(output)\n",
    "response.replace(\"<|endoftext|>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b16a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
