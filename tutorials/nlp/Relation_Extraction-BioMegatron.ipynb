{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_0K1lsW1dj9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nemo_toolkit[nlp]\n",
      "  Cloning https://github.com/NVIDIA/NeMo.git (to revision r1.0.0rc1) to /tmp/pip-install-ftqr8t3d/nemo-toolkit\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (1.19.2)\n",
      "Requirement already satisfied: onnx>=1.7.0 in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (1.7.0)\n",
      "Collecting pytorch-lightning<=1.1.5,>=1.1.0\n",
      "  Downloading pytorch_lightning-1.1.5-py3-none-any.whl (685 kB)\n",
      "\u001b[K     |████████████████████████████████| 685 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (2.8.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (1.8.0a0+17f8c32)\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (1.10.11)\n",
      "Requirement already satisfied: ruamel.yaml in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (0.15.87)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (0.23.2)\n",
      "Collecting omegaconf>=2.0.5\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Collecting hydra-core>=1.0.4\n",
      "  Downloading hydra_core-1.0.6-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers>=4.0.1\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece<1.0.0 in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (0.1.94)\n",
      "Collecting webdataset>=0.1.48\n",
      "  Downloading webdataset-0.1.49-py3-none-any.whl (29 kB)\n",
      "Collecting tqdm>=4.41.0\n",
      "  Downloading tqdm-4.58.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 3.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opencc\n",
      "  Downloading OpenCC-1.1.1.post1-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pangu\n",
      "  Downloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 11.7 MB/s eta 0:00:01   |███████▌                        | 4.5 MB 7.1 MB/s eta 0:00:03     |████████████████████████        | 14.5 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (1.16.11)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (3.0.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.2 in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (3.3.2)\n",
      "Requirement already satisfied: torchtext in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (0.8.0a0)\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (1.1.1)\n",
      "Collecting youtokentome\n",
      "  Downloading youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rapidfuzz\n",
      "  Downloading rapidfuzz-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (5.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.1 MB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gdown\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting megatron-lm==1.1.5\n",
      "  Downloading megatron_lm-1.1.5-py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 30.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: inflect in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (4.1.0)\n",
      "Requirement already satisfied: sacrebleu[ja] in /opt/conda/lib/python3.6/site-packages (from nemo_toolkit[nlp]) (1.2.10)\n",
      "\u001b[33m  WARNING: sacrebleu 1.2.10 does not provide the extra 'ja'\u001b[0m\n",
      "Collecting sacremoses>=0.0.43\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 28.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.6/site-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.13.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (1.15.0)\n",
      "Collecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
      "  Downloading fsspec-0.8.7-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (5.3.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (0.18.2)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->nemo_toolkit[nlp]) (0.7)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->nemo_toolkit[nlp]) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->nemo_toolkit[nlp]) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->nemo_toolkit[nlp]) (0.17.0)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.1.1-py3-none-any.whl (25 kB)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 44.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 32.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (20.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (2.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (2020.10.28)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (2.24.0)\n",
      "Collecting braceexpand\n",
      "  Downloading braceexpand-0.1.6-py2.py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->nemo_toolkit[nlp]) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->nemo_toolkit[nlp]) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.11 in /opt/conda/lib/python3.6/site-packages (from boto3->nemo_toolkit[nlp]) (1.19.11)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.6/site-packages (from h5py->nemo_toolkit[nlp]) (1.5.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (2020.6.20)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.1.1-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 34.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (1.3.1)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from youtokentome->nemo_toolkit[nlp]) (7.1.2)\n",
      "Requirement already satisfied: pybind11 in /opt/conda/lib/python3.6/site-packages (from megatron-lm==1.1.5->nemo_toolkit[nlp]) (2.6.0)\n",
      "Requirement already satisfied: typing in /opt/conda/lib/python3.6/site-packages (from sacrebleu[ja]->nemo_toolkit[nlp]) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf->onnx>=1.7.0->nemo_toolkit[nlp]) (50.3.0.post20201006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 37.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (3.3.3)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB)\n",
      "\u001b[K     |████████████████████████████████| 135 kB 42.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (0.35.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (0.11.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (1.33.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "  Downloading aiohttp-3.7.4-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.9\"->hydra-core>=1.0.4->nemo_toolkit[nlp]) (3.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (3.0.4)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 34.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 36.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning<=1.1.5,>=1.1.0->nemo_toolkit[nlp]) (20.2.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 38.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nemo-toolkit, wget, jieba, gdown, sacremoses, antlr4-python3-runtime, idna-ssl\n",
      "  Building wheel for nemo-toolkit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nemo-toolkit: filename=nemo_toolkit-1.0.0rc1-py3-none-any.whl size=730840 sha256=91ccd51325135e3885e2f54f3f10d4121ec7473bb361013d30bedccf56bc091d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-x103tstf/wheels/00/af/10/2663ae95beae199257a765c6c43902a095a6ea82feb78fbfaa\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9681 sha256=b185426ac4825e8e39cf6abbf774692ac2564330bfbcfcdbeedb80d8835babde\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/1d/93/c863ee832230df5cfc25ca497b3e88e0ee3ea9e44adc46ac62\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=26890f91f304fd075d87e686318f0f4046a7303344227586aef4578eeff08eff\n",
      "  Stored in directory: /root/.cache/pip/wheels/17/a7/8b/a7e03881534e78558920ac68aaeca05180c0e2c3d11c4fce3b\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9681 sha256=7b0eb2e6f26678f171848c4804366cd7de54a1e13ad5a1a06f4f3e94d266adc4\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/15/6e/df5f8336275e96e19599034a76f9cfd81c6ae15d2bf16c11ca\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=7073de24b7bd31e0ee65b330f6956bdcab6243ad6b830c8bbf57690bfbcc95d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=76092be37bd973752ecb95f368db4987afcae20cc27607e79abe9186c25ee16e\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/04/35/9449686f1c26ff16f6224dc942e108329f3782185802ec6b93\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=5ee59eb235cdd6e8095cc44f5901153995fca9b46dfdae8d3aa1f07429c3b338\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "Successfully built nemo-toolkit wget jieba gdown sacremoses antlr4-python3-runtime idna-ssl\n",
      "Installing collected packages: tensorboard-plugin-wit, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, multidict, yarl, async-timeout, idna-ssl, aiohttp, fsspec, tqdm, pytorch-lightning, wget, omegaconf, importlib-resources, antlr4-python3-runtime, hydra-core, tokenizers, sacremoses, transformers, braceexpand, webdataset, opencc, pangu, jieba, youtokentome, rapidfuzz, gdown, megatron-lm, nemo-toolkit, pillow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.15.9999+nv\n",
      "    Uninstalling tensorboard-1.15.9999+nv:\n",
      "      Successfully uninstalled tensorboard-1.15.9999+nv\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.31.1\n",
      "    Uninstalling tqdm-4.31.1:\n",
      "      Successfully uninstalled tqdm-4.31.1\n",
      "  Attempting uninstall: sacremoses\n",
      "    Found existing installation: sacremoses 0.0.35\n",
      "    Uninstalling sacremoses-0.0.35:\n",
      "      Successfully uninstalled sacremoses-0.0.35\n",
      "Successfully installed aiohttp-3.7.4 antlr4-python3-runtime-4.8 async-timeout-3.0.1 braceexpand-0.1.6 cachetools-4.2.1 fsspec-0.8.7 gdown-3.12.2 google-auth-1.27.0 google-auth-oauthlib-0.4.2 hydra-core-1.0.6 idna-ssl-1.1.0 importlib-resources-5.1.1 jieba-0.42.1 megatron-lm-1.1.5 multidict-5.1.0 nemo-toolkit-1.0.0rc1 oauthlib-3.1.0 omegaconf-2.0.6 opencc-1.1.1.post1 pangu-4.0.6.1 pillow-8.1.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch-lightning-1.1.5 rapidfuzz-1.1.2 requests-oauthlib-1.3.0 rsa-4.7.2 sacremoses-0.0.43 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tokenizers-0.10.1 tqdm-4.58.0 transformers-4.3.3 webdataset-0.1.49 wget-3.2 yarl-1.6.3 youtokentome-1.0.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell\n",
    "\n",
    "# install NeMo\n",
    "BRANCH = 'r1.0.0rc1'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting widgetsnbextension~=3.5.0\n",
      "  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 27.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets>=1.0.0; python_version >= \"3.6\"\n",
      "  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (5.0.8)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (7.16.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets) (5.3.4)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.6/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.1.4)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.7.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (50.3.0.post20201006)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.7)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (19.0.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.2.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: async-generator in /opt/conda/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.4)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed ipywidgets-7.6.3 jupyterlab-widgets-1.0.0 widgetsnbextension-3.5.1\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If you're not using Colab, you might need to upgrade jupyter notebook to avoid the following error:\n",
    "# 'ImportError: IProgress not found. Please update jupyter and ipywidgets.'\n",
    "\n",
    "! pip install ipywidgets\n",
    "! jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# Please restart the kernel after running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzqD2WDFOIN-"
   },
   "outputs": [],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "daYw_Xll2ZR9"
   },
   "source": [
    "In this tutorial, we are going to describe how to finetune BioMegatron - a [BERT](https://arxiv.org/abs/1810.04805)-like [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) model pre-trained on large biomedical text corpus ([PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts and full-text commercial use collection) - on [RE: Text mining chemical-protein interactions (CHEMPROT)](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vi/track-5/).\n",
    "\n",
    "The model size of Megatron-LM can be larger than BERT, up to multi-billion parameters, compared to 345 million parameters of BERT-large.\n",
    "There are some alternatives of BioMegatron, most notably [BioBERT](https://arxiv.org/abs/1901.08746). Compared to BioBERT BioMegatron is larger by model size and pre-trained on larger text corpus.\n",
    "\n",
    "A more general tutorial of using BERT-based models, including Megatron-LM, for downstream natural language processing tasks can be found [here](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/01_Pretrained_Language_Models_for_Downstream_Tasks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Description\n",
    "**Relation Extraction (RE)** can be regarded as a type of sentence classification.\n",
    "\n",
    "The task is to classify the relation of a [GENE] and [CHEMICAL] in a sentence, for example like the following:\n",
    "```html\n",
    "14967461.T1.T22\t<@CHEMICAL$> inhibitors currently under investigation include the small molecules <@GENE$> (Iressa, ZD1839) and erlotinib (Tarceva, OSI-774), as well as monoclonal antibodies such as cetuximab (IMC-225, Erbitux).\t<CPR:4>\n",
    "14967461.T2.T22\t<@CHEMICAL$> inhibitors currently under investigation include the small molecules gefitinib (<@GENE$>, ZD1839) and erlotinib (Tarceva, OSI-774), as well as monoclonal antibodies such as cetuximab (IMC-225, Erbitux).\t<CPR:4>\n",
    "```\n",
    "to one of the following class:\n",
    "\n",
    "| Relation Class      | Relations |\n",
    "| ----------- | ----------- |\n",
    "| CPR:3      |  Upregulator and activator       |\n",
    "| CPR:4   | Downregulator and inhibitor         |\n",
    "| CPR:5 | Agonist |\n",
    "| CPR:6 | Antagonist |\n",
    "| CPR:9 | Substrate and product of |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnuziSwJ1yEB"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Details of ChemProt Relation Extraction task and the original data can be found on the [BioCreative VI website](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vi/track-5/)\n",
    "\n",
    "ChemProt dataset pre-processed for easier consumption can be downloaded from [here](https://github.com/arwhirang/recursive_chemprot/blob/master/Demo/tree_LSTM/data/chemprot-data_treeLSTM.zip) or [here](https://github.com/ncbi-nlp/BLUE_Benchmark/releases/download/0.1/bert_data.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--wJ2891aIIE"
   },
   "outputs": [],
   "source": [
    "TASK = 'ChemProt'\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'DATA_DIR')\n",
    "RE_DATA_DIR = os.path.join(DATA_DIR, 'RE')\n",
    "WORK_DIR = os.path.join(os.getcwd(), 'WORK_DIR')\n",
    "MODEL_CONFIG = 'text_classification_config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, 'RE'), exist_ok=True)\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "wget.download('https://github.com/arwhirang/recursive_chemprot/blob/master/Demo/tree_LSTM/data/chemprot-data_treeLSTM.zip?raw=true',\n",
    "              os.path.join(DATA_DIR, 'data_re.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o {DATA_DIR}/data_re.zip -d {RE_DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qB0oLE4R9EhJ"
   },
   "outputs": [],
   "source": [
    "! ls -l $RE_DATA_DIR/RE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process dataset\n",
    "Let's convert the dataset into the format that is compatible for [NeMo text-classification module](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/text_classification/text_classification_with_bert.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/data/import_datasets.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python import_datasets.py --dataset_name=chemprot --source_data_dir=DATA_DIR/RE --target_data_dir=DATA_DIR/RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the training data \n",
    "! head -n 5 {RE_DATA_DIR}/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the label mapping\n",
    "! cat {RE_DATA_DIR}/label_mapping.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to have the mapping exactly like this - it can be different.\n",
    "We use the same [mapping used by BioBERT](https://github.com/dmis-lab/biobert/blob/master/run_re.py#L438) so that comparison can be more straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_whKCxfTMo6Y"
   },
   "source": [
    "# Model configuration\n",
    "\n",
    "Now, let's take a closer look at the model's configuration and learn to train the model.\n",
    "\n",
    "The model is defined in a config file which declares multiple important sections. They are:\n",
    "- **model**: All arguments that are related to the Model - language model, a classifier, optimizer and schedulers, datasets and any other related information\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1gA8PsJ13MJ"
   },
   "outputs": [],
   "source": [
    "# download the model's configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mX3KmWMvSUQw"
   },
   "outputs": [],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.train_ds.file_path = os.path.join(RE_DATA_DIR, 'train.tsv')\n",
    "config.model.validation_ds.file_path = os.path.join(RE_DATA_DIR, 'dev.tsv')\n",
    "config.model.task_name = 'chemprot'\n",
    "# Note: these are small batch-sizes - increase as appropriate to available GPU capacity\n",
    "config.model.train_ds.batch_size=8\n",
    "config.model.validation_ds.batch_size=8\n",
    "config.model.dataset.num_classes=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCgWzNBkaQLZ"
   },
   "source": [
    "# Model Training\n",
    "## Setting up Data within the config\n",
    "\n",
    "Among other things, the config file contains dictionaries called **dataset**, **train_ds** and **validation_ds**. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
    "\n",
    "We assume that both training and evaluation files are located in the same directory, and use the default names mentioned during the data download step. \n",
    "So, to start model training, we simply need to specify `model.dataset.data_dir`, like we are going to do below.\n",
    "\n",
    "Also notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user.\n",
    "\n",
    "Let's now add the data directory path, task name and output directory for saving predictions to the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQHCJN-ZaoLp"
   },
   "outputs": [],
   "source": [
    "config.model.task_name = TASK\n",
    "config.model.output_dir = WORK_DIR\n",
    "config.model.dataset.data_dir = RE_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nB96-3sTc3yk"
   },
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
    "\n",
    "Let's first instantiate a Trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tG4FzZ4Ui60"
   },
   "outputs": [],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knF6QeQQdMrH"
   },
   "outputs": [],
   "source": [
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "cuda = 1 if torch.cuda.is_available() else 0\n",
    "config.trainer.gpus = cuda\n",
    "\n",
    "# for PyTorch Native AMP set precision=16\n",
    "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "\n",
    "# remove distributed training flags\n",
    "config.trainer.accelerator = 'DDP'\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IlEMdVxdr6p"
   },
   "source": [
    "## Setting up a NeMo Experiment\n",
    "\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8uztqGAmdrYt"
   },
   "outputs": [],
   "source": [
    "config.exp_manager.exp_dir = WORK_DIR\n",
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tjLhUvL_o7_"
   },
   "source": [
    "Before initializing the model, we might want to modify some of the model configs. Here we are modifying it to use BioMegatron, [Megatron-LM BERT](https://arxiv.org/abs/1909.08053) pre-trained on [PubMed](https://pubmed.ncbi.nlm.nih.gov/) biomedical text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xeuc2i7Y_nP5"
   },
   "outputs": [],
   "source": [
    "# complete list of supported BERT-like models\n",
    "print(nemo_nlp.modules.get_pretrained_lm_models_list())\n",
    "\n",
    "# specify BERT-like model, you want to use, for example, \"megatron-bert-345m-uncased\" or 'bert-base-uncased'\n",
    "PRETRAINED_BERT_MODEL = \"biomegatron-bert-345m-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RK2xglXyAUOO"
   },
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzNZNAVRjDD-"
   },
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders we'll be prepared for training and evaluation.\n",
    "Also, the pretrained BERT model will be downloaded, note it can take up to a few minutes depending on the size of the chosen BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NgsGLydWo-6-"
   },
   "outputs": [],
   "source": [
    "model = nemo_nlp.models.TextClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQ592Tx4pzyB"
   },
   "source": [
    "## Monitoring training progress\n",
    "Optionally, you can create a Tensorboard visualization to monitor training progress.\n",
    "If you're not using Colab, refer to [https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) if you're facing issues with running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTJr16_pp0aS"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google import colab\n",
    "    COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUvnSpyjp0Dh"
   },
   "outputs": [],
   "source": [
    "# start model training\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ref1qSonGNhP"
   },
   "source": [
    "## Training Script\n",
    "\n",
    "If you have NeMo installed locally, you can also train the model with `examples/nlp/text_classification/text_classification_with_bert.py.`\n",
    "\n",
    "To run training script, use:\n",
    "\n",
    "`python text_classification_with_bert.py \\\n",
    " model.dataset.data_dir=PATH_TO_DATA_DIR \\\n",
    " model.task_name=TASK`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training could take several minutes and the results should look something like:\n",
    "\n",
    "```\n",
    "precision    recall  f1-score   support\n",
    "    \n",
    "               0     0.7328    0.8348    0.7805       115\n",
    "               1     0.9402    0.9291    0.9346      7950\n",
    "               2     0.8311    0.9146    0.8708       199\n",
    "               3     0.6400    0.6302    0.6351       457\n",
    "               4     0.8002    0.8317    0.8156      1093\n",
    "               5     0.7228    0.7518    0.7370       548\n",
    "    \n",
    "        accuracy                         0.8949     10362\n",
    "       macro avg     0.7778    0.8153    0.7956     10362\n",
    "    weighted avg     0.8963    0.8949    0.8954     10362\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Relation_Extraction-BioMegatron.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
