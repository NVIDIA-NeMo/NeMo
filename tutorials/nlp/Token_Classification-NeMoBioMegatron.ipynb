{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a434f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH='main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developmental-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\\n\\nInstructions for setting up Colab are as follows:\\n1. Open a new Python 3 notebook.\\n2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\\n3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\\n4. Run this cell to set up dependencies.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell\n",
    "\n",
    "# install NeMo\n",
    "#!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "challenging-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-01-13 03:39:00 experimental:27] Module <function get_argmin_mat at 0x7fe31f65d3a0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-13 03:39:00 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7fe31f7c8ca0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-13 03:39:00 experimental:27] Module <function parse_scale_configs at 0x7fe31f65dd30> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-13 03:39:00 experimental:27] Module <function get_embs_and_timestamps at 0x7fe31f7d40d0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-ethiopia",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to describe how to finetune BioMegatron - a [BERT](https://arxiv.org/abs/1810.04805)-like [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) model pre-trained on large biomedical text corpus ([PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts and full-text commercial use collection) - on the [NCBI Disease Dataset](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/) for Named Entity Recognition.\n",
    "\n",
    "The model size of Megatron-LM can be larger than BERT, up to multi-billion parameters, compared to 345 million parameters of BERT-large.\n",
    "There are some alternatives of BioMegatron, most notably [BioBERT](https://arxiv.org/abs/1901.08746). Compared to BioBERT BioMegatron is larger by model size and pre-trained on larger text corpus.\n",
    "\n",
    "A more general tutorial of using BERT-based models, including Megatron-LM, for downstream natural language processing tasks can be found [here](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/nlp/01_Pretrained_Language_Models_for_Downstream_Tasks.ipynb).\n",
    "\n",
    "# Task Description\n",
    "**Named entity recognition (NER)**, also referred to as entity chunking, identification or extraction, is the task of detecting and classifying key information (entities) in text.\n",
    "\n",
    "For instance, **given sentences from medical abstracts, what diseases are mentioned?**<br>\n",
    "In this case, our data input is sentences from the abstracts, and our labels are the precise locations of the named disease entities.  Take a look at the information provided for the dataset.\n",
    "\n",
    "For more details and general examples on Named Entity Recognition, please refer to the [Token Classification and Named Entity Recognition tutorial notebook](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/nlp/Token_Classification_Named_Entity_Recognition.ipynb).\n",
    "\n",
    "# Dataset\n",
    "\n",
    "The [NCBI-disease corpus](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/) is a set of 793 PubMed abstracts, annotated by 14 annotators. The annotations take the form of HTML-style tags inserted into the abstract text using the clearly defined rules.  The annotations identify named diseases, and can be used to fine-tune a language model to identify disease mentions in future abstracts, *whether those diseases were part of the original training set or not*.\n",
    "\n",
    "Here's an example of what an annotated abstract from the corpus looks like:\n",
    "\n",
    "```html\n",
    "10021369\tIdentification of APC2, a homologue of the <category=\"Modifier\">adenomatous polyposis coli tumour</category> suppressor .\tThe <category=\"Modifier\">adenomatous polyposis coli ( APC ) tumour</category>-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In <category=\"Modifier\">colon carcinoma</category> cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - <category=\"Modifier\">colon carcinoma</category> cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and <category=\"SpecificDisease\">cancer</category> .\n",
    "```\n",
    "\n",
    "In this example, we see the following tags within the abstract:\n",
    "```html\n",
    "<category=\"Modifier\">adenomatous polyposis coli tumour</category>\n",
    "<category=\"Modifier\">adenomatous polyposis coli ( APC ) tumour</category>\n",
    "<category=\"Modifier\">colon carcinoma</category>\n",
    "<category=\"Modifier\">colon carcinoma</category>\n",
    "<category=\"SpecificDisease\">cancer</category>\n",
    "```\n",
    "\n",
    "For our purposes, we will consider any identified category (such as \"Modifier\", \"Specific Disease\", and a few others) to generally be a \"disease\".\n",
    "\n",
    "Let's download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "federal-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"DATA_DIR\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, 'NER'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "relevant-juvenile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NCBI data...\n",
      "Archive:  DATA_DIR/NCBI_corpus.zip\n",
      "  inflating: DATA_DIR/NCBI_corpus_development.txt  \n",
      "  inflating: DATA_DIR/NCBI_corpus_testing.txt  \n",
      "  inflating: DATA_DIR/NCBI_corpus_training.txt  \n"
     ]
    }
   ],
   "source": [
    "print('Downloading NCBI data...')\n",
    "wget.download('https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/NCBI_corpus.zip', DATA_DIR)\n",
    "! unzip -o {DATA_DIR}/NCBI_corpus.zip -d {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "radical-castle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288106\tClustering of missense mutations in the <category=\"Modifier\">ataxia-telangiectasia</category> gene in a <category=\"SpecificDisease\">sporadic T-cell leukaemia</category>.\t<category=\"SpecificDisease\">Ataxia-telangiectasia</category> ( <category=\"SpecificDisease\">A-T</category> ) is a <category=\"DiseaseClass\">recessive multi-system disorder</category> caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of <category=\"DiseaseClass\">cancer</category> , especially <category=\"DiseaseClass\">lymphoid neoplasias</category> , is substantially elevated in <category=\"Modifier\">A-T</category> patients and has long been associated with chromosomal instability . By analysing <category=\"Modifier\">tumour</category> DNA from patients with <category=\"SpecificDisease\">sporadic T-cell prolymphocytic leukaemia</category> ( <category=\"SpecificDisease\">T-PLL</category> ) , a rare <category=\"DiseaseClass\">clonal malignancy</category> with similarities to a <category=\"SpecificDisease\">mature T-cell leukaemia</category> seen in <category=\"SpecificDisease\">A-T</category> , we demonstrate a high frequency of ATM mutations in <category=\"SpecificDisease\">T-PLL</category> . In marked contrast to the ATM mutation pattern in <category=\"SpecificDisease\">A-T</category> , the most frequent nucleotide changes in this <category=\"DiseaseClass\">leukaemia</category> were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated <category=\"SpecificDisease\">T-PLL</category> samples had a previously reported <category=\"Modifier\">A-T</category> allele . In contrast , no mutations were detected in the p53 gene , suggesting that this <category=\"Modifier\">tumour</category> suppressor is not frequently altered in this <category=\"DiseaseClass\">leukaemia</category> . Occasional missense mutations in ATM were also found in <category=\"Modifier\">tumour</category> DNA from patients with <category=\"SpecificDisease\">B-cell non-Hodgkins lymphomas</category> ( <category=\"SpecificDisease\">B-NHL</category> ) and a <category=\"Modifier\">B-NHL</category> cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated <category=\"DiseaseClass\">tumours</category> establishes somatic inactivation of this gene in the pathogenesis of <category=\"SpecificDisease\">sporadic T-PLL</category> and suggests that ATM acts as a <category=\"Modifier\">tumour</category> suppressor . As constitutional DNA was not available , a putative hereditary predisposition to <category=\"SpecificDisease\">T-PLL</category> will require further investigation . . \n"
     ]
    }
   ],
   "source": [
    "# If you want to see more examples, you can explore the text of the corpus using the file browser to the left, or open files directly, for example typing a command like the following in a code-cell:\n",
    "\n",
    "! head -1 $DATA_DIR/NCBI_corpus_testing.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-maine",
   "metadata": {},
   "source": [
    "We have two datasets derived from this corpus:  a text classification dataset and a named entity recognition (NER) dataset.  The text classification dataset labels the abstracts among three broad disease groupings.  We'll use this simple split to demonstrate the NLP text classification task.   The NER dataset labels individual words as diseases.  This dataset will be used for the NLP NER task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-numbers",
   "metadata": {},
   "source": [
    "## Pre-process dataset\n",
    "A pre-processed NCBI-disease dataset for NER can be found [here](https://github.com/spyysalo/ncbi-disease/tree/master/conll) or [here](https://github.com/dmis-lab/biobert#datasets).<br>\n",
    "We download the files under {DATA_DIR/NER} directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "present-interference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DATA_DIR/NER/test (1).tsv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_DATA_DIR = f'{DATA_DIR}/NER'\n",
    "wget.download('https://raw.githubusercontent.com/spyysalo/ncbi-disease/master/conll/train.tsv', NER_DATA_DIR)\n",
    "wget.download('https://raw.githubusercontent.com/spyysalo/ncbi-disease/master/conll/devel.tsv', NER_DATA_DIR)\n",
    "wget.download('https://raw.githubusercontent.com/spyysalo/ncbi-disease/master/conll/test.tsv', NER_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identical-figure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24M\n",
      "-rw-r--r-- 1 root root 1.4M Jan 10 21:40  cached_text_dev.txt_BertTokenizer_128_28996_-1\n",
      "-rw-r--r-- 1 root root 1.2M Jan 10 21:57  cached_text_dev.txt_BertTokenizer_128_31012_-1\n",
      "-rw-r--r-- 1 root root 8.3M Jan 10 21:40  cached_text_train.txt_BertTokenizer_128_28996_-1\n",
      "-rw-r--r-- 1 root root 8.2M Jan 10 21:57  cached_text_train.txt_BertTokenizer_128_31012_-1\n",
      "-rw-r--r-- 1 root root 196K Jan 10 21:28  dev.tsv\n",
      "-rw-r--r-- 1 root root 196K Jan 12 22:11 'devel (1).tsv'\n",
      "-rw-r--r-- 1 root root 196K Jan 13 03:39  devel.tsv\n",
      "-rw-r--r-- 1 root root   21 Jan 13 00:40  label_ids.csv\n",
      "-rw-r--r-- 1 root root  63K Jan 12 22:12  labels_dev.txt\n",
      "-rw-r--r-- 1 root root   51 Jan 10 21:40  labels_dev_label_stats.tsv\n",
      "-rw-r--r-- 1 root root  65K Jan 12 22:12  labels_test.txt\n",
      "-rw-r--r-- 1 root root 360K Jan 12 22:11  labels_train.txt\n",
      "-rw-r--r-- 1 root root   53 Jan 13 00:40  labels_train_label_stats.tsv\n",
      "-rw-r--r-- 1 root root   43 Jan 10 21:40  labels_train_weights.p\n",
      "-rw-r--r-- 1 root root 201K Jan 13 03:39 'test (1).tsv'\n",
      "-rw-r--r-- 1 root root 201K Jan 10 16:31  test.tsv\n",
      "-rw-r--r-- 1 root root 135K Jan 12 22:12  text_dev.txt\n",
      "-rw-r--r-- 1 root root 138K Jan 12 22:12  text_test.txt\n",
      "-rw-r--r-- 1 root root 760K Jan 12 22:11  text_train.txt\n",
      "-rw-r--r-- 1 root root 1.1M Jan 13 03:39 'train (1).tsv'\n",
      "-rw-r--r-- 1 root root 1.1M Jan 10 16:31  train.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $NER_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-donor",
   "metadata": {},
   "source": [
    "Convert these to a format that is compatible with [NeMo Token Classification module](https://github.com/NVIDIA/NeMo/blob/stable/examples/nlp/token_classification/token_classification_train.py), using the [conversion script](https://github.com/NVIDIA/NeMo/blob/stable/examples/nlp/token_classification/data/import_from_iob_format.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "utility-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv $NER_DATA_DIR/devel.tsv $NER_DATA_DIR/dev.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suited-jenny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import_from_iob_format (2).py'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/nlp/token_classification/data/import_from_iob_format.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sensitive-victoria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:39:12 import_from_iob_format:119] Processing DATA_DIR/NER/train.tsv\n",
      "[NeMo I 2022-01-13 03:39:12 import_from_iob_format:124] Processing of the DATA_DIR/NER/train.tsv is complete\n",
      "[NeMo I 2022-01-13 03:39:15 import_from_iob_format:119] Processing DATA_DIR/NER/dev.tsv\n",
      "[NeMo I 2022-01-13 03:39:15 import_from_iob_format:124] Processing of the DATA_DIR/NER/dev.tsv is complete\n",
      "[NeMo I 2022-01-13 03:39:18 import_from_iob_format:119] Processing DATA_DIR/NER/test.tsv\n",
      "[NeMo I 2022-01-13 03:39:18 import_from_iob_format:124] Processing of the DATA_DIR/NER/test.tsv is complete\n"
     ]
    }
   ],
   "source": [
    "! python import_from_iob_format.py --data_file=$NER_DATA_DIR/train.tsv\n",
    "! python import_from_iob_format.py --data_file=$NER_DATA_DIR/dev.tsv\n",
    "! python import_from_iob_format.py --data_file=$NER_DATA_DIR/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-debate",
   "metadata": {},
   "source": [
    "The NER task requires two files: the text sentences, and the labels.  Run the next two cells to see a sample of the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sound-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor . \n",
      "The adenomatous polyposis coli ( APC ) tumour - suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK - 3beta ) , axin / conductin and betacatenin . \n",
      "Complex formation induces the rapid degradation of betacatenin . \n",
      "In colon carcinoma cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf - 4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . \n",
      "Here , we report the identification and genomic structure of APC homologues . \n",
      "Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . \n",
      "Like APC , APC2 regulates the formation of active betacatenin - Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - colon carcinoma cells . \n",
      "Human APC2 maps to chromosome 19p13 . \n",
      "3 . \n",
      "APC and APC2 may therefore have comparable functions in development and cancer . \n"
     ]
    }
   ],
   "source": [
    "!head $NER_DATA_DIR/text_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spectacular-strain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O B-Disease I-Disease I-Disease I-Disease O O \n",
      "O B-Disease I-Disease I-Disease I-Disease I-Disease I-Disease I-Disease O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      "O O O O O O O O O \n",
      "O B-Disease I-Disease O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      "O O O O O O O O O O O O O \n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O B-Disease I-Disease O O \n",
      "O O O O O O O \n",
      "O O \n",
      "O O O O O O O O O O O B-Disease O \n"
     ]
    }
   ],
   "source": [
    "!head $NER_DATA_DIR/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-principal",
   "metadata": {},
   "source": [
    "### IOB Tagging\n",
    "We can see that the abstract has been broken into sentences.  Each sentence is then further parsed into words with labels that correspond to the original HTML-style tags in the corpus. \n",
    "\n",
    "The sentences and labels in the NER dataset map to each other with _inside, outside, beginning (IOB)_ tagging. Anything separated by white space is a word, including punctuation.  For the first sentence we have the following mapping:\n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "Recall the original corpus tags:\n",
    "```html\n",
    "Identification of APC2, a homologue of the <category=\"Modifier\">adenomatous polyposis coli tumour</category> suppressor .\n",
    "```\n",
    "The beginning word of the tagged text, \"adenomatous\", is now IOB-tagged with a <span style=\"font-family:verdana;font-size:110%;\">B</span> (beginning) tag, the other parts of the disease, \"polyposis coli tumour\" tagged with <span style=\"font-family:verdana;font-size:110%;\">I</span> (inside) tags, and everything else tagged as <span style=\"font-family:verdana;font-size:110%;\">O</span> (outside).\n",
    "\n",
    "# Model configuration\n",
    "\n",
    "Our Named Entity Recognition model is comprised of the pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a Token Classification layer.\n",
    "\n",
    "The model is defined in a config file which declares multiple important sections. They are:\n",
    "- **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813cc36",
   "metadata": {},
   "source": [
    "## Convert the Megatron-LM Weights to Nem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b8e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.modules.common.megatron.megatron_utils import MEGATRON_CONFIG_MAP\n",
    "import pathlib\n",
    "# specify BERT-like model, you want to use\n",
    "PRETRAINED_BERT_MODEL = \"biomegatron-bert-345m-cased\"\n",
    "\n",
    "checkpoint_url = MEGATRON_CONFIG_MAP[PRETRAINED_BERT_MODEL]['checkpoint']\n",
    "vocab_url = MEGATRON_CONFIG_MAP[PRETRAINED_BERT_MODEL]['vocab']\n",
    "checkpoint_filename = pathlib.Path(checkpoint_url).name\n",
    "vocab_filename = pathlib.Path(vocab_url).name\n",
    "if not pathlib.Path(checkpoint_filename).exists():\n",
    "    print('downloading from checkpoint url', checkpoint_url)\n",
    "    !wget $checkpoint_url\n",
    "if not pathlib.Path(vocab_filename).exists():\n",
    "    print('downloading from vocab url', vocab_url)\n",
    "    !wget $vocab_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b00ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is already exists\n"
     ]
    }
   ],
   "source": [
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "# Prepare the model parameters \n",
    "# download the model's configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "MODEL_CONFIG = \"megatron_bert_config.yaml\"\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/nlp/language_modeling/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ae5a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR/configs/megatron_bert_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "config.model.num_layers = 24\n",
    "config.model.hidden_size = 1024\n",
    "config.model.ffn_hidden_size = 4096\n",
    "config.model.num_attention_heads = 16\n",
    "config.model.tokenizer.vocab_file = vocab_filename\n",
    "config.model.tokenizer.type = 'BertWordPieceCase'\n",
    "config.model.tensor_model_parallel_size = 1\n",
    "config.model.data.data_prefix = ''\n",
    "config.model.max_position_embeddings = 512\n",
    "config.model.data.seq_length = 512\n",
    "config.cfg = {}\n",
    "config.cfg.cfg = config.model\n",
    "with open('hparams.yaml', 'w') as f:\n",
    "    f.write(OmegaConf.to_yaml(config.cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e1beda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-01-13 00:21:01 experimental:27] Module <function get_argmin_mat at 0x7f15f2590700> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-13 00:21:01 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7f15f2590790> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-13 00:21:01 experimental:27] Module <function parse_scale_configs at 0x7f15f259d1f0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-13 00:21:01 experimental:27] Module <function get_embs_and_timestamps at 0x7f15f259d280> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "I0113 00:21:01.651694 139736060577600 distributed_c10d.py:218] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "I0113 00:21:01.651954 139736060577600 distributed_c10d.py:252] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "converted 332.59M parameters\n",
      "[NeMo I 2022-01-13 00:21:01 tokenizer_utils:188] Getting Megatron tokenizer for pretrained model name: megatron-bert-345m-cased and custom vocab file: /NeMo/tutorials/nlp/vocab.txt\n",
      "[NeMo I 2022-01-13 00:21:01 tokenizer_utils:124] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-cased, vocab_file: /NeMo/tutorials/nlp/vocab.txt, special_tokens_dict: {}, and use_fast: False\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "[NeMo I 2022-01-13 00:21:05 megatron_bert_model:375] Padded vocab_size: 29056, original vocab_size: 28996, dummy tokens: 60.\n",
      "[NeMo W 2022-01-13 00:21:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:208: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['model.language_model.pooler.dense.weight', 'model.language_model.pooler.dense.bias', 'model.lm_head.bias', 'model.lm_head.dense.weight', 'model.lm_head.dense.bias', 'model.lm_head.layernorm.weight', 'model.lm_head.layernorm.bias', 'model.binary_head.weight', 'model.binary_head.bias']\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-01-13 00:21:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:212: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['model.qa_head.qa_dense.weight', 'model.qa_head.qa_dense.bias']\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2022-01-13 00:29:30 megatron_lm_ckpt_to_nemo:213] NeMo model saved to: /NeMo/tutorials/nlp/biomegatron.nemo\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PWD = os.getcwd()\n",
    "!python -m torch.distributed.run --nproc_per_node=1 ../../examples/nlp/language_modeling/megatron_lm_ckpt_to_nemo.py --checkpoint_folder=$PWD --checkpoint_name=$checkpoint_filename --hparams_file=$PWD/hparams.yaml --nemo_file_path=$PWD/biomegatron.nemo --model_type=bert --tensor_model_parallel_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-effort",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Setting up Data within the config\n",
    "\n",
    "Among other things, the config file contains dictionaries called dataset, train_ds and validation_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2c67a",
   "metadata": {},
   "source": [
    "\n",
    "We assume that both training and evaluation files are located in the same directory, and use the default names mentioned during the data download step. \n",
    "So, to start model training, we simply need to specify `model.dataset.data_dir`, like we are going to do below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34bbd1",
   "metadata": {},
   "source": [
    "\n",
    "Also notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user.\n",
    "\n",
    "Let's now add the data directory path to the config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-mauritius",
   "metadata": {},
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
    "\n",
    "Let's first instantiate a Trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "speaking-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "MODEL_CONFIG = \"token_classification_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "demanding-ballet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is already exists\n"
     ]
    }
   ],
   "source": [
    "# download the model's configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "criminal-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR/configs/token_classification_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "# Note: these are small batch-sizes - increase as appropriate to available GPU capacity\n",
    "config.model.train_ds.batch_size=8\n",
    "config.model.validation_ds.batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "informed-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this tutorial train and dev datasets are located in the same folder, so it is enought to add the path of the data directory to the config\n",
    "config.model.dataset.data_dir = os.path.join(DATA_DIR, 'NER')\n",
    "\n",
    "# if you want to decrease the size of your datasets, uncomment the lines below:\n",
    "# NUM_SAMPLES = 1000\n",
    "# config.model.train_ds.num_samples = NUM_SAMPLES\n",
    "# config.model.validation_ds.num_samples = NUM_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "divine-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model: null\n",
      "trainer:\n",
      "  gpus: 1\n",
      "  num_nodes: 1\n",
      "  max_epochs: 5\n",
      "  max_steps: null\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 0.0\n",
      "  precision: 16\n",
      "  accelerator: ddp\n",
      "  checkpoint_callback: false\n",
      "  logger: false\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "  resume_from_checkpoint: null\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: token_classification_model\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "model:\n",
      "  label_ids: null\n",
      "  class_labels:\n",
      "    class_labels_file: label_ids.csv\n",
      "  dataset:\n",
      "    data_dir: DATA_DIR/NER\n",
      "    class_balancing: null\n",
      "    max_seq_length: 128\n",
      "    pad_label: O\n",
      "    ignore_extra_tokens: false\n",
      "    ignore_start_end: false\n",
      "    use_cache: true\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "  train_ds:\n",
      "    text_file: text_train.txt\n",
      "    labels_file: labels_train.txt\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    batch_size: 8\n",
      "  validation_ds:\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 8\n",
      "  test_ds:\n",
      "    text_file: text_dev.txt\n",
      "    labels_file: labels_dev.txt\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    batch_size: 64\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    nemo_file: /NeMo/biomegatron_biomegatron_full_biovocab_cased_text_sentence/biomegatron.nemo\n",
      "    config_file: null\n",
      "    config: null\n",
      "  head:\n",
      "    num_fc_layers: 2\n",
      "    fc_dropout: 0.5\n",
      "    activation: relu\n",
      "    use_transformer_init: true\n",
      "  optim:\n",
      "    name: adam\n",
      "    lr: 5.0e-05\n",
      "    weight_decay: 0.0\n",
      "    sched:\n",
      "      name: WarmupAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      last_epoch: -1\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "computational-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer config - \n",
      "\n",
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 5\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "precision: 16\n",
      "accelerator: ddp\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "unique-genre",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "[NeMo W 2022-01-13 03:44:12 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:48: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-01-13 03:44:12 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "cuda = 1 if torch.cuda.is_available() else 0\n",
    "config.trainer.gpus = cuda\n",
    "\n",
    "# for PyTorch Native AMP set precision=16\n",
    "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "\n",
    "# remove distributed training flags\n",
    "config.trainer.accelerator = None\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-literature",
   "metadata": {},
   "source": [
    "## Setting up a NeMo Experiment\n",
    "\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "mathematical-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:44:15 exp_manager:283] Experiments will be logged at /NeMo/tutorials/nlp/nemo_experiments/token_classification_model/2022-01-13_03-44-15\n",
      "[NeMo I 2022-01-13 03:44:15 exp_manager:648] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-13 03:44:15 exp_manager:879] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2022-01-13 03:44:15 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/NeMo/tutorials/nlp/nemo_experiments/token_classification_model/2022-01-13_03-44-15'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "compact-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "# config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL\n",
    "config.model.language_model.nemo_file = 'biomegatron.nemo'\n",
    "config.model.language_model.pretrained_model_name = 'megatron-bert-cased'\n",
    "config.model.tokenizer.vocab_file='vocab.txt'\n",
    "config.model.tokenizer.tokenizer_model = 'BertWordPieceCase'\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-geometry",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders we'll be prepared for training and evaluation.\n",
    "Also, the pretrained BERT model will be downloaded, note it can take up to a few minutes depending on the size of the chosen BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "indoor-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:44:21 tokenizer_utils:124] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-cased, vocab_file: /NeMo/tutorials/nlp/vocab.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:54] Processing DATA_DIR/NER/labels_train.txt\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:90] Labels mapping {'O': 0, 'B-Disease': 1, 'I-Disease': 2} saved to : DATA_DIR/NER/label_ids.csv\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:99] Three most popular labels in DATA_DIR/NER/labels_train.txt:\n",
      "[NeMo I 2022-01-13 03:44:24 data_preprocessing:194] label: 0, 124819 out of 136086 (91.72%).\n",
      "[NeMo I 2022-01-13 03:44:24 data_preprocessing:194] label: 2, 6122 out of 136086 (4.50%).\n",
      "[NeMo I 2022-01-13 03:44:24 data_preprocessing:194] label: 1, 5145 out of 136086 (3.78%).\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:101] Total labels: 136086. Label frequencies - {0: 124819, 2: 6122, 1: 5145}\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:107] Class weights restored from DATA_DIR/NER/labels_train_weights.p\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_dataset:277] features restored from DATA_DIR/NER/cached_text_train.txt_BertTokenizer_128_28996_-1\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:54] Processing DATA_DIR/NER/labels_dev.txt\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Disease': 1, 'I-Disease': 2}\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:96] DATA_DIR/NER/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_dataset:277] features restored from DATA_DIR/NER/cached_text_dev.txt_BertTokenizer_128_28996_-1\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:54] Processing DATA_DIR/NER/labels_dev.txt\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B-Disease': 1, 'I-Disease': 2}\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_utils:96] DATA_DIR/NER/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-01-13 03:44:24 token_classification_dataset:277] features restored from DATA_DIR/NER/cached_text_dev.txt_BertTokenizer_128_28996_-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-13 03:44:24 modelPT:202] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2022-01-13 03:44:24 lm_utils:78] megatron-bert-cased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:44:35 tokenizer_utils:188] Getting Megatron tokenizer for pretrained model name: megatron-bert-345m-cased and custom vocab file: /NeMo/tutorials/nlp/vocab.txt\n",
      "[NeMo I 2022-01-13 03:44:35 tokenizer_utils:124] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-cased, vocab_file: /NeMo/tutorials/nlp/vocab.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:44:38 megatron_bert_model:375] Padded vocab_size: 29056, original vocab_size: 28996, dummy tokens: 60.\n",
      "[NeMo I 2022-01-13 03:44:40 tokenizer_utils:188] Getting Megatron tokenizer for pretrained model name: megatron-bert-345m-cased and custom vocab file: /NeMo/tutorials/nlp/vocab.txt\n",
      "[NeMo I 2022-01-13 03:44:40 tokenizer_utils:124] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-cased, vocab_file: /NeMo/tutorials/nlp/vocab.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:44:43 megatron_bert_model:375] Padded vocab_size: 29056, original vocab_size: 28996, dummy tokens: 60.\n",
      "[NeMo I 2022-01-13 03:44:44 save_restore_connector:149] Model MegatronBertModel was successfully restored from /NeMo/tutorials/nlp/biomegatron.nemo.\n"
     ]
    }
   ],
   "source": [
    "model_ner = nemo_nlp.models.TokenClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-pipeline",
   "metadata": {},
   "source": [
    "## Monitoring training progress\n",
    "Optionally, you can create a Tensorboard visualization to monitor training progress.\n",
    "If you're not using Colab, refer to [https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) if you're facing issues with running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "changed-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use tensorboard, please use this notebook in a Google Colab environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google import colab\n",
    "    COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "applied-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-13 03:45:06 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-01-13 03:45:06 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "[NeMo W 2022-01-13 03:45:06 modelPT:475] The lightning trainer received accelerator: <pytorch_lightning.accelerators.gpu.GPUAccelerator object at 0x7fe31ee61940>. We recommend to use 'ddp' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:45:06 modelPT:566] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-01-13 03:45:06 lr_scheduler:833] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fe31d24b0d0>\" \n",
      "    will be used during training (effective maximum steps = 3395) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 3395\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | MegatronBertModel    | 332 M \n",
      "1 | classifier            | TokenClassifier      | 1.1 M \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "333 M     Trainable params\n",
      "0         Non-trainable params\n",
      "333 M     Total params\n",
      "667.288   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be82a8395f4419d9e3eb7c113ecbc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-13 03:45:06 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:45:07 token_classification_model:188] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                        100.00       7.12      13.30        365\n",
      "    B-Disease (label_id: 1)                                  3.07     100.00       5.96         11\n",
      "    I-Disease (label_id: 2)                                  0.00       0.00       0.00         15\n",
      "    -------------------\n",
      "    micro avg                                                9.46       9.46       9.46        391\n",
      "    macro avg                                               34.36      35.71       6.42        391\n",
      "    weighted avg                                            93.44       9.46      12.58        391\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-13 03:45:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4190c3efd21949c7946ad7401d033fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d0bd06c36245edad1039ba108bc10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:46:10 token_classification_model:188] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         96.25      98.85      97.53      22092\n",
      "    B-Disease (label_id: 1)                                 58.31      28.97      38.71        787\n",
      "    I-Disease (label_id: 2)                                 64.98      52.94      58.34       1090\n",
      "    -------------------\n",
      "    micro avg                                               94.47      94.47      94.47      23969\n",
      "    macro avg                                               73.18      60.25      64.86      23969\n",
      "    weighted avg                                            93.58      94.47      93.82      23969\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 678: val_loss reached 0.19994 (best 0.19994), saving model to \"/NeMo/tutorials/nlp/nemo_experiments/token_classification_model/2022-01-13_03-44-15/checkpoints/token_classification_model--val_loss=0.1999-epoch=0.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f7f9e5fd1645f29b5df56502a71ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:47:26 token_classification_model:188] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.18      98.69      97.93      22092\n",
      "    B-Disease (label_id: 1)                                 57.07      58.45      57.75        787\n",
      "    I-Disease (label_id: 2)                                 77.47      51.74      62.05       1090\n",
      "    -------------------\n",
      "    micro avg                                               95.24      95.24      95.24      23969\n",
      "    macro avg                                               77.24      69.63      72.58      23969\n",
      "    weighted avg                                            94.97      95.24      94.98      23969\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1357: val_loss reached 0.16159 (best 0.16159), saving model to \"/NeMo/tutorials/nlp/nemo_experiments/token_classification_model/2022-01-13_03-44-15/checkpoints/token_classification_model--val_loss=0.1616-epoch=1.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4afa2beb45d41bda9e2322eb7effc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:48:41 token_classification_model:188] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.62      98.98      98.30      22092\n",
      "    B-Disease (label_id: 1)                                 64.88      65.95      65.41        787\n",
      "    I-Disease (label_id: 2)                                 81.69      57.71      67.63       1090\n",
      "    -------------------\n",
      "    micro avg                                               96.02      96.02      96.02      23969\n",
      "    macro avg                                               81.40      74.21      77.11      23969\n",
      "    weighted avg                                            95.82      96.02      95.82      23969\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2036: val_loss reached 0.14897 (best 0.14897), saving model to \"/NeMo/tutorials/nlp/nemo_experiments/token_classification_model/2022-01-13_03-44-15/checkpoints/token_classification_model--val_loss=0.1490-epoch=2.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dfeb0518dc4ffda97c366eb79c1279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:49:58 token_classification_model:188] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         98.13      98.69      98.41      22092\n",
      "    B-Disease (label_id: 1)                                 68.98      63.28      66.00        787\n",
      "    I-Disease (label_id: 2)                                 75.29      71.01      73.09       1090\n",
      "    -------------------\n",
      "    micro avg                                               96.27      96.27      96.27      23969\n",
      "    macro avg                                               80.80      77.66      79.17      23969\n",
      "    weighted avg                                            96.13      96.27      96.19      23969\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2715: val_loss reached 0.13950 (best 0.13950), saving model to \"/NeMo/tutorials/nlp/nemo_experiments/token_classification_model/2022-01-13_03-44-15/checkpoints/token_classification_model--val_loss=0.1395-epoch=3.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0fa88a1a4343aca3ec43a042b7ceb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:51:11 token_classification_model:188] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         98.34      98.65      98.50      22092\n",
      "    B-Disease (label_id: 1)                                 66.55      71.79      69.07        787\n",
      "    I-Disease (label_id: 2)                                 77.87      68.44      72.85       1090\n",
      "    -------------------\n",
      "    micro avg                                               96.40      96.40      96.40      23969\n",
      "    macro avg                                               80.92      79.63      80.14      23969\n",
      "    weighted avg                                            96.36      96.40      96.36      23969\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 3394: val_loss reached 0.15355 (best 0.13950), saving model to \"/NeMo/tutorials/nlp/nemo_experiments/token_classification_model/2022-01-13_03-44-15/checkpoints/token_classification_model--val_loss=0.1536-epoch=4.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# start model training\n",
    "trainer.fit(model_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-michael",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "To see how the model performs, we can run generate prediction similar to the way we did it earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "classical-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first create a subset of our dev data\n",
    "! head -n 100 $NER_DATA_DIR/text_dev.txt > $NER_DATA_DIR/sample_text_dev.txt\n",
    "! head -n 100 $NER_DATA_DIR/labels_dev.txt > $NER_DATA_DIR/sample_labels_dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-ranking",
   "metadata": {},
   "source": [
    "Now, let's generate predictions for the provided text file.\n",
    "If labels file is also specified, the model will evaluate the predictions and plot confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "twenty-abortion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:54:25 token_classification_dataset:121] Setting Max Seq length to: 81\n",
      "[NeMo I 2022-01-13 03:54:25 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-01-13 03:54:25 data_preprocessing:360] Min: 4 |                  Max: 81 |                  Mean: 36.03 |                  Median: 33.0\n",
      "[NeMo I 2022-01-13 03:54:25 data_preprocessing:366] 75 percentile: 47.00\n",
      "[NeMo I 2022-01-13 03:54:25 data_preprocessing:367] 99 percentile: 68.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-13 03:54:25 token_classification_dataset:150] 0 are longer than 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-13 03:54:25 token_classification_dataset:153] *** Example ***\n",
      "[NeMo I 2022-01-13 03:54:25 token_classification_dataset:154] i: 0\n",
      "[NeMo I 2022-01-13 03:54:25 token_classification_dataset:155] subtokens: [CLS] BR ##CA ##1 is secret ##ed and exhibits properties of a g ##rani ##n . [SEP]\n",
      "[NeMo I 2022-01-13 03:54:25 token_classification_dataset:156] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-01-13 03:54:25 token_classification_dataset:157] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-01-13 03:54:25 token_classification_dataset:158] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Half but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3624644/685687628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_ner.evaluate_from_file(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtext_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNER_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_text_dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabels_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNER_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_labels_dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0madd_confusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NeMo/nemo/collections/nlp/models/token_classification/token_classification_model.py\u001b[0m in \u001b[0;36mevaluate_from_file\u001b[0;34m(self, output_dir, text_file, labels_file, add_confusion_matrix, normalize_confusion_matrix, batch_size)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0mwith_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NeMo/nemo/collections/nlp/models/token_classification/token_classification_model.py\u001b[0m in \u001b[0;36m_infer\u001b[0;34m(self, queries, batch_size)\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtokens_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 logits = self.forward(\n\u001b[0m\u001b[1;32m    374\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NeMo/nemo/core/classes/common.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;31m# Call the method - this can be forward, or any other callable method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         instance._attach_and_validate_output_types(\n",
      "\u001b[0;32m/NeMo/nemo/collections/nlp/models/token_classification/token_classification_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnemo_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokentype_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             hidden_states = self.bert_model(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NeMo/nemo/collections/nlp/models/language_modeling/megatron_bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, attention_mask, tokentype_ids, lm_labels)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokentype_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokentype_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokentype_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NeMo/nemo/collections/nlp/models/language_modeling/megatron/bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, bert_model_input, attention_mask, tokentype_ids, lm_labels)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_position_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mlm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokentype_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokentype_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_binary_head\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NeMo/nemo/collections/nlp/modules/common/megatron/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_input_ids, enc_position_ids, enc_attn_mask, prompt_tags, dec_input_ids, dec_position_ids, dec_attn_mask, enc_dec_attn_mask, tokentype_ids, layer_past, get_key_value, pooling_sequence_index, enc_hidden_states, output_enc_hidden_only)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;31m# encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menc_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m             encoder_output = self.encoder(\n\u001b[0m\u001b[1;32m    697\u001b[0m                 \u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NeMo/nemo/collections/nlp/modules/common/megatron/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, get_key_value, encoder_output, enc_dec_attn_mask)\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# Reverting data format change [s b h] --> [b s h].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/apex/normalization/fused_layer_norm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmixed_dtype_fused_layer_norm_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/apex/normalization/fused_layer_norm.py\u001b[0m in \u001b[0;36mmixed_dtype_fused_layer_norm_affine\u001b[0;34m(input, weight, bias, normalized_shape, eps)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cast_if_autocast_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFusedLayerNormAffineMixedDtypesFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/apex/normalization/fused_layer_norm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, weight, bias, normalized_shape, eps)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mweight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mbias_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         output, mean, invvar = fused_layer_norm_cuda.forward_affine_mixed_dtypes(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Half but found Float"
     ]
    }
   ],
   "source": [
    "model_ner.evaluate_from_file(\n",
    "    text_file=os.path.join(NER_DATA_DIR, 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(NER_DATA_DIR, 'sample_labels_dev.txt'),\n",
    "    output_dir=exp_dir,\n",
    "    add_confusion_matrix=False,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")\n",
    "# Please check matplotlib version if encountering any error plotting confusion matrix:\n",
    "# https://stackoverflow.com/questions/63212347/importerror-cannot-import-name-png-from-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-typing",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "If you have NeMo installed locally, you can also train the model with `nlp/token_classification/token_classification_train.py.`\n",
    "\n",
    "To run training script, use:\n",
    "\n",
    "`python token_classification_train.py model.dataset.data_dir=PATH_TO_DATA_DIR PRETRAINED_BERT_MODEL=biomegatron-bert-345m-cased`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-electric",
   "metadata": {},
   "source": [
    "The training could take several minutes and the result should look something like\n",
    "```\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:82] Accuracy: 0.9882348032875798\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 weighted: 98.82\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 macro: 93.74\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 micro: 98.82\n",
    "[NeMo I 2020-05-22 17:13:49 token_classification_callback:89] precision    recall  f1-score   support\n",
    "    \n",
    "    O (label id: 0)     0.9938    0.9957    0.9947     22092\n",
    "    B (label id: 1)     0.8843    0.9034    0.8938       787\n",
    "    I (label id: 2)     0.9505    0.8982    0.9236      1090\n",
    "    \n",
    "           accuracy                         0.9882     23969\n",
    "          macro avg     0.9429    0.9324    0.9374     23969\n",
    "       weighted avg     0.9882    0.9882    0.9882     23969\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
