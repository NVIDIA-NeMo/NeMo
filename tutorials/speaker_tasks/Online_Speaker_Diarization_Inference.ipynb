{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4238d8",
   "metadata": {},
   "source": [
    "# Streaming multispeaker ASR and diarization tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa603a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "if False: \n",
    "    ## Install dependencies\n",
    "    !pip install wget\n",
    "    !apt-get install sox libsndfile1 ffmpeg\n",
    "    !pip install text-unidecode\n",
    "\n",
    "    # ## Install NeMo\n",
    "    BRANCH = 'main'\n",
    "    !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
    "\n",
    "    ## Install TorchAudio\n",
    "    !pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f27b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.insert(0,f'/home/taejinp/projects/online_diar/NeMo/')\n",
    "import nemo\n",
    "print(\"Nemo PATH:\", nemo.__path__)\n",
    "BRANCH = 'streaming_mulspk_asr'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be14c5",
   "metadata": {},
   "source": [
    "# Online Speaker Diarization\n",
    "\n",
    "Speaker diarization is the process of determining \"who spoke when\" in a given audio clip. Depending on the method of processing, speaker diarization can be categorized into two types:\n",
    "\n",
    "- **Offline Speaker Diarization**: This method assumes access to the entire audio clip. The transcription, indicating which speaker spoke at which time, is provided after processing the audio from start to end.\n",
    "\n",
    "- **Online Speaker Diarization**: In this approach, the system only gradually gains access to short segments of the audio, typically a few seconds long. The transcription is generated and displayed in real-time as the segmented audio is being processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import socket\n",
    "if socket.gethostname() == \"aiapps-06052021\":\n",
    "    sys.path.insert(0,'/home/taejinp/projects/streaming_mulspk_asr/NeMo')\n",
    "else:\n",
    "    sys.path.insert(0,'/your/path/to/NeMo/')\n",
    "    \n",
    "import nemo\n",
    "print(\"Using Nemo PATH:\", nemo.__path__[0])\n",
    "\n",
    "# !pip install gradio==2.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc838c",
   "metadata": {},
   "source": [
    "# Introduction to Online Speaker Diarization\n",
    "\n",
    "As covered in Speaker diarization inference tutorial, speaker diarization is the task of segmenting audio recordings by speaker labels and answers the question \"Who Speaks When?\".\n",
    "\n",
    "While offline speaker diarization has access to the entire audio file and return the speaker labels all at once, online speaker diarization is a streaming task that processes audio in small chunks. \n",
    "Since we only have access to a small chunk of audio at a time, the online speaker diarization system needs to maintain a memory buffer to store the history of the speakers in the past. At the sametime, the system needs to be able to detect new speakers that are not in the memory buffer.\n",
    "\n",
    "This tutorial will cover the followings:\n",
    "\n",
    "- How to run online speaker diarization with NeMo\n",
    "- How online speaker clustering and memory buffer works together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb24d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.speaker_utils import audio_rttm_map\n",
    "from nemo.core.config import hydra_runner\n",
    "import gradio as gr\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5cb01",
   "metadata": {},
   "source": [
    "Read yaml file for online diarization. You have to specifty the following items:\n",
    "    \n",
    "- input manifest file (If  simulation)\n",
    "- VAD model path\n",
    "- Speaker embedding extractor model path\n",
    "- Diarization Decoder model path (Coming soon)\n",
    "- Punctuation model path (automatically download from NGC)\n",
    "- Language model path (Coming soon)\n",
    "\n",
    "Download nemo models and specify the path to config struct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da444302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "\n",
    "YAML_FILE=\"/home/taejinp/projects/streaming_mulspk_asr/NeMo/examples/speaker_tasks/diarization/conf/inference/online_diar_infer_general.yaml\"\n",
    "cfg = omegaconf.OmegaConf.load(YAML_FILE)\n",
    "import socket\n",
    "\n",
    "cfg.diarizer.out_dir = \"./streaming_diar_output\"\n",
    "\n",
    "os.makedirs(cfg.diarizer.out_dir, exist_ok=True)\n",
    "cfg.diarizer.asr.parameters.colored_text = False\n",
    "print(f\"socket.gethostname() {socket.gethostname()}\")\n",
    "if socket.gethostname() == \"aiapps-06052021\":\n",
    "    cfg.diarizer.manifest_filepath = \"/home/taejinp/projects/data/diar_manifest_input/online_diar_demo_01.json\"\n",
    "    cfg.diarizer.vad.model_path = \"/home/taejinp/gdrive/model/VAD_models/mVAD_lin_marblenet-3x2x64-4N-256bs-50e-0.01lr-0.001wd.nemo\"\n",
    "    cfg.diarizer.speaker_embeddings.model_path = \"/home/taejinp/Downloads/titanet_target_fixed/titanet-l.nemo\"\n",
    "    cfg.diarizer.asr.model_path = \"/home/taejinp/gdrive/model/ASR_models/Conformer-CTC-BPE_large_Riva_ASR_set_3.0_ep60.nemo\"\n",
    "\n",
    "else:\n",
    "    # Please download the following models and run the code. \n",
    "    # Download CH109 dataset at: https://drive.google.com/drive/folders/1ksq10H-NZbKRfMjEP_WWyBF_G0iAJt6b?usp=sharing\n",
    "    cfg.diarizer.manifest_filepath = \"/your/path/to/ch109.json\"\n",
    "    # Download streaming VAD model at: https://drive.google.com/file/d/1ab42CaYeTkuJSMsMsMLbSS9m5e1isJzx/view?usp=sharing\n",
    "    cfg.diarizer.vad.model_path = \"/your/path/to/mVAD_lin_marblenet-3x2x64-4N-256bs-50e-0.01lr-0.001wd.nemo\"\n",
    "    # Download titanet-m model at: https://drive.google.com/file/d/1xAgjm0udKogPrlQF6cdHLobEKHLY9azA/view?usp=sharing\n",
    "    cfg.diarizer.speaker_embeddings.model_path = \"/your/path/to/titanet-m.nemo\"\n",
    "    # Download Conformer-CTC ASR model at: https://drive.google.com/file/d/1Xg075IbiwL0szI4_a8gYmCPaG1UsgR6E/view?usp=sharing\n",
    "    cfg.diarizer.asr.model_path = \"/your/path/to/Conformer-CTC-BPE_large_Riva_ASR_set_3.0_ep60.nemo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa1248",
   "metadata": {},
   "source": [
    "Now, let's setup the parameters for online diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fa81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import OnlineClusteringDiarizer\n",
    "import os\n",
    "\n",
    "params = {}\n",
    "params['use_cuda'] = True\n",
    "AUDIO_RTTM_MAP = audio_rttm_map(cfg.diarizer.manifest_filepath)\n",
    "\n",
    "diar = OnlineClusteringDiarizer(cfg)\n",
    "from nemo.collections.asr.parts.utils.diarization_utils import OnlineDiarWithASR, write_txt\n",
    "\n",
    "cfg.diarizer.simulation_uniq_id='citadel_ken'\n",
    "cfg.diarizer.out_dir = '/home/taejinp/projects/run_time/streaming_diar_output_univ'\n",
    "cfg.diarizer.asr.parameters.streaming_simulation=True\n",
    "cfg.diarizer.asr.parameters.enforce_real_time=True \n",
    "cfg.diarizer.asr.parameters.colored_text=False\n",
    " \n",
    "fn = os.path.join(cfg.diarizer.out_dir, \"print_script.sh\")\n",
    "\n",
    "diar.uniq_id = cfg.diarizer.simulation_uniq_id \n",
    "diar.single_audio_file_path = AUDIO_RTTM_MAP[diar.uniq_id]['audio_filepath']\n",
    "diar.rttm_file_path = AUDIO_RTTM_MAP[diar.uniq_id]['rttm_filepath']\n",
    "diar._init_segment_variables()\n",
    "\n",
    "\n",
    "online_diar_asr = OnlineDiarWithASR(cfg=cfg)\n",
    "diar = online_diar_asr.diar\n",
    "\n",
    "diar.device = online_diar_asr.device\n",
    "online_diar_asr.reset()\n",
    "\n",
    "cfg.diarizer.asr.parameters.streaming_simulation=True\n",
    "# cfg.diarizer.asr.parameters.streaming_simulation=False\n",
    "\n",
    "if not cfg.diarizer.asr.parameters.streaming_simulation:\n",
    "    cfg.diarizer.asr.parameters.enforce_real_time=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963b9e6",
   "metadata": {},
   "source": [
    "We can run simulated audio stream to check if streaming system is working properly. After you initiate the following function and while the function is running, you can check the transcription is being generated in realtime.  The path is ./streaming_diar_output/print_script.sh, and this can be viewed using \"streaming_diarization_viewer.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time\n",
    "box_layout = ipywidgets.Layout(height=\"500px\", width=\"90%\")\n",
    "widget = ipywidgets.Textarea(value='', disabled=True, layout=box_layout)\n",
    "display(widget)  # display widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diar.uniq_id = cfg.diarizer.simulation_uniq_id\n",
    "online_diar_asr.get_audio_rttm_map(diar.uniq_id)\n",
    "diar.single_audio_file_path = diar.AUDIO_RTTM_MAP[diar.uniq_id]['audio_filepath']\n",
    "online_diar_asr.rttm_file_path = diar.AUDIO_RTTM_MAP[diar.uniq_id]['rttm_filepath']\n",
    "\n",
    "diar._init_segment_variables()\n",
    "diar.device = online_diar_asr.device\n",
    "write_txt(f\"{diar._out_dir}/print_script.sh\", \"\")\n",
    "\n",
    "if cfg.diarizer.asr.parameters.streaming_simulation == True:\n",
    "    samplerate, sdata = wavfile.read(diar.single_audio_file_path)\n",
    "    if  diar.AUDIO_RTTM_MAP[diar.uniq_id]['offset'] and diar.AUDIO_RTTM_MAP[diar.uniq_id]['duration']:\n",
    "        \n",
    "        offset = samplerate*diar.AUDIO_RTTM_MAP[diar.uniq_id]['offset']\n",
    "        duration = samplerate*diar.AUDIO_RTTM_MAP[diar.uniq_id]['duration']\n",
    "        stt, end = int(offset), int(offset + duration)\n",
    "        sdata = sdata[stt:end]\n",
    "\n",
    "    for index in range(int(np.floor(sdata.shape[0]/online_diar_asr.n_frame_len))):\n",
    "        shift = online_diar_asr.CHUNK_SIZE\n",
    "        sample_audio = sdata[shift*index:shift*(index+1)]\n",
    "        online_diar_asr.buffer_counter = index\n",
    "        online_diar_asr.streaming_step(sample_audio)\n",
    "        \n",
    "        widget.value += f\" update {index}\"\n",
    "        fp = open(f'{diar._out_dir}/print_script.sh','r').read()\n",
    "        widget.value = fp\n",
    "else:\n",
    "    isTorch = torch.cuda.is_available()\n",
    "    iface = gr.Interface(\n",
    "    fn=online_diar_asr.audio_queue_launcher,\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"microphone\", type=\"numpy\", streaming=True), \n",
    "        \"state\",\n",
    "    ],\n",
    "    outputs=[\n",
    "        \"textbox\",\n",
    "        \"state\",\n",
    "    ],\n",
    "    layout=\"horizontal\",\n",
    "    theme=\"huggingface\",\n",
    "    title=f\"NeMo Streaming Conformer CTC Large - English, CUDA:{isTorch}\",\n",
    "    description=\"Demo for English speech recognition using Conformer Transducers\",\n",
    "    allow_flagging='never',\n",
    "    live=True,\n",
    "    )\n",
    "    iface.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da436545",
   "metadata": {},
   "source": [
    "Now, go to streaming_diarization_viewer.ipynb and check the realtime output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a145dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    fp = open(f'{diar._out_dir}/print_script.sh','r').read()\n",
    "    widget.value = fp\n",
    "    time.sleep(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a1c71140709e1092172cda15023df22961677c096d9c036172fd9c04bdbca4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
