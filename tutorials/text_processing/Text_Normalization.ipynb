{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5fA5qAm5Afg"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "<img src=\"https://raw.githubusercontent.com/NVIDIA/NeMo/tn_tutorial/tutorials/text_processing/images/task_overview.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-IrnmXMTevr"
   },
   "source": [
    "\n",
    "**Text normalization (TN)** is the task of converting text in canonical written form to it's verbalized form. For example, *10:00* -> *ten o'clock* or *10kg* -> *ten kilograms*.\n",
    "\n",
    "**Inverse text normalization (ITN)** does the reverse and converts normalized text back written form. For example, *in nineteen seventy five* -> *in 1975* and *one hundred and twenty three dollars* -> *$123*.\n",
    "\n",
    "A sentence can be split up into semiotic tokens stemming from a varity of classes, where the spoken form differs from the written form. Examples are *dates*, *decimals*, *cardinals*, *measures* etc. The good TN or ITN system will be able to handle a variety of **semiotic classes**.\n",
    "\n",
    "TN is used to in the pre-processing of Text-To-Speech (TTS) systems, whereas ITN is used to post-process Automatic Speech Recognition (ASR) outputs. Audio-based TN can be used to normalize ASR training data for better ASR accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXRARM8XtK_g"
   },
   "source": [
    "# NeMo package overview\n",
    "\n",
    "`nemo_text_processing` is a Python package automatically installed with [`NeMo`](https://github.com/NVIDIA/NeMo). `nemo_text_processing` supports \n",
    "- TN\n",
    "- audio-based TN\n",
    "- ITN\n",
    "\n",
    "The toolkit is based on weighted finite-state\n",
    "transducer (WFST) grammars. The tools uses [`Pynini`](https://www.openfst.org/twiki/bin/view/GRM/PyniniDocs) to construct WFSTs. \n",
    "\n",
    "`nemo_text_processing` supports a wide range of grammars across of number of languages. TODO: reference to docs\n",
    "\n",
    "The toolkit is modular and easily extendable. A tutorial for system details and customization can be found in the [WFST tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/text_processing/WFST_Tutorial.ipynb). The Python environment allows integration into an existing Python application. \n",
    "\n",
    "A sentence can be split up into semiotic tokens stemming from a varity of classes, where the spoken form differs from the written form. Examples are *dates*, *decimals*, *cardinals*, *measures* etc. \n",
    "\n",
    "The Python system can be seamlessly deployment into C++. The pipeline is shown below.\n",
    "The WFST-based grammars can be exported into an [OpenFST](https://www.openfst.org/) Archive File (FAR) and dropped into [`Sparrowhawk`](https://github.com/google/sparrowhawk)  -- an open-source version of [Kestrel TTS text normalization system](https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/kestrel-tts-text-normalization-system/F0C18A3F596B75D83B75C479E23795DA). As an example, `nemo_text_processing` is used in [NVIDIA RIVA](https://www.nvidia.com/en-us/ai-data-science/products/riva-enterprise/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NVIDIA/NeMo/tn_tutorial/tutorials/text_processing/images/deployment_pipeline.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IT1Xr9iW2Xr"
   },
   "source": [
    "# How to use\n",
    "### 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install NeMo, which installs both nemo and nemo_text_processing package\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nemo_text_processing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import of nemo_text_processing an other dependencies\n",
    "import nemo_text_processing\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bfs7fa9lXDDh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-28 20:41:26 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-28 20:41:26 tokenize_and_classify:92] Creating ClassifyFst grammars.\n"
     ]
    }
   ],
   "source": [
    "# create text normalization instance that works on cased input\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "normalizer = Normalizer(input_case='cased', lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Run TN on input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens { name: \"We\" } tokens { name: \"paid\" } tokens { money { currency_maj: \"dollars\" integer_part: \"one hundred and twenty three\" } } tokens { name: \"for\" } tokens { name: \"this\" } tokens { name: \"desk\" }  tokens { name: \".\" }\n",
      "We paid one hundred and twenty three dollars for this desk.\n"
     ]
    }
   ],
   "source": [
    "# run normalization on example string input\n",
    "written = \"We paid $123 for this desk.\"\n",
    "normalized = normalizer.normalize(written, verbose=True, punct_post_process=True)\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intermediate semtiotic class information is shown if verbose=True\n",
    "#### 2.1 Run TN on input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UD-OuFmEOX3T"
   },
   "outputs": [],
   "source": [
    "# create temporary data folder and example input file\n",
    "DATA_DIR = 'tmp_data_dir'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "INPUT_FILE = f'{DATA_DIR}/inference.txt'\n",
    "! echo -e 'The alarm went off at 10:00a.m. \\nI received $123' > $INPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d4T0gXHwY3JZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alarm went off at 10:00a.m. \r\n",
      "I received $123\r\n"
     ]
    }
   ],
   "source": [
    "# check input file was properly created\n",
    "! cat $INPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The alarm went off at 10:00a.m.', 'I received $123']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load input file into 'data' - a list of strings\n",
    "data = []\n",
    "with open(INPUT_FILE, 'r') as fp:\n",
    "    for line in fp:\n",
    "        data.append(line.strip())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F5wSJTI8ZFRg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The alarm went off at ten AM',\n",
       " 'I received one hundred and twenty three dollars']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run normalization on 'data'\n",
    "normalizer.normalize_list(data, punct_post_process=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMT5lkPYzZHK"
   },
   "source": [
    "#### 2.2 Evaluate TN on written-normalized text pairs \n",
    "\n",
    "The evaluation data needs to have the following format:\n",
    "\n",
    "'on 22 july 2022 they worked until 12:00' and the normalization is represented as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAIN\ton\t<self>\r\n",
      "DATE\t22 july 2012\tthe twenty second of july twenty twelve\r\n",
      "PLAIN\tthey\t<self>\r\n",
      "PLAIN\tworked\t<self>\r\n",
      "PLAIN\tuntil\t<self>\r\n",
      "TIME\t12:00\ttwelve o'clock\r\n",
      "<eos>\t<eos>\r\n"
     ]
    }
   ],
   "source": [
    "# example evaluation sentence\n",
    "eval_text =  \"\"\"PLAIN\\ton\\t<self>\n",
    "DATE\\t22 july 2012\\tthe twenty second of july twenty twelve\n",
    "PLAIN\\tthey\\t<self>\n",
    "PLAIN\\tworked\\t<self>\n",
    "PLAIN\\tuntil\\t<self>\n",
    "TIME\\t12:00\\ttwelve o'clock\n",
    "<eos>\\t<eos>\n",
    "\"\"\"\n",
    "EVAL_FILE = f'{DATA_DIR}/eval.txt'\n",
    "with open(EVAL_FILE, 'w') as fp:\n",
    "    fp.write(eval_text)\n",
    "! cat $EVAL_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMT5lkPYzZHK"
   },
   "source": [
    "That is, every sentence is broken into semiotic tokens line by line and concluded by end of sentence token `<eos>`. In case of a plain token it's `[SEMIOTIC CLASS] [TAB] [WRITTEN] [TAB] <self>`, otherwise `[SEMIOTIC CLASS] [TAB] [WRITTEN] [TAB] [NORMALIZED]`.\n",
    "This format was introduced in [Google Text normalization dataset](https://arxiv.org/abs/1611.00068). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('on 22 july 2012 they worked until 12:00', \"on the twenty second of july twenty twelve they worked until twelve o'clock\")]\n"
     ]
    }
   ],
   "source": [
    "# Parse evaluation file into written and normalized sentence pairs\n",
    "from nemo_text_processing.text_normalization.data_loader_utils import load_files, training_data_to_sentences\n",
    "eval_data = load_files([EVAL_FILE])\n",
    "sentences_un_normalized, sentences_normalized, sentences_class_types = training_data_to_sentences(eval_data)\n",
    "print(list(zip(sentences_un_normalized, sentences_normalized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"on the twenty second of july twenty twelve they worked until twelve o'clock\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run prediction\n",
    "sentences_prediction = normalizer.normalize_list(sentences_un_normalized)\n",
    "print(sentences_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# measure sentence accuracy\n",
    "from nemo_text_processing.text_normalization.data_loader_utils import evaluate\n",
    "sentences_accuracy = evaluate(\n",
    "            preds=sentences_prediction, labels=sentences_normalized, input=sentences_un_normalized\n",
    "        )\n",
    "print(\"- Accuracy: \" + str(sentences_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4zjeVVv-UXR"
   },
   "source": [
    "You can also break down evaluation accuracy by semiotic class, for that use the script [`NeMo/nemo_text_processing/text_normalization/run_evaluate.py`](https://github.com/NVIDIA/NeMo/blob/main/nemo_text_processing/text_normalization/run_evaluate.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inverse Text Normalization\n",
    "ITN supports equivalent API as TN. Here we are only going to show inverse normalization on input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-04-28 20:41:45 tokenize_and_classify:70] Creating ClassifyFst grammars.\n"
     ]
    }
   ],
   "source": [
    "# create inverse text normalization instance\n",
    "from nemo_text_processing.inverse_text_normalization.inverse_normalize import InverseNormalizer\n",
    "inverse_normalizer = InverseNormalizer(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens { name: \"we\" } tokens { name: \"paid\" } tokens { money { integer_part: \"123\" currency: \"$\" } } tokens { name: \"for\" } tokens { name: \"this\" } tokens { name: \"desk\" }\n",
      "we paid $123 for this desk\n"
     ]
    }
   ],
   "source": [
    "# run ITN on example string input\n",
    "spoken = \"we paid one hundred twenty three dollars for this desk\"\n",
    "un_normalized = inverse_normalizer.inverse_normalize(spoken, verbose=True)\n",
    "print(un_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Audio-based Text Normalization\n",
    "Audio-based text normalization uses extended WFST grammars to provide a range of possible normalization options.\n",
    "The following example shows the workflow: (Disclaimer: exact values in graphic do not need to be real system's behavior)\n",
    "1. text \"627\" is sent to extended TN WFST grammar\n",
    "2. grammar output 5 different options of verbalization based on text input alone\n",
    "3. in case an audio file is presented we compare the audio transcript with the verbalization options to find out which normalization is correct based on character error rate. The transcript is generated using a pretrained NeMo ASR model. \n",
    "\n",
    "More information can be found at https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/wfst_text_normalization.html#audio-based-text-normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NVIDIA/NeMo/tn_tutorial/tutorials/text_processing/images/audio_based_tn.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows an example of how to generate multiple normalization options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import non-deterministic WFST-based TN module\n",
    "from nemo_text_processing.text_normalization.normalize_with_audio import NormalizerWithAudio\n",
    "\n",
    "# initialize normalizer\n",
    "normalizer = NormalizerWithAudio(\n",
    "        lang=\"en\",\n",
    "        input_case=\"cased\",\n",
    "        overwrite_cache=False,\n",
    "        cache_dir=\"cache_dir\",\n",
    "    )\n",
    "# create up to 10 normalization options\n",
    "print(normalizer.normalize(\"123\", n_tagged=10, punct_post_process=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L85ZaUJ_4TkF"
   },
   "source": [
    "# C++ deployment\n",
    "\n",
    "Sparrowhawk is based on C++ which operates similar to NeMo's Python TN and ITN.\n",
    "To deploy the grammars you need:\n",
    "- [Docker](https://www.docker.com/) \n",
    "- download [NeMo source code](https://github.com/NVIDIA/NeMo) which includes grammars\n",
    "\n",
    "Run [NeMo/tools/text_processing_deployment/export_grammars.sh](https://github.com/NVIDIA/NeMo/blob/main/tools/text_processing_deployment/export_grammars.sh).\n",
    "\n",
    "More details can be found under https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/wfst_text_processing_deployment.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENMDNl9C4TkF"
   },
   "source": [
    "# Tutorial on how to customize grammars\n",
    "\n",
    "https://github.com/NVIDIA/NeMo/blob/main/tutorials/text_processing/WFST_Tutorial.ipynb\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/intro.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcvT3P2lQ_GS"
   },
   "source": [
    "# References and Further Reading:\n",
    "\n",
    "\n",
    "- [Zhang, Yang, Bakhturina, Evelina, Gorman, Kyle and Ginsburg, Boris. \"NeMo Inverse Text Normalization: From Development To Production.\" (2021)](https://arxiv.org/abs/2104.05055)\n",
    "- [Ebden, Peter, and Richard Sproat. \"The Kestrel TTS text normalization system.\" Natural Language Engineering 21.3 (2015): 333.](https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/kestrel-tts-text-normalization-system/F0C18A3F596B75D83B75C479E23795DA)\n",
    "- [Gorman, Kyle. \"Pynini: A Python library for weighted finite-state grammar compilation.\" Proceedings of the SIGFSM Workshop on Statistical NLP and Weighted Automata. 2016.](https://www.aclweb.org/anthology/W16-2409.pdf)\n",
    "- [Mohri, Mehryar, Fernando Pereira, and Michael Riley. \"Weighted finite-state transducers in speech recognition.\" Computer Speech & Language 16.1 (2002): 69-88.](https://cs.nyu.edu/~mohri/postscript/csl01.pdf)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lcvT3P2lQ_GS"
   ],
   "name": "Text_Normalization_Tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
