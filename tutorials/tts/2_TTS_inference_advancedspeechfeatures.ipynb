{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS Inference\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    ">\n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    ">\n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    ">\n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# # If you're using Google Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode\n",
    "# BRANCH = 'main'\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[tts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please run the below cell to setup this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Reduce logging messages for this notebook\n",
    "from nemo.utils import logging\n",
    "logging.setLevel(logging.ERROR)\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.helpers.helpers import regulate_len\n",
    "\n",
    "# Load the models from NGC\n",
    "fastpitch = FastPitchModel.from_pretrained(\"tts_en_fastpitch\").eval().cuda()\n",
    "hifigan = HifiGanModel.from_pretrained(\"tts_hifigan\").eval().cuda()\n",
    "sr = 22050\n",
    "\n",
    "def display_pitch(audio, pitch, sr=22050):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    spec = np.abs(librosa.stft(audio[0], n_fft=1024))\n",
    "    ax.plot(pitch.cpu().numpy()[0], color='cyan', linewidth=1)\n",
    "    librosa.display.specshow(np.log(spec+1e-12), y_axis='log')\n",
    "    ipd.display(ipd.Audio(audio, rate=sr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration Control\n",
    "\n",
    "This section is applicable to models that use a duration predictor module. This module is called the Length Regulator and was introduce in FastSpeech [1]. A list of NeMo models that support duration predictors are as follows:\n",
    "\n",
    "- [FastPitch](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/models/fastpitch.py)\n",
    "- [FastPitch_HifiGan_E2E](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_e2e_fastpitchhifigan)\n",
    "- [FastSpeech2](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_fastspeech_2)\n",
    "- [FastSpeech2_HifiGan_E2E](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_e2e_fastspeech2hifigan)\n",
    "- [TalkNet](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_talknet)\n",
    "- [Glow-TTS](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_glowtts)\n",
    "\n",
    "While each model has their own implementation of this duration predictor, all of them follow a simple convolutional architecture. The input is the encoded tokens, and the output of the module is a value that represents how many frames in the decoder correspond to each token. It is essentially a hard attention mechanism.\n",
    "\n",
    "Since each model outputs a duration value per token, it is simple to slow down or increase the speech rate by increasing or decreasing these values. Consider the following:\n",
    "\n",
    "```python\n",
    "def regulate_len(durations, pace=1.0):\n",
    "    durations = durations.float() / pace\n",
    "    # The output from the duration predictor module is still a float\n",
    "    # If we want the speech to be faster, we can increase the pace and make each token duration shorter\n",
    "    # Alternatively we can slow down the pace by decreasing the pace parameter\n",
    "    return durations.long()  # Lastly, we need to make the durations integers for subsequent processes\n",
    "```\n",
    "\n",
    "Let's try this out with FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define what we want the model to say\n",
    "input_string = \"Hey, I am speaking at different paces!\"  # Feel free to change it and experiment\n",
    "\n",
    "# Define a quick helper function to go from string to audio\n",
    "def str_to_audio(inp, pace=1.0):\n",
    "    with torch.no_grad():\n",
    "        tokens = fastpitch.parse(inp)\n",
    "        spec = fastpitch.generate_spectrogram(tokens=tokens, pace=pace)\n",
    "        audio = hifigan.convert_spectrogram_to_audio(spec=spec).to('cpu').numpy()\n",
    "    return audio\n",
    "\n",
    "# Let's run fastpitch normally\n",
    "audio = str_to_audio(input_string)\n",
    "print(f\"This is fastpitch speaking at the regular pace of 1.0. This example is {len(audio[0])/sr:.3f} seconds long.\")\n",
    "ipd.display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "# We can speed up the speech by adjusting the pace\n",
    "audio = str_to_audio(input_string, pace=1.2)\n",
    "print(f\"This is fastpitch speaking at the faster pace of 1.2. This example is {len(audio[0])/sr:.3f} seconds long.\")\n",
    "ipd.display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "# We can slow down the speech  by adjusting the pace\n",
    "audio = str_to_audio(input_string, pace=0.75)\n",
    "print(f\"This is fastpitch speaking at the faster pace of 0.75. This example is {len(audio[0])/sr:.3f} seconds long.\")\n",
    "ipd.display(ipd.Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch Control\n",
    "\n",
    "The newer spectrogram generator models predict the pitch for certain words. Since these models predict pitch, we can adjust the predicted pitch in a similar manner to the predicted durations like in the previous section. A list of NeMo models that support pitch control are as follows:\n",
    "\n",
    "- [FastPitch](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/models/fastpitch.py)\n",
    "- [FastPitch_HifiGan_E2E](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_e2e_fastpitchhifigan)\n",
    "- [FastSpeech2](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_fastspeech_2)\n",
    "- [FastSpeech2_HifiGan_E2E](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_e2e_fastspeech2hifigan)\n",
    "- [TalkNet](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_talknet)\n",
    "\n",
    "### FastPitch\n",
    "\n",
    "As with the previous tutorial, we will focus on FastPitch. FastPitch differs from some other models as it predicts a pitch difference to a normalized (mean 0, std 1) speaker pitch. Other models will just predict the unnormalized pitch. Looking at a simplified version of the FastPitch model, we see\n",
    "\n",
    "```python\n",
    "# Predict pitch\n",
    "pitch_predicted = self.pitch_predictor(enc_out, enc_mask)  # Predicts a pitch that is normalized with speaker statistics \n",
    "pitch_emb = self.pitch_emb(pitch.unsqueeze(1))  # A simple 1D convolution to map the float pitch to a embedding pitch\n",
    "\n",
    "enc_out = enc_out + pitch_emb.transpose(1, 2)  # We add the pitch to the encoder output\n",
    "spec, *_ = self.decoder(input=len_regulated, seq_lens=dec_lens)  # We send the sum to the decoder to get the spectrogram\n",
    "```\n",
    "\n",
    "Let's see the `pitch_predicted` for a sample text. You can run the below cell. You should get an image that looks like for the input `Hey, what is my pitch?`:\n",
    "<img src=\"files/fastpitch-pitch.png\">\n",
    "Notice that the last word `pitch` has an increase in pitch to stress that it is a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from nemo.collections.tts.helpers.helpers import regulate_len\n",
    "%matplotlib inline\n",
    "\n",
    "#Define what we want the model to say\n",
    "input_string = \"Hey, what is my pitch?\"  # Feel free to change it and experiment\n",
    "\n",
    "# Run inference to get spectrogram and pitch\n",
    "with torch.no_grad():\n",
    "    tokens = fastpitch.parse(input_string)\n",
    "    spec, _, durs_predicted, _, pitch, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None)\n",
    "    audio = hifigan.convert_spectrogram_to_audio(spec=spec).to('cpu').numpy()\n",
    "    # FastPitch predicts one pitch value per token. To plot it, we have to expand the token length to the spectrogram length\n",
    "    pitch, _ = regulate_len(durs_predicted, pitch.unsqueeze(-1))\n",
    "    pitch = pitch.squeeze(-1)\n",
    "    # Note we have to unnormalize the pitch with LJSpeech's mean and std\n",
    "    pitch = pitch * 65.72037058703644 + 214.72202032404294\n",
    "\n",
    "# Let's plot the predicted pitch and how it affects the predicted audio\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "spec = np.abs(librosa.stft(audio[0], n_fft=1024))\n",
    "ax.plot(pitch.cpu().numpy()[0], color='cyan', linewidth=1)\n",
    "librosa.display.specshow(np.log(spec+1e-12), y_axis='log')\n",
    "ipd.display(ipd.Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Control\n",
    "\n",
    "Now that we see how the pitch affects the predicted spectrogram, we can now adjust it to add some effects. We will explore 4 different manipulations:\n",
    "\n",
    "1) Pitch shift\n",
    "\n",
    "2) Pitch flatten\n",
    "\n",
    "3) Pitch inversion\n",
    "\n",
    "4) Pitch amplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we look at pitch shift. To shift the pitch up or down by some Hz, we can just add or subtract as needed\n",
    "# Let's shift the pitch down by 50 Hz\n",
    "# First, let's run the previous example and then shift down\n",
    "\n",
    "#Define what we want the model to say\n",
    "input_string = \"Hey, what is my pitch?\"  # Feel free to change it and experiment\n",
    "\n",
    "# Run inference to get spectrogram and pitch\n",
    "with torch.no_grad():\n",
    "    tokens = fastpitch.parse(input_string)\n",
    "    spec_norm, _, durs_norm_pred, _, pitch, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None)\n",
    "    audio_norm = hifigan.convert_spectrogram_to_audio(spec=spec_norm).to('cpu').numpy()\n",
    "    \n",
    "    # Note we have to unnormalize the pitch with LJSpeech's mean and std\n",
    "    pitch = pitch * 65.72037058703644 + 214.72202032404294\n",
    "    pitch_norm = pitch\n",
    "    \n",
    "    # Now let's pitch shift down by 50Hz\n",
    "    pitch_shift = pitch - 50\n",
    "    pitch = (pitch_shift - 214.72202032404294) / 65.72037058703644\n",
    "    \n",
    "    # Now we can pass it to the model\n",
    "    spec_shift, _, durs_shift_pred, _, pitch, *_ = fastpitch(text=tokens, durs=None, pitch=pitch, speaker=None)\n",
    "    audio_shift = hifigan.convert_spectrogram_to_audio(spec=spec_shift).to('cpu').numpy()\n",
    "    \n",
    "    # FastPitch predicts one pitch value per token. To plot it, we have to expand the token length to the spectrogram length\n",
    "    pitch_shift, _ = regulate_len(durs_shift_pred, pitch_shift.unsqueeze(-1))\n",
    "    pitch_shift = pitch_shift.squeeze(-1)\n",
    "    pitch_norm, _ = regulate_len(durs_norm_pred, pitch_norm.unsqueeze(-1))\n",
    "    pitch_norm = pitch_norm.squeeze(-1)\n",
    "    \n",
    "\n",
    "# Let's see both results\n",
    "print(\"The first unshifted sample\")\n",
    "display_pitch(audio_norm, pitch_norm)\n",
    "\n",
    "print(\"The second shifted sample. This sample is much deeper than the previous.\")\n",
    "display_pitch(audio_shift, pitch_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second we look at pitch flattening. To flattern the pitch, we just set it to 0.\n",
    "# First, let's run the previous example and then compare it to flattening\n",
    "\n",
    "#Define what we want the model to say\n",
    "input_string = \"Hey, what is my pitch?\"  # Feel free to change it and experiment\n",
    "\n",
    "# Run inference to get spectrogram and pitch\n",
    "with torch.no_grad():\n",
    "    tokens = fastpitch.parse(input_string)\n",
    "    spec_norm, _, durs_norm_pred, _, pitch, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None)\n",
    "    audio_norm = hifigan.convert_spectrogram_to_audio(spec=spec_norm).to('cpu').numpy()\n",
    "    \n",
    "    # Note we have to unnormalize the pitch with LJSpeech's mean and std\n",
    "    pitch = pitch * 65.72037058703644 + 214.72202032404294\n",
    "    pitch_norm = pitch\n",
    "    \n",
    "    # Now let's set the pitch to 0\n",
    "    pitch_flat = pitch * 0\n",
    "    \n",
    "    # Now we can pass it to the model\n",
    "    spec_flat, _, durs_flat_pred, _, pitch, *_ = fastpitch(text=tokens, durs=None, pitch=pitch_flat, speaker=None)\n",
    "    audio_flat = hifigan.convert_spectrogram_to_audio(spec=spec_flat).to('cpu').numpy()\n",
    "    pitch_flat = pitch_flat + 214.72202032404294\n",
    "    \n",
    "    # FastPitch predicts one pitch value per token. To plot it, we have to expand the token length to the spectrogram length\n",
    "    pitch_flat, _ = regulate_len(durs_flat_pred, pitch_flat.unsqueeze(-1))\n",
    "    pitch_flat = pitch_flat.squeeze(-1)\n",
    "    pitch_norm, _ = regulate_len(durs_norm_pred, pitch_norm.unsqueeze(-1))\n",
    "    pitch_norm = pitch_norm.squeeze(-1)\n",
    "    \n",
    "\n",
    "# Let's see both results\n",
    "print(\"The first unaltered sample\")\n",
    "display_pitch(audio_norm, pitch_norm)\n",
    "\n",
    "print(\"The second flattened sample. This sample is more monotone than the previous.\")\n",
    "display_pitch(audio_flat, pitch_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third we look at pitch flattening. To invert the pitch, we just multiply it by -1.\n",
    "# First, let's run the previous example and then compare it to inversion\n",
    "\n",
    "#Define what we want the model to say\n",
    "input_string = \"Hey, what is my pitch?\"  # Feel free to change it and experiment\n",
    "\n",
    "# Run inference to get spectrogram and pitch\n",
    "with torch.no_grad():\n",
    "    tokens = fastpitch.parse(input_string)\n",
    "    spec_norm, _, durs_norm_pred, _, pitch, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None)\n",
    "    audio_norm = hifigan.convert_spectrogram_to_audio(spec=spec_norm).to('cpu').numpy()\n",
    "    \n",
    "    # Note we have to unnormalize the pitch with LJSpeech's mean and std\n",
    "    pitch_norm = pitch * 65.72037058703644 + 214.72202032404294\n",
    "    \n",
    "    # Now let's invert the pitch\n",
    "    pitch_invert = pitch * -1\n",
    "    \n",
    "    # Now we can pass it to the model\n",
    "    spec_inv, _, durs_inv_pred, _, pitch_inv, *_ = fastpitch(text=tokens, durs=None, pitch=pitch_invert, speaker=None)\n",
    "    audio_inv = hifigan.convert_spectrogram_to_audio(spec=spec_inv).to('cpu').numpy()\n",
    "    pitch_inv = pitch_invert * 65.72037058703644 + 214.72202032404294\n",
    "    \n",
    "    # FastPitch predicts one pitch value per token. To plot it, we have to expand the token length to the spectrogram length\n",
    "    pitch_inv, _ = regulate_len(durs_inv_pred, pitch_inv.unsqueeze(-1))\n",
    "    pitch_inv = pitch_inv.squeeze(-1)\n",
    "    pitch_norm, _ = regulate_len(durs_norm_pred, pitch_norm.unsqueeze(-1))\n",
    "    pitch_norm = pitch_norm.squeeze(-1)\n",
    "    \n",
    "\n",
    "# Let's see both results\n",
    "print(\"The first unaltered sample\")\n",
    "display_pitch(audio_norm, pitch_norm)\n",
    "\n",
    "print(\"The second inverted sample. This sample sounds less like a question and more like a statement.\")\n",
    "display_pitch(audio_inv, pitch_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, we look at pitch amplifying. To ameplify the pitch, we just multiply it by a positive constant.\n",
    "# First, let's run the previous example and then compare it to amplification.\n",
    "\n",
    "#Define what we want the model to say\n",
    "input_string = \"Hey, what is my pitch?\"  # Feel free to change it and experiment\n",
    "\n",
    "# Run inference to get spectrogram and pitch\n",
    "with torch.no_grad():\n",
    "    tokens = fastpitch.parse(input_string)\n",
    "    spec_norm, _, durs_norm_pred, _, pitch, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None)\n",
    "    audio_norm = hifigan.convert_spectrogram_to_audio(spec=spec_norm).to('cpu').numpy()\n",
    "    \n",
    "    # Note we have to unnormalize the pitch with LJSpeech's mean and std\n",
    "    pitch_norm = pitch * 65.72037058703644 + 214.72202032404294\n",
    "    \n",
    "    # Now let's amplify the pitch\n",
    "    pitch_amp = pitch * 1.5\n",
    "    \n",
    "    # Now we can pass it to the model\n",
    "    spec_amp, _, durs_amp_pred, _, _, *_ = fastpitch(text=tokens, durs=None, pitch=pitch_amp, speaker=None)\n",
    "    audio_amp = hifigan.convert_spectrogram_to_audio(spec=spec_amp).to('cpu').numpy()\n",
    "    pitch_amp = pitch_amp * 65.72037058703644 + 214.72202032404294\n",
    "    \n",
    "    # FastPitch predicts one pitch value per token. To plot it, we have to expand the token length to the spectrogram length\n",
    "    pitch_amp, _ = regulate_len(durs_amp_pred, pitch_amp.unsqueeze(-1))\n",
    "    pitch_amp = pitch_amp.squeeze(-1)\n",
    "    pitch_norm, _ = regulate_len(durs_norm_pred, pitch_norm.unsqueeze(-1))\n",
    "    pitch_norm = pitch_norm.squeeze(-1)\n",
    "    \n",
    "\n",
    "# Let's see both results\n",
    "print(\"The first unaltered sample\")\n",
    "display_pitch(audio_norm, pitch_norm)\n",
    "\n",
    "print(\"The second amplified sample.\")\n",
    "display_pitch(audio_amp, pitch_amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now that we understand how to control the duration and pitch of TTS models. We can show how to adjust the voice to sound more solemn (slower speed + lower pitch), or more excited (higher speed + higher pitch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define what we want the model to say\n",
    "input_string = \"I want to pass on my condolences for your loss.\"\n",
    "\n",
    "# Run inference to get spectrogram and pitch\n",
    "with torch.no_grad():\n",
    "    tokens = fastpitch.parse(input_string)\n",
    "    spec_norm, _, durs_norm_pred, _, pitch_norm, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None)\n",
    "    audio_norm = hifigan.convert_spectrogram_to_audio(spec=spec_norm).to('cpu').numpy()\n",
    "    \n",
    "    # TODO\n",
    "    new_pitch = (pitch_norm)*0.75-0.75\n",
    "    new_pitch[0][-5] += 0.2\n",
    "    spec_sol, _, durs_sol_pred, _, _, _, _, _, _, pitch_sol = fastpitch(\n",
    "        text=tokens, durs=None, pitch=new_pitch, speaker=None, pace=0.9)\n",
    "    audio_sol = hifigan.convert_spectrogram_to_audio(spec=spec_sol).to('cpu').numpy()\n",
    "    \n",
    "    pitch_sol_t = pitch_sol\n",
    "    \n",
    "    pitch_sol, _ = regulate_len(durs_sol_pred, pitch_sol.unsqueeze(-1), pace=0.9)\n",
    "    pitch_sol = pitch_sol.squeeze(-1)\n",
    "    pitch_norm, _ = regulate_len(durs_norm_pred, pitch_norm.unsqueeze(-1))\n",
    "    pitch_norm = pitch_norm.squeeze(-1)\n",
    "    pitch_norm = pitch_norm * 65.72037058703644 + 214.72202032404294\n",
    "    pitch_sol = pitch_sol * 65.72037058703644 + 214.72202032404294\n",
    "    \n",
    "# Let's see both results\n",
    "print(\"The first unaltered sample\")\n",
    "display_pitch(audio_norm, pitch_norm)\n",
    "\n",
    "\n",
    "print(\"The second solumn sample\")\n",
    "display_pitch(audio_sol, pitch_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define what we want the model to say\n",
    "input_string = \"Congratulations on your promotion.\"\n",
    "\n",
    "# Run inference to get spectrogram and pitch\n",
    "with torch.no_grad():\n",
    "    tokens = fastpitch.parse(input_string)\n",
    "    spec_norm, _, durs_norm_pred, _, pitch_norm, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None)\n",
    "    audio_norm = hifigan.convert_spectrogram_to_audio(spec=spec_norm).to('cpu').numpy()\n",
    "    \n",
    "    # TODO\n",
    "    new_pitch = (pitch_norm)*1.7+0.5\n",
    "    spec_sol, _, durs_sol_pred, _, _, _, _, _, _, pitch_sol = fastpitch(\n",
    "        text=tokens, durs=None, pitch=new_pitch, speaker=None, pace=1.1)\n",
    "    audio_sol = hifigan.convert_spectrogram_to_audio(spec=spec_sol).to('cpu').numpy()\n",
    "    \n",
    "    pitch_sol_t = pitch_sol\n",
    "    \n",
    "    pitch_sol, _ = regulate_len(durs_sol_pred, pitch_sol.unsqueeze(-1), pace=1.1)\n",
    "    pitch_sol = pitch_sol.squeeze(-1)\n",
    "    pitch_norm, _ = regulate_len(durs_norm_pred, pitch_norm.unsqueeze(-1))\n",
    "    pitch_norm = pitch_norm.squeeze(-1)\n",
    "    pitch_norm = pitch_norm * 65.72037058703644 + 214.72202032404294\n",
    "    pitch_sol = pitch_sol * 65.72037058703644 + 214.72202032404294\n",
    "    \n",
    "# Let's see both results\n",
    "print(\"The first unaltered sample\")\n",
    "display_pitch(audio_norm, pitch_norm)\n",
    "\n",
    "\n",
    "print(\"The second solumn sample\")\n",
    "display_pitch(audio_sol, pitch_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://arxiv.org/abs/1905.09263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Module |\n",
    "|---|---|\n",
    "|[FastPitch](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/models/fastpitch.py)|[TemporalPredictor](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/modules/fastpitch.py#L117)|\n",
    "|[FastPitch_HifiGan_E2E](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_e2e_fastpitchhifigan)|[TemporalPredictor](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/modules/fastpitch.py#L117)|\n",
    "|[FastSpeech2](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_fastspeech_2)|[VarianceAdaptor](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/modules/fastspeech2.py)|\n",
    "|[FastSpeech2_HifiGan_E2E](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_e2e_fastspeech2hifigan)|[VarianceAdaptor](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/modules/fastspeech2.py)|\n",
    "|[TalkNet](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_talknet)| [ConvASREncoder](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/modules/conv_asr.py#L53) |\n",
    "|[Glow-TTS](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_en_glowtts)| [TextEncoder](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/modules/glow_tts.py#L63) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_sol_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
