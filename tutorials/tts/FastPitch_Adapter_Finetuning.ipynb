{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7754dcf",
   "metadata": {},
   "source": [
    "# FastPitch Adapter Finetuning\n",
    "\n",
    "This notebook is designed to provide a guide on how to run FastPitch Adapter Finetuning Pipeline. It contains the following sections:\n",
    "1. **Transform pre-trained FastPitch checkpoint to adapter-compatible checkpoint**\n",
    "2. **Fine-tune FastPitch on adaptation data**: fine-tune pre-trained multi-speaker FastPitch for a new speaker\n",
    "* Dataset Preparation: download dataset and extract manifest files. (duration more than 15 mins)\n",
    "* Preprocessing: add absolute audio paths in manifest, calculate pitch stats.\n",
    "* Training: fine-tune frozen multispeaker FastPitch with trainable adapters.\n",
    "3. **Fine-tune HiFiGAN on adaptation data**: fine-tune a vocoder for the fine-tuned multi-speaker FastPitch\n",
    "* Dataset Preparation: extract mel-spectrograms from fine-tuned FastPitch.\n",
    "* Training: fine-tune HiFiGAN with fine-tuned adaptation data.\n",
    "4. **Inference**: generate speech from adpated FastPitch\n",
    "* Load Model: load pre-trained multi-speaker FastPitch with fine-tuned adapters.\n",
    "* Output Audio: generate audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91190b36",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2023 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71341824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login #PASTE_WANDB_APIKEY_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_PRETRAINED_FASTPITCH_CHECKPOINT = \"\"\n",
    "YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb47145",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "# Store all python script\n",
    "codedir = 'NeMoTTS' \n",
    "# Store all manifest and audios\n",
    "datadir = 'NeMoTTS_dataset'\n",
    "# Store all related text-normalized files\n",
    "normdir = 'NeMoTTS_normalize_files'\n",
    "# Store all supplementary files\n",
    "suppdir = \"NeMoTTS_sup_data\"\n",
    "# Store all config files\n",
    "confdir = \"NeMoTTS_conf\"\n",
    "# Store all training logs\n",
    "logsdir = \"NeMoTTS_logs\"\n",
    "# Store all mel-spectrograms for vocoder training\n",
    "melsdir = \"NeMoTTS_mels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7560cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40e507",
   "metadata": {},
   "source": [
    "# 1. Transform pre-trained checkpoint to adapter-compatible checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f55cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core import adapter_mixins\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88696b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_config_to_support_adapter(config) -> DictConfig:\n",
    "    with open_dict(config):\n",
    "        enc_adapter_metadata = adapter_mixins.get_registered_adapter(config.input_fft._target_)\n",
    "        if enc_adapter_metadata is not None:\n",
    "            config.input_fft._target_ = enc_adapter_metadata.adapter_class_path\n",
    "\n",
    "        dec_adapter_metadata = adapter_mixins.get_registered_adapter(config.output_fft._target_)\n",
    "        if dec_adapter_metadata is not None:\n",
    "            config.output_fft._target_ = dec_adapter_metadata.adapter_class_path\n",
    "\n",
    "        pitch_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.pitch_predictor._target_)\n",
    "        if pitch_predictor_adapter_metadata is not None:\n",
    "            config.pitch_predictor._target_ = pitch_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        duration_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.duration_predictor._target_)\n",
    "        if duration_predictor_adapter_metadata is not None:\n",
    "            config.duration_predictor._target_ = duration_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        aligner_adapter_metadata = adapter_mixins.get_registered_adapter(config.alignment_module._target_)\n",
    "        if aligner_adapter_metadata is not None:\n",
    "            config.alignment_module._target_ = aligner_adapter_metadata.adapter_class_path\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bfaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)\n",
    "state['hyper_parameters']['cfg'] = update_model_config_to_support_adapter(state['hyper_parameters']['cfg'])\n",
    "torch.save(state, YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b65006",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copyfile(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT, \"FastPitch.pt\")\n",
    "shutil.copyfile(YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT, \"HifiGan.pt\")\n",
    "YOUR_PRETRAINED_FASTPITCH_CHECKPOINT = \"FastPitch.pt\"\n",
    "YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT = \"HifiGan.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb540c9",
   "metadata": {},
   "source": [
    "# 2. Fine-tune FastPitch on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e8a04",
   "metadata": {},
   "source": [
    "## a. Data Preparation\n",
    "For our tutorial, we use small part of VCTK dataset with a new target speaker (p267). Usually, the audios should have total duration more than 15 mintues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1442ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {datadir} && cd {datadir} && wget https://vctk-subset.s3.amazonaws.com/vctk_subset.tar.gz && tar zxf vctk_subset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ef048",
   "metadata": {},
   "outputs": [],
   "source": [
    "manidir = f\"{datadir}/vctk_subset\"\n",
    "!ls {manidir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b5f1a",
   "metadata": {},
   "source": [
    "For simplicity, we use original dev set as training set and original test set as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = os.path.abspath(os.path.join(manidir, 'train.json'))\n",
    "valid_manifest = os.path.abspath(os.path.join(manidir, 'dev.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e0e87",
   "metadata": {},
   "source": [
    "## b. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02029ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files\n",
    "!mkdir -p {normdir} && cd {normdir} \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/heteronyms-052722 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc7db6",
   "metadata": {},
   "source": [
    "### Add absolute file path in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0700d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        for line in f: lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def json_writer(manifest, filename):\n",
    "    with open(filename, 'w') as fout:\n",
    "        for m in manifest: fout.write(json.dumps(m) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643cf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(train_datas, train_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c29e7",
   "metadata": {},
   "source": [
    "### Calibrate speaker id to start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2dad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['old_speaker'], m['speaker'] = m['speaker'], 0\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['old_speaker'], m['speaker'] = m['speaker'], 0\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a7a6e",
   "metadata": {},
   "source": [
    "### Calculate Pitch Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7179f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch(sample):    \n",
    "    rel_audio_path = Path(sample[\"audio_filepath\"]).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    \n",
    "    if os.path.exists(pitch_filepath):\n",
    "        pitch = torch.load(pitch_filepath).numpy()\n",
    "\n",
    "    else:\n",
    "        features = wave_model.process(\n",
    "            sample[\"audio_filepath\"]\n",
    "        )\n",
    "        voiced_tuple = librosa.pyin(\n",
    "            features.numpy(),\n",
    "            fmin=librosa.note_to_hz('C2'),\n",
    "            fmax=librosa.note_to_hz('C7'),\n",
    "            frame_length=2048,\n",
    "            sr=44100,\n",
    "            fill_na=0.0,\n",
    "        )\n",
    "        pitch = voiced_tuple[0]\n",
    "        torch.save(torch.from_numpy(pitch).float(), pitch_filepath)\n",
    "    \n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)\n",
    "pitch_dir = os.path.join(suppdir, 'pitch')\n",
    "os.makedirs(suppdir, exist_ok=True)\n",
    "os.makedirs(pitch_dir, exist_ok=True)\n",
    "\n",
    "train_pitchs = []\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "for m in tqdm(train_datas): train_pitchs.append(get_pitch(m))\n",
    "    \n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "for m in tqdm(valid_datas): get_pitch(m)\n",
    "\n",
    "train_pitchs = np.concatenate(train_pitchs)\n",
    "pitch_mean = float(np.mean(train_pitchs))\n",
    "pitch_std = float(np.std(train_pitchs))\n",
    "\n",
    "with open(os.path.join(manidir, 'pitch_stats.json'), 'w') as f:\n",
    "    json.dump({'pitch':[pitch_mean, pitch_std]}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87cceb",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7724add",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {confdir} && cd {confdir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/fastpitch_align_44100_adapter.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a969914",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {codedir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/fastpitch_finetune_adapters.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96071e54",
   "metadata": {},
   "source": [
    "### Important notes\n",
    "* **+init_from_ptl_ckpt**: initialize with a multi-speaker FastPitch checkpoint\n",
    "* **~model.speaker_encoder.lookup_module**: remove the pre-trained looked-up speaker embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f1cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 100 epochs (15 mins)\n",
    "!(python {codedir}/fastpitch_finetune_adapters.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=fastpitch_align_44100_adapter.yaml \\\n",
    "+init_from_ptl_ckpt={YOUR_PRETRAINED_FASTPITCH_CHECKPOINT} \\\n",
    "sample_rate=44100 \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id', 'reference_audio']\" \\\n",
    "sup_data_path={suppdir} \\\n",
    "pitch_mean={pitch_mean} \\\n",
    "pitch_std={pitch_std} \\\n",
    "phoneme_dict_path={normdir}/cmudict-0.7b_nv22.10 \\\n",
    "heteronyms_path={normdir}/heteronyms-052722 \\\n",
    "~model.speaker_encoder.lookup_module \\\n",
    "model.speaker_encoder.gst_module._target_=\"nemo.collections.tts.modules.submodules.GlobalStyleToken\" \\\n",
    "model.input_fft.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.output_fft.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.duration_predictor.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.pitch_predictor.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.alignment_module.condition_types=\"['add']\" \\\n",
    "model.train_ds.dataloader_params.batch_size=8 \\\n",
    "model.validation_ds.dataloader_params.batch_size=8 \\\n",
    "model.train_ds.dataloader_params.num_workers=8 \\\n",
    "model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "+model.text_tokenizer.add_blank_at=True \\\n",
    "model.optim.name=adam \\\n",
    "model.optim.lr=2e-4 \\\n",
    "model.optim.weight_decay=0.0 \\\n",
    "~model.optim.sched \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "+exp_manager.create_wandb_logger=True \\\n",
    "+exp_manager.wandb_logger_kwargs.name=\"tutorial-FastPitch-finetune-adaptation\" \\\n",
    "+exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    "+exp_manager.checkpoint_callback_params.save_top_k=-1 \\\n",
    "trainer.max_epochs=10 \\\n",
    "trainer.check_val_every_n_epoch=10 \\\n",
    "trainer.log_every_n_steps=1 \\\n",
    "trainer.devices=1 \\\n",
    "trainer.strategy=ddp \\\n",
    "trainer.precision=32 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31286dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. NeMoTTS_logs/FastPitch/Y-M-D_H-M-S\n",
    "last_checkpoint_dir = sorted(list([i for i in (Path(logsdir) / \"FastPitch\").iterdir() if i.is_dir()]))[-1] / \"checkpoints\"\n",
    "YOUR_FINETUNED_ADAPTER_CHECKPOINT = list(last_checkpoint_dir.glob('adapters.pt'))[0]\n",
    "YOUR_FINETUNED_ADAPTER_CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8f037",
   "metadata": {},
   "source": [
    "# 4. Fine-tune HiFiGAN on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40026019",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6edacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spectrogram(index, manifest, speaker_to_index, base_data_dir):\n",
    "    \n",
    "    record = manifest[index]\n",
    "    audio_file = record[\"audio_filepath\"]\n",
    "    \n",
    "    if '.wav' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(melsdir, audio_file.split(\"/\")[-1].replace(\".wav\", \".npy\")))\n",
    "    \n",
    "    if '.flac' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(melsdir, audio_file.split(\"/\")[-1].replace(\".flac\", \".npy\")))\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    \n",
    "    if \"normalized_text\" in record:\n",
    "        text = spec_model.parse(record[\"normalized_text\"], normalize=False)\n",
    "    else:\n",
    "        text = spec_model.parse(record['text'])\n",
    "        \n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=spec_model.device).unsqueeze(0)\n",
    "    \n",
    "    audio = wave_model.process(audio_file).unsqueeze(0).to(device=spec_model.device)\n",
    "    audio_len = torch.tensor(audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len) \n",
    "    \n",
    "    attn_prior = torch.from_numpy(beta_binomial_interpolator(spect_len.item(), text_len.item())).unsqueeze(0).to(spec_model.device)\n",
    "        \n",
    "    reference_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    reference_sample = manifest[random.sample(reference_pool, 1)[0]]\n",
    "    reference_audio = wave_model.process(reference_sample[\"audio_filepath\"]).unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_audio_length = torch.tensor(reference_audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_spec, reference_spec_len = spec_model.preprocessor(input_signal=reference_audio, length=reference_audio_length)  \n",
    "    \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        spectrogram = spec_model.forward(\n",
    "          text=text, \n",
    "          input_lens=text_len,\n",
    "          spec=spect, \n",
    "          mel_lens=spect_len, \n",
    "          attn_prior=attn_prior,\n",
    "          reference_spec=reference_spec,\n",
    "          reference_spec_lens=reference_spec_len,\n",
    "        )[0]\n",
    "    \n",
    "    spec = spectrogram[0].to('cpu').numpy()\n",
    "    np.save(save_path, spec)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained FastPitch Weights\n",
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)\n",
    "\n",
    "# Load Adapter Weights\n",
    "spec_model.load_adapters(YOUR_FINETUNED_ADAPTER_CHECKPOINT)\n",
    "spec_model.eval().cuda()\n",
    "\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(melsdir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(train_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(train_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, train_datas, speaker_to_index, base_data_dir)\n",
    "\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "\n",
    "# Valid\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(valid_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(valid_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, valid_datas, speaker_to_index, base_data_dir)\n",
    "\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68926d1",
   "metadata": {},
   "source": [
    "## b. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {confdir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/hifigan/hifigan_44100.yaml\n",
    "!cd {confdir} && mkdir -p model/train_ds && cd model/train_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/train_ds/train_ds_finetune.yaml \n",
    "!cd {confdir} && mkdir -p model/validation_ds && cd model/validation_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/validation_ds/val_ds_finetune.yaml\n",
    "!cd {confdir} && mkdir -p model/generator && cd model/generator && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/generator/v1_44100.yaml\n",
    "!cd {codedir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/hifigan_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 500 epochs (30 mins)\n",
    "!(python {codedir}/hifigan_finetune.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=hifigan_44100.yaml \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "+init_from_ptl_ckpt={YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT} \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.optim.lr=0.0001 \\\n",
    "+trainer.max_epochs=5 \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "model/train_ds=train_ds_finetune \\\n",
    "model/validation_ds=val_ds_finetune \\\n",
    "trainer.devices=1 \\\n",
    "trainer.strategy='ddp' \\\n",
    "trainer.precision=16 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "exp_manager.create_wandb_logger=True \\\n",
    "exp_manager.wandb_logger_kwargs.name=\"tutorial-HiFiGAN-finetune-adaptation\" \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. NeMoTTS_logs/HifiGan/Y-M-D_H-M-S/checkpoints/HifiGan--val_loss=XXX-epoch=XXX.ckpt\n",
    "last_checkpoint_dir = sorted(list([i for i in (Path(logsdir) / \"HifiGan\").iterdir() if i.is_dir()]))[-1] / \"checkpoints\"\n",
    "YOUR_FINETUNED_HIFIGAN_ON_ADAPTATION_CHECKPOINT = list(last_checkpoint_dir.glob('*-last.ckpt'))[0]\n",
    "YOUR_FINETUNED_HIFIGAN_ON_ADAPTATION_CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fe2e3",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7274970d",
   "metadata": {},
   "source": [
    "## a. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75982012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastPitch\n",
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)\n",
    "spec_model.load_adapters(YOUR_FINETUNED_ADAPTER_CHECKPOINT)\n",
    "# spec_model.freeze()\n",
    "# spec_model.unfreeze_enabled_adapters()\n",
    "spec_model = spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fb6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiGAN\n",
    "vocoder_model = HifiGanModel.load_from_checkpoint(checkpoint_path=YOUR_FINETUNED_HIFIGAN_ON_ADAPTATION_CHECKPOINT).eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45533e1a",
   "metadata": {},
   "source": [
    "## b. Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_spectrogram(audio_path, wave_model, spec_gen_model):\n",
    "    features = wave_model.process(audio_path, trim=False)\n",
    "    audio, audio_length = features, torch.tensor(features.shape[0]).long()\n",
    "    audio = audio.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    audio_length = audio_length.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram, spec_len = spec_gen_model.preprocessor(input_signal=audio, length=audio_length)\n",
    "    return spectrogram, spec_len\n",
    "\n",
    "def gen_spectrogram(text, spec_gen_model, reference_spec, reference_spec_lens):\n",
    "    parsed = spec_gen_model.parse(text)\n",
    "    with torch.no_grad():    \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, \n",
    "                                                          reference_spec=reference_spec, \n",
    "                                                          reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    return spectrogram\n",
    "  \n",
    "def synth_audio(vocoder_model, spectrogram):    \n",
    "    with torch.no_grad():  \n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d23c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Audio\n",
    "with open(train_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        reference_record = json.loads(line)\n",
    "        break\n",
    "        \n",
    "# Validatation Audio\n",
    "num_val = 3\n",
    "val_records = []\n",
    "with open(valid_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append(json.loads(line))\n",
    "        if len(val_records) >= num_val:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8485d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val_record in enumerate(val_records):\n",
    "    reference_spec, reference_spec_lens = gt_spectrogram(reference_record['audio_filepath'], wave_model, spec_model)\n",
    "    reference_spec = reference_spec.to(spec_model.device)\n",
    "    spec_pred = gen_spectrogram(val_record['text'], spec_model,\n",
    "                                reference_spec=reference_spec, \n",
    "                                reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    audio_gen = synth_audio(vocoder_model, spec_pred)\n",
    "    \n",
    "    audio_ref = ipd.Audio(reference_record['audio_filepath'], rate=sample_rate)\n",
    "    audio_gt = ipd.Audio(val_record['audio_filepath'], rate=sample_rate)\n",
    "    audio_gen = ipd.Audio(audio_gen, rate=sample_rate)\n",
    "    \n",
    "    print(\"------\")\n",
    "    print(f\"Text: {val_record['text']}\")\n",
    "    print('Reference Audio')\n",
    "    ipd.display(audio_ref)\n",
    "    print('Ground Truth Audio')\n",
    "    ipd.display(audio_gt)\n",
    "    print('Synthesized Audio')\n",
    "    ipd.display(audio_gen)\n",
    "    plt.imshow(spec_pred[0].to('cpu').numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c137592",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(YOUR_FINETUNED_ADAPTER_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(YOUR_FINETUNED_HIFIGAN_ON_ADAPTATION_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4b8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
