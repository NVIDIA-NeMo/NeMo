{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef81c313",
   "metadata": {},
   "source": [
    "# FastPitch Adapter Finetuning\n",
    "\n",
    "This notebook is designed to provide a guide on how to run FastPitch Adapter Finetuning Pipeline. It contains the following sections:\n",
    "1. **Transform pre-trained FastPitch checkpoint to adapter-compatible checkpoint**\n",
    "2. **Fine-tune FastPitch on adaptation data**: fine-tune pre-trained multi-speaker FastPitch for a new speaker\n",
    "* Dataset Preparation: download dataset and extract manifest files. (duration more than 15 mins)\n",
    "* Preprocessing: add absolute audio paths in manifest and extract Supplementary Data.\n",
    "* Training: fine-tune frozen multispeaker FastPitch with trainable adapters.\n",
    "3. **Fine-tune HiFiGAN on adaptation data**: fine-tune a vocoder for the fine-tuned multi-speaker FastPitch\n",
    "* Dataset Preparation: extract mel-spectrograms from fine-tuned FastPitch.\n",
    "* Training: fine-tune HiFiGAN with fine-tuned adaptation data.\n",
    "4. **Inference**: generate speech from adpated FastPitch\n",
    "* Load Model: load pre-trained multi-speaker FastPitch with **fine-tuned adapters**.\n",
    "* Output Audio: generate audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72971e",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce36784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# BRANCH = 'main'\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "# # Download local version of NeMo scripts. If you are running locally and want to use your own local NeMo code,\n",
    "# # comment out the below lines and set `code_dir` to your local path.\n",
    "code_dir = 'NeMoTTS' \n",
    "!git clone https://github.com/NVIDIA/NeMo.git {code_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login #PASTE_WANDB_APIKEY_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .nemo files for your pre-trained FastPitch and HiFiGAN\n",
    "pretrained_fastpitch_checkpoint = \"\"\n",
    "finetuned_hifigan_on_multispeaker_checkpoint = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "# Store all manifest and audios\n",
    "data_dir = 'NeMoTTS_dataset'\n",
    "# Store all supplementary files\n",
    "supp_dir = \"NeMoTTS_sup_data\"\n",
    "# Store all training logs\n",
    "logs_dir = \"NeMoTTS_logs\"\n",
    "# Store all mel-spectrograms for vocoder training\n",
    "mels_dir = \"NeMoTTS_mels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ed8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(code_dir, exist_ok=True)\n",
    "code_dir = os.path.abspath(code_dir)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "data_dir = os.path.abspath(data_dir)\n",
    "os.makedirs(supp_dir, exist_ok=True)\n",
    "supp_dir = os.path.abspath(supp_dir)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "logs_dir = os.path.abspath(logs_dir)\n",
    "os.makedirs(mels_dir, exist_ok=True)\n",
    "mels_dir = os.path.abspath(mels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afc9e1",
   "metadata": {},
   "source": [
    "# 1. Transform pre-trained checkpoint to adapter-compatible checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.core import adapter_mixins\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88654e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_config_to_support_adapter(config) -> DictConfig:\n",
    "    with open_dict(config):\n",
    "        enc_adapter_metadata = adapter_mixins.get_registered_adapter(config.input_fft._target_)\n",
    "        if enc_adapter_metadata is not None:\n",
    "            config.input_fft._target_ = enc_adapter_metadata.adapter_class_path\n",
    "\n",
    "        dec_adapter_metadata = adapter_mixins.get_registered_adapter(config.output_fft._target_)\n",
    "        if dec_adapter_metadata is not None:\n",
    "            config.output_fft._target_ = dec_adapter_metadata.adapter_class_path\n",
    "\n",
    "        pitch_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.pitch_predictor._target_)\n",
    "        if pitch_predictor_adapter_metadata is not None:\n",
    "            config.pitch_predictor._target_ = pitch_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        duration_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.duration_predictor._target_)\n",
    "        if duration_predictor_adapter_metadata is not None:\n",
    "            config.duration_predictor._target_ = duration_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        aligner_adapter_metadata = adapter_mixins.get_registered_adapter(config.alignment_module._target_)\n",
    "        if aligner_adapter_metadata is not None:\n",
    "            config.alignment_module._target_ = aligner_adapter_metadata.adapter_class_path\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastPitchModel.restore_from(pretrained_fastpitch_checkpoint)\n",
    "model.cfg = update_model_config_to_support_adapter(model.cfg)\n",
    "model.save_to('Pretrained-FastPitch.nemo')\n",
    "shutil.copyfile(finetuned_hifigan_on_multispeaker_checkpoint, \"Pretrained-HifiGan.nemo\")\n",
    "\n",
    "pretrained_fastpitch_checkpoint = os.path.abspath(\"Pretrained-FastPitch.nemo\")\n",
    "finetuned_hifigan_on_multispeaker_checkpoint = os.path.abspath(\"Pretrained-HifiGan.nemo\")\n",
    "#     state = torch.load(pretrained_fastpitch_checkpoint)\n",
    "#     state['hyper_parameters']['cfg'] = update_model_config_to_support_adapter(state['hyper_parameters']['cfg'])\n",
    "#     torch.save(state, pretrained_fastpitch_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879a60c",
   "metadata": {},
   "source": [
    "# 2. Fine-tune FastPitch on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c7f02",
   "metadata": {},
   "source": [
    "## a. Data Preparation\n",
    "For our tutorial, we use small part of VCTK dataset with a new target speaker (p267). Usually, the audios should have total duration more than 15 mintues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {data_dir} && wget https://vctk-subset.s3.amazonaws.com/vctk_subset.tar.gz && tar zxf vctk_subset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "manidir = f\"{data_dir}/vctk_subset\"\n",
    "!ls {manidir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = os.path.abspath(os.path.join(manidir, 'train.json'))\n",
    "valid_manifest = os.path.abspath(os.path.join(manidir, 'dev.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df877a",
   "metadata": {},
   "source": [
    "## b. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34a373",
   "metadata": {},
   "source": [
    "### Add absolute file path in manifest\n",
    "We use absoluate path for audio_filepath to get the audio during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = read_manifest(train_manifest)\n",
    "for m in train_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "write_manifest(train_manifest, train_datas)\n",
    "\n",
    "valid_datas = read_manifest(valid_manifest)\n",
    "for m in valid_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "write_manifest(valid_manifest, valid_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0f45a",
   "metadata": {},
   "source": [
    "### Extract Supplementary Data\n",
    "\n",
    "As mentioned in the [FastPitch and MixerTTS training tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/FastPitch_MixerTTS_Training.ipynb) - To accelerate and stabilize our training, we also need to extract pitch for every audio, estimate pitch statistics (mean, std, min, and max). To do this, all we need to do is iterate over our data one time, via `extract_sup_data.py` script.\n",
    "\n",
    "Note: This is an optional step, if skipped, it will be automatically executed within the first epoch of training FastPitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87661e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {code_dir} && python scripts/dataset_processing/tts/extract_sup_data.py \\\n",
    "    manifest_filepath={train_manifest} \\\n",
    "    sup_data_path={supp_dir} \\\n",
    "    dataset.sample_rate={sample_rate} \\\n",
    "    dataset.n_fft=2048 \\\n",
    "    dataset.win_length=2048 \\\n",
    "    dataset.hop_length=512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939bde2",
   "metadata": {},
   "source": [
    "After running the above command line, you will observe a new folder NeMoTTS_sup_data/pitch and printouts of pitch statistics like below. Specify these values to the FastPitch training configurations. We will be there in the following section.\n",
    "```bash\n",
    "PITCH_MEAN=175.48513793945312, PITCH_STD=42.3786735534668\n",
    "PITCH_MIN=65.4063949584961, PITCH_MAX=270.8517761230469\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {code_dir} && python scripts/dataset_processing/tts/extract_sup_data.py \\\n",
    "    manifest_filepath={valid_manifest} \\\n",
    "    sup_data_path={supp_dir} \\\n",
    "    dataset.sample_rate={sample_rate} \\\n",
    "    dataset.n_fft=2048 \\\n",
    "    dataset.win_length=2048 \\\n",
    "    dataset.hop_length=512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26e91b",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_dict_path = os.path.abspath(os.path.join(code_dir, \"scripts\", \"tts_dataset_files\", \"cmudict-0.7b_nv22.10\"))\n",
    "heteronyms_path = os.path.abspath(os.path.join(code_dir, \"scripts\", \"tts_dataset_files\", \"heteronyms-052722\"))\n",
    "\n",
    "# Copy and Paste the PITCH_MEAN and PITCH_STD from previous steps (train_manifest) to overide pitch_mean and pitch_std configs below.\n",
    "PITCH_MEAN=175.48513793945312\n",
    "PITCH_STD=42.3786735534668"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d599575",
   "metadata": {},
   "source": [
    "### Important notes\n",
    "* `+init_from_ptl_ckpt`: initialize with a multi-speaker FastPitch checkpoint\n",
    "* `~model.speaker_encoder.lookup_module`: remove the pre-trained looked-up speaker embedding\n",
    "* Other optional arguments based on your preference:\n",
    "    * batch_size\n",
    "    * exp_manager\n",
    "    * trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17638cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 100 epochs\n",
    "!cd {code_dir} && python examples/tts/fastpitch_finetune_adapters.py \\\n",
    "--config-name=fastpitch_align_44100_adapter.yaml \\\n",
    "+init_from_nemo_model={pretrained_fastpitch_checkpoint} \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id', 'reference_audio']\" \\\n",
    "sup_data_path={supp_dir} \\\n",
    "pitch_mean={PITCH_MEAN} \\\n",
    "pitch_std={PITCH_STD} \\\n",
    "~model.speaker_encoder.lookup_module \\\n",
    "model.train_ds.dataloader_params.batch_size=8 \\\n",
    "model.validation_ds.dataloader_params.batch_size=8 \\\n",
    "model.optim.name=adam \\\n",
    "model.optim.lr=2e-4 \\\n",
    "~model.optim.sched \\\n",
    "exp_manager.exp_dir={logs_dir} \\\n",
    "+exp_manager.create_wandb_logger=True \\\n",
    "+exp_manager.wandb_logger_kwargs.name=\"tutorial-FastPitch-finetune-adaptation\" \\\n",
    "+exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    "+exp_manager.checkpoint_callback_params.save_top_k=-1 \\\n",
    "trainer.max_epochs=10 \\\n",
    "trainer.check_val_every_n_epoch=10 \\\n",
    "trainer.log_every_n_steps=1 \\\n",
    "trainer.devices=1 \\\n",
    "trainer.strategy=ddp \\\n",
    "trainer.precision=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ae669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. NeMoTTS_logs/FastPitch/Y-M-D_H-M-S/checkpoints/adapters.pt\n",
    "last_checkpoint_dir = sorted(list([i for i in (Path(logs_dir) / \"FastPitch\").iterdir() if i.is_dir()]))[-1] / \"checkpoints\"\n",
    "finetuned_adapter_checkpoint = list(last_checkpoint_dir.glob('adapters.pt'))[0]\n",
    "print(finetuned_adapter_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dcbc8f",
   "metadata": {},
   "source": [
    "# 4. Fine-tune HiFiGAN on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37090445",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation\n",
    "Generate mel-spectrograms for HiFiGAN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45efcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spectrogram(index, manifest, speaker_to_index):\n",
    "    \n",
    "    record = manifest[index]\n",
    "    audio_file = record[\"audio_filepath\"]\n",
    "    \n",
    "    if '.wav' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mels_dir, audio_file.split(\"/\")[-1].replace(\".wav\", \".npy\")))\n",
    "    \n",
    "    if '.flac' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mels_dir, audio_file.split(\"/\")[-1].replace(\".flac\", \".npy\")))\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    \n",
    "    if \"normalized_text\" in record:\n",
    "        text = spec_model.parse(record[\"normalized_text\"], normalize=False)\n",
    "    else:\n",
    "        text = spec_model.parse(record['text'])\n",
    "        \n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=spec_model.device).unsqueeze(0)\n",
    "    \n",
    "    audio = wave_model.process(audio_file).unsqueeze(0).to(device=spec_model.device)\n",
    "    audio_len = torch.tensor(audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len) \n",
    "    \n",
    "    attn_prior = torch.from_numpy(beta_binomial_interpolator(spect_len.item(), text_len.item())).unsqueeze(0).to(spec_model.device)\n",
    "        \n",
    "    reference_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    reference_sample = manifest[random.sample(reference_pool, 1)[0]]\n",
    "    reference_audio = wave_model.process(reference_sample[\"audio_filepath\"]).unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_audio_length = torch.tensor(reference_audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_spec, reference_spec_len = spec_model.preprocessor(input_signal=reference_audio, length=reference_audio_length)  \n",
    "    \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        spectrogram = spec_model.forward(\n",
    "          text=text, \n",
    "          input_lens=text_len,\n",
    "          spec=spect, \n",
    "          mel_lens=spect_len, \n",
    "          attn_prior=attn_prior,\n",
    "          reference_spec=reference_spec,\n",
    "          reference_spec_lens=reference_spec_len,\n",
    "        )[0]\n",
    "    \n",
    "    spec = spectrogram[0].to('cpu').numpy()\n",
    "    np.save(save_path, spec)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)\n",
    "\n",
    "# Pretrained FastPitch Weights\n",
    "spec_model = FastPitchModel.restore_from(pretrained_fastpitch_checkpoint)\n",
    "\n",
    "# Load Adapter Weights\n",
    "spec_model.load_adapters(finetuned_adapter_checkpoint)\n",
    "spec_model.eval().cuda()\n",
    "\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(mels_dir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "train_datas = read_manifest(train_manifest)\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(train_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(train_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, train_datas, speaker_to_index)\n",
    "\n",
    "write_manifest(train_manifest, train_datas)\n",
    "\n",
    "\n",
    "# Valid\n",
    "valid_datas = read_manifest(valid_manifest)\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(valid_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(valid_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, valid_datas, speaker_to_index)\n",
    "\n",
    "write_manifest(valid_manifest, valid_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa78c31",
   "metadata": {},
   "source": [
    "## b. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 500 epochs\n",
    "!cd {code_dir} && python examples/tts/hifigan_finetune.py \\\n",
    "--config-name=hifigan_44100.yaml \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "+init_from_nemo_model={finetuned_hifigan_on_multispeaker_checkpoint} \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.optim.lr=0.0001 \\\n",
    "model/train_ds=train_ds_finetune \\\n",
    "model/validation_ds=val_ds_finetune \\\n",
    "+trainer.max_epochs=5 \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "trainer.devices=-1 \\\n",
    "trainer.strategy='ddp' \\\n",
    "trainer.precision=16 \\\n",
    "exp_manager.exp_dir={logs_dir} \\\n",
    "exp_manager.create_wandb_logger=True \\\n",
    "exp_manager.wandb_logger_kwargs.name=\"tutorial-HiFiGAN-finetune-multispeaker\" \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"NeMo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. NeMoTTS_logs/HifiGan/Y-M-D_H-M-S/checkpoints/HifiGan.nemo\n",
    "last_checkpoint_dir = sorted(list([i for i in (Path(logs_dir) / \"HifiGan\").iterdir() if i.is_dir()]))[-1] / \"checkpoints\"\n",
    "finetuned_hifigan_on_adaptation_checkpoint = list(last_checkpoint_dir.glob('*.nemo'))[0]\n",
    "finetuned_hifigan_on_adaptation_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd425ef",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de65974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981087a2",
   "metadata": {},
   "source": [
    "## a. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be586e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastPitch\n",
    "spec_model = FastPitchModel.restore_from(pretrained_fastpitch_checkpoint)\n",
    "spec_model.load_adapters(finetuned_adapter_checkpoint)\n",
    "spec_model = spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab16ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiGAN\n",
    "vocoder_model = HifiGanModel.restore_from(finetuned_hifigan_on_adaptation_checkpoint).eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf26212",
   "metadata": {},
   "source": [
    "## b. Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_spectrogram(audio_path, wave_model, spec_gen_model):\n",
    "    features = wave_model.process(audio_path, trim=False)\n",
    "    audio, audio_length = features, torch.tensor(features.shape[0]).long()\n",
    "    audio = audio.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    audio_length = audio_length.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram, spec_len = spec_gen_model.preprocessor(input_signal=audio, length=audio_length)\n",
    "    return spectrogram, spec_len\n",
    "\n",
    "def gen_spectrogram(text, spec_gen_model, reference_spec, reference_spec_lens):\n",
    "    parsed = spec_gen_model.parse(text)\n",
    "    with torch.no_grad():    \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed,                                                           \n",
    "                                                          reference_spec=reference_spec, \n",
    "                                                          reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    return spectrogram\n",
    "  \n",
    "def synth_audio(vocoder_model, spectrogram):    \n",
    "    with torch.no_grad():  \n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Audio\n",
    "with open(train_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        reference_record = json.loads(line)\n",
    "        break\n",
    "        \n",
    "# Validatation Audio\n",
    "num_val = 3\n",
    "val_records = []\n",
    "with open(valid_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append(json.loads(line))\n",
    "        if len(val_records) >= num_val:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val_record in enumerate(val_records):\n",
    "    reference_spec, reference_spec_lens = gt_spectrogram(reference_record['audio_filepath'], wave_model, spec_model)\n",
    "    reference_spec = reference_spec.to(spec_model.device)\n",
    "    spec_pred = gen_spectrogram(val_record['text'], spec_model,\n",
    "                                reference_spec=reference_spec, \n",
    "                                reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    audio_gen = synth_audio(vocoder_model, spec_pred)\n",
    "    \n",
    "    audio_ref = ipd.Audio(reference_record['audio_filepath'], rate=sample_rate)\n",
    "    audio_gt = ipd.Audio(val_record['audio_filepath'], rate=sample_rate)\n",
    "    audio_gen = ipd.Audio(audio_gen, rate=sample_rate)\n",
    "    \n",
    "    print(\"------\")\n",
    "    print(f\"Text: {val_record['text']}\")\n",
    "    print('Reference Audio')\n",
    "    ipd.display(audio_ref)\n",
    "    print('Ground Truth Audio')\n",
    "    ipd.display(audio_gt)\n",
    "    print('Synthesized Audio')\n",
    "    ipd.display(audio_gen)\n",
    "    plt.imshow(spec_pred[0].to('cpu').numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fintuned_fastpitch = 'fastpitch.nemo'\n",
    "fintuned_hifigan = 'hifigan.nemo'\n",
    "spec_model.save_to(fintuned_fastpitch)\n",
    "vocoder_model.save_to(fintuned_hifigan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FastPitch checkpoint: {pretrained_fastpitch_checkpoint}\")\n",
    "print(f\"Adapter checkpoint: {finetuned_adapter_checkpoint}\")\n",
    "print(f\"HiFi-Gan checkpoint: {finetuned_hifigan_on_adaptation_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e656010",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FastPitch nemo file: {fintuned_fastpitch}\")\n",
    "print(f\"HiFi-Gan nemo file: {fintuned_hifigan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e710f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
