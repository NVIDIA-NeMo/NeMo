{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbe848f",
   "metadata": {},
   "source": [
    "# FastPitch MultiSpeaker Pretraining\n",
    "\n",
    "This notebook is designed to provide a guide on how to run FastPitch MultiSpeaker Pretraining Pipeline. It contains the following sections:\n",
    "1. **Pre-train FastPitch on multi-speaker data**: pre-train a multi-speaker FastPitch\n",
    "* Dataset Preparation: download dataset and extract manifest files.\n",
    "* Preprocessing: add absolute audio paths in manifest, calculate pitch stats.\n",
    "* Training: pre-train multispeaker FastPitch\n",
    "  * Input: we introduce additional speaker id and reference audio.\n",
    "  * Speaker: we have looked-up speaker embedding and speaker encoder. \n",
    "  * Condition: we can condition phoneme encoder, pitch/duration predictors, mel-spectrogram decoder, aligner with add and layernorm operation.\n",
    "2. **Fine-tune HiFiGAN on multi-speaker data**: fine-tune a vocoder for the pre-trained multi-speaker FastPitch\n",
    "* Dataset Preparation: extract mel-spectrograms from pre-trained FastPitch.\n",
    "* Training: fine-tune HiFiGAN with pre-trained multi-speaker data.\n",
    "3. **Inference**: generate speech from pre-trained multi-speaker FastPitch\n",
    "* Load Model: load pre-trained multi-speaker FastPitch.\n",
    "* Output Audio: generate audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa175ad",
   "metadata": {},
   "source": [
    "# License\n",
    "> Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b90dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e34752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login #PASTE_WANDB_APIKEY_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9105f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "# Store all python script\n",
    "codedir = 'NeMoTTS' \n",
    "# Store all manifest and audios\n",
    "datadir = 'NeMoTTS_dataset'\n",
    "# Store all related text-normalized files\n",
    "normdir = 'NeMoTTS_normalize_files'\n",
    "# Store all supplementary files\n",
    "suppdir = \"NeMoTTS_sup_data\"\n",
    "# Store all config files\n",
    "confdir = \"NeMoTTS_conf\"\n",
    "# Store all training logs\n",
    "logsdir = \"NeMoTTS_logs\"\n",
    "# Store all mel-spectrograms for vocoder training\n",
    "melsdir = \"NeMoTTS_mels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c46dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3478f9",
   "metadata": {},
   "source": [
    "# 1. Pre-train FastPitch on multi-speaker data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba034e43",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation\n",
    "For our tutorial, we use the subset of VCTK dataset with 5 speakers (p225-p229). The audios have 48 kHz sampling rate, we downsample to 44.1 kHz in this tutorial. \n",
    "You can read more about dataset [here](https://datashare.ed.ac.uk/handle/10283/2950)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {datadir} && cd {datadir} && wget https://vctk-subset.s3.amazonaws.com/vctk_subset_multispeaker.tar.gz && tar zxf vctk_subset_multispeaker.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "manidir = f\"{datadir}/vctk_subset_multispeaker\"\n",
    "!ls {manidir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239e1d2",
   "metadata": {},
   "source": [
    "For simplicity, we use original dev set as training set and original test set as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c33763",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = os.path.abspath(os.path.join(manidir, 'train.json'))\n",
    "valid_manifest = os.path.abspath(os.path.join(manidir, 'dev.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0824a5",
   "metadata": {},
   "source": [
    "## b. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac16425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files\n",
    "!mkdir -p {normdir} && cd {normdir} \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/heteronyms-052722 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b11129",
   "metadata": {},
   "source": [
    "### Add absoluate audio path in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c062437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        for line in f: lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def json_writer(manifest, filename):\n",
    "    with open(filename, 'w') as fout:\n",
    "        for m in manifest: fout.write(json.dumps(m) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e10b53",
   "metadata": {},
   "source": [
    "### Calibrate speaker id to start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "speaker2id = {s: _id for _id, s in enumerate(set([m['speaker'] for m in train_datas]))}\n",
    "for m in train_datas: m['old_speaker'], m['speaker'] = m['speaker'], speaker2id[m['speaker']]\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['old_speaker'], m['speaker'] = m['speaker'], speaker2id[m['speaker']]\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c74cab",
   "metadata": {},
   "source": [
    "### Calculate Pitch Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3344194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899abf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch(sample):    \n",
    "    rel_audio_path = Path(sample[\"audio_filepath\"]).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    \n",
    "    if os.path.exists(pitch_filepath):\n",
    "        pitch = torch.load(pitch_filepath).numpy()\n",
    "\n",
    "    else:\n",
    "        features = wave_model.process(\n",
    "            sample[\"audio_filepath\"]\n",
    "        )\n",
    "        voiced_tuple = librosa.pyin(\n",
    "            features.numpy(),\n",
    "            fmin=librosa.note_to_hz('C2'),\n",
    "            fmax=librosa.note_to_hz('C7'),\n",
    "            frame_length=2048,\n",
    "            sr=sample_rate,\n",
    "            fill_na=0.0,\n",
    "        )\n",
    "        pitch = voiced_tuple[0]\n",
    "        torch.save(torch.from_numpy(pitch).float(), pitch_filepath)\n",
    "    \n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)\n",
    "pitch_dir = os.path.join(suppdir, 'pitch')\n",
    "os.makedirs(suppdir, exist_ok=True)\n",
    "os.makedirs(pitch_dir, exist_ok=True)\n",
    "\n",
    "train_pitchs = []\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "for m in tqdm(train_datas): train_pitchs.append(get_pitch(m))\n",
    "    \n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "for m in tqdm(valid_datas): get_pitch(m)\n",
    "\n",
    "train_pitchs = np.concatenate(train_pitchs)\n",
    "pitch_mean = float(np.mean(train_pitchs))\n",
    "pitch_std = float(np.std(train_pitchs))\n",
    "\n",
    "with open(os.path.join(manidir, 'pitch_stats.json'), 'w') as f:\n",
    "    json.dump({'pitch':[pitch_mean, pitch_std]}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ee532",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f954beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {confdir} && cd {confdir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/fastpitch_align_44100_adapter.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {codedir} && cd {codedir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/fastpitch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12307264",
   "metadata": {},
   "source": [
    "### Important notes\n",
    "* [Data] **speaker_id** in **sup_data_types**: each data has an unique speaker index (start from 0) in the input.\n",
    "* [Data] **reference_audio** in **sup_data_types**: each data has a reference audio (from the same speaker) in the input.\n",
    "* [Speaker] **model.speaker_encoder.lookup_module**: model creates lookup table to get speaker embedding from speaker id.\n",
    "* [Speaker] **model.speaker_encoder.lookup_module.n_speakers**: model gets the speaker size. \n",
    "* [Speaker] **model.speaker_encoder.gst_module**: model creates global style token to extract speaker information from reference audio.\n",
    "* [Condition] **condition_types=\"['add', 'layernorm']\"**: insert conditions with `add` operation to inputs and `layernorm` operation to layernorms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 200 epochs\n",
    "!(python {codedir}/fastpitch.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=fastpitch_align_44100_adapter.yaml \\\n",
    "+init_from_pretrained_model=\"tts_en_fastpitch\" \\\n",
    "sample_rate={sample_rate} \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id','reference_audio']\" \\\n",
    "sup_data_path={suppdir} \\\n",
    "pitch_mean={pitch_mean} \\\n",
    "pitch_std={pitch_std} \\\n",
    "phoneme_dict_path={normdir}/cmudict-0.7b_nv22.10 \\\n",
    "heteronyms_path={normdir}/heteronyms-052722 \\\n",
    "model.speaker_encoder.lookup_module._target_=\"nemo.collections.tts.modules.submodules.SpeakerLookupTable\" \\\n",
    "model.speaker_encoder.lookup_module.n_speakers=5 \\\n",
    "model.speaker_encoder.gst_module._target_=\"nemo.collections.tts.modules.submodules.GlobalStyleToken\" \\\n",
    "model.input_fft.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.output_fft.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.duration_predictor.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.pitch_predictor.condition_types=\"['add', 'layernorm']\" \\\n",
    "model.alignment_module.condition_types=\"['add']\" \\\n",
    "model.train_ds.dataloader_params.batch_size=8 \\\n",
    "model.validation_ds.dataloader_params.batch_size=8 \\\n",
    "model.train_ds.dataloader_params.num_workers=8 \\\n",
    "model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "model.train_ds.dataset.max_duration=20 \\\n",
    "model.validation_ds.dataset.max_duration=20 \\\n",
    "model.validation_ds.dataset.min_duration=0.1 \\\n",
    "+model.text_tokenizer.add_blank_at=True \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "+exp_manager.create_wandb_logger=True \\\n",
    "+exp_manager.wandb_logger_kwargs.name=\"tutorial-FastPitch-pretrain-multispeaker\" \\\n",
    "+exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    "trainer.max_epochs=20 \\\n",
    "trainer.check_val_every_n_epoch=20 \\\n",
    "trainer.log_every_n_steps=1 \\\n",
    "trainer.devices=-1 \\\n",
    "trainer.strategy=ddp \\\n",
    "trainer.precision=32 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. NeMoTTS_logs/FastPitch/Y-M-D_H-M-S/checkpoints/FastPitch--val_loss=XXX-epoch=XXX-last.ckpt\n",
    "last_checkpoint_dir = sorted(list([i for i in (Path(logsdir) / \"FastPitch\").iterdir() if i.is_dir()]))[-1] / \"checkpoints\"\n",
    "YOUR_PRETRAINED_FASTPITCH_CHECKPOINT = list(last_checkpoint_dir.glob('*-last.ckpt'))[0]\n",
    "YOUR_PRETRAINED_FASTPITCH_CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a9913",
   "metadata": {},
   "source": [
    "# 2. Fine-tune HiFiGAN on multi-speaker data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88364f18",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68712a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spectrogram(index, manifest, speaker_to_index, base_data_dir):\n",
    "    \n",
    "    record = manifest[index]\n",
    "    audio_file = record[\"audio_filepath\"]\n",
    "    \n",
    "    if '.wav' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(melsdir, audio_file.split(\"/\")[-1].replace(\".wav\", \".npy\")))\n",
    "    \n",
    "    if '.flac' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(melsdir, audio_file.split(\"/\")[-1].replace(\".flac\", \".npy\")))\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    \n",
    "    if \"normalized_text\" in record:\n",
    "        text = spec_model.parse(record[\"normalized_text\"], normalize=False)\n",
    "    else:\n",
    "        text = spec_model.parse(record['text'])\n",
    "        \n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=spec_model.device).unsqueeze(0)\n",
    "    \n",
    "    audio = wave_model.process(audio_file).unsqueeze(0).to(device=spec_model.device)\n",
    "    audio_len = torch.tensor(audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len) \n",
    "    attn_prior = torch.from_numpy(beta_binomial_interpolator(spect_len.item(), text_len.item())).unsqueeze(0).to(spec_model.device)\n",
    "    \n",
    "    speaker = torch.tensor([record['speaker']]).to(spec_model.device)\n",
    "    \n",
    "    reference_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    reference_sample = manifest[random.sample(reference_pool, 1)[0]]\n",
    "    reference_audio = wave_model.process(reference_sample[\"audio_filepath\"]).unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_audio_length = torch.tensor(reference_audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_spec, reference_spec_len = spec_model.preprocessor(input_signal=reference_audio, length=reference_audio_length)  \n",
    "            \n",
    "    with torch.no_grad():\n",
    "        spectrogram = spec_model.forward(\n",
    "          text=text, \n",
    "          input_lens=text_len,\n",
    "          spec=spect, \n",
    "          mel_lens=spect_len, \n",
    "          attn_prior=attn_prior,\n",
    "          speaker=speaker,\n",
    "          reference_spec=reference_spec,\n",
    "          reference_spec_lens=reference_spec_len\n",
    "        )[0]\n",
    "    \n",
    "    spec = spectrogram[0].to('cpu').numpy()\n",
    "    np.save(save_path, spec)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT).eval().cuda()\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8809ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(melsdir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(train_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(train_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, train_datas, speaker_to_index, base_data_dir)\n",
    "\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "\n",
    "# Valid\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(valid_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(valid_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, valid_datas, speaker_to_index, base_data_dir)\n",
    "\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5740a15",
   "metadata": {},
   "source": [
    "## b. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af905c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {confdir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/hifigan/hifigan_44100.yaml\n",
    "!cd {confdir} && mkdir -p model/train_ds && cd model/train_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/train_ds/train_ds_finetune.yaml \n",
    "!cd {confdir} && mkdir -p model/validation_ds && cd model/validation_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/validation_ds/val_ds_finetune.yaml\n",
    "!cd {confdir} && mkdir -p model/generator && cd model/generator && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/generator/v1_44100.yaml\n",
    "!cd {codedir} && wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/hifigan_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 100 epochs\n",
    "!(python {codedir}/hifigan_finetune.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=hifigan_44100.yaml \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "+init_from_pretrained_model=\"tts_en_hifitts_hifigan_ft_fastpitch\" \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.optim.lr=0.0001 \\\n",
    "+trainer.max_epochs=5 \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "model/train_ds=train_ds_finetune \\\n",
    "model/validation_ds=val_ds_finetune \\\n",
    "trainer.devices=-1 \\\n",
    "trainer.strategy='ddp' \\\n",
    "trainer.precision=16 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "exp_manager.create_wandb_logger=True \\\n",
    "exp_manager.wandb_logger_kwargs.name=\"tutorial-HiFiGAN-finetune-multispeaker\" \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d427f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. NeMoTTS_logs/HifiGan/Y-M-D_H-M-S/checkpoints/HifiGan--val_loss=XXX-epoch=XXX-last.ckpt\n",
    "last_checkpoint_dir = sorted(list([i for i in (Path(logsdir) / \"HifiGan\").iterdir() if i.is_dir()]))[-1] / \"checkpoints\"\n",
    "YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT = list(last_checkpoint_dir.glob('*-last.ckpt'))[0]\n",
    "YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec6e21",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64490c8a",
   "metadata": {},
   "source": [
    "## a. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d78337",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastPitch\n",
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiGAN\n",
    "vocoder_model = HifiGanModel.load_from_checkpoint(checkpoint_path=YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT).eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c726e4",
   "metadata": {},
   "source": [
    "## b. Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2974168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_spectrogram(audio_path, wave_model, spec_gen_model):\n",
    "    features = wave_model.process(audio_path, trim=False)\n",
    "    audio, audio_length = features, torch.tensor(features.shape[0]).long()\n",
    "    audio = audio.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    audio_length = audio_length.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram, spec_len = spec_gen_model.preprocessor(input_signal=audio, length=audio_length)\n",
    "    return spectrogram, spec_len\n",
    "\n",
    "def gen_spectrogram(text, spec_gen_model, speaker, reference_spec, reference_spec_lens):\n",
    "    parsed = spec_gen_model.parse(text)\n",
    "    speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():    \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, \n",
    "                                                          speaker=speaker, \n",
    "                                                          reference_spec=reference_spec, \n",
    "                                                          reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    return spectrogram\n",
    "  \n",
    "def synth_audio(vocoder_model, spectrogram):    \n",
    "    with torch.no_grad():  \n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fcb433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Audio\n",
    "reference_records = []\n",
    "with open(train_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        reference_records.append(json.loads(line))\n",
    "\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(reference_records): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "        \n",
    "# Validatation Audio\n",
    "num_val = 3\n",
    "val_records = []\n",
    "with open(valid_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append(json.loads(line))\n",
    "        if len(val_records) >= num_val:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val_record in enumerate(val_records):\n",
    "    reference_record = reference_records[speaker_to_index[val_record['speaker']][0]]\n",
    "    reference_spec, reference_spec_lens = gt_spectrogram(reference_record['audio_filepath'], wave_model, spec_model)\n",
    "    reference_spec = reference_spec.to(spec_model.device)\n",
    "    spec_pred = gen_spectrogram(val_record['text'], \n",
    "                                spec_model,\n",
    "                                speaker=val_record['speaker'], \n",
    "                                reference_spec=reference_spec, \n",
    "                                reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    audio_gen = synth_audio(vocoder_model, spec_pred)\n",
    "    \n",
    "    audio_ref = ipd.Audio(reference_record['audio_filepath'], rate=sample_rate)\n",
    "    audio_gt = ipd.Audio(val_record['audio_filepath'], rate=sample_rate)\n",
    "    audio_gen = ipd.Audio(audio_gen, rate=sample_rate)\n",
    "    \n",
    "    print(\"------\")\n",
    "    print(f\"Text: {val_record['text']}\")\n",
    "    print('Reference Audio')\n",
    "    ipd.display(audio_ref)\n",
    "    print('Ground Truth Audio')\n",
    "    ipd.display(audio_gt)\n",
    "    print('Synthesized Audio')\n",
    "    ipd.display(audio_gen)\n",
    "    plt.imshow(spec_pred[0].to('cpu').numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49ee8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
