{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voicebox Speech Editing\n",
    "This tutorial is about generating edited speech and zero-shot TTS. The main code is in `scripts/voicebox_edit/test_edit.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Our code includes evaluating speaker similarity with WavLM-TDNN. To make sure this part of code won't cause trouble,\n",
    "1. run `pip install bitarray git+https://github.com/facebookresearch/fairseq.git#fairseq --no-deps` for installing fairseq, then \n",
    "2. place the [speaker_verification](https://github.com/microsoft/UniSpeech/tree/e3043e2021d49429a406be09b9b8432febcdec73/downstreams/speaker_verification) folder at `scripts/voicebox_edit/speaker_verification`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Voicebox From Checkpoints\n",
    "Voicebox include a duration prediction model for phone length prediction, and an audio model as the main part for speech editing and zero-shot TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.voicebox_edit.test_edit import MainExc\n",
    "\n",
    "# checkpoint paths\n",
    "vb_ckpt_path = \"nemo_experiments/checkpoints/a100-GS_XL-DAC-pymha-unet-warmup/checkpoints/vb-val_loss/vb=0.2913-epoch=167-step=500000-last.ckpt\"\n",
    "dp_ckpt_path = \"nemo_experiments/checkpoints/dp_no_sil_spn=1.4410-epoch=8.ckpt\"\n",
    "# output path\n",
    "output_path = \"nemo_experiments/gen_dataset\"\n",
    "\n",
    "# run the main script\n",
    "main_exc = MainExc(vb_ckpt_path=vb_ckpt_path, dp_ckpt_path=dp_ckpt_path, gen_data_dir=output_path, sample_std=0.95)\n",
    "# load the model checkpoint\n",
    "print(main.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Since our code support separate training for the duration prediction model and the voicebox audio model, we therefore might need two separate checkpoints.\n",
    "- The duration model and the voicebox audio model are loaded separately, so there is no need to make sure their configurations exactly match each other. Just ensure they're using the same acoustic feature is enough.\n",
    "- There might be warnings about missing keys when checkpoint loading, this is cause by loading checkpoints saved with the old version of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SINE Dataset\n",
    "Paper ref: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate json file for LLM to generate transcript edits\n",
    "main_exc.gen_v3_transcript_json()\n",
    "\n",
    "# LLM generated response, we use zephyr-7b-beta\n",
    "gpt_file = \"nemo_experiments/data_1a_medium.json\"\n",
    "\n",
    "# generate dataset\n",
    "main_exc.gen_v3(gpt_file=gpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate RealEdit Dataset with Voicebox\n",
    "First ask VoiceCraft's author for the RealEdit dataset, then download [`RealEdit.txt`](https://github.com/jasonppy/VoiceCraft/blob/master/RealEdit.txt), then fix the end of line 188 from \"5\\t5\\tsubstitution|substitution\" into \"5|14,15\\t5|15,16\\tsubstitution|insertion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RealEdit dataset\n",
    "realedit_dir = \"nemo_experiments/RealEdit\"\n",
    "\n",
    "# RealEdit.txt (recommand rename to RealEdit.tsv for better format understanding)\n",
    "filepath = \"nemo_experiments/RealEdit/RealEdit.txt\"\n",
    "\n",
    "# dataset output path\n",
    "output_dir = \"nemo_experiments/gen_dataset\"\n",
    "\n",
    "# generate RealEdit dataset\n",
    "main_exc.gen_RealEdit(realedit_dir=realedit_dir, filepath=filepath, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: during generation, we would simultaneously evaluate each generated audio (WER and speaker similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Speech Editing / Zero-Shot TTS Examples\n",
    "Use `main_exc.infer.riva_demo(data)` to do speech editing or zero-shot TTS.\n",
    "Check `main_exc.riva_demo()` for a typical generation pipeline, which is as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# main_exc.riva_demo()\n",
    "datas = self.dataprocessor.get_riva_demo_data(output_dir)\n",
    "for data in datas:\n",
    "    ori_mel, edit_mel = main_exc.infer.riva_demo(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, first create a list of editing metadata with the following format, then generate accordingly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# datas format\n",
    "datas = [\n",
    "    ...,\n",
    "    {\n",
    "        \"audio_path\": f\"{realedit_dir}/Original/{row.wav_fn}\",  # original audio path to be edited or as a reference for zero-shot TTS\n",
    "        \"text\": row.orig_transcript,                            # original transcript\n",
    "        \"textgrid_path\": textgrid_path,                         # (optional) textgrid path for the original audio. If not provided, the script will generate it.\n",
    "        \"from\": a_data[\"from\"],                                 # original transcript part to be substituted from\n",
    "        \"to\": t_data,                                           # original transcript part to be substituted to\n",
    "        \"edit_type\": row.type,                                  # (optional) edit type: \"substitution\", \"insertion\", \"deletion\". Default is \"substitution\".\n",
    "        \"out_ori_path\": f\"{output_dir}_ori/{row.wav_fn}\",       # output path for saving the original audio\n",
    "        \"out_gen_path\": f\"{output_dir}/{row.wav_fn}\",           # output path for saving the edited audio\n",
    "        \"out_tts_path\": f\"{output_dir}/tts_{row.wav_fn}\",       # (optional) output path for saving the zero-shot TTS audio used for cut-and-paste editing\n",
    "    },\n",
    "]\n",
    "for data in datas:\n",
    "    ori_mel, edit_mel = main_exc.infer.riva_demo(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above \"out_tts_path\" is actually for cut-and-paste speech editing, which takes the original full utterance as a reference and generates the full utterance of the edited transcript. If you're willing to do the actual zero-shot TTS, please set the \"from\" as the last few words of your original transcript, then set \"to\" to your new transcript."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
