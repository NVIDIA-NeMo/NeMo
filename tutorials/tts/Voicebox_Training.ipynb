{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voicebox Training\n",
    "Before starting this tutorial, please ensure you have training datasets prepared according to [tutorials/tts/Voicebox_MFA.ipynb](Voicebox_MFA.ipynb).\n",
    "\n",
    "We use hierarchical structure for configuration files, so it might be difficult to understand directly. Therefore we provide logged model configuration at [examples/tts/conf/voicebox/mel_spec-LibriHeavy.yaml](/examples/tts/conf/voicebox/mel_spec-LibriHeavy.yaml) and [examples/tts/conf/voicebox/DAC-GigaSpeech.yaml](/examples/tts/conf/voicebox/DAC-GigaSpeech.yaml), to show how to override the default arguments.\n",
    "\n",
    "In the following, we will show how to train with several settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAC feature + GigaSpeech\n",
    "The following is part of the slurm script for training, to trace the original configuration files, checkout [experiment/a100-GS-DAC/base.yaml](/examples/tts/conf/voicebox/experiment/a100-GS-DAC/base.yaml) and [experiment/tricks/adam_warmup_cos_anneal.yaml](/examples/tts/conf/voicebox/experiment/tricks/adam_warmup_cos_anneal.yaml). Also check out [examples/tts/conf/voicebox/DAC-GigaSpeech.yaml](/examples/tts/conf/voicebox/DAC-GigaSpeech.yaml) for a flattened model config (which has a smaller model size for debugging purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! WANDB_API_KEY=YOUR_API_KEY PYTHONPATH=. python3 examples/tts/voicebox.py \\\n",
    "    experiment=\\[a100-GS-DAC/base,tricks/adam_warmup_cos_anneal\\] \\\n",
    "    ++model.voicebox.use_unet_skip_connection=True \\\n",
    "    ++model.voicebox.pytorch_mha=True \\\n",
    "    ++exp_manager.create_wandb_logger=True \\\n",
    "    ++exp_manager.create_tensorboard_logger=False \\\n",
    "    ++exp_manager.resume_if_exists=True \\\n",
    "    ++exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "    ++exp_manager.max_time_per_run=\"00:03:50:00\" \\\n",
    "    ++exp_manager.wandb_logger_kwargs.project=${PROJECT_NAME} \\\n",
    "    ++exp_manager.wandb_logger_kwargs.name=${EXP_NAME} \\\n",
    "    ++exp_manager.checkpoint_callback_params.every_n_train_steps=${EVERY_N_TRAIN_STEPS} \\\n",
    "    ++exp_manager.checkpoint_callback_params.every_n_epochs=${EVERY_N_EPOCHS} \\\n",
    "    ++exp_manager.checkpoint_callback_params.always_save_nemo=${ALWAYS_SAVE_NEMO} \\\n",
    "    ++exp_manager.checkpoint_callback_params.save_nemo_on_train_end=${SAVE_NEMO_ON_TRAIN_END} \\\n",
    "    ++exp_manager.checkpoint_callback_params.filename=\"'vb-{val_loss/vb:.4f}-{epoch}-{step}'\" \\\n",
    "    trainer.devices=-1 \\\n",
    "    trainer.num_nodes=$SLURM_JOB_NUM_NODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel-Spectrogram + LibriHeavy\n",
    "The following is part of the slurm script for training, to trace the original configuration files, checkout [experiment/a100-GS-DAC/mel-LH.yaml](/examples/tts/conf/voicebox/experiment/a100-GS-DAC/mel-LH.yaml) and [experiment/tricks/adam_warmup_cos_anneal.yaml](/examples/tts/conf/voicebox/experiment/tricks/adam_warmup_cos_anneal.yaml). Also check out [examples/tts/conf/voicebox/mel_spec-LibriHeavy.yaml](/examples/tts/conf/voicebox/mel_spec-LibriHeavy.yaml) for a flattened model config (which has a smaller model size for debugging purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! WANDB_API_KEY=YOUR_API_KEY PYTHONPATH=. python3 examples/tts/voicebox.py \\\n",
    "    experiment=\\[a100-GS-DAC/mel-LH,tricks/adam_warmup_cos_anneal\\] \\\n",
    "    ++model.voicebox.use_unet_skip_connection=True \\\n",
    "    ++model.voicebox.pytorch_mha=True \\\n",
    "    ++model.validation_ds.max_duration=20 \\\n",
    "    ++exp_manager.create_wandb_logger=True \\\n",
    "    ++exp_manager.create_tensorboard_logger=False \\\n",
    "    ++exp_manager.resume_if_exists=True \\\n",
    "    ++exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "    ++exp_manager.max_time_per_run=\"00:03:50:00\" \\\n",
    "    ++exp_manager.wandb_logger_kwargs.project=${PROJECT_NAME} \\\n",
    "    ++exp_manager.wandb_logger_kwargs.name=${EXP_NAME} \\\n",
    "    ++exp_manager.checkpoint_callback_params.every_n_train_steps=${EVERY_N_TRAIN_STEPS} \\\n",
    "    ++exp_manager.checkpoint_callback_params.every_n_epochs=${EVERY_N_EPOCHS} \\\n",
    "    ++exp_manager.checkpoint_callback_params.always_save_nemo=${ALWAYS_SAVE_NEMO} \\\n",
    "    ++exp_manager.checkpoint_callback_params.save_nemo_on_train_end=${SAVE_NEMO_ON_TRAIN_END} \\\n",
    "    ++exp_manager.checkpoint_callback_params.filename=\"'vb-{val_loss/vb:.4f}-{epoch}-{step}'\" \\\n",
    "    trainer.devices=-1 \\\n",
    "    trainer.num_nodes=$SLURM_JOB_NUM_NODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration Predictor Training\n",
    "Simply add the following to your voicebox training command:\n",
    "```python\n",
    "    ++model.validation_ds.max_duration=20 \\\n",
    "    ~model.freeze_updates.modules.duration_predictor=-1 \\\n",
    "    ++model.freeze_updates.modules.voicebox=-1 \\\n",
    "```\n",
    "Note that `model.duration_predictor.audio_enc_dec` should be exactly the same as `model.voicebox.audio_enc_dec`, to ensure the phoneme length matches what the Voicebox audio model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! WANDB_API_KEY=YOUR_API_KEY PYTHONPATH=. python3 examples/tts/voicebox.py \\\n",
    "    experiment=\\[a100-GS-DAC/base,tricks/adam_warmup_cos_anneal\\] \\\n",
    "    ++model.voicebox.use_unet_skip_connection=True \\\n",
    "    ++model.voicebox.pytorch_mha=True \\\n",
    "    ++model.validation_ds.max_duration=20 \\\n",
    "    ~model.freeze_updates.modules.duration_predictor=-1 \\\n",
    "    ++model.freeze_updates.modules.voicebox=-1 \\\n",
    "    ++exp_manager.checkpoint_callback_params.monitor=val_loss/dp_no_sil_spn \\\n",
    "    ++exp_manager.create_wandb_logger=True \\\n",
    "    ++exp_manager.create_tensorboard_logger=False \\\n",
    "    ++exp_manager.resume_if_exists=True \\\n",
    "    ++exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "    ++exp_manager.max_time_per_run=\"00:03:50:00\" \\\n",
    "    ++exp_manager.wandb_logger_kwargs.project=${PROJECT_NAME} \\\n",
    "    ++exp_manager.wandb_logger_kwargs.name=${EXP_NAME} \\\n",
    "    ++exp_manager.checkpoint_callback_params.every_n_train_steps=${EVERY_N_TRAIN_STEPS} \\\n",
    "    ++exp_manager.checkpoint_callback_params.every_n_epochs=${EVERY_N_EPOCHS} \\\n",
    "    ++exp_manager.checkpoint_callback_params.always_save_nemo=${ALWAYS_SAVE_NEMO} \\\n",
    "    ++exp_manager.checkpoint_callback_params.save_nemo_on_train_end=${SAVE_NEMO_ON_TRAIN_END} \\\n",
    "    ++exp_manager.checkpoint_callback_params.filename=\"'vb-{val_loss/vb:.4f}-{epoch}-{step}'\" \\\n",
    "    trainer.devices=-1 \\\n",
    "    trainer.num_nodes=$SLURM_JOB_NUM_NODES"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
