name: "ECAPA_TDNN"

model:

  sample_rate: 16000

  train_ds:
    manifest_filepath: ???
    sample_rate: ${model.sample_rate}
    labels: null
    batch_size: 64
    num_workers: 8
    shuffle: True
    drop_last: true
    augmentor:
      noise:
        manifest_path: ???
        prob: 0.5
        min_snr_db: 0
        max_snr_db: 15

      speed:
        prob: 0.5
        sr: ${model.sample_rate}
        resample_type: 'kaiser_fast'
        min_speed_rate: 0.95
        max_speed_rate: 1.05

      impulse:
        prob: 0.5
        manifest_path: ???

  validation_ds:
    manifest_filepath: ???
    sample_rate: ${model.sample_rate}
    labels: null
    batch_size: 128
    num_workers: 8
    shuffle: False
    drop_last: true

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    normalize: "per_feature"
    window_size: 0.025
    sample_rate: ${model.sample_rate}
    window_stride: 0.01
    window: "hann"
    features: 80
    n_fft: 512
    frame_splicing: 1
    dither: 0.00001
    stft_conv: false

  spec_augment:
    _target_: nemo.collections.asr.modules.SpectrogramAugmentation
    freq_masks: 3
    freq_width: 4
    time_masks: 5
    time_width: 0.03

  encoder:
    _target_: nemo.collections.asr.modules.ECAPAEncoder
    feat_in: ${model.preprocessor.features}
    filters: [512,512,512,512,1536]
    kernel_sizes: [5,3,3,3,1]
    dilations: [1,1,1,1,1]
    scale: 8

  decoder:
    _target_: nemo.collections.asr.modules.SpeakerDecoder
    feat_in: 1536
    num_classes: 7205
    pool_mode: 'attention' #xvector,tap or attention
    attention_channels: 3072
    emb_sizes: 192

  loss:
    _target_: nemo.collections.asr.losses.angularloss.AngularSoftmaxLoss # you could also use cross-entrophy loss
    scale: 30
    margin: 0.2

  optim:
    name: adamw
    lr: 0.001
    weight_decay: 0.0002

    # scheduler setup
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.1
      min_lr: 0.00001

trainer:
  devices: -1 # number of gpus (trained on four nodes - each node has 8 gpus)
  max_epochs: 250
  max_steps: -1 # computed at runtime if not set
  num_nodes: 1
  accelerator: gpu
  strategy: ddp
  accumulate_grad_batches: 1
  deterministic: False
  enable_checkpointing: False
  logger: False
  log_every_n_steps: 10  # Interval of logging.
  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  gradient_clip_val: 1.0

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    # in case of multiple validation sets, first one is used
    monitor: "val_loss"
    mode: "min"
    save_top_k: 1

  # you need to set these two to True to continue the training
  resume_if_exists: true
  resume_ignore_no_checkpoint: true

  # You may use this section to create a W&B logger
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: null
    project: null
